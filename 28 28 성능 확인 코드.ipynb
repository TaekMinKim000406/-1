{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd3d0ee-c3ff-4f59-bacb-62a4254eec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "픽셀을 28 x 28일 때 성능을 검증하기 위한 코드입니다.\n",
    "원래는 정의해 놓은 함수를 불러와서 사용해야 했으나, 검증 과정에서 일부 함수를 직접 코드를 복사해와서 테스트 한 코드도 많습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "438e0800-231c-4fc0-8a2e-6ddf6613aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(current_dir)\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "image_folder_path = './G1020/Images/'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af23b24-5e35-46b5-9abc-50e3dd65f3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed images shape: (1020, 1, 28, 28)\n",
      "Labels shape: (1020,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('G1020.csv')\n",
    "image_files = df['imageID'].tolist()  # 이미지 파일 이름이 있는 열 이름\n",
    "labels = df['binaryLabels'].values  # 레이블이 있는 열 이름\n",
    "\n",
    "\n",
    "# 이미지 크기 설정\n",
    "target_size = (28, 28)\n",
    "\n",
    "# 이미지 불러오기 및 전처리 함수\n",
    "def load_and_preprocess_image(image_path, target_size=(28, 28)):\n",
    "    # 이미지 읽기 (기본 BGR 형식)\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # 흑백으로 읽기\n",
    "    if image is None:\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # 이미지 크기 조정 (28x28)\n",
    "    image = cv2.resize(image, target_size)\n",
    "    \n",
    "    # 정규화 (0~1 범위로 스케일링)\n",
    "    image = image / 255.0\n",
    "    \n",
    "    # 차원을 추가하여 (1, 28, 28)로 만들기\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# 모든 이미지를 불러와서 리스트에 저장\n",
    "images = [load_and_preprocess_image(image_folder_path+img_path, target_size) for img_path in image_files]\n",
    "images = np.array([img for img in images if img is not None])  # None 값 제거\n",
    "\n",
    "print(f\"Processed images shape: {images.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba7ecdaf-92cd-4d9f-8ef6-82668d1d5b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = int((input_size - filter_size + 2 * filter_pad) / filter_stride + 1)\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b3'] = np.zeros(hidden_size)        \n",
    "        self.params['W4'] = weight_init_std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b4'] = np.zeros(hidden_size)\n",
    "        self.params['W5'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b5'] = np.zeros(output_size)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.layers['Relu3'] = Relu()  # 추가된 ReLU 활성화 함수\n",
    "        self.layers['Affine3'] = Affine(self.params['W4'], self.params['b4'])\n",
    "        self.layers['Relu4'] = Relu()  # 추가된 ReLU 활성화 함\n",
    "        self.layers['Affine4'] = Affine(self.params['W5'], self.params['b5'])\n",
    "\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "        # for idx in (1, 2, 3,4,5):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        grads['W4'], grads['b4'] = self.layers['Affine3'].dW, self.layers['Affine3'].db\n",
    "        grads['W5'], grads['b5'] = self.layers['Affine4'].dW, self.layers['Affine4'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fea0a628-bb4f-41e6-8cb7-62122a6a1621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.300803272950098\n",
      "=== epoch:1, train acc:0.7, test acc:0.65 ===\n",
      "train loss:2.298882390785243\n",
      "train loss:2.2967263456152085\n",
      "train loss:2.2940187194229975\n",
      "train loss:2.290784882541292\n",
      "train loss:2.2864484700638927\n",
      "train loss:2.2796501963985305\n",
      "train loss:2.265035361444203\n",
      "train loss:2.247697226529353\n",
      "train loss:2.2165315608839435\n",
      "train loss:2.1780456498026424\n",
      "train loss:2.115740964265553\n",
      "train loss:2.0314443772859767\n",
      "train loss:1.9299488396764055\n",
      "train loss:1.6407425800398499\n",
      "train loss:1.389671495126504\n",
      "train loss:1.2421294987329223\n",
      "train loss:1.0254760422462321\n",
      "train loss:0.7142180101166379\n",
      "train loss:0.8975504013894401\n",
      "train loss:0.6193962273020703\n",
      "train loss:1.165636264280297\n",
      "train loss:0.7362177617594363\n",
      "train loss:0.7033095832589078\n",
      "train loss:0.5160771383905955\n",
      "train loss:0.6791765825737472\n",
      "train loss:0.685520622300128\n",
      "train loss:0.6575167026355455\n",
      "train loss:0.6782074385431407\n",
      "train loss:0.6701157715408161\n",
      "train loss:0.525056065088166\n",
      "train loss:0.6512592873486155\n",
      "train loss:0.5229011001175222\n",
      "train loss:0.3340646389724985\n",
      "train loss:0.5845068551175978\n",
      "train loss:0.05023882564502502\n",
      "train loss:0.7057694684409686\n",
      "train loss:1.345136026408978\n",
      "train loss:0.6983831970593888\n",
      "train loss:0.7612337730014114\n",
      "train loss:0.6972159795012695\n",
      "train loss:0.7411251746115399\n",
      "train loss:0.8982536341782243\n",
      "train loss:0.9776872334690246\n",
      "train loss:0.9921991545690159\n",
      "train loss:0.8074338626324273\n",
      "train loss:0.7194716433353501\n",
      "train loss:0.6312253991355832\n",
      "train loss:0.5823682991325215\n",
      "train loss:0.6090200826390078\n",
      "train loss:0.47481207769574835\n",
      "train loss:0.954279969136433\n",
      "train loss:0.38929697968111304\n",
      "train loss:0.6442416359633012\n",
      "train loss:0.6104342584516195\n",
      "train loss:0.6195195998095044\n",
      "train loss:0.6005707300498023\n",
      "train loss:0.34369980538511263\n",
      "train loss:0.3725886399540884\n",
      "train loss:0.6710307837659478\n",
      "train loss:0.7803895620801713\n",
      "train loss:0.625538032452816\n",
      "train loss:0.5694554771967251\n",
      "train loss:0.6178190787425896\n",
      "train loss:0.5194475349364934\n",
      "train loss:0.6768252645729705\n",
      "train loss:1.0825603183687722\n",
      "train loss:0.6632498137630989\n",
      "train loss:0.6622043821814158\n",
      "train loss:0.7529203376125946\n",
      "train loss:0.5921018753068759\n",
      "train loss:0.729832099798473\n",
      "train loss:0.9976163483983532\n",
      "train loss:0.7302161054164518\n",
      "train loss:0.6903638014806357\n",
      "train loss:0.8130445241357604\n",
      "train loss:0.7715272452196958\n",
      "train loss:0.5708432204313934\n",
      "train loss:0.5282422817653942\n",
      "train loss:0.5460758398943539\n",
      "train loss:0.7360695113341454\n",
      "train loss:0.7086417684184647\n",
      "train loss:0.6135241056255156\n",
      "train loss:0.6737056901265088\n",
      "train loss:0.545241055082087\n",
      "train loss:0.6190493543868267\n",
      "train loss:0.7550743221744922\n",
      "train loss:0.5323551211938706\n",
      "train loss:0.5178246190397815\n",
      "train loss:0.2826944229200969\n",
      "train loss:0.6411680937512831\n",
      "train loss:0.3964635529227506\n",
      "train loss:0.48273097311938995\n",
      "train loss:0.8962772868628447\n",
      "train loss:0.3703919310340454\n",
      "train loss:0.6280641681354049\n",
      "train loss:0.34595092989060683\n",
      "train loss:0.4704507125986481\n",
      "train loss:0.663550287988263\n",
      "train loss:0.6753212623115675\n",
      "train loss:0.39348749874749844\n",
      "train loss:0.3388703481566787\n",
      "train loss:0.6983824954140757\n",
      "train loss:0.5930174680590288\n",
      "train loss:0.8897255102882969\n",
      "train loss:0.6329778002266303\n",
      "train loss:0.7139290822486904\n",
      "train loss:0.4869877890588589\n",
      "train loss:0.4927258439322643\n",
      "train loss:0.5250699230023235\n",
      "train loss:0.46310912184737807\n",
      "train loss:0.32988472149086345\n",
      "train loss:0.46168904946359907\n",
      "train loss:0.8340152801584614\n",
      "train loss:0.40854384071558697\n",
      "train loss:0.25738770441285724\n",
      "train loss:0.19321289448740384\n",
      "train loss:0.16396706692761867\n",
      "train loss:0.975510485359672\n",
      "train loss:0.5529516476652521\n",
      "train loss:0.5250723086491145\n",
      "train loss:0.6637120486934152\n",
      "train loss:0.6371659990071741\n",
      "train loss:0.8716855469088435\n",
      "train loss:0.48135860201600045\n",
      "train loss:0.5558353853479512\n",
      "train loss:0.7638611780646152\n",
      "train loss:0.436244006411621\n",
      "train loss:0.43310799206072376\n",
      "train loss:0.6146810625913129\n",
      "train loss:0.44889019974709266\n",
      "train loss:0.6192458134768082\n",
      "train loss:0.5016824006164646\n",
      "train loss:0.7050338257760737\n",
      "train loss:0.5137420213375968\n",
      "train loss:0.5209782829281833\n",
      "train loss:0.4237492612954899\n",
      "train loss:0.7211753902665032\n",
      "train loss:0.7398198921849949\n",
      "train loss:0.7114600388889833\n",
      "train loss:0.6204724889174893\n",
      "train loss:0.5034320338862429\n",
      "train loss:0.4265209115680199\n",
      "train loss:0.7904233771296099\n",
      "train loss:0.49231983316992894\n",
      "train loss:0.7844696318911426\n",
      "train loss:0.8002629605512507\n",
      "train loss:0.7654286408167217\n",
      "train loss:0.6805579701866511\n",
      "train loss:0.6698948339419842\n",
      "train loss:0.5700003990222247\n",
      "train loss:0.6731050047841249\n",
      "train loss:0.5186950704780081\n",
      "train loss:0.5042393577565965\n",
      "train loss:0.6156700066609647\n",
      "train loss:0.6666186126431152\n",
      "train loss:0.5990005347686197\n",
      "train loss:0.5731053810065506\n",
      "train loss:0.6164645263000604\n",
      "train loss:0.6034787509435883\n",
      "train loss:0.5441171979169561\n",
      "train loss:0.4410166054340007\n",
      "train loss:0.6949535301274936\n",
      "train loss:0.39007172888889824\n",
      "train loss:0.7283024715048507\n",
      "train loss:0.6149891163517872\n",
      "train loss:0.6234614938355718\n",
      "train loss:0.78917451385691\n",
      "train loss:0.4712675803052693\n",
      "train loss:0.3813454363962763\n",
      "train loss:0.63497696894746\n",
      "train loss:0.4789202259846241\n",
      "train loss:0.5274195526644277\n",
      "train loss:0.8476461169458945\n",
      "train loss:0.5231645084497041\n",
      "train loss:0.8550222000942844\n",
      "train loss:0.6031729202389136\n",
      "train loss:0.6288195940544565\n",
      "train loss:0.8847391390420812\n",
      "train loss:0.6174059579101526\n",
      "train loss:0.6670220394889196\n",
      "train loss:0.6745122019224963\n",
      "train loss:0.5802492405040713\n",
      "train loss:0.6258900603279123\n",
      "train loss:0.6505881644661861\n",
      "train loss:0.6191126291760246\n",
      "train loss:0.6744177502654798\n",
      "train loss:0.5984752797929511\n",
      "train loss:0.5763311296100601\n",
      "train loss:0.6426139320512017\n",
      "train loss:0.562855056573969\n",
      "train loss:0.8020576270792314\n",
      "train loss:0.6447082401513076\n",
      "train loss:0.5429432422021438\n",
      "train loss:0.5230541235159134\n",
      "train loss:0.5068833053538463\n",
      "train loss:0.50701363826742\n",
      "train loss:0.7352163341086732\n",
      "train loss:0.3978398455235878\n",
      "train loss:0.7659786941420111\n",
      "train loss:0.41925357292455956\n",
      "train loss:0.6622833754098475\n",
      "train loss:0.32472192958908924\n",
      "train loss:0.8781292318149017\n",
      "train loss:0.48289649348803143\n",
      "train loss:0.7750830385629763\n",
      "train loss:0.5169865265676503\n",
      "train loss:0.6204371189796719\n",
      "train loss:0.5131600244363126\n",
      "train loss:0.6095793993287217\n",
      "train loss:0.6111597672562723\n",
      "train loss:0.6003786698924106\n",
      "train loss:0.7859829463654124\n",
      "train loss:0.6734589545679963\n",
      "train loss:0.4169594505123526\n",
      "train loss:0.6671698397724488\n",
      "train loss:0.5742935021120663\n",
      "train loss:0.5954641307276564\n",
      "train loss:0.526509790165364\n",
      "train loss:0.4047859021275747\n",
      "train loss:0.7659371061264542\n",
      "train loss:0.6934090336975111\n",
      "train loss:0.7464785441458366\n",
      "train loss:0.6091649862214442\n",
      "train loss:0.5492842526980193\n",
      "train loss:0.7028101978671397\n",
      "train loss:0.5240165279806631\n",
      "train loss:0.6095643346836879\n",
      "train loss:0.7007956865076451\n",
      "train loss:0.7262863478607262\n",
      "train loss:0.5901450310650442\n",
      "train loss:0.5859767730097458\n",
      "train loss:0.520212378793808\n",
      "train loss:0.6794998034243731\n",
      "train loss:0.4480402422764751\n",
      "train loss:0.7781442239441625\n",
      "train loss:0.667067559351179\n",
      "train loss:0.628350452148795\n",
      "train loss:0.6736749022768646\n",
      "train loss:0.5010319597453567\n",
      "train loss:0.5658109345291723\n",
      "train loss:0.433482473234469\n",
      "train loss:0.4835237739645943\n",
      "train loss:0.5855111091127119\n",
      "train loss:0.3674928838257169\n",
      "train loss:0.7464480529045986\n",
      "train loss:0.5427018041614973\n",
      "train loss:0.6296622745259277\n",
      "train loss:0.5472643698063939\n",
      "train loss:0.5783973202123767\n",
      "train loss:0.8962848643803818\n",
      "train loss:0.6257525269525057\n",
      "train loss:0.5158069747098292\n",
      "train loss:0.7842928882656566\n",
      "train loss:0.5901240312864104\n",
      "train loss:0.48587019773156853\n",
      "train loss:0.5580285181371771\n",
      "train loss:0.6121715007241734\n",
      "train loss:0.6753416032400363\n",
      "train loss:0.5949148459386204\n",
      "train loss:0.7351084064738297\n",
      "train loss:0.5735241507863628\n",
      "train loss:0.6094547610650752\n",
      "train loss:0.559712433707366\n",
      "train loss:0.5995228425757648\n",
      "train loss:0.48474866564923963\n",
      "train loss:0.5462201759513695\n",
      "train loss:0.6173812122958754\n",
      "train loss:0.6295154308432771\n",
      "train loss:0.40777092459021275\n",
      "train loss:0.49351390262313305\n",
      "train loss:0.3785218255433031\n",
      "train loss:1.105145402037151\n",
      "train loss:0.8197346124383523\n",
      "train loss:0.9110123735838369\n",
      "train loss:0.6327357189375229\n",
      "train loss:0.6470095480215181\n",
      "train loss:0.725085671315472\n",
      "train loss:0.520533387432607\n",
      "train loss:0.6136125278489271\n",
      "train loss:0.5733014527088793\n",
      "train loss:0.6687196716514686\n",
      "train loss:0.7154688176605626\n",
      "train loss:0.8148549392070518\n",
      "train loss:0.7033585338621215\n",
      "train loss:0.6424288147759674\n",
      "train loss:0.6470006130541333\n",
      "train loss:0.7364503299798072\n",
      "train loss:0.5623294186513051\n",
      "train loss:0.637617800868557\n",
      "train loss:0.6449437928828363\n",
      "train loss:0.6448276458584791\n",
      "train loss:0.5450191569720418\n",
      "train loss:0.4928358882636233\n",
      "train loss:0.386041593223693\n",
      "train loss:0.42488616686979075\n",
      "train loss:0.5054729192370986\n",
      "train loss:0.6511130531605158\n",
      "train loss:0.5127060648216851\n",
      "train loss:0.5218171350645351\n",
      "train loss:0.7045623918003121\n",
      "train loss:0.7395054943693132\n",
      "train loss:0.6974758460539203\n",
      "train loss:0.8192143884905194\n",
      "train loss:1.0293381780830204\n",
      "train loss:0.6064427781920229\n",
      "train loss:0.52491113252217\n",
      "train loss:0.619924510214668\n",
      "train loss:0.6199240691384572\n",
      "train loss:0.58467858905218\n",
      "train loss:0.5933866786368025\n",
      "train loss:0.6098128299062899\n",
      "train loss:0.6904909986318675\n",
      "train loss:0.5867634460897261\n",
      "train loss:0.7333711320556936\n",
      "train loss:0.5249948571822497\n",
      "train loss:0.6903369744187776\n",
      "train loss:0.7222208759509442\n",
      "train loss:0.6013757902711048\n",
      "train loss:0.7414333919340383\n",
      "train loss:0.5378707967274117\n",
      "train loss:0.6283956157773777\n",
      "train loss:0.6176982299543463\n",
      "train loss:0.6760074526839628\n",
      "train loss:0.7100068943652493\n",
      "train loss:0.5854976137082539\n",
      "train loss:0.7754468634711393\n",
      "train loss:0.7397735143071807\n",
      "train loss:0.603007750476209\n",
      "train loss:0.5877231338948478\n",
      "train loss:0.7362574872178893\n",
      "train loss:0.49097731558074553\n",
      "train loss:0.5955727709069419\n",
      "train loss:0.5342101247149044\n",
      "train loss:0.6846022709902422\n",
      "train loss:0.7603613454802302\n",
      "train loss:0.600533207891357\n",
      "train loss:0.5052595046533959\n",
      "train loss:0.6831272922531137\n",
      "train loss:0.45042089373165\n",
      "train loss:0.6853793296008025\n",
      "train loss:0.5135990681886647\n",
      "train loss:0.5752497777049846\n",
      "train loss:0.46498880093891926\n",
      "train loss:0.5013457985122856\n",
      "train loss:0.5905187489548639\n",
      "train loss:0.35180756438863814\n",
      "train loss:0.20820760872702912\n",
      "train loss:0.49299068685964204\n",
      "train loss:0.7155832286298738\n",
      "train loss:0.8976494464290925\n",
      "train loss:0.9744697597215894\n",
      "train loss:0.6629789896716812\n",
      "train loss:0.5137434611911478\n",
      "train loss:0.3882476966697571\n",
      "train loss:0.4984472879859849\n",
      "train loss:0.6004600669743974\n",
      "train loss:0.6795603072581619\n",
      "train loss:0.6220872256464958\n",
      "train loss:0.6768596829532184\n",
      "train loss:0.7202212096453204\n",
      "train loss:0.6709288413947823\n",
      "train loss:0.6623688433006772\n",
      "train loss:0.6764520647682605\n",
      "train loss:0.6251224308503447\n",
      "train loss:0.6403186114213746\n",
      "train loss:0.5669722534492265\n",
      "train loss:0.6487773660085655\n",
      "train loss:0.6407642190498749\n",
      "train loss:0.5710511704185579\n",
      "train loss:0.5231965902691283\n",
      "train loss:0.8134176813017515\n",
      "train loss:0.6738674672657569\n",
      "train loss:0.6954276769998327\n",
      "train loss:0.6663469219247279\n",
      "train loss:0.6291617376013008\n",
      "train loss:0.5992245103567353\n",
      "train loss:0.7016702958635026\n",
      "train loss:0.4501829436978378\n",
      "train loss:0.6878907342602737\n",
      "train loss:0.6419267859997134\n",
      "train loss:0.711491432398689\n",
      "train loss:0.3526863044824784\n",
      "train loss:0.608045947753396\n",
      "train loss:0.7088532541674253\n",
      "train loss:0.6099936858939048\n",
      "train loss:0.3919293456598154\n",
      "train loss:0.6531313589072256\n",
      "train loss:0.7228214048720719\n",
      "train loss:0.6943581329810814\n",
      "train loss:0.5014944702008484\n",
      "train loss:0.7033112191349066\n",
      "train loss:0.40765165998136055\n",
      "train loss:0.4370059421817231\n",
      "train loss:0.5346402697686592\n",
      "train loss:0.5031439786858111\n",
      "train loss:0.6041168683239776\n",
      "train loss:0.37646404555157054\n",
      "train loss:0.2106347923066158\n",
      "train loss:0.5983261001659536\n",
      "train loss:0.6449586085879466\n",
      "train loss:0.5092389415334525\n",
      "train loss:0.3518374064374048\n",
      "train loss:0.49890107955273544\n",
      "train loss:0.6733023909455194\n",
      "train loss:0.4916011353837412\n",
      "train loss:0.4860677148585225\n",
      "train loss:0.34579167092815466\n",
      "train loss:0.9463967371503109\n",
      "train loss:0.6345871004650407\n",
      "train loss:0.41206421579493935\n",
      "train loss:0.5141658144043776\n",
      "train loss:0.6864221278547922\n",
      "train loss:0.8347493151865418\n",
      "train loss:0.6769168517011149\n",
      "train loss:0.7674229698885153\n",
      "train loss:0.5292265565453829\n",
      "train loss:0.6619826693444173\n",
      "train loss:0.6022652438327123\n",
      "train loss:0.742745788148636\n",
      "train loss:0.6417979360138247\n",
      "train loss:0.5385454408185963\n",
      "train loss:0.5738659291492192\n",
      "train loss:0.7352532187087598\n",
      "train loss:0.5537649149446155\n",
      "train loss:0.7049689755943603\n",
      "train loss:0.6450411130374588\n",
      "train loss:0.7354290392783741\n",
      "train loss:0.7350024754241437\n",
      "train loss:0.6226095280622724\n",
      "train loss:0.6017814481194124\n",
      "train loss:0.5380368798396686\n",
      "train loss:0.3878021003223545\n",
      "train loss:0.44318938345891973\n",
      "train loss:0.5133428375844538\n",
      "train loss:0.4781954020408749\n",
      "train loss:0.6684282698916294\n",
      "train loss:0.7496228538544593\n",
      "train loss:0.9286645691672426\n",
      "train loss:0.3645840641486139\n",
      "train loss:0.6329301022507314\n",
      "train loss:0.3798265761291014\n",
      "train loss:0.8626141346345371\n",
      "train loss:0.628357817173118\n",
      "train loss:0.6440896871692241\n",
      "train loss:0.5945320962349696\n",
      "train loss:0.6144135710924299\n",
      "train loss:0.6026304454817918\n",
      "train loss:0.45395097433427567\n",
      "train loss:0.6818092890886346\n",
      "train loss:0.5991033893410529\n",
      "train loss:0.5405159719450883\n",
      "train loss:0.3778466927667968\n",
      "train loss:0.679002243476619\n",
      "train loss:0.7699657222625971\n",
      "train loss:0.7780410197282345\n",
      "train loss:0.7531306242064005\n",
      "train loss:0.7267105255856976\n",
      "train loss:0.545069531529044\n",
      "train loss:0.5638924515567311\n",
      "train loss:0.5462256684533685\n",
      "train loss:0.6171573911105682\n",
      "train loss:0.3831684144107034\n",
      "train loss:0.6002079698456623\n",
      "train loss:0.4160794139569954\n",
      "train loss:0.626772525856822\n",
      "train loss:0.644632618324416\n",
      "train loss:0.745122384815206\n",
      "train loss:0.6041844628774148\n",
      "train loss:0.740852757491403\n",
      "train loss:0.6406204944175848\n",
      "train loss:0.5999741011984581\n",
      "train loss:0.6961346719003212\n",
      "train loss:0.5298598547678414\n",
      "train loss:0.42527288435956356\n",
      "train loss:0.6093584088135138\n",
      "train loss:0.49945695035152227\n",
      "train loss:0.8037385120915482\n",
      "train loss:0.6982660074567959\n",
      "train loss:0.5211196101390982\n",
      "train loss:0.6023587076962779\n",
      "train loss:0.5669490123527561\n",
      "train loss:0.6204093227015595\n",
      "train loss:0.6851130229942406\n",
      "train loss:0.533609857195658\n",
      "train loss:0.41763228551549386\n",
      "train loss:0.5291951808176557\n",
      "train loss:0.615867691702064\n",
      "train loss:0.525931775788276\n",
      "train loss:0.6083625414672775\n",
      "train loss:0.8118316362522922\n",
      "train loss:0.5177810939917593\n",
      "train loss:0.6941444828708754\n",
      "train loss:0.6187077004052035\n",
      "train loss:0.6189019476180034\n",
      "train loss:0.6175555852630314\n",
      "train loss:0.8438046195401675\n",
      "train loss:0.5452329575914165\n",
      "train loss:0.6114417869979836\n",
      "train loss:0.6184658567359339\n",
      "train loss:0.6047996941289627\n",
      "train loss:0.633902237333352\n",
      "train loss:0.6759543895632497\n",
      "train loss:0.5565812638031\n",
      "train loss:0.5501127680368849\n",
      "train loss:0.7478259657248134\n",
      "train loss:0.6101002360918307\n",
      "train loss:0.7481153289374781\n",
      "train loss:0.734296684192988\n",
      "train loss:0.6727799982138638\n",
      "train loss:0.7717864810912127\n",
      "train loss:0.6222001195211446\n",
      "train loss:0.5789351587792987\n",
      "train loss:0.5768271301892308\n",
      "train loss:0.6201805469166212\n",
      "train loss:0.6905160639991357\n",
      "train loss:0.6679299996685807\n",
      "train loss:0.6713998306629705\n",
      "train loss:0.6297618001502444\n",
      "train loss:0.49352901400699734\n",
      "train loss:0.5318169052354589\n",
      "train loss:0.4927277057632578\n",
      "train loss:0.5232942011662493\n",
      "train loss:0.3926390515874391\n",
      "train loss:0.6430004060757307\n",
      "train loss:0.49403773872505286\n",
      "train loss:0.5338606685309137\n",
      "train loss:1.1352817533449193\n",
      "train loss:0.9913526017807632\n",
      "train loss:0.39999184737636273\n",
      "train loss:0.39450294594068414\n",
      "train loss:0.5944381864576174\n",
      "train loss:0.5006726035538982\n",
      "train loss:0.5958999929206965\n",
      "train loss:0.69470905903272\n",
      "train loss:0.5397324495134525\n",
      "train loss:0.5391340328095945\n",
      "train loss:0.5859107275520956\n",
      "train loss:0.6323343572363662\n",
      "train loss:0.6056911919233474\n",
      "train loss:0.6227503518324815\n",
      "train loss:0.7066712592264918\n",
      "train loss:0.6970678202638013\n",
      "train loss:0.5455094961255023\n",
      "train loss:0.6242986359178252\n",
      "train loss:0.5455660407316328\n",
      "train loss:0.7020694836342832\n",
      "train loss:0.48102309098841134\n",
      "train loss:0.6775663459662783\n",
      "train loss:0.4359508276962846\n",
      "train loss:0.5224654952108234\n",
      "train loss:0.5827971850389027\n",
      "train loss:0.4982467939189574\n",
      "train loss:0.36167498814465815\n",
      "train loss:0.601833583315212\n",
      "train loss:0.6551991754142477\n",
      "train loss:0.17453443747528888\n",
      "train loss:0.6409173290314618\n",
      "train loss:0.1422458967943696\n",
      "train loss:0.5463383256715193\n",
      "train loss:0.7421366442006144\n",
      "train loss:0.8708718662019533\n",
      "train loss:0.5410416084017482\n",
      "train loss:0.6713342042656872\n",
      "train loss:0.5217689727810383\n",
      "train loss:0.6282154753946456\n",
      "train loss:0.45870095938995414\n",
      "train loss:0.4291552318566799\n",
      "train loss:0.6242832790806437\n",
      "train loss:0.5378054629229831\n",
      "train loss:0.6852634206116637\n",
      "train loss:0.6109442117062054\n",
      "train loss:0.5387077509441236\n",
      "train loss:0.6730388768113147\n",
      "train loss:0.6020533413996342\n",
      "train loss:0.5873868420536358\n",
      "train loss:0.745902598387852\n",
      "train loss:0.6070650227343105\n",
      "train loss:0.6573994554233736\n",
      "train loss:0.6911123911924278\n",
      "train loss:0.46218986476494156\n",
      "train loss:0.4672486374447707\n",
      "train loss:0.4526979284077991\n",
      "train loss:0.7458672294969113\n",
      "train loss:0.5158827369969653\n",
      "train loss:0.6258995594929255\n",
      "train loss:0.6603467053653591\n",
      "train loss:0.7446017506302136\n",
      "train loss:0.6115438348470865\n",
      "train loss:0.7349875633081703\n",
      "train loss:0.8918349193062793\n",
      "train loss:0.4702368636659024\n",
      "train loss:0.5381604974523324\n",
      "train loss:0.4560289086282001\n",
      "train loss:0.5188222210934614\n",
      "train loss:0.6802634096624918\n",
      "train loss:0.6301155310003652\n",
      "train loss:0.8384551017257964\n",
      "train loss:0.6277811350623119\n",
      "train loss:0.620248205030197\n",
      "train loss:0.6649753723661378\n",
      "train loss:0.5561558181355934\n",
      "train loss:0.5236051007452607\n",
      "train loss:0.7969839535175876\n",
      "train loss:0.4219167830342859\n",
      "train loss:0.6294280638623865\n",
      "train loss:0.693146677867168\n",
      "train loss:0.8689437998072403\n",
      "train loss:0.47391552708440055\n",
      "train loss:0.5166277665110326\n",
      "train loss:0.7562394820166398\n",
      "train loss:0.5431342457177136\n",
      "train loss:0.7637398187587315\n",
      "train loss:0.6650354768335631\n",
      "train loss:0.46034030391236946\n",
      "train loss:0.5377514399520108\n",
      "train loss:0.6278791758345794\n",
      "train loss:0.5933225045958522\n",
      "train loss:0.6187376403196014\n",
      "train loss:0.6831752087077254\n",
      "train loss:0.4258518046539817\n",
      "train loss:0.6221373409425205\n",
      "train loss:0.6246809620821969\n",
      "train loss:0.397684756602139\n",
      "train loss:0.6361341613331064\n",
      "train loss:0.5259437986094109\n",
      "train loss:0.5976432237946033\n",
      "train loss:0.7015042436524481\n",
      "train loss:0.5222765724573295\n",
      "train loss:0.4000129834665399\n",
      "train loss:0.36841532266436683\n",
      "train loss:0.5376586613587403\n",
      "train loss:0.5920796772853948\n",
      "train loss:0.3299168194429033\n",
      "train loss:0.30968071893417437\n",
      "train loss:0.6363087712504032\n",
      "train loss:0.9982406984123188\n",
      "train loss:0.7842354104699567\n",
      "train loss:0.7402762395087377\n",
      "train loss:0.7049109662880416\n",
      "train loss:0.5327682887254153\n",
      "train loss:0.5991938680958958\n",
      "train loss:0.5029040792479832\n",
      "train loss:0.5746548930233276\n",
      "train loss:0.5056335861899159\n",
      "train loss:0.7234727831355008\n",
      "train loss:0.6795701682674845\n",
      "train loss:0.5736731643379689\n",
      "train loss:0.7789303743988584\n",
      "train loss:0.6781870357986776\n",
      "train loss:0.6321117883136791\n",
      "train loss:0.6292360607402706\n",
      "train loss:0.5276972752671326\n",
      "train loss:0.6819257800002528\n",
      "train loss:0.5164910772447169\n",
      "train loss:0.5566444142443434\n",
      "train loss:0.5507831867485847\n",
      "train loss:0.5921817111997656\n",
      "train loss:0.43333372426754424\n",
      "train loss:0.41487123245739504\n",
      "train loss:0.5966683021774017\n",
      "train loss:0.8063398503079643\n",
      "train loss:0.7164126816742267\n",
      "train loss:0.9932164218717195\n",
      "train loss:0.7438363321996243\n",
      "train loss:0.6198946290720746\n",
      "train loss:0.6642284923191052\n",
      "train loss:0.4504575126322233\n",
      "train loss:0.6165640455075497\n",
      "train loss:0.5302150599247508\n",
      "train loss:0.6714961527297035\n",
      "train loss:0.5084802815665795\n",
      "train loss:0.5408034740375428\n",
      "train loss:0.6773757063340866\n",
      "train loss:0.7027330947028194\n",
      "train loss:0.6323798124530022\n",
      "train loss:0.6304830754305135\n",
      "train loss:0.5433015455222154\n",
      "train loss:0.6689427249362758\n",
      "train loss:0.5253619073250639\n",
      "train loss:0.4408165306703945\n",
      "train loss:0.8485423663391206\n",
      "train loss:0.869111563918775\n",
      "train loss:0.44373138473522555\n",
      "train loss:0.4519684117073573\n",
      "train loss:0.5365481080477321\n",
      "train loss:0.31153344870496724\n",
      "train loss:0.39866581718840727\n",
      "train loss:0.5229497602051254\n",
      "train loss:0.5260420192032847\n",
      "train loss:0.6255811063599201\n",
      "train loss:0.9482223484761032\n",
      "train loss:0.6330542775535999\n",
      "train loss:0.34078980247324275\n",
      "train loss:0.7290099127892787\n",
      "train loss:0.6771320997471684\n",
      "train loss:0.7594402616708906\n",
      "train loss:0.4244813876406268\n",
      "train loss:0.5194410918374099\n",
      "train loss:0.6034641429062548\n",
      "train loss:0.38257662436195994\n",
      "train loss:0.42507654856374993\n",
      "train loss:0.9339412982399811\n",
      "train loss:0.4766899687168751\n",
      "train loss:0.8817926573765991\n",
      "train loss:0.5275136124025945\n",
      "train loss:0.3582032907440859\n",
      "train loss:0.686752477105355\n",
      "train loss:0.5015072694190128\n",
      "train loss:0.5999527148430459\n",
      "train loss:0.5795074855065503\n",
      "train loss:0.5847886208777208\n",
      "train loss:0.7049200921606313\n",
      "train loss:0.5956281063984362\n",
      "train loss:0.6960365634510896\n",
      "train loss:0.49174882922604535\n",
      "train loss:0.5710684280690815\n",
      "train loss:0.5940206663014366\n",
      "train loss:0.4122038287065638\n",
      "train loss:0.5069598406360146\n",
      "train loss:0.7049200702238549\n",
      "train loss:0.8392098768215549\n",
      "train loss:0.5828397605107962\n",
      "train loss:0.7060975602670518\n",
      "train loss:0.7007934833587972\n",
      "train loss:0.7492401316792175\n",
      "train loss:0.7473818331421641\n",
      "train loss:0.6885623695774934\n",
      "train loss:0.5852232337837008\n",
      "train loss:0.5388368330215265\n",
      "train loss:0.591669826567348\n",
      "train loss:0.7508685212810797\n",
      "train loss:0.6156190197260253\n",
      "train loss:0.5018711456136694\n",
      "train loss:0.5900927734331622\n",
      "train loss:0.5950463773764871\n",
      "train loss:0.5498332304474948\n",
      "train loss:0.6173289767424304\n",
      "train loss:0.6014949136649983\n",
      "train loss:0.757431847737894\n",
      "train loss:0.8586795873661701\n",
      "train loss:0.6113609822555428\n",
      "train loss:0.43720166064291666\n",
      "train loss:0.8001946533456226\n",
      "train loss:0.6373293429166096\n",
      "train loss:0.5216963647435169\n",
      "train loss:0.6114728952096308\n",
      "train loss:0.6119999055166019\n",
      "train loss:0.7071540506047775\n",
      "train loss:0.6978956288691818\n",
      "train loss:0.7850210619150364\n",
      "train loss:0.59764431352609\n",
      "train loss:0.5935373472756602\n",
      "train loss:0.6127047159352771\n",
      "train loss:0.7535289948499623\n",
      "train loss:0.7265630548179024\n",
      "train loss:0.5237492391837825\n",
      "train loss:0.5189427944467588\n",
      "train loss:0.5034323430903195\n",
      "train loss:0.6139664186436636\n",
      "train loss:0.4764354133550935\n",
      "train loss:0.5115854903352057\n",
      "train loss:0.6025119632195753\n",
      "train loss:0.7052197696498658\n",
      "train loss:0.5090688320885436\n",
      "train loss:0.7299533291093188\n",
      "train loss:0.5992664262098119\n",
      "train loss:0.49648958171438523\n",
      "train loss:0.5125692543384426\n",
      "train loss:0.3650886199712697\n",
      "train loss:0.7267384231573362\n",
      "train loss:0.6500516373077316\n",
      "train loss:0.7789194159383168\n",
      "train loss:0.6060906411749718\n",
      "train loss:0.5986557830054627\n",
      "train loss:0.4060635502097445\n",
      "train loss:0.38508146457440756\n",
      "train loss:0.5094115473035604\n",
      "train loss:0.820323808976213\n",
      "train loss:0.6299987399551819\n",
      "train loss:0.6355441572023679\n",
      "train loss:0.5860738416467045\n",
      "train loss:0.6992298707296383\n",
      "train loss:0.5299300387722841\n",
      "train loss:0.5213940605624748\n",
      "train loss:0.5416453253369405\n",
      "train loss:0.5344217697424781\n",
      "train loss:0.6174398012116834\n",
      "train loss:0.4389908901695427\n",
      "train loss:0.3999236125304967\n",
      "train loss:0.39794743759642376\n",
      "train loss:0.5120710749082821\n",
      "train loss:0.7603940384240819\n",
      "train loss:0.9773675093367032\n",
      "train loss:0.3804997614517188\n",
      "train loss:0.5869404909873495\n",
      "train loss:0.49319803218339925\n",
      "train loss:0.48030464410110413\n",
      "train loss:0.5017591336411247\n",
      "train loss:0.5043905222649865\n",
      "train loss:0.5207722189819742\n",
      "train loss:0.37827118669492305\n",
      "train loss:0.49473974176668756\n",
      "train loss:0.65686115761236\n",
      "train loss:0.7416821535329404\n",
      "train loss:0.3869075285112962\n",
      "train loss:0.7703610957232316\n",
      "train loss:0.386408553841532\n",
      "train loss:0.7141205382134918\n",
      "train loss:0.632120391901196\n",
      "train loss:0.6057627692524133\n",
      "train loss:0.4356363888300073\n",
      "train loss:0.6954697696571043\n",
      "train loss:0.507022680284217\n",
      "train loss:0.3633250314644213\n",
      "train loss:0.5987258521169675\n",
      "train loss:0.7644438989066622\n",
      "train loss:0.5238491278718218\n",
      "train loss:0.42863510898322776\n",
      "train loss:0.5135895113996263\n",
      "train loss:0.3922326319649262\n",
      "train loss:0.3744404862974727\n",
      "train loss:1.0143325841158168\n",
      "train loss:0.6337930262224596\n",
      "train loss:0.8393565580975807\n",
      "train loss:0.6153655481714437\n",
      "train loss:0.6816075535170103\n",
      "train loss:0.6099268873903879\n",
      "train loss:0.5066863365925389\n",
      "train loss:0.6014090299818888\n",
      "train loss:0.5266444201928479\n",
      "train loss:0.6798751681540338\n",
      "train loss:0.4636360692078836\n",
      "train loss:0.6149955784767742\n",
      "train loss:0.5536783636406651\n",
      "train loss:0.44999534409333924\n",
      "train loss:0.5185550116442246\n",
      "train loss:0.7799833536896374\n",
      "train loss:0.4877950414831414\n",
      "train loss:0.5478111249084576\n",
      "train loss:0.40612585925732114\n",
      "train loss:0.6336706682940998\n",
      "train loss:0.6600052183715952\n",
      "train loss:1.0400181110685256\n",
      "train loss:0.7184775804174875\n",
      "train loss:0.3236981940718494\n",
      "train loss:0.41908060108258327\n",
      "train loss:0.4060694745852868\n",
      "train loss:0.3952043040893837\n",
      "train loss:0.9004476041342713\n",
      "train loss:0.5826317363131327\n",
      "train loss:0.5857541460218562\n",
      "train loss:0.5385948377885211\n",
      "train loss:0.4997908185131755\n",
      "train loss:0.5012548223176716\n",
      "train loss:0.5015963967407525\n",
      "train loss:0.7327479748214041\n",
      "train loss:0.4993312781724848\n",
      "train loss:0.7421022028627987\n",
      "train loss:0.8138034982708413\n",
      "train loss:0.6107239799090363\n",
      "train loss:0.6786798001330302\n",
      "train loss:0.531064190103328\n",
      "train loss:0.6748283781423116\n",
      "train loss:0.4673365662156151\n",
      "train loss:0.6725240439313236\n",
      "train loss:0.5277777415425811\n",
      "train loss:0.6964509518322994\n",
      "train loss:0.5492280434040604\n",
      "train loss:0.6958238062455294\n",
      "train loss:0.6053718837831334\n",
      "train loss:0.5466156306815757\n",
      "train loss:0.7373944069372831\n",
      "train loss:0.6768822308832029\n",
      "train loss:0.6703703149047506\n",
      "train loss:0.7551648209300122\n",
      "train loss:0.743655510079745\n",
      "train loss:0.7246163169323682\n",
      "train loss:0.5692215742198748\n",
      "train loss:0.515432464033485\n",
      "train loss:0.6315763530891599\n",
      "train loss:0.686423769679441\n",
      "train loss:0.488340832994491\n",
      "train loss:0.5366837841298878\n",
      "train loss:0.4640066686214538\n",
      "train loss:0.6451798221485274\n",
      "train loss:0.5858675112836704\n",
      "train loss:0.5079714711652398\n",
      "train loss:0.5125466714366897\n",
      "train loss:0.6211558077992251\n",
      "train loss:0.7078011143074847\n",
      "train loss:0.5037218521456552\n",
      "train loss:0.5994313874045556\n",
      "train loss:0.4778582788812592\n",
      "train loss:0.20016669323338818\n",
      "train loss:0.7902261491492023\n",
      "train loss:0.8357576383627727\n",
      "train loss:0.6070604451918312\n",
      "train loss:0.739971375584426\n",
      "train loss:0.6293250439806404\n",
      "train loss:0.6839251903993017\n",
      "train loss:0.5343001428070606\n",
      "train loss:0.7529852595782625\n",
      "train loss:0.5704734399727722\n",
      "train loss:0.6880671363309843\n",
      "train loss:0.574766936588569\n",
      "train loss:0.513456667987243\n",
      "train loss:0.5782994582789425\n",
      "train loss:0.5743813979015903\n",
      "train loss:0.5952199366244046\n",
      "train loss:0.7190415058814935\n",
      "train loss:0.6165123246323297\n",
      "train loss:0.5539119737433867\n",
      "train loss:0.6186554288683003\n",
      "train loss:0.5340101620530472\n",
      "train loss:0.6923109060076347\n",
      "train loss:0.6333889582740285\n",
      "train loss:0.5386525980405626\n",
      "train loss:0.3124208711663516\n",
      "train loss:0.39395587614495353\n",
      "train loss:0.21954288711614217\n",
      "train loss:0.4536046660193788\n",
      "train loss:0.6749331922313583\n",
      "train loss:0.3420589511086081\n",
      "train loss:0.5712678429718483\n",
      "train loss:0.7772432603309825\n",
      "train loss:0.6758969001759447\n",
      "train loss:0.28956660121030275\n",
      "train loss:0.8225148870822527\n",
      "train loss:0.5034240913143467\n",
      "train loss:0.3846748012868991\n",
      "train loss:0.24673214461535437\n",
      "train loss:0.7511954254201235\n",
      "train loss:0.5984297517872399\n",
      "train loss:0.5130472498909607\n",
      "train loss:0.7157978907246454\n",
      "train loss:0.5123297945853392\n",
      "train loss:0.7076236432576782\n",
      "train loss:0.6198451112222814\n",
      "train loss:0.7002735843300113\n",
      "train loss:0.5571647599391129\n",
      "train loss:0.7244786487219274\n",
      "train loss:0.6875004167730282\n",
      "train loss:0.6758495085121434\n",
      "train loss:0.6246422005177743\n",
      "train loss:0.4527194566194477\n",
      "train loss:0.673779314935095\n",
      "train loss:0.554662212918996\n",
      "train loss:0.6820100560159588\n",
      "train loss:0.7503297741342456\n",
      "train loss:0.4839736218844304\n",
      "train loss:0.6115914126693496\n",
      "train loss:0.8016341205894856\n",
      "train loss:0.6011047824955396\n",
      "train loss:0.548693430312682\n",
      "train loss:0.5391890034083684\n",
      "train loss:0.5874564297732798\n",
      "train loss:0.44897798597199046\n",
      "train loss:0.6457065614284125\n",
      "train loss:0.38073705750710296\n",
      "train loss:0.3970802932774945\n",
      "train loss:0.6231439865763531\n",
      "train loss:0.49489496788026655\n",
      "train loss:0.7747348651998389\n",
      "train loss:0.34533011739569575\n",
      "train loss:0.6787532032111981\n",
      "train loss:0.973520654987343\n",
      "train loss:0.5074997913867877\n",
      "train loss:0.6113717401065604\n",
      "train loss:0.5786873816250248\n",
      "train loss:0.5920883751927908\n",
      "train loss:0.7699141019920602\n",
      "train loss:0.41824870291572747\n",
      "train loss:0.4731075367700785\n",
      "train loss:0.7575530583764775\n",
      "train loss:0.7571895033997331\n",
      "train loss:0.6133548988921713\n",
      "train loss:0.5759613907729746\n",
      "train loss:0.5808699032119653\n",
      "train loss:0.5656256779601947\n",
      "train loss:0.6250999875521713\n",
      "train loss:0.6294743841833648\n",
      "train loss:0.6062000307480836\n",
      "train loss:0.6746880985924688\n",
      "train loss:0.39297646550064025\n",
      "train loss:0.5303999755423021\n",
      "train loss:0.41028448312272847\n",
      "train loss:0.37985078464676275\n",
      "train loss:0.6154495293687897\n",
      "train loss:0.4835020733422223\n",
      "train loss:0.4881801911108755\n",
      "train loss:0.7996820040895619\n",
      "train loss:0.6666973151061695\n",
      "train loss:0.37575938258575725\n",
      "train loss:0.628973612367959\n",
      "train loss:0.5464467722935509\n",
      "train loss:0.689963454609412\n",
      "train loss:0.5733939527271337\n",
      "train loss:0.5100189904346115\n",
      "train loss:0.7368239220061163\n",
      "train loss:0.3073955255115972\n",
      "train loss:0.832911845576675\n",
      "train loss:0.5307086048265821\n",
      "train loss:0.5346596184389509\n",
      "train loss:0.3644385203943453\n",
      "train loss:0.7796340101950368\n",
      "train loss:0.9143053304681509\n",
      "train loss:0.6966126797012456\n",
      "train loss:0.6393794477525998\n",
      "train loss:0.42921476402320496\n",
      "train loss:0.553594144658486\n",
      "train loss:0.6699160203930575\n",
      "train loss:0.4792436048127463\n",
      "train loss:0.5833621544568072\n",
      "train loss:0.533840839036861\n",
      "train loss:0.6165883361117416\n",
      "train loss:0.5975659860566468\n",
      "train loss:0.49688768240805137\n",
      "train loss:0.8653976417140111\n",
      "train loss:0.500690351767177\n",
      "train loss:0.8314260862165235\n",
      "train loss:0.7111494247355777\n",
      "train loss:0.40731396172184847\n",
      "train loss:0.6636960618133085\n",
      "train loss:0.6107061871851529\n",
      "train loss:0.6115009452081658\n",
      "train loss:0.7801022100954224\n",
      "train loss:0.5946336116779043\n",
      "train loss:0.4568440148477559\n",
      "train loss:0.5436304137467667\n",
      "train loss:0.6974482927474702\n",
      "train loss:0.4349664813345428\n",
      "train loss:0.6154597690515062\n",
      "train loss:0.41603545814308057\n",
      "train loss:0.49398882399298694\n",
      "train loss:0.3889317509345131\n",
      "train loss:0.7707453016032868\n",
      "train loss:0.8296584192280163\n",
      "train loss:0.6376627681954827\n",
      "train loss:0.3661393898750003\n",
      "train loss:0.7210280049443197\n",
      "train loss:0.4048812318874201\n",
      "train loss:0.38795334603317266\n",
      "train loss:0.37733640643884664\n",
      "train loss:0.8805139249633823\n",
      "train loss:0.6118272835732923\n",
      "train loss:0.6367001880787959\n",
      "train loss:0.8054231236618442\n",
      "train loss:0.5135631992601182\n",
      "train loss:0.7144415364297626\n",
      "train loss:0.5326046416910726\n",
      "train loss:0.6156084007980526\n",
      "train loss:0.46829326681568106\n",
      "train loss:0.6772953256174841\n",
      "train loss:0.5212597473007539\n",
      "train loss:0.45108296338544146\n",
      "train loss:0.7643856955998727\n",
      "train loss:0.4375187274989524\n",
      "train loss:0.684235067806766\n",
      "train loss:0.61656454493399\n",
      "train loss:0.5948911950446928\n",
      "train loss:0.7637067717860094\n",
      "train loss:0.6253987345034514\n",
      "train loss:0.3311804871813772\n",
      "train loss:0.7325977380680617\n",
      "train loss:0.7057046325021065\n",
      "train loss:0.5985622719426705\n",
      "train loss:0.6088656307696265\n",
      "train loss:0.5674708286902603\n",
      "train loss:0.6321930876190642\n",
      "train loss:0.7783760130148958\n",
      "train loss:0.5868356414458777\n",
      "train loss:0.6071280102598041\n",
      "train loss:0.460756259946206\n",
      "train loss:0.5753907800984314\n",
      "train loss:0.6102299438409888\n",
      "train loss:0.46702080715973227\n",
      "train loss:0.5052131369487367\n",
      "train loss:0.6241120644378148\n",
      "train loss:0.7190210591567682\n",
      "train loss:0.5100757476633184\n",
      "train loss:0.5148581842482998\n",
      "train loss:0.38264857010744235\n",
      "train loss:0.7145988314590442\n",
      "train loss:0.6613732493174992\n",
      "train loss:0.7536331414452377\n",
      "train loss:0.8255398753763405\n",
      "train loss:0.6973195209817413\n",
      "train loss:0.5336981983634713\n",
      "train loss:0.6003930693674274\n",
      "train loss:0.6181204400164354\n",
      "train loss:0.6769041319667066\n",
      "train loss:0.6147463906740563\n",
      "train loss:0.6281365252643293\n",
      "train loss:0.676641406162046\n",
      "train loss:0.5668556349694885\n",
      "train loss:0.661900832868813\n",
      "train loss:0.5662007175214256\n",
      "train loss:0.6191020749531955\n",
      "train loss:0.553221673263174\n",
      "train loss:0.6147002902846135\n",
      "train loss:0.593322473165796\n",
      "train loss:0.6720419243690522\n",
      "train loss:0.5335643356450366\n",
      "train loss:0.8021998445031802\n",
      "train loss:0.34714533880237763\n",
      "train loss:0.7865640349118213\n",
      "train loss:0.6653335334680137\n",
      "train loss:0.6900413417100519\n",
      "train loss:0.8915380553470404\n",
      "train loss:0.6203671266314116\n",
      "train loss:0.6877943915420003\n",
      "train loss:0.5348400494584108\n",
      "train loss:0.380601300997415\n",
      "train loss:0.5974725026842711\n",
      "train loss:0.447728746862089\n",
      "train loss:0.6186625853711184\n",
      "train loss:0.610451574697455\n",
      "train loss:0.5148368544225808\n",
      "train loss:0.8252495657190243\n",
      "train loss:0.5177773326852076\n",
      "train loss:0.6148469327302977\n",
      "train loss:0.41288396222412693\n",
      "train loss:0.6926253307567587\n",
      "train loss:0.8288055605559217\n",
      "train loss:0.7096623628642862\n",
      "train loss:0.5755013934332082\n",
      "train loss:0.689909573884893\n",
      "train loss:0.5280904883551771\n",
      "train loss:0.3535775119458971\n",
      "train loss:0.32812152692455937\n",
      "train loss:0.7897354672324989\n",
      "train loss:0.7406101346132645\n",
      "train loss:0.424125391780623\n",
      "train loss:0.6198180444329424\n",
      "train loss:0.41153661999894\n",
      "train loss:0.3894932798089659\n",
      "train loss:0.9156332160286258\n",
      "train loss:0.8318877691395533\n",
      "train loss:0.5200302343857355\n",
      "train loss:0.7388110572593639\n",
      "train loss:0.7676504637395882\n",
      "train loss:0.519037736060677\n",
      "train loss:0.6002996072552385\n",
      "train loss:0.45759275833764673\n",
      "train loss:0.6170238170513521\n",
      "train loss:0.5962065558801188\n",
      "train loss:0.7350661829166272\n",
      "train loss:0.6052539431099759\n",
      "train loss:0.5303616331462669\n",
      "train loss:0.806832881738719\n",
      "train loss:0.6105630231393546\n",
      "train loss:0.39786955988961575\n",
      "train loss:0.6067362397580396\n",
      "train loss:0.4566695833527098\n",
      "train loss:0.5813191149154828\n",
      "train loss:0.8002894100177305\n",
      "train loss:0.47832756108657665\n",
      "train loss:0.5024136171865295\n",
      "train loss:0.7217928111040168\n",
      "train loss:0.7226588752490057\n",
      "train loss:0.5160564930049232\n",
      "train loss:0.6315470670505698\n",
      "train loss:0.6947405800766033\n",
      "train loss:0.5399484739659548\n",
      "train loss:0.8848486030666078\n",
      "train loss:0.5187552450885409\n",
      "train loss:0.7924606566871355\n",
      "train loss:0.6096743284130535\n",
      "train loss:0.5499995228784553\n",
      "train loss:0.4896824897526029\n",
      "train loss:0.5914490927823375\n",
      "train loss:0.6199036462107504\n",
      "train loss:0.3891721730763428\n",
      "train loss:0.6944018943257195\n",
      "train loss:0.5320269892263314\n",
      "train loss:0.5941646927912878\n",
      "train loss:0.806309753567384\n",
      "train loss:0.601605430317715\n",
      "train loss:0.41898405941855543\n",
      "train loss:0.7259127145216897\n",
      "train loss:0.6937432539030283\n",
      "train loss:0.565931631176584\n",
      "train loss:0.6190492037526608\n",
      "train loss:0.3122756031871191\n",
      "train loss:0.9405099702570935\n",
      "train loss:0.41637231395523777\n",
      "train loss:0.38921312518746004\n",
      "train loss:0.5178420528255951\n",
      "train loss:0.6324335620028432\n",
      "train loss:0.46076123689841264\n",
      "train loss:0.7328272255461334\n",
      "train loss:0.48914900886694906\n",
      "train loss:0.630618107378623\n",
      "train loss:0.7120169910759928\n",
      "train loss:0.4825167612019435\n",
      "train loss:0.4781375283145654\n",
      "train loss:0.46064491270637065\n",
      "train loss:0.7478161522865809\n",
      "train loss:0.49676417210267826\n",
      "train loss:0.38446245628786135\n",
      "train loss:0.5413494478330423\n",
      "train loss:0.6117307945989541\n",
      "train loss:0.6256575603577093\n",
      "train loss:0.5278305289467402\n",
      "train loss:0.7072075135294471\n",
      "train loss:0.5008148312596516\n",
      "train loss:0.593140245390958\n",
      "train loss:0.6108677314776166\n",
      "train loss:0.38744690508963175\n",
      "train loss:0.5153459483458781\n",
      "train loss:0.503301134626204\n",
      "train loss:0.27533473821728444\n",
      "train loss:0.21426007247794607\n",
      "train loss:0.9619476105914835\n",
      "train loss:0.7413666298337639\n",
      "train loss:0.5051766717279523\n",
      "train loss:0.73384345041576\n",
      "train loss:0.6399693307170446\n",
      "train loss:0.5211833861117277\n",
      "train loss:0.7973882812519266\n",
      "train loss:0.41474440414968383\n",
      "train loss:0.6360295476942273\n",
      "train loss:0.43697061402869286\n",
      "train loss:0.532215489179458\n",
      "train loss:0.6418814471570118\n",
      "train loss:0.6139215693047293\n",
      "train loss:0.5276684031333307\n",
      "train loss:0.6883591564750311\n",
      "train loss:0.6231483004595001\n",
      "train loss:0.6866230124052961\n",
      "train loss:0.6720142222925455\n",
      "train loss:0.6605411446842382\n",
      "train loss:0.5947201375050648\n",
      "train loss:0.6703962441629263\n",
      "train loss:0.7899238392499673\n",
      "train loss:0.6790556819406743\n",
      "train loss:0.5989675574451693\n",
      "train loss:0.7737702200068558\n",
      "train loss:0.4939341406003508\n",
      "train loss:0.663890389450622\n",
      "train loss:0.5732638528274843\n",
      "train loss:0.49127939370792967\n",
      "train loss:0.5480013934014509\n",
      "train loss:0.7570039587311213\n",
      "train loss:0.6066498128189656\n",
      "train loss:0.6782190363184988\n",
      "train loss:0.454525246535012\n",
      "train loss:0.6993091129555129\n",
      "train loss:0.5311916878208166\n",
      "train loss:0.7620191443206751\n",
      "train loss:0.6887616595929403\n",
      "train loss:0.7439540376761442\n",
      "train loss:0.6507372989029238\n",
      "train loss:0.6035988080680068\n",
      "train loss:0.605057337358648\n",
      "train loss:0.6159786054094106\n",
      "train loss:0.4493313052800064\n",
      "train loss:0.7663851539353704\n",
      "train loss:0.5947595114317736\n",
      "train loss:0.5378652354801041\n",
      "train loss:0.7107250060662229\n",
      "train loss:0.448685719159717\n",
      "train loss:0.43595298696872237\n",
      "train loss:0.5821441240742283\n",
      "train loss:0.4315712490340772\n",
      "train loss:0.6132464012932304\n",
      "train loss:0.6115967213493033\n",
      "train loss:0.7246439590143299\n",
      "train loss:0.739366676071212\n",
      "train loss:0.5072615034291056\n",
      "train loss:0.6001886085379059\n",
      "train loss:0.6990811149658942\n",
      "train loss:0.7634980838728588\n",
      "train loss:0.4286356577153153\n",
      "train loss:0.43294257469053166\n",
      "train loss:0.6065771196546879\n",
      "train loss:0.5310259231973308\n",
      "train loss:0.7031105185072238\n",
      "train loss:0.6704029780252945\n",
      "train loss:0.5341468375421105\n",
      "train loss:0.5417935441853496\n",
      "train loss:0.5983971301034798\n",
      "train loss:0.604152630143089\n",
      "train loss:0.5017333765662747\n",
      "train loss:0.5859515412400754\n",
      "train loss:0.2895965696956039\n",
      "train loss:0.7532043355110923\n",
      "train loss:0.5389732027361581\n",
      "train loss:0.7333785396122771\n",
      "train loss:0.5980586856508577\n",
      "train loss:0.6137028574007731\n",
      "train loss:0.6984742447839156\n",
      "train loss:0.3881256404572407\n",
      "train loss:0.6259242471889774\n",
      "train loss:0.7172631054153362\n",
      "train loss:0.5319406663512958\n",
      "train loss:0.6738090276555171\n",
      "train loss:0.4074019497178032\n",
      "train loss:0.5324164773485607\n",
      "train loss:0.5152784071542242\n",
      "train loss:0.6028945288782864\n",
      "train loss:0.676609233412113\n",
      "train loss:0.38697795867512175\n",
      "train loss:0.4147414269923565\n",
      "train loss:0.4069606906168065\n",
      "train loss:0.534115576240972\n",
      "train loss:0.2744737468159908\n",
      "train loss:0.6860665884814567\n",
      "train loss:0.8899500059579243\n",
      "train loss:0.9729303046335239\n",
      "train loss:0.617695497656296\n",
      "train loss:0.406884126121603\n",
      "train loss:0.6059655311235772\n",
      "train loss:0.51857634375836\n",
      "train loss:0.590060473880049\n",
      "train loss:0.6908642596453204\n",
      "train loss:0.6758011066447864\n",
      "train loss:0.685946047264629\n",
      "train loss:0.559094519095584\n",
      "train loss:0.500808561024767\n",
      "train loss:0.5457922434644276\n",
      "train loss:0.6048596492409395\n",
      "train loss:0.6362311907849816\n",
      "train loss:0.6113859158005785\n",
      "train loss:0.6207488515115103\n",
      "train loss:0.44816746722733536\n",
      "train loss:0.4465273606249343\n",
      "train loss:0.5903490258365061\n",
      "train loss:0.7430221493128626\n",
      "train loss:0.5087812602993986\n",
      "train loss:0.6998169755798658\n",
      "train loss:0.38062225794816357\n",
      "train loss:0.6151058949755839\n",
      "train loss:0.3547048466400654\n",
      "train loss:0.385566904640738\n",
      "train loss:0.9495507320083799\n",
      "train loss:0.48936363481880385\n",
      "train loss:0.6332484371717062\n",
      "train loss:0.5054872953260967\n",
      "train loss:0.6389378860332815\n",
      "train loss:0.755533607048655\n",
      "train loss:0.7635475242004294\n",
      "train loss:0.634036786748769\n",
      "train loss:0.5296825973145401\n",
      "train loss:0.7538267283266741\n",
      "train loss:0.4214627342046233\n",
      "train loss:0.8103694869994189\n",
      "train loss:0.5552440283077626\n",
      "train loss:0.5101921575188588\n",
      "train loss:0.6815114390196733\n",
      "train loss:0.6641556634174755\n",
      "train loss:0.6175154418132369\n",
      "train loss:0.6198146322370206\n",
      "train loss:0.7734690856123595\n",
      "train loss:0.5296128985514943\n",
      "train loss:0.6225016930356047\n",
      "train loss:0.5517186270200556\n",
      "train loss:0.8598615603884721\n",
      "train loss:0.8014024088499406\n",
      "train loss:0.47645944340297824\n",
      "train loss:0.6693241846813758\n",
      "train loss:0.48687911506964693\n",
      "train loss:0.5560175160678577\n",
      "train loss:0.7499192614426347\n",
      "train loss:0.667822315977067\n",
      "train loss:0.6733768335502014\n",
      "train loss:0.6888948171019287\n",
      "train loss:0.6105067066936215\n",
      "train loss:0.44766389550900626\n",
      "train loss:0.42877536351760687\n",
      "train loss:0.683764412383051\n",
      "train loss:0.44273366283252075\n",
      "train loss:0.39954612623423397\n",
      "train loss:0.40083500820737444\n",
      "train loss:0.5997970938055824\n",
      "train loss:0.3193012190848134\n",
      "train loss:0.646499906536388\n",
      "train loss:0.5212411144955647\n",
      "train loss:0.7092107168196243\n",
      "train loss:0.30808087063783546\n",
      "train loss:0.35193408943588894\n",
      "train loss:0.8551835962011569\n",
      "train loss:0.3443563498851524\n",
      "train loss:0.3797710022230616\n",
      "train loss:0.8003829597363923\n",
      "train loss:0.750346451963722\n",
      "train loss:0.5989551660358858\n",
      "train loss:0.40957655780287705\n",
      "train loss:0.5394546002818034\n",
      "train loss:0.7458253803302315\n",
      "train loss:0.4776157545858915\n",
      "train loss:0.38987864560049784\n",
      "train loss:0.8249391673522283\n",
      "train loss:0.6660731733923082\n",
      "train loss:0.615063959441386\n",
      "train loss:0.7347179969312521\n",
      "train loss:0.6131650064555626\n",
      "train loss:0.687266536083284\n",
      "train loss:0.6063286395671678\n",
      "train loss:0.675328871467223\n",
      "train loss:0.7248681062755734\n",
      "train loss:0.6239095117028503\n",
      "train loss:0.679830374574318\n",
      "train loss:0.5620688766316448\n",
      "train loss:0.6072560747204634\n",
      "train loss:0.6728686868544351\n",
      "train loss:0.6673145252053833\n",
      "train loss:0.6041757256749556\n",
      "train loss:0.5595382893530246\n",
      "train loss:0.6076956311761604\n",
      "train loss:0.6167013337292252\n",
      "train loss:0.6900378406010363\n",
      "train loss:0.4418943096258346\n",
      "train loss:0.6054600159237477\n",
      "train loss:0.5148456583363664\n",
      "train loss:0.41749193589030853\n",
      "train loss:0.3945351385235685\n",
      "train loss:0.5748889742038765\n",
      "train loss:0.48585633516783416\n",
      "train loss:0.49352898720684263\n",
      "train loss:0.5317372410652473\n",
      "train loss:0.8155145568568084\n",
      "train loss:0.622797074907713\n",
      "train loss:0.6406103996552558\n",
      "train loss:0.5213900441614825\n",
      "train loss:0.3794468683723628\n",
      "train loss:0.6346974667647934\n",
      "train loss:0.6050300420024517\n",
      "train loss:0.4841276855084134\n",
      "train loss:0.40963429964037845\n",
      "train loss:0.7091307192159992\n",
      "train loss:0.7015202142372559\n",
      "train loss:0.5364309646405591\n",
      "train loss:0.7670332702679635\n",
      "train loss:0.6178040983907885\n",
      "train loss:0.7323255676439198\n",
      "train loss:0.4869549969271275\n",
      "train loss:0.6003850548442504\n",
      "train loss:0.5484854731845762\n",
      "train loss:0.679831790431354\n",
      "train loss:0.4691766348100404\n",
      "train loss:0.5365059283448457\n",
      "train loss:0.5260910801499188\n",
      "train loss:0.5283011120098672\n",
      "train loss:0.5748299286984272\n",
      "train loss:0.48705116873866994\n",
      "train loss:0.3859894722168344\n",
      "train loss:0.7660133966892134\n",
      "train loss:0.6288136377137545\n",
      "train loss:0.8984602038617442\n",
      "train loss:0.6172106127289674\n",
      "train loss:0.619080589374064\n",
      "train loss:0.3841487350510905\n",
      "train loss:0.7306203445611188\n",
      "train loss:0.7158853912008192\n",
      "train loss:0.7645837051745966\n",
      "train loss:0.5956730302027176\n",
      "train loss:0.6120832051488818\n",
      "train loss:0.7765064493462789\n",
      "train loss:0.5554369271776124\n",
      "train loss:0.48908053879763924\n",
      "train loss:0.6193444894802473\n",
      "train loss:0.5475686576446798\n",
      "train loss:0.6663478422976128\n",
      "train loss:0.6704909614893549\n",
      "train loss:0.6697666000742586\n",
      "train loss:0.5551578575068835\n",
      "train loss:0.6150622125713305\n",
      "train loss:0.46954640828669236\n",
      "train loss:0.5419682734172098\n",
      "train loss:0.5201223687622548\n",
      "train loss:0.6162153446807819\n",
      "train loss:0.4062296336640996\n",
      "train loss:0.3919075757627059\n",
      "train loss:0.5231212523358977\n",
      "train loss:0.6486327736592561\n",
      "train loss:0.4881452843056112\n",
      "train loss:0.3490394385977539\n",
      "train loss:0.7037849006129424\n",
      "train loss:0.8647430791209546\n",
      "train loss:0.773026497131293\n",
      "train loss:0.6395571548513242\n",
      "train loss:0.4964521012955673\n",
      "train loss:0.5269417920780626\n",
      "train loss:0.6070451668472923\n",
      "train loss:0.4558406882572732\n",
      "train loss:0.4289351925134309\n",
      "train loss:0.36113269964942163\n",
      "train loss:0.8447698815652039\n",
      "train loss:0.691191677433836\n",
      "train loss:0.6275369971286662\n",
      "train loss:0.614222771041053\n",
      "train loss:0.5714047111670527\n",
      "train loss:0.5201439807092321\n",
      "train loss:0.5124601514872441\n",
      "train loss:0.861506569216998\n",
      "train loss:0.5039816266645973\n",
      "train loss:0.5168301451646037\n",
      "train loss:0.5527401804476202\n",
      "train loss:0.7216485325449458\n",
      "train loss:0.5023752135269915\n",
      "train loss:0.4848166615641721\n",
      "train loss:0.797157850751921\n",
      "train loss:0.32523520213960133\n",
      "train loss:0.6282630684033256\n",
      "train loss:0.6929990313542778\n",
      "train loss:0.4999562378195952\n",
      "train loss:0.614297987995152\n",
      "train loss:0.5047153008574893\n",
      "train loss:0.6648408473367546\n",
      "train loss:0.623574369622462\n",
      "train loss:0.7416878242316158\n",
      "train loss:0.7101135309197796\n",
      "train loss:0.7812714890524053\n",
      "train loss:0.5167693424986263\n",
      "train loss:0.5187179993286648\n",
      "train loss:0.6877613886738261\n",
      "train loss:0.8000389997036385\n",
      "train loss:0.6859926350285036\n",
      "train loss:0.6260561945081837\n",
      "train loss:0.5978304526232615\n",
      "train loss:0.7117241355794328\n",
      "train loss:0.5301144888273043\n",
      "train loss:0.5227225133141559\n",
      "train loss:0.5702747590056015\n",
      "train loss:0.7444385829522113\n",
      "train loss:0.5715467176941014\n",
      "train loss:0.6695751853686347\n",
      "train loss:0.6232668617694923\n",
      "train loss:0.6681957483862317\n",
      "train loss:0.6119811207502391\n",
      "train loss:0.7390893894595133\n",
      "train loss:0.6639506115067336\n",
      "train loss:0.5427759091475509\n",
      "train loss:0.36563839353133454\n",
      "train loss:0.5951550085369304\n",
      "train loss:0.7160622106661406\n",
      "train loss:0.4988742955068154\n",
      "train loss:0.5104800511023242\n",
      "train loss:0.7064005648271348\n",
      "train loss:0.6481242424066651\n",
      "train loss:0.36902763908642594\n",
      "train loss:0.6162646929101822\n",
      "train loss:0.3509000531617421\n",
      "train loss:0.7139123884610697\n",
      "train loss:0.4869455061485106\n",
      "train loss:0.4839883005404678\n",
      "train loss:0.6223569547741038\n",
      "train loss:0.4889583133324826\n",
      "train loss:0.7997693433813755\n",
      "train loss:0.502549633882011\n",
      "train loss:0.5273933169540299\n",
      "train loss:0.5964521782008448\n",
      "train loss:0.8063192137414081\n",
      "train loss:0.5117699142318073\n",
      "train loss:0.427829148053236\n",
      "train loss:0.5222619635754144\n",
      "train loss:0.5071040819882167\n",
      "train loss:0.607098519295038\n",
      "train loss:0.6027980221119172\n",
      "train loss:0.7041996008152828\n",
      "train loss:0.4987440002272603\n",
      "train loss:0.43378438677467984\n",
      "train loss:0.5094567415136814\n",
      "train loss:0.6340304249735735\n",
      "train loss:0.4037501402322367\n",
      "train loss:0.8402189703025285\n",
      "train loss:0.5153174864039515\n",
      "train loss:0.38564079423398223\n",
      "train loss:0.7588454109476132\n",
      "train loss:0.7350526030295211\n",
      "train loss:0.6189019112515063\n",
      "train loss:0.5750951070346558\n",
      "train loss:0.6122488025093812\n",
      "train loss:0.5309633231803991\n",
      "train loss:0.5875545480765123\n",
      "train loss:0.4178772844691487\n",
      "train loss:0.5125832081888708\n",
      "train loss:0.7010778443981622\n",
      "train loss:0.6257688630054614\n",
      "train loss:0.6008081826450058\n",
      "train loss:0.5130901421358782\n",
      "train loss:0.4048567210228965\n",
      "train loss:0.6110192504849723\n",
      "train loss:0.6202851203672058\n",
      "train loss:0.619212086497025\n",
      "train loss:0.6208309873279779\n",
      "train loss:0.5044452859441623\n",
      "train loss:0.5449333408330447\n",
      "train loss:0.5371331696423746\n",
      "train loss:0.5212096592979246\n",
      "train loss:0.5214871296580101\n",
      "train loss:0.5132294643902974\n",
      "train loss:0.3886127685843416\n",
      "train loss:0.5948028308183966\n",
      "train loss:0.6288172302042108\n",
      "train loss:0.4943845532514525\n",
      "train loss:0.3609674767693244\n",
      "train loss:0.4805531316352166\n",
      "train loss:0.9497909918086949\n",
      "train loss:0.6615276625288403\n",
      "train loss:0.3694265821283095\n",
      "train loss:0.257448666164445\n",
      "train loss:0.6324629327411192\n",
      "train loss:0.3651836907328055\n",
      "train loss:0.6369930302621115\n",
      "train loss:0.8247650167618271\n",
      "train loss:0.7105285640446135\n",
      "train loss:0.5121782487094243\n",
      "train loss:0.5129367969856704\n",
      "train loss:0.5093652508828685\n",
      "train loss:0.5819928834284529\n",
      "train loss:0.6010859643310129\n",
      "train loss:0.7744946290920686\n",
      "train loss:0.531089836381698\n",
      "train loss:0.6060411630133075\n",
      "train loss:0.6106337833078295\n",
      "train loss:0.5862532772653883\n",
      "train loss:0.44064264307385337\n",
      "train loss:0.7867049232512462\n",
      "train loss:0.5179768140719266\n",
      "train loss:0.42323432315873866\n",
      "train loss:0.4167018316130364\n",
      "train loss:0.7053141413827861\n",
      "train loss:0.6152492279693588\n",
      "train loss:0.7955737160707952\n",
      "train loss:0.5431212316434553\n",
      "train loss:0.38266648617785526\n",
      "train loss:0.606209249140273\n",
      "train loss:0.6092581896465715\n",
      "train loss:0.7000342406202104\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6617647058823529\n"
     ]
    }
   ],
   "source": [
    "from common.trainer import Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41bf42c2-d463-4083-97b1-5b44df752e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Hue images shape: (1020, 1, 28, 28)\n",
      "Labels shape: (1020,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('G1020.csv')\n",
    "image_files = df['imageID'].tolist()  # 이미지 파일 이름이 있는 열 이름\n",
    "labels = df['binaryLabels'].values  # 레이블이 있는 열 이름\n",
    "\n",
    "# 이미지 크기 설정\n",
    "target_size = (28, 28)\n",
    "\n",
    "# cv2로 이미지 읽고 전처리\n",
    "def load_and_extract_hue(image_path, target_size):\n",
    "    image = cv2.imread(image_path)  # 이미지 읽기 (기본 BGR 형식)\n",
    "    if image is None:\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return None\n",
    "    image = cv2.resize(image, target_size)  # 이미지 크기 조정\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)  # BGR을 HSV로 변환\n",
    "    hue_channel = hsv_image[:, :, 0]  # Hue 채널만 추출\n",
    "    hue_channel = hue_channel / 180.0  # Hue 값 정규화 (0~1 범위로 스케일링, Hue 범위는 0-179)\n",
    "    return hue_channel\n",
    "\n",
    "# 모든 이미지를 불러와서 리스트에 저장\n",
    "hue_images = [load_and_extract_hue(image_folder_path + img_path, target_size) for img_path in image_files]\n",
    "hue_images = np.array([img for img in hue_images if img is not None])  # None 값 제거\n",
    "\n",
    "# 차원 추가하여 (1020, 1, 128, 128) 형태로 변환\n",
    "hue_images = hue_images[:, np.newaxis, :, :]\n",
    "\n",
    "print(f\"Processed Hue images shape: {hue_images.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f5bec08-60f2-47cd-ada6-0d2386d808f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2998602638334167\n",
      "=== epoch:1, train acc:0.7, test acc:0.65 ===\n",
      "train loss:2.295370330187135\n",
      "train loss:2.2888145706905396\n",
      "train loss:2.2778996305229873\n",
      "train loss:2.2555070292030237\n",
      "train loss:2.234920183588287\n",
      "train loss:2.1947911513570335\n",
      "train loss:2.1551512257486967\n",
      "train loss:2.0552867723339387\n",
      "train loss:2.0179849263991274\n",
      "train loss:1.9067493226918835\n",
      "train loss:1.785065347923328\n",
      "train loss:1.5873058487475011\n",
      "train loss:1.4581537769916273\n",
      "train loss:1.2636267089883366\n",
      "train loss:1.2251661951109598\n",
      "train loss:0.7659249284554346\n",
      "train loss:0.712054079773794\n",
      "train loss:0.7691612056074145\n",
      "train loss:0.5934447851069408\n",
      "train loss:0.7152877040142341\n",
      "train loss:0.9690796665957601\n",
      "train loss:0.8859399633192634\n",
      "train loss:0.5373714122178896\n",
      "train loss:0.8300782836841927\n",
      "train loss:0.5238847499384778\n",
      "train loss:0.6103711892762114\n",
      "train loss:0.7637493489532398\n",
      "train loss:0.6221800310141692\n",
      "train loss:0.5273797812913722\n",
      "train loss:0.5680836707586787\n",
      "train loss:0.6284336572889216\n",
      "train loss:0.5021984296349665\n",
      "train loss:0.47316294242354073\n",
      "train loss:0.6916390944904219\n",
      "train loss:0.41953063106831834\n",
      "train loss:0.6553554749509682\n",
      "train loss:0.4939661403989251\n",
      "train loss:0.19127515961567995\n",
      "train loss:0.7140697752268623\n",
      "train loss:0.5011075360279047\n",
      "train loss:0.7019618648339604\n",
      "train loss:0.5399933579428977\n",
      "train loss:0.8601760719114153\n",
      "train loss:0.7901890533250834\n",
      "train loss:0.6039978264677777\n",
      "train loss:0.7626843952724357\n",
      "train loss:0.5994942892638365\n",
      "train loss:0.6830043912919207\n",
      "train loss:0.670179729976941\n",
      "train loss:0.6438936541871872\n",
      "train loss:0.6193677565904023\n",
      "train loss:0.6909117103820184\n",
      "train loss:0.4204154583831943\n",
      "train loss:0.7155871776305485\n",
      "train loss:0.6509783785510849\n",
      "train loss:0.6318261743339566\n",
      "train loss:0.7279723636212437\n",
      "train loss:0.47523214100512484\n",
      "train loss:0.5016713258240291\n",
      "train loss:0.6464479612747107\n",
      "train loss:0.7678831244331463\n",
      "train loss:0.3458190511170443\n",
      "train loss:0.4919117170546383\n",
      "train loss:0.3650621347437326\n",
      "train loss:0.6132747071896045\n",
      "train loss:0.4884206843859065\n",
      "train loss:0.831532527720983\n",
      "train loss:0.5183510440782136\n",
      "train loss:0.40237075964946695\n",
      "train loss:1.0697903509061086\n",
      "train loss:0.4022363123098285\n",
      "train loss:0.6995053177460713\n",
      "train loss:0.5959940428423833\n",
      "train loss:0.6286027573373358\n",
      "train loss:0.6840976355367359\n",
      "train loss:0.6146024941024749\n",
      "train loss:0.525026077251409\n",
      "train loss:0.7138275670103532\n",
      "train loss:0.6368216345945015\n",
      "train loss:0.6237815729937018\n",
      "train loss:0.6154173280868526\n",
      "train loss:0.6741336604549839\n",
      "train loss:0.47834246386413976\n",
      "train loss:0.6838267425679381\n",
      "train loss:0.5264820030613508\n",
      "train loss:0.6095026440819045\n",
      "train loss:0.5027587662543402\n",
      "train loss:0.6142173416348735\n",
      "train loss:0.370385604563774\n",
      "train loss:0.49908117144170144\n",
      "train loss:0.621556661500642\n",
      "train loss:0.7878784603939878\n",
      "train loss:0.33456849194551047\n",
      "train loss:1.066897178174409\n",
      "train loss:0.8933015701027982\n",
      "train loss:0.7150703432117995\n",
      "train loss:0.5050118001074664\n",
      "train loss:0.6954876314930083\n",
      "train loss:0.6036450029183563\n",
      "train loss:0.6135223547244618\n",
      "train loss:0.6285129828576068\n",
      "train loss:0.6936423810354685\n",
      "train loss:0.7074615858222467\n",
      "train loss:0.6520838996055291\n",
      "train loss:0.6230174634616256\n",
      "train loss:0.7047828190281258\n",
      "train loss:0.6232189405691486\n",
      "train loss:0.6712421773785548\n",
      "train loss:0.596363235515721\n",
      "train loss:0.6437159064523778\n",
      "train loss:0.5772913538760404\n",
      "train loss:0.5398379915627717\n",
      "train loss:0.538560035631515\n",
      "train loss:0.5226565618597446\n",
      "train loss:0.763942291022707\n",
      "train loss:0.6237895310746197\n",
      "train loss:0.6337342598725011\n",
      "train loss:0.3767894397694051\n",
      "train loss:0.7647495977421676\n",
      "train loss:0.48887594751695984\n",
      "train loss:0.5981917340937443\n",
      "train loss:0.660831325201064\n",
      "train loss:0.6523230263220214\n",
      "train loss:0.532962197711882\n",
      "train loss:0.6403983351767255\n",
      "train loss:0.9469396915632423\n",
      "train loss:0.7215396284295156\n",
      "train loss:0.70514219544584\n",
      "train loss:0.7555071106863072\n",
      "train loss:0.6243480339729868\n",
      "train loss:0.5003610151920366\n",
      "train loss:0.632488171305196\n",
      "train loss:0.5174504974171812\n",
      "train loss:0.6198680464584159\n",
      "train loss:0.5179568690479041\n",
      "train loss:0.6856735216827505\n",
      "train loss:0.5635936975254021\n",
      "train loss:0.595081448880033\n",
      "train loss:0.5221527829358654\n",
      "train loss:0.7044479009297231\n",
      "train loss:0.6030649039864313\n",
      "train loss:0.9829840767805751\n",
      "train loss:0.8844754832375749\n",
      "train loss:0.6123470424932638\n",
      "train loss:0.37408292954723243\n",
      "train loss:0.7897100159871379\n",
      "train loss:0.694165603140297\n",
      "train loss:0.4108460023572471\n",
      "train loss:0.46288475349224045\n",
      "train loss:0.5190904823182579\n",
      "train loss:0.6676126089529685\n",
      "train loss:0.690063480091794\n",
      "train loss:0.6070796709410243\n",
      "train loss:0.33221723262218206\n",
      "train loss:0.30290663914391824\n",
      "train loss:0.6333696473910548\n",
      "train loss:0.6975258251675102\n",
      "train loss:0.5008174574420157\n",
      "train loss:0.7680956786054072\n",
      "train loss:0.6148022042597392\n",
      "train loss:0.8059940499773146\n",
      "train loss:0.8579450287416002\n",
      "train loss:0.3958446873771155\n",
      "train loss:0.5964478175103773\n",
      "train loss:0.4335951657173429\n",
      "train loss:0.2906608319766358\n",
      "train loss:0.9012650463163144\n",
      "train loss:0.6310554555428899\n",
      "train loss:0.7198943293176041\n",
      "train loss:0.7112351717691718\n",
      "train loss:0.35952393457675785\n",
      "train loss:0.6146749393915896\n",
      "train loss:0.6368036719727331\n",
      "train loss:0.45924990098036744\n",
      "train loss:0.6221007399140619\n",
      "train loss:0.6893767093259198\n",
      "train loss:0.6136131691791797\n",
      "train loss:0.7051164758684351\n",
      "train loss:0.7632573817457631\n",
      "train loss:0.7535910479428833\n",
      "train loss:0.5539583520384355\n",
      "train loss:0.5694042193892277\n",
      "train loss:0.6881659786663463\n",
      "train loss:0.5002356836001925\n",
      "train loss:0.6183943554370839\n",
      "train loss:0.7476298101905071\n",
      "train loss:0.5558201013981521\n",
      "train loss:0.7079589738713119\n",
      "train loss:0.6253941689958171\n",
      "train loss:0.620157850031372\n",
      "train loss:0.6237025085668002\n",
      "train loss:0.6908696004072274\n",
      "train loss:0.5301481573542794\n",
      "train loss:0.5492727531160588\n",
      "train loss:0.542045974896468\n",
      "train loss:0.7498947355472609\n",
      "train loss:0.8591004087192425\n",
      "train loss:0.7530588201393249\n",
      "train loss:0.7676643402376025\n",
      "train loss:0.6800501003421591\n",
      "train loss:0.6795775402937413\n",
      "train loss:0.5974790868328196\n",
      "train loss:0.5066145726680469\n",
      "train loss:0.529466029341916\n",
      "train loss:0.5753112982740214\n",
      "train loss:0.7263689353480867\n",
      "train loss:0.6235122236712745\n",
      "train loss:0.710717654881079\n",
      "train loss:0.5838928859550514\n",
      "train loss:0.4625314403834665\n",
      "train loss:0.9447387947111846\n",
      "train loss:0.7188745600294053\n",
      "train loss:0.5939543881068732\n",
      "train loss:0.5341198858877702\n",
      "train loss:0.6038062423213557\n",
      "train loss:0.42009779190417645\n",
      "train loss:0.525685014643205\n",
      "train loss:0.7021844399235907\n",
      "train loss:0.7970166993923733\n",
      "train loss:0.7122720966564\n",
      "train loss:0.5472527147085302\n",
      "train loss:0.5093376782892587\n",
      "train loss:0.708678208780583\n",
      "train loss:0.43470438441728004\n",
      "train loss:0.8003083986553795\n",
      "train loss:0.5357149982985063\n",
      "train loss:0.6917617698109102\n",
      "train loss:0.6197115332784808\n",
      "train loss:0.5190251931202127\n",
      "train loss:0.6917841916773994\n",
      "train loss:0.45863418944312445\n",
      "train loss:0.6084218168345943\n",
      "train loss:0.5577389167450255\n",
      "train loss:0.3070110842640833\n",
      "train loss:0.4071484523098247\n",
      "train loss:0.3909538933995349\n",
      "train loss:0.8761835452731693\n",
      "train loss:0.640287926190017\n",
      "train loss:0.35827605510142635\n",
      "train loss:0.20881122321251394\n",
      "train loss:0.942681295765216\n",
      "train loss:0.653745249762324\n",
      "train loss:0.49929648457582215\n",
      "train loss:0.655598838942878\n",
      "train loss:0.8930314815788349\n",
      "train loss:0.5269507878329566\n",
      "train loss:0.3867777992852529\n",
      "train loss:0.3728795711491458\n",
      "train loss:0.5247516875823637\n",
      "train loss:0.7060290892634116\n",
      "train loss:0.8107988997864333\n",
      "train loss:0.5162904721275708\n",
      "train loss:0.5080448601673035\n",
      "train loss:0.5142452866704307\n",
      "train loss:0.7132658744593268\n",
      "train loss:0.6017533897119975\n",
      "train loss:0.6728824858974185\n",
      "train loss:0.6909904197079203\n",
      "train loss:0.6862795346239848\n",
      "train loss:0.7488199768057175\n",
      "train loss:0.6759550471857257\n",
      "train loss:0.6767727911630061\n",
      "train loss:0.7040377853672821\n",
      "train loss:0.6040330422833519\n",
      "train loss:0.6493960176005044\n",
      "train loss:0.6095820048208906\n",
      "train loss:0.6012501815634821\n",
      "train loss:0.6375092567536902\n",
      "train loss:0.6196597757175992\n",
      "train loss:0.676392807129693\n",
      "train loss:0.5515829556747548\n",
      "train loss:0.812818232049592\n",
      "train loss:0.731646941628739\n",
      "train loss:0.5615623055270981\n",
      "train loss:0.6303966451675551\n",
      "train loss:0.538830784064387\n",
      "train loss:0.7640558997548841\n",
      "train loss:0.536750492818572\n",
      "train loss:0.5188039191445563\n",
      "train loss:0.6913448600605182\n",
      "train loss:0.5387298014303401\n",
      "train loss:0.5383084246635079\n",
      "train loss:0.7203515586955467\n",
      "train loss:0.5963206199059605\n",
      "train loss:0.5000940403438361\n",
      "train loss:0.3893434755768888\n",
      "train loss:0.5041131030649005\n",
      "train loss:0.3844454160789855\n",
      "train loss:0.6499286368568173\n",
      "train loss:0.6223056508676175\n",
      "train loss:0.5151511151622409\n",
      "train loss:1.0253777289077033\n",
      "train loss:1.0096142297695339\n",
      "train loss:0.3879425553856829\n",
      "train loss:0.4129891540003608\n",
      "train loss:0.891710561510622\n",
      "train loss:0.6959848511764559\n",
      "train loss:0.6186956629402761\n",
      "train loss:0.6760495449743006\n",
      "train loss:0.6725214178866756\n",
      "train loss:0.6287485974226411\n",
      "train loss:0.6810225826691768\n",
      "train loss:0.6038405168111514\n",
      "train loss:0.5962934676942916\n",
      "train loss:0.6402214129540288\n",
      "train loss:0.5342536919376377\n",
      "train loss:0.677349248080981\n",
      "train loss:0.6238334611375528\n",
      "train loss:0.6285632500527598\n",
      "train loss:0.4857947678536241\n",
      "train loss:0.6119356269662066\n",
      "train loss:0.6974834314777685\n",
      "train loss:0.7826164046523415\n",
      "train loss:0.6828327765068368\n",
      "train loss:0.43607540718482907\n",
      "train loss:0.6008842651691451\n",
      "train loss:0.7716630199283946\n",
      "train loss:0.6405250654540595\n",
      "train loss:0.7089493595727862\n",
      "train loss:0.5082253515564107\n",
      "train loss:0.4162429047412063\n",
      "train loss:0.5226081845544553\n",
      "train loss:0.7097860957529945\n",
      "train loss:0.6239322860568995\n",
      "train loss:0.513556555437687\n",
      "train loss:0.7762118596230934\n",
      "train loss:0.5044561581269137\n",
      "train loss:0.722786565115605\n",
      "train loss:0.42285240722866096\n",
      "train loss:0.42489265491237777\n",
      "train loss:0.7059681669100784\n",
      "train loss:0.6025428973205635\n",
      "train loss:0.5160377792773682\n",
      "train loss:0.5790236007874279\n",
      "train loss:0.6229061067983308\n",
      "train loss:0.397461354088513\n",
      "train loss:0.5145787727417044\n",
      "train loss:0.5088634788046118\n",
      "train loss:0.3825163188407601\n",
      "train loss:0.5297935944333062\n",
      "train loss:0.7839228691122696\n",
      "train loss:0.2294364768820977\n",
      "train loss:0.7745636467431333\n",
      "train loss:0.6489811173587839\n",
      "train loss:0.6364215105644486\n",
      "train loss:0.4934474148097944\n",
      "train loss:0.8611350281759377\n",
      "train loss:0.27103383595654906\n",
      "train loss:0.6389393422387097\n",
      "train loss:0.5123058555616776\n",
      "train loss:0.3861272532593265\n",
      "train loss:0.5269675925139058\n",
      "train loss:0.5202219960149015\n",
      "train loss:0.824354830267702\n",
      "train loss:0.4926145519065619\n",
      "train loss:0.5908235608245065\n",
      "train loss:0.6200017010739074\n",
      "train loss:0.7151275966194205\n",
      "train loss:0.8880262962657863\n",
      "train loss:0.5452148863135233\n",
      "train loss:0.6283749028901402\n",
      "train loss:0.6153349533486907\n",
      "train loss:0.5556535811556639\n",
      "train loss:0.5500644079388021\n",
      "train loss:0.4692737467677198\n",
      "train loss:0.6083733267401898\n",
      "train loss:0.7451124961987811\n",
      "train loss:0.618364122301255\n",
      "train loss:0.4665405355497875\n",
      "train loss:0.4605379846090926\n",
      "train loss:0.7003719784963951\n",
      "train loss:0.6089144666292359\n",
      "train loss:0.6007677586061648\n",
      "train loss:0.6287849501151831\n",
      "train loss:0.6058586831922522\n",
      "train loss:0.6198690407890147\n",
      "train loss:0.8106961881285585\n",
      "train loss:0.6978004137311773\n",
      "train loss:0.523928820753007\n",
      "train loss:0.6931255695081252\n",
      "train loss:0.5391081964964964\n",
      "train loss:0.6705755910330178\n",
      "train loss:0.6077243208579051\n",
      "train loss:0.45427693129460334\n",
      "train loss:0.4470369514315286\n",
      "train loss:0.5313945321503646\n",
      "train loss:0.6111423746885479\n",
      "train loss:0.6008378028550185\n",
      "train loss:0.5013595886433772\n",
      "train loss:0.6187323204242758\n",
      "train loss:0.7208946278620156\n",
      "train loss:0.6111235290932985\n",
      "train loss:0.758653603791333\n",
      "train loss:0.5955108290855987\n",
      "train loss:0.8085977259923945\n",
      "train loss:0.425847771925923\n",
      "train loss:0.7831196195687096\n",
      "train loss:0.8144688595335603\n",
      "train loss:0.4012497647243739\n",
      "train loss:0.6077532042927278\n",
      "train loss:0.536916354725742\n",
      "train loss:0.6052617212007108\n",
      "train loss:0.534275217668796\n",
      "train loss:0.6914392133910617\n",
      "train loss:0.38860523769231525\n",
      "train loss:0.5175722738744831\n",
      "train loss:0.5139648896080995\n",
      "train loss:0.5270071629193366\n",
      "train loss:0.5061631614874434\n",
      "train loss:0.4874924385993726\n",
      "train loss:0.38483513630530275\n",
      "train loss:0.6379086884521672\n",
      "train loss:0.9679589574716785\n",
      "train loss:0.9079816318879088\n",
      "train loss:0.24426015172489116\n",
      "train loss:0.6297934841545795\n",
      "train loss:0.4906833009338486\n",
      "train loss:0.49682777331280176\n",
      "train loss:0.5265574351158151\n",
      "train loss:0.7218412350725831\n",
      "train loss:0.6290288248172535\n",
      "train loss:0.39129564038704234\n",
      "train loss:0.3760479268486198\n",
      "train loss:0.4998419254834524\n",
      "train loss:0.5962398135337141\n",
      "train loss:0.615325913160399\n",
      "train loss:0.8265376022875864\n",
      "train loss:1.0327192471409823\n",
      "train loss:0.6139190485011373\n",
      "train loss:0.6973780309402667\n",
      "train loss:0.5422256709552304\n",
      "train loss:0.5490193596529033\n",
      "train loss:0.549493061532635\n",
      "train loss:0.4931506836796672\n",
      "train loss:0.8711578055832963\n",
      "train loss:0.49610819183485705\n",
      "train loss:0.5496779726784059\n",
      "train loss:0.7419905679366584\n",
      "train loss:0.42949814227815775\n",
      "train loss:0.47069770388229576\n",
      "train loss:0.5591254938174577\n",
      "train loss:0.44632625201112475\n",
      "train loss:0.7271069200569124\n",
      "train loss:0.3008462641412151\n",
      "train loss:0.5305003476067985\n",
      "train loss:0.644052427187604\n",
      "train loss:0.9685347371658608\n",
      "train loss:0.3769842979748055\n",
      "train loss:0.774019622943595\n",
      "train loss:0.6111644638652295\n",
      "train loss:0.609890160099393\n",
      "train loss:0.61618023075345\n",
      "train loss:0.7498080986406876\n",
      "train loss:0.8051198090376823\n",
      "train loss:0.8794635441163056\n",
      "train loss:0.5343055196928315\n",
      "train loss:0.7515450918853117\n",
      "train loss:0.6022494777903462\n",
      "train loss:0.6095719296829545\n",
      "train loss:0.568639082039121\n",
      "train loss:0.5400024795759162\n",
      "train loss:0.5877697667843356\n",
      "train loss:0.6304213962684586\n",
      "train loss:0.5691947037258748\n",
      "train loss:0.6159162069014512\n",
      "train loss:0.6121053130371197\n",
      "train loss:0.6144431148722798\n",
      "train loss:0.604073735634568\n",
      "train loss:0.6185745273778166\n",
      "train loss:0.536856975296281\n",
      "train loss:0.6928831511197233\n",
      "train loss:0.6964072584943757\n",
      "train loss:0.6874154159396266\n",
      "train loss:0.7034755286638289\n",
      "train loss:0.6020051122062664\n",
      "train loss:0.6144877931901552\n",
      "train loss:0.7013301077710238\n",
      "train loss:0.6695035700747802\n",
      "train loss:0.6057849534067099\n",
      "train loss:0.6053217773343171\n",
      "train loss:0.3653151626972564\n",
      "train loss:0.5366508379179146\n",
      "train loss:0.43578429866562357\n",
      "train loss:0.7197883106714603\n",
      "train loss:0.6032291483751426\n",
      "train loss:0.41268872489228253\n",
      "train loss:0.41984386968739296\n",
      "train loss:0.8218274177409018\n",
      "train loss:0.6397903976754635\n",
      "train loss:0.4928599265077607\n",
      "train loss:0.7420539812601012\n",
      "train loss:0.8476394170139734\n",
      "train loss:0.8027344906230856\n",
      "train loss:0.5236450857110093\n",
      "train loss:0.5079937614938131\n",
      "train loss:0.5361378229826654\n",
      "train loss:0.6933895912000856\n",
      "train loss:0.534973943518258\n",
      "train loss:0.5286479223210137\n",
      "train loss:0.5322004036237006\n",
      "train loss:0.6939609725397913\n",
      "train loss:0.7009320575346767\n",
      "train loss:0.45847604317462914\n",
      "train loss:0.440519321933816\n",
      "train loss:0.6944128166107214\n",
      "train loss:0.6946278375317878\n",
      "train loss:0.4348903671518961\n",
      "train loss:0.5996541207001931\n",
      "train loss:0.5040336371007753\n",
      "train loss:0.5124197015202693\n",
      "train loss:0.40883734207225997\n",
      "train loss:0.719958973073109\n",
      "train loss:0.6228767074235773\n",
      "train loss:0.6367823924451865\n",
      "train loss:0.621530105245774\n",
      "train loss:0.6131745399812534\n",
      "train loss:0.7431289177324243\n",
      "train loss:0.49052534091848\n",
      "train loss:0.633838136142999\n",
      "train loss:0.5871173335799157\n",
      "train loss:0.30296527339321866\n",
      "train loss:0.7077184859104564\n",
      "train loss:0.6172000342819645\n",
      "train loss:0.31230126430095995\n",
      "train loss:0.8501058806507981\n",
      "train loss:0.9052707299452318\n",
      "train loss:0.6995805085826934\n",
      "train loss:0.5218568631658907\n",
      "train loss:0.4462812053099573\n",
      "train loss:0.6199888831710841\n",
      "train loss:0.6153711485639992\n",
      "train loss:0.6917689094712287\n",
      "train loss:0.46440944626604364\n",
      "train loss:0.607823320459161\n",
      "train loss:0.4684014949632475\n",
      "train loss:0.5183791693285478\n",
      "train loss:0.5219946811149038\n",
      "train loss:0.611025156159367\n",
      "train loss:0.5240889216792922\n",
      "train loss:0.5117024812337709\n",
      "train loss:0.7950116797865896\n",
      "train loss:0.7041435042198076\n",
      "train loss:0.5111822434395086\n",
      "train loss:0.3934150961172795\n",
      "train loss:0.7426017286219733\n",
      "train loss:0.7290749091002345\n",
      "train loss:0.2911512100362132\n",
      "train loss:0.6147409028854744\n",
      "train loss:0.4955019700097106\n",
      "train loss:0.8453187950168697\n",
      "train loss:0.7252699279172308\n",
      "train loss:0.30019513847606777\n",
      "train loss:0.7269687897430483\n",
      "train loss:0.4174698966351369\n",
      "train loss:0.6478162778336904\n",
      "train loss:0.42558193286876617\n",
      "train loss:0.7215506970757853\n",
      "train loss:0.6246494381148178\n",
      "train loss:0.49511708103196084\n",
      "train loss:0.6046528683135628\n",
      "train loss:0.5119858777777069\n",
      "train loss:0.5208716616297983\n",
      "train loss:0.7224025850630089\n",
      "train loss:0.2989522268578866\n",
      "train loss:0.6151883773301434\n",
      "train loss:0.7340212613047157\n",
      "train loss:0.5111630123318409\n",
      "train loss:0.6974852957887114\n",
      "train loss:0.4889730175918447\n",
      "train loss:0.5324945474027444\n",
      "train loss:0.6234045981308871\n",
      "train loss:0.5988800421955485\n",
      "train loss:0.7203970945557707\n",
      "train loss:0.8161131416786824\n",
      "train loss:0.5055847947200536\n",
      "train loss:0.7082602919674748\n",
      "train loss:0.6122349431105727\n",
      "train loss:0.45335130116312594\n",
      "train loss:0.3685017664602965\n",
      "train loss:0.5464005153369659\n",
      "train loss:0.6976290408744099\n",
      "train loss:0.6020378480572609\n",
      "train loss:0.5256560036917509\n",
      "train loss:0.7032635224991111\n",
      "train loss:0.8623127594436315\n",
      "train loss:0.6189177551003493\n",
      "train loss:0.702410251082741\n",
      "train loss:0.45161873209790926\n",
      "train loss:0.6163892956380136\n",
      "train loss:0.6249654466710083\n",
      "train loss:0.6079882492593659\n",
      "train loss:0.6101741999529766\n",
      "train loss:0.542322546345819\n",
      "train loss:0.6884426474770469\n",
      "train loss:0.45516097886269496\n",
      "train loss:0.9257681249525873\n",
      "train loss:0.6130615535566386\n",
      "train loss:0.6059272190166038\n",
      "train loss:0.6114492122085258\n",
      "train loss:0.6018211006829505\n",
      "train loss:0.7416762136124838\n",
      "train loss:0.6091610354972137\n",
      "train loss:0.5523614840711207\n",
      "train loss:0.6797045722041368\n",
      "train loss:0.6813005678646864\n",
      "train loss:0.5569215219146845\n",
      "train loss:0.6685988471768377\n",
      "train loss:0.5453997892457758\n",
      "train loss:0.5414955856193739\n",
      "train loss:0.6228285593657837\n",
      "train loss:0.5319398012264289\n",
      "train loss:0.6044846664574648\n",
      "train loss:0.6141332055245579\n",
      "train loss:0.524489740392398\n",
      "train loss:0.5163974544434093\n",
      "train loss:0.4111514601624022\n",
      "train loss:0.7134433736624367\n",
      "train loss:0.393753757027061\n",
      "train loss:0.6176673006184017\n",
      "train loss:0.865858084647672\n",
      "train loss:0.7504358983082878\n",
      "train loss:0.708543994150694\n",
      "train loss:0.7106520454256675\n",
      "train loss:0.5206906356053251\n",
      "train loss:0.42484813892300266\n",
      "train loss:0.5148887091707105\n",
      "train loss:0.61430645572273\n",
      "train loss:0.5445340521022358\n",
      "train loss:0.6926845167393568\n",
      "train loss:0.33376394692030154\n",
      "train loss:0.7038243961566296\n",
      "train loss:0.5948648535321986\n",
      "train loss:0.6027645664282576\n",
      "train loss:0.7236536290634252\n",
      "train loss:0.5169522311687448\n",
      "train loss:0.6341295014077661\n",
      "train loss:0.6048817127036045\n",
      "train loss:0.5153391692768643\n",
      "train loss:0.6173869861554933\n",
      "train loss:0.8680728898844743\n",
      "train loss:0.5249968525150854\n",
      "train loss:0.4300206505342035\n",
      "train loss:0.7195657608169558\n",
      "train loss:0.6959396474358256\n",
      "train loss:0.6265658414237067\n",
      "train loss:0.5205088325722274\n",
      "train loss:0.677238392397039\n",
      "train loss:0.6179794027068828\n",
      "train loss:0.6850013016708272\n",
      "train loss:0.6861753430756758\n",
      "train loss:0.751400925272993\n",
      "train loss:0.681613321398146\n",
      "train loss:0.6771685096642001\n",
      "train loss:0.49819972813478197\n",
      "train loss:0.49951757196203517\n",
      "train loss:0.6744438977324035\n",
      "train loss:0.6135394146586076\n",
      "train loss:0.7345618857929005\n",
      "train loss:0.6822194530642458\n",
      "train loss:0.6149314205946395\n",
      "train loss:0.6737178522959207\n",
      "train loss:0.6827271981102315\n",
      "train loss:0.5649425517117395\n",
      "train loss:0.5637118137552803\n",
      "train loss:0.6219809390603765\n",
      "train loss:0.6752263753150493\n",
      "train loss:0.6075437177712393\n",
      "train loss:0.8082779990157298\n",
      "train loss:0.6762905479203551\n",
      "train loss:0.6156671833613763\n",
      "train loss:0.5496497467603267\n",
      "train loss:0.4808571933917823\n",
      "train loss:0.606159750429027\n",
      "train loss:0.5300548967159772\n",
      "train loss:0.44870652664922545\n",
      "train loss:0.5138725334769683\n",
      "train loss:0.4037035648824176\n",
      "train loss:0.5027405658501148\n",
      "train loss:0.3868652125136296\n",
      "train loss:0.49759656552925924\n",
      "train loss:0.8169125927699188\n",
      "train loss:0.5236802268292378\n",
      "train loss:0.676742411019976\n",
      "train loss:0.6372011000936586\n",
      "train loss:0.3600106589281812\n",
      "train loss:0.48654552359787984\n",
      "train loss:0.32361552060976495\n",
      "train loss:0.6282505621208845\n",
      "train loss:0.8001298467591786\n",
      "train loss:0.6343664823133105\n",
      "train loss:0.5047269905041121\n",
      "train loss:0.3610623710750073\n",
      "train loss:0.49894580876745903\n",
      "train loss:0.5022574499658018\n",
      "train loss:0.8618333716348981\n",
      "train loss:0.5043100643197935\n",
      "train loss:0.6228126818671091\n",
      "train loss:0.29055774507971516\n",
      "train loss:0.5172968619124358\n",
      "train loss:0.7167571546557637\n",
      "train loss:0.6173082465072387\n",
      "train loss:0.7050402013363919\n",
      "train loss:0.5137384301143205\n",
      "train loss:0.5252585051619594\n",
      "train loss:0.43250588959860164\n",
      "train loss:0.6041257646935323\n",
      "train loss:0.8059573406129952\n",
      "train loss:0.3376456771958041\n",
      "train loss:0.6105638025191688\n",
      "train loss:0.6073520510899353\n",
      "train loss:0.7954637688794558\n",
      "train loss:0.7828197607698553\n",
      "train loss:0.7780351385269106\n",
      "train loss:0.5508277435938311\n",
      "train loss:0.6149639571337062\n",
      "train loss:0.6122662232888342\n",
      "train loss:0.6219759003601562\n",
      "train loss:0.8077261670030348\n",
      "train loss:0.743187576845771\n",
      "train loss:0.5638373722168258\n",
      "train loss:0.6731160883520826\n",
      "train loss:0.5247572783160233\n",
      "train loss:0.5293996602550088\n",
      "train loss:0.5637090018555477\n",
      "train loss:0.7248846814702774\n",
      "train loss:0.7338737261048446\n",
      "train loss:0.784178943111249\n",
      "train loss:0.5645657229391713\n",
      "train loss:0.5679074823496669\n",
      "train loss:0.5652252498514125\n",
      "train loss:0.6764482210069165\n",
      "train loss:0.42359992761703247\n",
      "train loss:0.6195653798300003\n",
      "train loss:0.6199840084153643\n",
      "train loss:0.6156300811300877\n",
      "train loss:0.601513891241668\n",
      "train loss:0.514283552742732\n",
      "train loss:0.6134589402323816\n",
      "train loss:0.7974449633052197\n",
      "train loss:0.5238313520238096\n",
      "train loss:0.8028415600340711\n",
      "train loss:0.6218775052816109\n",
      "train loss:0.606851984628873\n",
      "train loss:0.3171314645891904\n",
      "train loss:0.4971228787983164\n",
      "train loss:0.7062799220644632\n",
      "train loss:0.5155691419237207\n",
      "train loss:0.5038482780846814\n",
      "train loss:0.5004759071676359\n",
      "train loss:0.5062496030974727\n",
      "train loss:0.6177334347415319\n",
      "train loss:0.7406259813608285\n",
      "train loss:0.594239433436231\n",
      "train loss:0.9532039516455774\n",
      "train loss:0.2879306221976539\n",
      "train loss:0.517810196308153\n",
      "train loss:0.8103462912782486\n",
      "train loss:0.6172665686687924\n",
      "train loss:0.7859039275549342\n",
      "train loss:0.6133054178369352\n",
      "train loss:0.8409635595127212\n",
      "train loss:0.5391501948531126\n",
      "train loss:0.7360262797088681\n",
      "train loss:0.5625325850907827\n",
      "train loss:0.7283495710827663\n",
      "train loss:0.5234931870507015\n",
      "train loss:0.5761956948157454\n",
      "train loss:0.6245671990717157\n",
      "train loss:0.5775075436883872\n",
      "train loss:0.7175637809994287\n",
      "train loss:0.6763020019912835\n",
      "train loss:0.6801293454753579\n",
      "train loss:0.465819755805012\n",
      "train loss:0.6837694856948532\n",
      "train loss:0.5002506698236511\n",
      "train loss:0.6736792831243327\n",
      "train loss:0.48664882199883214\n",
      "train loss:0.6857049899664783\n",
      "train loss:0.7678249380610769\n",
      "train loss:0.6122303955163623\n",
      "train loss:0.45392026526943435\n",
      "train loss:0.610041767478809\n",
      "train loss:0.4206943703047238\n",
      "train loss:0.4096465295783124\n",
      "train loss:0.72149621374065\n",
      "train loss:0.8415582774750889\n",
      "train loss:0.6196319652492707\n",
      "train loss:0.7280952656865607\n",
      "train loss:0.8150361402571334\n",
      "train loss:0.7844404841536294\n",
      "train loss:0.5262840600153993\n",
      "train loss:0.5288423962302908\n",
      "train loss:0.9264280362153476\n",
      "train loss:0.5353990578086334\n",
      "train loss:0.6274876249157688\n",
      "train loss:0.6819264787957882\n",
      "train loss:0.5620022333795037\n",
      "train loss:0.6154464176858772\n",
      "train loss:0.5611050872011669\n",
      "train loss:0.6811929270900439\n",
      "train loss:0.6201017354443381\n",
      "train loss:0.5695037776724965\n",
      "train loss:0.5514209913941898\n",
      "train loss:0.6826669834840909\n",
      "train loss:0.48826794033766274\n",
      "train loss:0.484317117363452\n",
      "train loss:0.534023538588285\n",
      "train loss:0.4465892862743601\n",
      "train loss:0.617566732225062\n",
      "train loss:0.8006400411013539\n",
      "train loss:0.6326781923100329\n",
      "train loss:0.7108964316434012\n",
      "train loss:0.4170452193693917\n",
      "train loss:0.7127166971484604\n",
      "train loss:0.5144336742412655\n",
      "train loss:0.5141574509446237\n",
      "train loss:0.6203142736260732\n",
      "train loss:0.7161457535500602\n",
      "train loss:0.5125787781603937\n",
      "train loss:0.2789516370922219\n",
      "train loss:0.4851179236728676\n",
      "train loss:0.7292037913627863\n",
      "train loss:0.5144390909413694\n",
      "train loss:0.8628818284427338\n",
      "train loss:0.8318402089038596\n",
      "train loss:0.6211650572386442\n",
      "train loss:0.8122409935612682\n",
      "train loss:0.6880286084982805\n",
      "train loss:0.4558530142521905\n",
      "train loss:0.7613315097680171\n",
      "train loss:0.6736460485448602\n",
      "train loss:0.48499639745285733\n",
      "train loss:0.5501460191485675\n",
      "train loss:0.6182967162826793\n",
      "train loss:0.9001554454537644\n",
      "train loss:0.7768868135961815\n",
      "train loss:0.721901180379972\n",
      "train loss:0.5878887223898208\n",
      "train loss:0.6733256057437386\n",
      "train loss:0.5920910197546422\n",
      "train loss:0.5943753584672579\n",
      "train loss:0.6360240732309104\n",
      "train loss:0.5915175260557755\n",
      "train loss:0.6296716891765706\n",
      "train loss:0.6702394206841372\n",
      "train loss:0.5363674070324291\n",
      "train loss:0.5747074522055831\n",
      "train loss:0.6184859569536225\n",
      "train loss:0.618613121934834\n",
      "train loss:0.6851903493388843\n",
      "train loss:0.8104271262384664\n",
      "train loss:0.6769459192359991\n",
      "train loss:0.6804655056130408\n",
      "train loss:0.6121504072661439\n",
      "train loss:0.48149092227495904\n",
      "train loss:0.45984105103981987\n",
      "train loss:0.5484099026594361\n",
      "train loss:0.4350648482751501\n",
      "train loss:0.7981276365803832\n",
      "train loss:0.8931609863690604\n",
      "train loss:0.7930630357209631\n",
      "train loss:0.6067400910404102\n",
      "train loss:0.7045170508911165\n",
      "train loss:0.6123851841877332\n",
      "train loss:0.6083062067827021\n",
      "train loss:0.46676254478466667\n",
      "train loss:0.4518157288059326\n",
      "train loss:0.6916812233867463\n",
      "train loss:0.7685299497630302\n",
      "train loss:0.771116636042079\n",
      "train loss:0.610755074551625\n",
      "train loss:0.5380263628370675\n",
      "train loss:0.6147446501746696\n",
      "train loss:0.6131362340397872\n",
      "train loss:0.46582251870806024\n",
      "train loss:0.6163055220717802\n",
      "train loss:0.5404672585342479\n",
      "train loss:0.6776947741935102\n",
      "train loss:0.5305639204663414\n",
      "train loss:0.6113519554824626\n",
      "train loss:0.5975529833987352\n",
      "train loss:0.6123169449236726\n",
      "train loss:0.782435550609615\n",
      "train loss:0.7010555052821781\n",
      "train loss:0.43711379093272906\n",
      "train loss:0.6920553081695002\n",
      "train loss:0.3412120505687522\n",
      "train loss:0.5175586218369146\n",
      "train loss:0.6141767797454459\n",
      "train loss:0.811942461471931\n",
      "train loss:0.7175830386775918\n",
      "train loss:0.7110654797367137\n",
      "train loss:0.696023636359066\n",
      "train loss:0.4335852470804733\n",
      "train loss:0.6133871153622559\n",
      "train loss:0.6191117681333383\n",
      "train loss:0.5203270086325145\n",
      "train loss:0.6871012781767342\n",
      "train loss:0.7692397867913675\n",
      "train loss:0.6108181662810146\n",
      "train loss:0.60997045420371\n",
      "train loss:0.5389901365233271\n",
      "train loss:0.6134524713280516\n",
      "train loss:0.6852555489636897\n",
      "train loss:0.4659190833092948\n",
      "train loss:0.6133332786861608\n",
      "train loss:0.5371580234084784\n",
      "train loss:0.6097550731538175\n",
      "train loss:0.527802345087891\n",
      "train loss:0.6120286822043123\n",
      "train loss:0.6061989268767503\n",
      "train loss:0.5204686786894122\n",
      "train loss:0.6169700041693422\n",
      "train loss:0.6091870687910547\n",
      "train loss:0.9080619033211796\n",
      "train loss:0.5157137677037722\n",
      "train loss:0.6903147884360863\n",
      "train loss:0.7084052679178903\n",
      "train loss:0.5130884577900536\n",
      "train loss:0.7022682543862583\n",
      "train loss:0.6137341082489759\n",
      "train loss:0.6082910426153021\n",
      "train loss:0.4540787112742901\n",
      "train loss:0.44145482897329247\n",
      "train loss:0.6941772047677948\n",
      "train loss:0.8618136471797676\n",
      "train loss:0.44790625769677145\n",
      "train loss:0.6117552088343252\n",
      "train loss:0.5276876079162867\n",
      "train loss:0.6919511944199933\n",
      "train loss:0.6119722107921276\n",
      "train loss:0.6880534290671089\n",
      "train loss:0.7723840630846847\n",
      "train loss:0.543070541966515\n",
      "train loss:0.6114353411544947\n",
      "train loss:0.6058074760245286\n",
      "train loss:0.5418552233566631\n",
      "train loss:0.5388503541613796\n",
      "train loss:0.45049130827970796\n",
      "train loss:0.4340317804853676\n",
      "train loss:0.41972665131605824\n",
      "train loss:0.5062490350506105\n",
      "train loss:0.39981882338433816\n",
      "train loss:0.37815816065834695\n",
      "train loss:0.633058357023965\n",
      "train loss:0.3559445324278766\n",
      "train loss:0.494395215795101\n",
      "train loss:0.3400789415085453\n",
      "train loss:0.5166035565111093\n",
      "train loss:0.6952894042660205\n",
      "train loss:0.681294871075877\n",
      "train loss:0.855694270641061\n",
      "train loss:0.4812286723966003\n",
      "train loss:0.9879800783942703\n",
      "train loss:0.7514783195464623\n",
      "train loss:0.7366059276159607\n",
      "train loss:0.8098280240922927\n",
      "train loss:0.44583944716955515\n",
      "train loss:0.4627478146031282\n",
      "train loss:0.6812604986779371\n",
      "train loss:0.6208405573897229\n",
      "train loss:0.565174278901555\n",
      "train loss:0.519100749821275\n",
      "train loss:0.6796475463890572\n",
      "train loss:0.627071646746778\n",
      "train loss:0.5304560971260626\n",
      "train loss:0.5783345150418551\n",
      "train loss:0.5197466283217804\n",
      "train loss:0.7834437254016539\n",
      "train loss:0.6189578029760078\n",
      "train loss:0.6197726043950909\n",
      "train loss:0.6147774540503063\n",
      "train loss:0.5566852889162265\n",
      "train loss:0.5502681907567845\n",
      "train loss:0.5485572756923828\n",
      "train loss:0.7537906581150008\n",
      "train loss:0.4553373541037816\n",
      "train loss:0.5302639639873195\n",
      "train loss:0.6968664685249264\n",
      "train loss:0.5102844073833435\n",
      "train loss:0.7109998759916175\n",
      "train loss:0.5227794567253088\n",
      "train loss:0.613344380735721\n",
      "train loss:0.5119349726540734\n",
      "train loss:0.39056043568922766\n",
      "train loss:0.7239073158338828\n",
      "train loss:0.7236159931418377\n",
      "train loss:0.49425691262679805\n",
      "train loss:1.0563971301037538\n",
      "train loss:0.39483784426975005\n",
      "train loss:0.3949938238307389\n",
      "train loss:0.5038830160497677\n",
      "train loss:0.6200217704115953\n",
      "train loss:0.6993623271225599\n",
      "train loss:0.6273378850956244\n",
      "train loss:0.6320689813856193\n",
      "train loss:0.5182848045663742\n",
      "train loss:0.6231070563379593\n",
      "train loss:0.5159900106483468\n",
      "train loss:0.5155033959393557\n",
      "train loss:0.4092327965778222\n",
      "train loss:0.6167719346179565\n",
      "train loss:0.6244257431224043\n",
      "train loss:0.6095895572683647\n",
      "train loss:0.5217726774728838\n",
      "train loss:0.7169346409240756\n",
      "train loss:0.6959793570365045\n",
      "train loss:0.7973639061949348\n",
      "train loss:0.33503478937803827\n",
      "train loss:0.610066425322633\n",
      "train loss:0.6081005740027263\n",
      "train loss:0.4225024114899695\n",
      "train loss:0.7252510710934598\n",
      "train loss:0.3296946941896182\n",
      "train loss:0.41210313028718043\n",
      "train loss:0.5033428275522672\n",
      "train loss:0.6144893790778956\n",
      "train loss:0.5114887387193742\n",
      "train loss:0.7463597509976388\n",
      "train loss:0.8478174216026643\n",
      "train loss:1.0340919859275857\n",
      "train loss:0.6024409863303204\n",
      "train loss:0.5222237741449249\n",
      "train loss:0.6053219397803191\n",
      "train loss:0.6111297589997833\n",
      "train loss:0.532878707135263\n",
      "train loss:0.4597567069602345\n",
      "train loss:0.7603888202455099\n",
      "train loss:0.5412558497359787\n",
      "train loss:0.611389539111211\n",
      "train loss:0.6153799463737277\n",
      "train loss:0.5430476519102958\n",
      "train loss:0.6869663746137693\n",
      "train loss:0.6128940781374699\n",
      "train loss:0.6837965962649692\n",
      "train loss:0.6115085764784658\n",
      "train loss:0.7555100465870036\n",
      "train loss:0.5444501782673993\n",
      "train loss:0.5433007592174961\n",
      "train loss:0.6807157831408517\n",
      "train loss:0.6793922079020795\n",
      "train loss:0.6133071395589\n",
      "train loss:0.687000234479541\n",
      "train loss:0.6771046573950165\n",
      "train loss:0.6145800319461387\n",
      "train loss:0.4794217465289453\n",
      "train loss:0.7503234667492384\n",
      "train loss:0.6138237637984474\n",
      "train loss:0.5409615488652934\n",
      "train loss:0.4637726905408354\n",
      "train loss:0.4565051058481854\n",
      "train loss:0.605945732418276\n",
      "train loss:0.33871408984384094\n",
      "train loss:0.5115493502464384\n",
      "train loss:0.6194601668915644\n",
      "train loss:0.731600929493904\n",
      "train loss:0.3964800514467316\n",
      "train loss:0.6261419209390644\n",
      "train loss:0.7634054735578106\n",
      "train loss:0.507786029992938\n",
      "train loss:0.6263565158611646\n",
      "train loss:0.7423560833335101\n",
      "train loss:0.4946999972635836\n",
      "train loss:0.8532384097865764\n",
      "train loss:0.7176037986282409\n",
      "train loss:0.6196382735116479\n",
      "train loss:0.5192292869490404\n",
      "train loss:0.6923875144356002\n",
      "train loss:0.531058435942351\n",
      "train loss:0.5280102421479292\n",
      "train loss:0.5287499573691343\n",
      "train loss:0.4439222381629163\n",
      "train loss:0.6123953735694931\n",
      "train loss:0.6916761111587595\n",
      "train loss:0.5112448293320278\n",
      "train loss:0.5114696233072029\n",
      "train loss:0.4362264135385966\n",
      "train loss:0.7030858566629694\n",
      "train loss:0.6134667110534202\n",
      "train loss:0.8968492101756578\n",
      "train loss:0.6089445467910831\n",
      "train loss:0.42722077654294505\n",
      "train loss:0.5976049954546468\n",
      "train loss:0.6217723553334261\n",
      "train loss:0.6128561158086446\n",
      "train loss:0.522965300413626\n",
      "train loss:0.6144648173166424\n",
      "train loss:0.783451394471689\n",
      "train loss:0.525897365807074\n",
      "train loss:0.6097756934446741\n",
      "train loss:0.6061361268217615\n",
      "train loss:0.5219492822642242\n",
      "train loss:0.6100862721274485\n",
      "train loss:0.517034765649324\n",
      "train loss:0.42498334185842135\n",
      "train loss:0.6212594640700358\n",
      "train loss:0.6158476798621351\n",
      "train loss:0.6124698421314315\n",
      "train loss:0.7231036050117666\n",
      "train loss:0.5163459689978848\n",
      "train loss:0.7033892028730524\n",
      "train loss:0.6305584431093622\n",
      "train loss:0.5178117498605737\n",
      "train loss:0.5184343689917338\n",
      "train loss:0.521573010789442\n",
      "train loss:0.3141524328791063\n",
      "train loss:0.4003694550008359\n",
      "train loss:0.7377273067698878\n",
      "train loss:0.7352064102302587\n",
      "train loss:0.6219529499429798\n",
      "train loss:0.834182990598233\n",
      "train loss:0.7225690925951399\n",
      "train loss:0.8916153970742705\n",
      "train loss:0.607371382555584\n",
      "train loss:0.530104669733721\n",
      "train loss:0.6105535742263459\n",
      "train loss:0.6097945513761938\n",
      "train loss:0.6137147942553007\n",
      "train loss:0.545465150329432\n",
      "train loss:0.6824755768356737\n",
      "train loss:0.551013781449644\n",
      "train loss:0.6190271640026126\n",
      "train loss:0.5523457217908245\n",
      "train loss:0.8066999941162672\n",
      "train loss:0.6175779382164551\n",
      "train loss:0.6788783012354498\n",
      "train loss:0.48963682282898413\n",
      "train loss:0.4850984180871748\n",
      "train loss:0.48004044163567006\n",
      "train loss:0.5442292170857981\n",
      "train loss:0.6817986611852884\n",
      "train loss:0.5346911472337313\n",
      "train loss:0.6083770030094532\n",
      "train loss:0.6954904790052872\n",
      "train loss:0.8643781500592457\n",
      "train loss:0.7810027143672668\n",
      "train loss:0.5260651027281443\n",
      "train loss:0.5259504960182111\n",
      "train loss:0.7770141312399979\n",
      "train loss:0.4448548405046978\n",
      "train loss:0.441322529062507\n",
      "train loss:0.6027789851846663\n",
      "train loss:0.6929655781753172\n",
      "train loss:0.6132093043914435\n",
      "train loss:0.42699617526016054\n",
      "train loss:0.701989285561851\n",
      "train loss:0.3222955234466347\n",
      "train loss:0.30446967842205025\n",
      "train loss:0.6196105046401096\n",
      "train loss:0.9528672675663901\n",
      "train loss:0.6287710018481982\n",
      "train loss:0.5177426687250419\n",
      "train loss:0.724423603136916\n",
      "train loss:0.6038346112025966\n",
      "train loss:0.5055036804340417\n",
      "train loss:0.40497222919970693\n",
      "train loss:0.5095347925651854\n",
      "train loss:0.5098540715134754\n",
      "train loss:0.9356018311342865\n",
      "train loss:0.7104053270280402\n",
      "train loss:0.6120360959572213\n",
      "train loss:0.6128159010543649\n",
      "train loss:0.6179808664432954\n",
      "train loss:0.5224363387101435\n",
      "train loss:0.6128462535539462\n",
      "train loss:0.4330276711427107\n",
      "train loss:0.8581601678597103\n",
      "train loss:0.6877440297934413\n",
      "train loss:0.7568541093649991\n",
      "train loss:0.6094595999672374\n",
      "train loss:0.8818408456088201\n",
      "train loss:0.7375377843458424\n",
      "train loss:0.7322491829092945\n",
      "train loss:0.627396206837391\n",
      "train loss:0.674635517454264\n",
      "train loss:0.5371826468126978\n",
      "train loss:0.6295280471815223\n",
      "train loss:0.7154390224304413\n",
      "train loss:0.7158358553704949\n",
      "train loss:0.6357171011529782\n",
      "train loss:0.6368194607041635\n",
      "train loss:0.5962059193997674\n",
      "train loss:0.6765504087351589\n",
      "train loss:0.5949983282239659\n",
      "train loss:0.63351703670698\n",
      "train loss:0.5850019727824806\n",
      "train loss:0.6731169617460863\n",
      "train loss:0.5313519179982669\n",
      "train loss:0.5730820400839195\n",
      "train loss:0.6715870249968543\n",
      "train loss:0.6149465011407178\n",
      "train loss:0.4918157806178239\n",
      "train loss:0.6141236024926302\n",
      "train loss:0.6161298864465845\n",
      "train loss:0.6157512457028804\n",
      "train loss:0.6896185256395463\n",
      "train loss:0.6093467856915229\n",
      "train loss:0.5234177668192311\n",
      "train loss:0.6996210626055742\n",
      "train loss:0.42206954110882017\n",
      "train loss:0.5073312417868244\n",
      "train loss:0.6136800050426722\n",
      "train loss:0.6126023102053282\n",
      "train loss:0.8373516996805493\n",
      "train loss:0.5032531626324632\n",
      "train loss:0.6123928984991531\n",
      "train loss:0.49614891023024754\n",
      "train loss:0.3904516694359876\n",
      "train loss:0.6290116004809764\n",
      "train loss:0.5199565324526476\n",
      "train loss:0.7317954054244616\n",
      "train loss:0.6281582644844403\n",
      "train loss:0.5151201097504501\n",
      "train loss:0.6216849815272821\n",
      "train loss:0.5084490526475468\n",
      "train loss:0.5077680931899475\n",
      "train loss:0.51417345029794\n",
      "train loss:0.6164154210147204\n",
      "train loss:0.5090224847034559\n",
      "train loss:0.5075135475215609\n",
      "train loss:0.38990371906746385\n",
      "train loss:0.37856766746276327\n",
      "train loss:0.5006850753573348\n",
      "train loss:0.3733683362120346\n",
      "train loss:0.776744515239225\n",
      "train loss:0.8882133618124772\n",
      "train loss:0.6356680191809501\n",
      "train loss:0.3731710299318988\n",
      "train loss:0.5075920421483122\n",
      "train loss:0.37481539600356983\n",
      "train loss:0.7354318947773447\n",
      "train loss:0.5048457558440425\n",
      "train loss:0.6157547433478658\n",
      "train loss:0.8359077224101048\n",
      "train loss:0.6153573948758837\n",
      "train loss:0.5135349695895801\n",
      "train loss:0.7182449486039701\n",
      "train loss:0.7939033847595298\n",
      "train loss:0.6170215665646575\n",
      "train loss:0.532575996315756\n",
      "train loss:0.7607055221890078\n",
      "train loss:0.5415734731787942\n",
      "train loss:0.5491096968873533\n",
      "train loss:0.6119055703548588\n",
      "train loss:0.548334271807229\n",
      "train loss:0.6197877992626907\n",
      "train loss:0.547907979171595\n",
      "train loss:0.6169212585444874\n",
      "train loss:0.7463241624106285\n",
      "train loss:0.6770754175617364\n",
      "train loss:0.6171326998746033\n",
      "train loss:0.5509858357086881\n",
      "train loss:0.47878814735020664\n",
      "train loss:0.5453198137754146\n",
      "train loss:0.6128589727789482\n",
      "train loss:0.5290991171348571\n",
      "train loss:0.5267807332301758\n",
      "train loss:0.6074843026468916\n",
      "train loss:0.5235610849701231\n",
      "train loss:0.7946081811712213\n",
      "train loss:0.3129890419859149\n",
      "train loss:0.5133916442625883\n",
      "train loss:0.49840623056622135\n",
      "train loss:0.3853094700395019\n",
      "train loss:0.6184586978927916\n",
      "train loss:0.35928772943381937\n",
      "train loss:0.6321657892331792\n",
      "train loss:0.3589999547551076\n",
      "train loss:0.7945098146199187\n",
      "train loss:0.18565439308599482\n",
      "train loss:0.6596703462099995\n",
      "train loss:0.8313428994484594\n",
      "train loss:0.7981406448083226\n",
      "train loss:0.33816780887246856\n",
      "train loss:0.5166652529152067\n",
      "train loss:0.5072919755421925\n",
      "train loss:0.634149917920235\n",
      "train loss:0.7465986458588725\n",
      "train loss:0.7460210907849695\n",
      "train loss:0.5047448866176968\n",
      "train loss:0.512124747034068\n",
      "train loss:0.5120940980961349\n",
      "train loss:0.3221569442345738\n",
      "train loss:0.5101509428240933\n",
      "train loss:0.5085260831630082\n",
      "train loss:0.7131156740771798\n",
      "train loss:0.5135026243691657\n",
      "train loss:0.6117057608614077\n",
      "train loss:0.7036117101190531\n",
      "train loss:0.7169335839623721\n",
      "train loss:0.4265654727795224\n",
      "train loss:0.6103031788691152\n",
      "train loss:0.6151361110248111\n",
      "train loss:0.5244331453439534\n",
      "train loss:0.7016752628531047\n",
      "train loss:0.6105712043090112\n",
      "train loss:0.6980206777067418\n",
      "train loss:0.6106338434111749\n",
      "train loss:0.6061701635172586\n",
      "train loss:0.44047735593541537\n",
      "train loss:0.609884873206375\n",
      "train loss:0.7791962516521465\n",
      "train loss:0.5329756569793991\n",
      "train loss:0.35799298042821226\n",
      "train loss:0.4365917944220552\n",
      "train loss:0.42631126753137655\n",
      "train loss:0.41104865488764597\n",
      "train loss:0.39656625784915844\n",
      "train loss:0.7435753796209669\n",
      "train loss:0.6235666384927766\n",
      "train loss:0.49841744006842303\n",
      "train loss:0.8685160307308548\n",
      "train loss:0.7519646923272181\n",
      "train loss:0.6291158165730287\n",
      "train loss:0.6262073455533206\n",
      "train loss:0.5072875846293192\n",
      "train loss:0.40080798979776944\n",
      "train loss:0.5041277629340333\n",
      "train loss:0.8309663170916854\n",
      "train loss:0.5111886734988474\n",
      "train loss:0.5033100858762772\n",
      "train loss:0.30037601227570276\n",
      "train loss:0.5084386351623172\n",
      "train loss:0.392177408193105\n",
      "train loss:0.6152605673426847\n",
      "train loss:0.38327195665997915\n",
      "train loss:0.37541849999811766\n",
      "train loss:0.4902640446957796\n",
      "train loss:0.6399703768444023\n",
      "train loss:0.6468730047691864\n",
      "train loss:0.7748520662480208\n",
      "train loss:0.7688162314004644\n",
      "train loss:0.6349243943280587\n",
      "train loss:0.3727853165665224\n",
      "train loss:0.6219131591381346\n",
      "train loss:0.5078428368541346\n",
      "train loss:0.39362504893440736\n",
      "train loss:0.5065380966179139\n",
      "train loss:0.5119458250222035\n",
      "train loss:0.6207649589119294\n",
      "train loss:0.5067252830554525\n",
      "train loss:0.383856643449264\n",
      "train loss:0.3897203139947048\n",
      "train loss:0.3782387303917588\n",
      "train loss:0.508728584947197\n",
      "train loss:0.36333745845240345\n",
      "train loss:0.6515310564924065\n",
      "train loss:0.8997200275951931\n",
      "train loss:0.5028886122007361\n",
      "train loss:0.7576347412895987\n",
      "train loss:0.7498194239888554\n",
      "train loss:0.49994253952537876\n",
      "train loss:0.728043183843826\n",
      "train loss:0.3994061532513144\n",
      "train loss:0.4017163589609982\n",
      "train loss:0.40459912529738185\n",
      "train loss:0.7220060366085621\n",
      "train loss:0.7155760890746939\n",
      "train loss:0.5038871156630764\n",
      "train loss:0.5097279453465473\n",
      "train loss:0.6046881277746321\n",
      "train loss:0.6088540226874659\n",
      "train loss:0.5197137609617383\n",
      "train loss:0.613027012816602\n",
      "train loss:0.6093265496078689\n",
      "train loss:0.5166745559927715\n",
      "train loss:0.3146890642701427\n",
      "train loss:0.41099474145485393\n",
      "train loss:0.7187925472651383\n",
      "train loss:0.6257214600934002\n",
      "train loss:0.280113480045441\n",
      "train loss:0.6125283549559268\n",
      "train loss:0.6166816053861399\n",
      "train loss:0.6191176550611289\n",
      "train loss:0.25314284321317426\n",
      "train loss:0.36933778667481915\n",
      "train loss:1.0276400827183032\n",
      "train loss:0.7529871152382054\n",
      "train loss:0.6335410380452247\n",
      "train loss:0.6162731551724833\n",
      "train loss:0.6101174866299329\n",
      "train loss:0.6152608525493831\n",
      "train loss:0.4072103232821819\n",
      "train loss:0.4056672307452992\n",
      "train loss:0.4040741447652062\n",
      "train loss:0.5110715857047197\n",
      "train loss:0.6090541055115664\n",
      "train loss:0.2831688773577596\n",
      "train loss:0.7275822706366094\n",
      "train loss:0.6183782871396828\n",
      "train loss:0.8427921412245363\n",
      "train loss:0.5077854317786178\n",
      "train loss:0.6170176148907823\n",
      "train loss:0.612314138760411\n",
      "train loss:0.4063721666100033\n",
      "train loss:0.7178881558424371\n",
      "train loss:0.9070473598596097\n",
      "train loss:0.7089387560126964\n",
      "train loss:0.7853378098940443\n",
      "train loss:0.5315076926107997\n",
      "train loss:0.6861791239694822\n",
      "train loss:0.613510611199771\n",
      "train loss:0.6094989183701467\n",
      "train loss:0.5481673494789937\n",
      "train loss:0.6787895934908907\n",
      "train loss:0.6780406272330988\n",
      "train loss:0.49768052225863935\n",
      "train loss:0.6756942248809722\n",
      "train loss:0.6167872357952351\n",
      "train loss:0.555999467283722\n",
      "train loss:0.5548508335129341\n",
      "train loss:0.6120716389867138\n",
      "train loss:0.6167224868669878\n",
      "train loss:0.5463277338198795\n",
      "train loss:0.609481067054542\n",
      "train loss:0.6085796730957991\n",
      "train loss:0.6112582757698599\n",
      "train loss:0.535600616666408\n",
      "train loss:0.5303676526864634\n",
      "train loss:0.5287699178331107\n",
      "train loss:0.5206455246744148\n",
      "train loss:0.5130108023467391\n",
      "train loss:0.8062836379446112\n",
      "train loss:0.40937500580439856\n",
      "train loss:0.6086016212838746\n",
      "train loss:0.2691555604886482\n",
      "train loss:0.8554874785991944\n",
      "train loss:0.8334571112545985\n",
      "train loss:0.37754365381870725\n",
      "train loss:0.5088719424317264\n",
      "train loss:0.7372817233689495\n",
      "train loss:0.8502515540201921\n",
      "train loss:0.5100056573170981\n",
      "train loss:0.9275938571042645\n",
      "train loss:0.8025451938677177\n",
      "train loss:0.42644206875032664\n",
      "train loss:0.6163388006955738\n",
      "train loss:0.6140282549044553\n",
      "train loss:0.6113335466838945\n",
      "train loss:0.46049989933673424\n",
      "train loss:0.6854410836155689\n",
      "train loss:0.39612182032457477\n",
      "train loss:0.6846425930280701\n",
      "train loss:0.5412424786453327\n",
      "train loss:0.5385414539599974\n",
      "train loss:0.4524981909834761\n",
      "train loss:0.6152043383161335\n",
      "train loss:0.4448076006355512\n",
      "train loss:0.608812384429008\n",
      "train loss:0.5163056680570883\n",
      "train loss:0.5240174789944552\n",
      "train loss:0.4071684755323092\n",
      "train loss:0.6111518536534088\n",
      "train loss:0.7272073745891173\n",
      "train loss:0.6072342078193504\n",
      "train loss:0.5094441793287865\n",
      "train loss:0.9680758581045735\n",
      "train loss:0.8379425456401833\n",
      "train loss:0.8087407107303953\n",
      "train loss:0.6074234623444812\n",
      "train loss:0.6081354341412897\n",
      "train loss:0.6921801119123294\n",
      "train loss:0.6850840208508817\n",
      "train loss:0.6174348239800151\n",
      "train loss:0.6112251966630633\n",
      "train loss:0.6125718736696716\n",
      "train loss:0.5453657600106191\n",
      "train loss:0.7413951298362589\n",
      "train loss:0.6104556884188328\n",
      "train loss:0.6196731862596618\n",
      "train loss:0.6765868981730144\n",
      "train loss:0.503416436399753\n",
      "train loss:0.6741368377434431\n",
      "train loss:0.5617059028140192\n",
      "train loss:0.6208188028745086\n",
      "train loss:0.6184109217611726\n",
      "train loss:0.6753715771127068\n",
      "train loss:0.42892827618117507\n",
      "train loss:0.5527346562767279\n",
      "train loss:0.477162044597515\n",
      "train loss:0.8235831473486099\n",
      "train loss:0.46682177137767333\n",
      "train loss:0.5271360385350482\n",
      "train loss:0.44077554849003964\n",
      "train loss:0.5146054484876387\n",
      "train loss:0.4195544308509575\n",
      "train loss:0.7118992366598819\n",
      "train loss:0.6129425318306272\n",
      "train loss:0.5079891972576307\n",
      "train loss:0.6331474316456831\n",
      "train loss:0.6166353811585867\n",
      "train loss:0.4981265009914478\n",
      "train loss:0.24336178263098712\n",
      "train loss:0.5031920213489147\n",
      "train loss:0.7594102216729494\n",
      "train loss:0.5026334348384391\n",
      "train loss:0.504872593116623\n",
      "train loss:0.21322051200229214\n",
      "train loss:0.49663907767533877\n",
      "train loss:1.0917206598623435\n",
      "train loss:0.9170729731541843\n",
      "train loss:0.37550790649505805\n",
      "train loss:0.5043213649253551\n",
      "train loss:0.6341910594340876\n",
      "train loss:0.8374696364449215\n",
      "train loss:0.3956579565642854\n",
      "train loss:0.6117922308001241\n",
      "train loss:0.7070640479781336\n",
      "train loss:0.5221632536576051\n",
      "train loss:0.6998377402061225\n",
      "train loss:0.34930445435920826\n",
      "train loss:0.5217104487385148\n",
      "train loss:0.6116714117899784\n",
      "train loss:0.4416813801783245\n",
      "train loss:0.5214248208867487\n",
      "train loss:0.6067308698467506\n",
      "train loss:0.5078039283844605\n",
      "train loss:0.7100099420040504\n",
      "train loss:0.4238736860455363\n",
      "train loss:0.42215749554241466\n",
      "train loss:0.49217540445647356\n",
      "train loss:0.726178172054803\n",
      "train loss:0.5981882520411099\n",
      "train loss:0.5103262622593864\n",
      "train loss:0.4952178600202595\n",
      "train loss:0.5055969396887356\n",
      "train loss:0.6202203419714628\n",
      "train loss:0.6207692670901237\n",
      "train loss:0.4976365136084027\n",
      "train loss:0.6285832609597327\n",
      "train loss:0.7357375879908459\n",
      "train loss:0.5020174772018757\n",
      "train loss:0.8506717498274756\n",
      "train loss:0.5090231972927934\n",
      "train loss:0.6078194088200302\n",
      "train loss:0.5100706169964642\n",
      "train loss:0.714115517375242\n",
      "train loss:0.6120278072934825\n",
      "train loss:0.5078534534467758\n",
      "train loss:0.6956170227041814\n",
      "train loss:0.612941464203703\n",
      "train loss:0.524607377922549\n",
      "train loss:0.7856034281004125\n",
      "train loss:0.5272454432340457\n",
      "train loss:0.5249048514262262\n",
      "train loss:0.8437806299613337\n",
      "train loss:0.6045231413784726\n",
      "train loss:0.46531632637953424\n",
      "train loss:0.6115299769250229\n",
      "train loss:0.6836045467079548\n",
      "train loss:0.45655032561326914\n",
      "train loss:0.7576408789314499\n",
      "train loss:0.4624132968457638\n",
      "train loss:0.6826762194275322\n",
      "train loss:0.7603008971126329\n",
      "train loss:0.46391811198918387\n",
      "train loss:0.6831469605516315\n",
      "train loss:0.6020586586680852\n",
      "train loss:0.7634027194601777\n",
      "train loss:0.6110618491354861\n",
      "train loss:0.6812295366742952\n",
      "train loss:0.692936506838428\n",
      "train loss:0.7436665172257464\n",
      "train loss:0.6806254858609546\n",
      "train loss:0.6128191893407215\n",
      "train loss:0.5561218123358884\n",
      "train loss:0.6163045053587095\n",
      "train loss:0.7388377747562933\n",
      "train loss:0.6776873038849341\n",
      "train loss:0.6707950792469477\n",
      "train loss:0.49757617521890196\n",
      "train loss:0.4915735873801415\n",
      "train loss:0.5525170422544118\n",
      "train loss:0.6770224847248872\n",
      "train loss:0.5420044366402774\n",
      "train loss:0.46896790487132894\n",
      "train loss:0.603790598122424\n",
      "train loss:0.69252066946461\n",
      "train loss:0.5289789941130578\n",
      "train loss:0.6115734000992079\n",
      "train loss:0.5185597764605171\n",
      "train loss:0.4245000650163889\n",
      "train loss:0.7127965014677278\n",
      "train loss:0.8913257299130108\n",
      "train loss:0.31566070414404623\n",
      "train loss:0.6176483995042695\n",
      "train loss:0.615524626913705\n",
      "train loss:0.616541143909468\n",
      "train loss:0.2913115434437594\n",
      "train loss:0.2688473694698526\n",
      "train loss:0.7446381313684448\n",
      "train loss:0.737089351672557\n",
      "train loss:0.6384416220751772\n",
      "train loss:0.6219966542007161\n",
      "train loss:0.73841742038351\n",
      "train loss:0.608430496543164\n",
      "train loss:0.8186002314495182\n",
      "train loss:0.7173067706575513\n",
      "train loss:0.711496509269816\n",
      "train loss:0.5274876518431336\n",
      "train loss:0.8588905268160028\n",
      "train loss:0.5293471737254979\n",
      "train loss:0.6110545674879225\n",
      "train loss:0.6856671974471349\n",
      "train loss:0.6820509781825896\n",
      "train loss:0.6742580828486513\n",
      "train loss:0.6786905030910731\n",
      "train loss:0.5510244919556426\n",
      "train loss:0.5656902079188498\n",
      "train loss:0.6746805130682129\n",
      "train loss:0.5178976862460556\n",
      "train loss:0.5059248058685287\n",
      "train loss:0.7248791227317006\n",
      "train loss:0.5646481353759402\n",
      "train loss:0.5534189675826966\n",
      "train loss:0.6751662587000077\n",
      "train loss:0.7365024461722606\n",
      "train loss:0.6762144110506605\n",
      "train loss:0.4989703720054024\n",
      "train loss:0.5543474738464808\n",
      "train loss:0.5514568103137159\n",
      "train loss:0.4709125511138515\n",
      "train loss:0.6143734131967992\n",
      "train loss:0.8409998826442407\n",
      "train loss:0.4588501945333566\n",
      "train loss:0.6894385664964764\n",
      "train loss:0.6074989075586011\n",
      "train loss:0.5281901239942988\n",
      "train loss:0.683643066919059\n",
      "train loss:0.5190466410874871\n",
      "train loss:0.7844306025075547\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6617647058823529\n"
     ]
    }
   ],
   "source": [
    "from common.trainer import Trainer\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(hue_images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "499ebdce-8cbb-45f7-b359-c70cadf04fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), #필터 수 조절해서 성능 조절\n",
    "                 conv_param_1 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=10):\n",
    "        # 가중치 초기화===========\n",
    "        # 각 층의 뉴런 하나당 앞 층의 몇 개 뉴런과 연결되는가（TODO: 자동 계산되게 바꿀 것）\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLU를 사용할 때의 권장 초깃값\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = wight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = wight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a137bbbe-f34b-4ad7-bba6-1d0830cc1cfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.236009654231396\n",
      "=== epoch:1, train acc:0.18872549019607843, test acc:0.2549019607843137 ===\n",
      "train loss:2.157605452988102\n",
      "train loss:2.105294772900327\n",
      "train loss:1.9779932031732241\n",
      "train loss:1.819595078525812\n",
      "train loss:1.7723165942125378\n",
      "train loss:1.7456789644232333\n",
      "train loss:1.7176090466960974\n",
      "train loss:1.7339974282477428\n",
      "train loss:1.763286660694815\n",
      "train loss:1.988294663312246\n",
      "train loss:1.748271531562744\n",
      "train loss:1.6627061434625923\n",
      "train loss:1.7506131837112548\n",
      "train loss:1.7559101046598244\n",
      "train loss:1.6741601653035099\n",
      "train loss:1.5533335726147501\n",
      "train loss:1.640819854708955\n",
      "train loss:1.6888797970753684\n",
      "train loss:1.5070705932287052\n",
      "train loss:1.6788526770864554\n",
      "train loss:1.645980652414771\n",
      "train loss:1.8150505464965243\n",
      "train loss:1.5539968440357947\n",
      "train loss:1.6604793538807112\n",
      "train loss:1.7130409841886836\n",
      "train loss:1.6052917978367676\n",
      "train loss:1.6539645326258097\n",
      "train loss:1.4561149186482565\n",
      "train loss:1.6180875582276844\n",
      "train loss:1.6083518950258664\n",
      "train loss:1.5669393146208943\n",
      "train loss:1.5588472197919003\n",
      "train loss:1.5340648281923221\n",
      "train loss:1.7134188487288347\n",
      "train loss:1.6262736905543602\n",
      "train loss:1.5310386274987966\n",
      "train loss:1.6378808164365293\n",
      "train loss:1.5193484231549925\n",
      "train loss:1.5885240487890044\n",
      "train loss:1.6272464420518518\n",
      "train loss:1.71072340533952\n",
      "train loss:1.6036121288165743\n",
      "train loss:1.4838499012973678\n",
      "train loss:1.4017123356884826\n",
      "train loss:1.3530435833697678\n",
      "train loss:1.47103778773452\n",
      "train loss:1.5013144073291034\n",
      "train loss:1.555509219478583\n",
      "train loss:1.5114063592081137\n",
      "train loss:1.6129215705470759\n",
      "train loss:1.441910019779076\n",
      "train loss:1.509974941135559\n",
      "train loss:1.5384227776154933\n",
      "train loss:1.4875389659360383\n",
      "train loss:1.6256126661377348\n",
      "train loss:1.6343281399345506\n",
      "train loss:1.558160960788754\n",
      "train loss:1.5134878992642555\n",
      "train loss:1.3827986882090681\n",
      "train loss:1.3071364758691224\n",
      "train loss:1.5986697543372992\n",
      "train loss:1.4963167423319093\n",
      "train loss:1.5995636100670705\n",
      "train loss:1.6158172036334575\n",
      "train loss:1.69486765751708\n",
      "train loss:1.5020368615955346\n",
      "train loss:1.3548265770231687\n",
      "train loss:1.553413707394867\n",
      "train loss:1.6396935200054015\n",
      "train loss:1.5435659956619268\n",
      "train loss:1.5216214275667557\n",
      "train loss:1.488170723837316\n",
      "train loss:1.4962813802947534\n",
      "train loss:1.565345079173912\n",
      "train loss:1.4030033634494459\n",
      "train loss:1.5005865388628397\n",
      "train loss:1.4975987970692075\n",
      "train loss:1.6483641006987113\n",
      "train loss:1.5623874678408611\n",
      "train loss:1.526872108678406\n",
      "train loss:1.3805538976581562\n",
      "train loss:1.3570584206434786\n",
      "train loss:1.2266399900052474\n",
      "train loss:1.508638635893886\n",
      "train loss:1.4883412531166809\n",
      "train loss:1.5381691426497683\n",
      "train loss:1.583940228993587\n",
      "train loss:1.413114262216221\n",
      "train loss:1.5480079566125164\n",
      "train loss:1.5168257334919182\n",
      "train loss:1.4161223282666626\n",
      "train loss:1.4397362466687684\n",
      "train loss:1.5512872142991037\n",
      "train loss:1.333931028800759\n",
      "train loss:1.4833980048823299\n",
      "train loss:1.498417414839071\n",
      "train loss:1.4924089977631507\n",
      "train loss:1.5615020499224896\n",
      "train loss:1.6434778969043107\n",
      "train loss:1.494517721567262\n",
      "train loss:1.4061516007753372\n",
      "train loss:1.4016635402811177\n",
      "train loss:1.4711761946494386\n",
      "train loss:1.611921309965784\n",
      "train loss:1.4909937718214334\n",
      "train loss:1.5826014489438744\n",
      "train loss:1.6661680247562218\n",
      "train loss:1.395491862222602\n",
      "train loss:1.5989024508031153\n",
      "train loss:1.4836915530407844\n",
      "train loss:1.3176288829021474\n",
      "train loss:1.3567008273427055\n",
      "train loss:1.4081920814605307\n",
      "train loss:1.2895175063350017\n",
      "train loss:1.4641723186019535\n",
      "train loss:1.3588123341979588\n",
      "train loss:1.4905045649146538\n",
      "train loss:1.3048971585682767\n",
      "train loss:1.5812725322529124\n",
      "train loss:1.3475143158759764\n",
      "train loss:1.5166485524800415\n",
      "train loss:1.19103507404622\n",
      "train loss:1.45496864817365\n",
      "train loss:1.4341135548983557\n",
      "train loss:1.3677713095883848\n",
      "train loss:1.3459950110535328\n",
      "train loss:1.4062901091805071\n",
      "train loss:1.5050473961124493\n",
      "train loss:1.3224329397155514\n",
      "train loss:1.6773819779458952\n",
      "train loss:1.6243106388931017\n",
      "train loss:1.5031684069883258\n",
      "train loss:1.3465230677511046\n",
      "train loss:1.5194358370350118\n",
      "train loss:1.359313824211697\n",
      "train loss:1.3074740656038162\n",
      "train loss:1.509268845425719\n",
      "train loss:1.4220361255949958\n",
      "train loss:1.498983031495401\n",
      "train loss:1.450953817404956\n",
      "train loss:1.5427255261814252\n",
      "train loss:1.2832337842918384\n",
      "train loss:1.4445832220886399\n",
      "train loss:1.1783620879063386\n",
      "train loss:1.4645345519537714\n",
      "train loss:1.4517391238021964\n",
      "train loss:1.3291401688314897\n",
      "train loss:1.6463734672919048\n",
      "train loss:1.4121952624467227\n",
      "train loss:1.3343177293914363\n",
      "train loss:1.4265744546513779\n",
      "train loss:1.3921394415343966\n",
      "train loss:1.3963529011625744\n",
      "train loss:1.3063390286478578\n",
      "train loss:1.6164152168574895\n",
      "train loss:1.49560173801384\n",
      "train loss:1.3679807207813652\n",
      "train loss:1.4295179581027317\n",
      "train loss:1.4079736241183927\n",
      "train loss:1.5453539579929712\n",
      "train loss:1.6266132782066685\n",
      "train loss:1.366034346081002\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6617647058823529\n"
     ]
    }
   ],
   "source": [
    "network = DeepConvNet()  \n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b3ea926-d2b2-409f-8249-700cf0854ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.5196078431372549\n",
      "혼동 행렬:\n",
      " [[83 55]\n",
      " [43 23]]\n",
      "분류 보고서:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.60      0.63       138\n",
      "           1       0.29      0.35      0.32        66\n",
      "\n",
      "    accuracy                           0.52       204\n",
      "   macro avg       0.48      0.47      0.47       204\n",
      "weighted avg       0.54      0.52      0.53       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mymethod.functions import *\n",
    "\n",
    "\n",
    "x_train_reshaped = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train_reshaped, y_train)\n",
    "\n",
    "y_pred = model.predict(x_test_reshaped)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"정확도:\", accuracy)\n",
    "print(\"혼동 행렬:\\n\", conf_matrix)\n",
    "print(\"분류 보고서:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1adf167-488d-42de-809a-15e713a14e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.5490196078431373\n",
      "혼동 행렬:\n",
      " [[88 50]\n",
      " [42 24]]\n",
      "분류 보고서:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       138\n",
      "           1       0.32      0.36      0.34        66\n",
      "\n",
      "    accuracy                           0.55       204\n",
      "   macro avg       0.50      0.50      0.50       204\n",
      "weighted avg       0.56      0.55      0.56       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(x_train_reshaped , y_train)\n",
    "y_pred = lda.predict(x_test_reshaped)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"정확도:\", accuracy)\n",
    "print(\"혼동 행렬:\\n\", conf_matrix)\n",
    "print(\"분류 보고서:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6bec05c-d16c-495e-b503-f4eb56d27dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.4117647058823529\n",
      "혼동 행렬:\n",
      " [[ 27 111]\n",
      " [  9  57]]\n",
      "분류 보고서:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.20      0.31       138\n",
      "           1       0.34      0.86      0.49        66\n",
      "\n",
      "    accuracy                           0.41       204\n",
      "   macro avg       0.54      0.53      0.40       204\n",
      "weighted avg       0.62      0.41      0.37       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(x_train_reshaped , y_train)\n",
    "y_pred = nb.predict(x_test_reshaped)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"정확도:\", accuracy)\n",
    "print(\"혼동 행렬:\\n\", conf_matrix)\n",
    "print(\"분류 보고서:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26dcfcea-ea0f-497b-8050-43ff21af00aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = hue_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bb5d95a-0919-4856-a790-a2acf57c8621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class ConvNet3Layer:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = int((input_size - filter_size + 2 * filter_pad) / filter_stride + 1)\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b3'] = np.zeros(hidden_size)        \n",
    "        self.params['W4'] = weight_init_std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b4'] = np.zeros(hidden_size)\n",
    "        self.params['W5'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b5'] = np.zeros(output_size)\n",
    "        \n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        \n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "\n",
    "        self.layers['Conv2'] = Convolution(self.params['W3'], self.params['b3'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Pool2'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "\n",
    "        self.layers['Affine2'] = Affine(self.params['W4'], self.params['b4'])\n",
    "        self.layers['Relu3'] = Relu()  # 추가된 ReLU 활성화 함수\n",
    "        self.layers['Affine3'] = Affine(self.params['W5'], self.params['b5'])\n",
    "        \n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3,4,5):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Conv2'].dW, self.layers['Conv2'].db\n",
    "        grads['W4'], grads['b4'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        grads['W5'], grads['b5'] = self.layers['Affine3'].dW, self.layers['Affine3'].db\n",
    "        \n",
    "\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bce8e1b-3612-4f39-92c0-5f8e41ca4b82",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 16\u001b[0m\n\u001b[0;32m      9\u001b[0m network \u001b[38;5;241m=\u001b[39m ConvNet3Layer(input_dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m), \n\u001b[0;32m     10\u001b[0m                         conv_param1 \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter_num\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m30\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstride\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m},\n\u001b[0;32m     11\u001b[0m                         hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, weight_init_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m     12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(network, x_train, y_train, x_test, y_test,\n\u001b[0;32m     13\u001b[0m                   epochs\u001b[38;5;241m=\u001b[39mmax_epochs, mini_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     14\u001b[0m                   optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer_param\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.001\u001b[39m},\n\u001b[0;32m     15\u001b[0m                   evaluate_sample_num_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\4학년 2학기\\딥러닝\\과제\\common\\trainer.py:71\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[1;32m---> 71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step()\n\u001b[0;32m     73\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39maccuracy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_test)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\4학년 2학기\\딥러닝\\과제\\common\\trainer.py:44\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_train[batch_mask]\n\u001b[0;32m     42\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_train[batch_mask]\n\u001b[1;32m---> 44\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mgradient(x_batch, t_batch)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mparams, grads)\n\u001b[0;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mloss(x_batch, t_batch)\n",
      "Cell \u001b[1;32mIn[21], line 85\u001b[0m, in \u001b[0;36mConvNet3Layer.gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# backward\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     dout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[21], line 56\u001b[0m, in \u001b[0;36mConvNet3Layer.loss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 56\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_layer\u001b[38;5;241m.\u001b[39mforward(y, t)\n",
      "Cell \u001b[1;32mIn[21], line 51\u001b[0m, in \u001b[0;36mConvNet3Layer.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m---> 51\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\4학년 2학기\\딥러닝\\과제\\common\\layers.py:216\u001b[0m, in \u001b[0;36mConvolution.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    215\u001b[0m     FN, C, FH, FW \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 216\u001b[0m     N, C, H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    217\u001b[0m     out_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m((H \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad \u001b[38;5;241m-\u001b[39m FH) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride)\n\u001b[0;32m    218\u001b[0m     out_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m((W \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad \u001b[38;5;241m-\u001b[39m FW) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "from common.trainer import Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param1 = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc3ca5-2913-41ed-b4a8-c61597c8baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), #필터 수 조절해서 성능 조절\n",
    "                 conv_param_1 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=10):\n",
    "        # 가중치 초기화===========\n",
    "        # 각 층의 뉴런 하나당 앞 층의 몇 개 뉴런과 연결되는가（TODO: 자동 계산되게 바꿀 것）\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLU를 사용할 때의 권장 초깃값\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = wight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = wight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
