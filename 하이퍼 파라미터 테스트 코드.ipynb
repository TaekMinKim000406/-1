{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7eccf8-a06b-4ad0-a45e-aed1c7dbaa5e",
   "metadata": {},
   "source": [
    "최적의 하이퍼 파라미터를 찾기 위한 코드입니다.\n",
    "원래는 정의해 놓은 함수를 불러와서 사용해야 했으나, 검증 과정에서 일부 함수를 직접 코드를 복사해와서 테스트 한 코드도 많습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b68f9fdc-5a78-4528-85ed-833ffc00d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(current_dir)\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "image_folder_path = './G1020/Images/'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "068cf94c-2ff7-45a1-a60b-9a0d62b7e119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Hue images shape: (1020, 1, 28, 28)\n",
      "Labels shape: (1020,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('G1020.csv')\n",
    "image_files = df['imageID'].tolist()  # 이미지 파일 이름이 있는 열 이름\n",
    "labels = df['binaryLabels'].values  # 레이블이 있는 열 이름\n",
    "\n",
    "# 이미지 크기 설정\n",
    "target_size = (28, 28)\n",
    "\n",
    "# cv2로 이미지 읽고 전처리\n",
    "def load_and_extract_hue(image_path, target_size):\n",
    "    image = cv2.imread(image_path)  # 이미지 읽기 (기본 BGR 형식)\n",
    "    if image is None:\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return None\n",
    "    image = cv2.resize(image, target_size)  # 이미지 크기 조정\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)  # BGR을 HSV로 변환\n",
    "    hue_channel = hsv_image[:, :, 0]  # Hue 채널만 추출\n",
    "    hue_channel = hue_channel / 180.0  # Hue 값 정규화 (0~1 범위로 스케일링, Hue 범위는 0-179)\n",
    "    return hue_channel\n",
    "\n",
    "# 모든 이미지를 불러와서 리스트에 저장\n",
    "images = [load_and_extract_hue(image_folder_path + img_path, target_size) for img_path in image_files]\n",
    "images = np.array([img for img in images if img is not None])  # None 값 제거\n",
    "\n",
    "# 차원 추가하여 (1020, 1, 128, 128) 형태로 변환\n",
    "images = images[:, np.newaxis, :, :]\n",
    "\n",
    "print(f\"Processed Hue images shape: {images.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eae2368-07b5-4043-a45d-1054a0efab7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299666680757359\n",
      "=== epoch:1, train acc:0.3, test acc:0.35 ===\n",
      "train loss:2.295161261354918\n",
      "train loss:2.2880640116511612\n",
      "train loss:2.276487901024723\n",
      "train loss:2.260578256199971\n",
      "train loss:2.235414098245895\n",
      "train loss:2.1930039392795697\n",
      "train loss:2.153535015514454\n",
      "train loss:2.069305504835655\n",
      "train loss:2.0053253755997895\n",
      "train loss:1.9018579775991884\n",
      "train loss:1.7394360760706604\n",
      "train loss:1.6390295968158433\n",
      "train loss:1.445256297352058\n",
      "train loss:1.3545503045823604\n",
      "train loss:1.1011055508901206\n",
      "train loss:0.9585805251529672\n",
      "train loss:0.8868065741958517\n",
      "train loss:0.7706085991645261\n",
      "train loss:0.7420649120967686\n",
      "train loss:0.703541613771742\n",
      "train loss:0.6226815200294726\n",
      "train loss:0.6118299370762117\n",
      "train loss:0.6065407164582828\n",
      "train loss:0.5371766957795578\n",
      "train loss:0.6305270337593951\n",
      "train loss:0.8820547594930874\n",
      "train loss:0.5089407745664396\n",
      "train loss:0.5931466385162885\n",
      "train loss:0.39112273042725204\n",
      "train loss:0.6468999280650344\n",
      "train loss:0.785948530049672\n",
      "train loss:0.5380857910596817\n",
      "train loss:0.6113033842047513\n",
      "train loss:0.6267044632221948\n",
      "train loss:0.6266005165973338\n",
      "train loss:0.5505463526110694\n",
      "train loss:0.528894011150843\n",
      "train loss:0.5066172840182573\n",
      "train loss:0.7340107465874863\n",
      "train loss:0.6388289537328232\n",
      "train loss:0.5078747874432741\n",
      "train loss:0.6335224258032447\n",
      "train loss:0.4730374063650885\n",
      "train loss:0.3581495946088681\n",
      "train loss:0.6756211185431293\n",
      "train loss:0.7350701276350257\n",
      "train loss:0.5021048451411845\n",
      "train loss:0.280402148045307\n",
      "train loss:0.37953137929252895\n",
      "train loss:0.49202184427547546\n",
      "train loss:0.5123173831869601\n",
      "train loss:0.7662483587998878\n",
      "train loss:0.8100463764773954\n",
      "train loss:0.5093908262813256\n",
      "train loss:0.7174546812405429\n",
      "train loss:0.5291702713913591\n",
      "train loss:0.6229912128759622\n",
      "train loss:0.7129899417055843\n",
      "train loss:0.683633182726741\n",
      "train loss:0.6922088187501086\n",
      "train loss:0.7114741633974077\n",
      "train loss:0.7256390121574646\n",
      "train loss:0.6819241656393341\n",
      "train loss:0.6809907637566214\n",
      "train loss:0.6305478341093901\n",
      "train loss:0.7020308861849089\n",
      "train loss:0.6379182716813737\n",
      "train loss:0.3738546504808205\n",
      "train loss:0.36270341625047664\n",
      "train loss:0.808372760439771\n",
      "train loss:0.8109757281444068\n",
      "train loss:0.6717812232556989\n",
      "train loss:0.5163959918076797\n",
      "train loss:0.3319607270914608\n",
      "train loss:0.8088245927925447\n",
      "train loss:0.5048015055571297\n",
      "train loss:0.38882480066811165\n",
      "train loss:0.7626211984724274\n",
      "train loss:0.7421172071106945\n",
      "train loss:0.5007884779546854\n",
      "train loss:0.6250204698322415\n",
      "train loss:0.7313211650247944\n",
      "train loss:0.5469257561516704\n",
      "train loss:0.6426246639244677\n",
      "train loss:0.592932153235658\n",
      "train loss:0.6801441094407941\n",
      "train loss:0.7100374532422384\n",
      "train loss:0.6297443845261704\n",
      "train loss:0.7166330500518849\n",
      "train loss:0.5730225781849033\n",
      "train loss:0.5726999258124043\n",
      "train loss:0.6303693547105151\n",
      "train loss:0.5241021700059753\n",
      "train loss:0.589483656541075\n",
      "train loss:0.8188653707860437\n",
      "train loss:0.7117519081816689\n",
      "train loss:0.493177022644382\n",
      "train loss:0.6168044442174383\n",
      "train loss:0.380125710666091\n",
      "train loss:0.6158878536218289\n",
      "train loss:0.8331627994660671\n",
      "train loss:0.4184173157674323\n",
      "train loss:0.5984631640348227\n",
      "train loss:0.7254280813835481\n",
      "train loss:0.6035211383872745\n",
      "train loss:0.8094097698097406\n",
      "train loss:0.6145489692553743\n",
      "train loss:0.6787784606410521\n",
      "train loss:0.6254845064686071\n",
      "train loss:0.5239624091424686\n",
      "train loss:0.5851661626870561\n",
      "train loss:0.6769696806478855\n",
      "train loss:0.6737933673189094\n",
      "train loss:0.7329079053028582\n",
      "train loss:0.6717850265478169\n",
      "train loss:0.5084869844916128\n",
      "train loss:0.6783931193779056\n",
      "train loss:0.6779945735814865\n",
      "train loss:0.6079301574149782\n",
      "train loss:0.45857189278287847\n",
      "train loss:0.5287418360535724\n",
      "train loss:0.29908396620499406\n",
      "train loss:0.3557339911613178\n",
      "train loss:0.9184169100267866\n",
      "train loss:0.6732987305233139\n",
      "train loss:0.6403870276059228\n",
      "train loss:1.0502485280285172\n",
      "train loss:0.6283399749937587\n",
      "train loss:0.735196970450543\n",
      "train loss:0.5007814901512382\n",
      "train loss:0.4156568520958322\n",
      "train loss:0.4497795277249523\n",
      "train loss:0.42339809016567564\n",
      "train loss:0.6239018551545786\n",
      "train loss:0.30435783781495734\n",
      "train loss:0.41372628409062484\n",
      "train loss:0.7699490006045379\n",
      "train loss:0.8933257721023432\n",
      "train loss:0.6417814650361746\n",
      "train loss:0.38737202659135994\n",
      "train loss:0.597111368814174\n",
      "train loss:0.5308752779842452\n",
      "train loss:0.403214828432536\n",
      "train loss:0.6145113446769721\n",
      "train loss:0.7150246326857376\n",
      "train loss:0.8437110010422211\n",
      "train loss:0.7187743928246128\n",
      "train loss:0.519995003386251\n",
      "train loss:0.4461933871466309\n",
      "train loss:0.46024846742440884\n",
      "train loss:0.5381351355035238\n",
      "train loss:0.6163254795573722\n",
      "train loss:0.3885381180775228\n",
      "train loss:0.40563705055705457\n",
      "train loss:0.38995357276021514\n",
      "train loss:0.4915357586222789\n",
      "train loss:0.8011112558226714\n",
      "train loss:0.7359331711505183\n",
      "train loss:0.7588965213460089\n",
      "train loss:0.47941364015773535\n",
      "train loss:0.5139090941521499\n",
      "train loss:0.5114198443568608\n",
      "train loss:0.738058637460028\n",
      "train loss:0.23490542753965382\n",
      "train loss:0.609248480360899\n",
      "train loss:0.5394739652303293\n",
      "train loss:0.7454477159070657\n",
      "train loss:0.637855114216353\n",
      "train loss:0.6372402927539187\n",
      "train loss:0.41448251769519456\n",
      "train loss:0.6080114680192052\n",
      "train loss:0.7253102319105712\n",
      "train loss:0.6235852028010869\n",
      "train loss:0.45641507448290186\n",
      "train loss:0.60193340097678\n",
      "train loss:0.5122611486151218\n",
      "train loss:0.5263250872059735\n",
      "train loss:0.5240139990994745\n",
      "train loss:0.6104273927918873\n",
      "train loss:0.40958873391476763\n",
      "train loss:0.6518794932111136\n",
      "train loss:0.5405065631413388\n",
      "train loss:0.7139820754596513\n",
      "train loss:0.6080427161791597\n",
      "train loss:0.860083218137655\n",
      "train loss:0.5263899897288571\n",
      "train loss:0.5218141882486156\n",
      "train loss:0.803131809695099\n",
      "train loss:0.8599221552777395\n",
      "train loss:0.5951623717469736\n",
      "train loss:0.6727182999492332\n",
      "train loss:0.6641409215327286\n",
      "train loss:0.584410019251422\n",
      "train loss:0.667160582936114\n",
      "train loss:0.715814761680711\n",
      "train loss:0.6771284679602904\n",
      "train loss:0.6505017560693697\n",
      "train loss:0.6454025108422706\n",
      "train loss:0.6213108228384226\n",
      "train loss:0.6053723282715595\n",
      "train loss:0.5357076420814617\n",
      "train loss:0.6229628212542447\n",
      "train loss:0.5470811679419212\n",
      "train loss:0.540252521010524\n",
      "train loss:0.41447031422468744\n",
      "train loss:0.3887998291690039\n",
      "train loss:0.8978608880046428\n",
      "train loss:0.38009511667076046\n",
      "train loss:0.9414539123753789\n",
      "train loss:0.48954973933419377\n",
      "train loss:0.35216063257070224\n",
      "train loss:0.34545948316689484\n",
      "train loss:0.8088460783331086\n",
      "train loss:0.8630273438474031\n",
      "train loss:0.5069246255976626\n",
      "train loss:0.6278075475455397\n",
      "train loss:0.6091344063677654\n",
      "train loss:0.3709118365823464\n",
      "train loss:0.7347028805721774\n",
      "train loss:0.7144055994494474\n",
      "train loss:0.6272868940879681\n",
      "train loss:0.7268758163671387\n",
      "train loss:0.5258678141395059\n",
      "train loss:0.5360706427123059\n",
      "train loss:0.46694881035918845\n",
      "train loss:0.5443096448078053\n",
      "train loss:0.6756117256188359\n",
      "train loss:0.4514198634629893\n",
      "train loss:0.6211515184831005\n",
      "train loss:0.4349420955596033\n",
      "train loss:0.7902601731361083\n",
      "train loss:0.6270814002732814\n",
      "train loss:0.6143405588093572\n",
      "train loss:0.522482613041739\n",
      "train loss:0.7812140306462909\n",
      "train loss:0.4148641979326282\n",
      "train loss:0.7791363016067703\n",
      "train loss:0.9554892801800365\n",
      "train loss:0.7777565526562238\n",
      "train loss:0.6254649841369004\n",
      "train loss:0.6124534401714026\n",
      "train loss:0.6254963945371501\n",
      "train loss:0.6260519126663718\n",
      "train loss:0.5783366841338161\n",
      "train loss:0.5680287013469428\n",
      "train loss:0.6218426246449779\n",
      "train loss:0.7033057184748394\n",
      "train loss:0.43756068035792595\n",
      "train loss:0.5380503760769797\n",
      "train loss:0.6352325931959821\n",
      "train loss:0.6862813015823364\n",
      "train loss:0.5138717973609825\n",
      "train loss:0.6184181535461075\n",
      "train loss:0.7086351093024805\n",
      "train loss:0.70483414525364\n",
      "train loss:0.5149159114599426\n",
      "train loss:0.5047369037913219\n",
      "train loss:0.28274010325908383\n",
      "train loss:0.6152620986203151\n",
      "train loss:0.5306273094100825\n",
      "train loss:0.8820515684994155\n",
      "train loss:0.4075781914992425\n",
      "train loss:0.7830597909503099\n",
      "train loss:0.6199726285862822\n",
      "train loss:0.7376668374215426\n",
      "train loss:0.6968755799054644\n",
      "train loss:0.5259912602782009\n",
      "train loss:0.6327804028930416\n",
      "train loss:0.6879547176106543\n",
      "train loss:0.46084526467548714\n",
      "train loss:0.7760121198578227\n",
      "train loss:0.4847782687340742\n",
      "train loss:0.6222238943887843\n",
      "train loss:0.7519844513917844\n",
      "train loss:0.7407386265908423\n",
      "train loss:0.5509731836041392\n",
      "train loss:0.6237446007843768\n",
      "train loss:0.6787277966686615\n",
      "train loss:0.6747792712326441\n",
      "train loss:0.6762094359629034\n",
      "train loss:0.5675053915285198\n",
      "train loss:0.5794545996386267\n",
      "train loss:0.6262481022475993\n",
      "train loss:0.5067248077918773\n",
      "train loss:0.8053830806439798\n",
      "train loss:0.5507278776393865\n",
      "train loss:0.5327163491509421\n",
      "train loss:0.7183070964048801\n",
      "train loss:0.44256736156409165\n",
      "train loss:0.5121468627579226\n",
      "train loss:0.5309857877678885\n",
      "train loss:0.5127314711173931\n",
      "train loss:0.6315756157570783\n",
      "train loss:0.5251978995202478\n",
      "train loss:0.6279557745609677\n",
      "train loss:0.5045874126888544\n",
      "train loss:0.37395996644059265\n",
      "train loss:0.48323316835037566\n",
      "train loss:0.5275073596726603\n",
      "train loss:0.3403143169507739\n",
      "train loss:1.2599375417765155\n",
      "train loss:0.6389050214848446\n",
      "train loss:0.8612377767214923\n",
      "train loss:0.4000309876614231\n",
      "train loss:0.6135987950352134\n",
      "train loss:0.6964771396095231\n",
      "train loss:0.7711606132449371\n",
      "train loss:0.6138850024479908\n",
      "train loss:0.556404485274974\n",
      "train loss:0.7352411404027125\n",
      "train loss:0.6772160498720452\n",
      "train loss:0.6774789275831588\n",
      "train loss:0.6456264376771201\n",
      "train loss:0.578222299072207\n",
      "train loss:0.6126793176044748\n",
      "train loss:0.562540532289188\n",
      "train loss:0.5893317600660766\n",
      "train loss:0.7854120456136189\n",
      "train loss:0.5131813577606331\n",
      "train loss:0.5580375958803157\n",
      "train loss:0.5387367845008952\n",
      "train loss:0.4514581723564719\n",
      "train loss:0.5990062801307044\n",
      "train loss:0.5057169535836059\n",
      "train loss:0.7170383166260792\n",
      "train loss:0.38541897625960414\n",
      "train loss:0.4936191221299532\n",
      "train loss:0.7804055645378775\n",
      "train loss:0.2209746647557819\n",
      "train loss:0.983127994260809\n",
      "train loss:0.530749439757123\n",
      "train loss:0.9380981741248618\n",
      "train loss:0.887008814074882\n",
      "train loss:0.6061010978363491\n",
      "train loss:0.5088820573097971\n",
      "train loss:0.41367406573939886\n",
      "train loss:0.5240024350840358\n",
      "train loss:0.6960430514676763\n",
      "train loss:0.5262117514177612\n",
      "train loss:0.5422852607891193\n",
      "train loss:0.5343953905777331\n",
      "train loss:0.5312283082120024\n",
      "train loss:0.5360944886553489\n",
      "train loss:0.7126636322422586\n",
      "train loss:0.7622935986613892\n",
      "train loss:0.5353355984005773\n",
      "train loss:0.6250403638172599\n",
      "train loss:0.3532926712859338\n",
      "train loss:0.6304719318892522\n",
      "train loss:0.535107510607318\n",
      "train loss:0.7114738231078922\n",
      "train loss:0.49820384652784994\n",
      "train loss:0.6266216356954357\n",
      "train loss:0.5183640557754622\n",
      "train loss:0.7228500995689329\n",
      "train loss:0.7259657240657534\n",
      "train loss:0.40880795679929915\n",
      "train loss:0.29308759641617294\n",
      "train loss:0.2908508825114504\n",
      "train loss:0.38805034843759423\n",
      "train loss:0.653726034549587\n",
      "train loss:0.3539348987029872\n",
      "train loss:0.5042693001546577\n",
      "train loss:0.9854002261489437\n",
      "train loss:0.6450359593001452\n",
      "train loss:0.4888537114083881\n",
      "train loss:0.34816505453473534\n",
      "train loss:0.18042256706067655\n",
      "train loss:0.48846922112924707\n",
      "train loss:0.8330528854891368\n",
      "train loss:0.3399689568948016\n",
      "train loss:0.8478770361665839\n",
      "train loss:0.34557841911578596\n",
      "train loss:0.6399917184025463\n",
      "train loss:0.5071972787225741\n",
      "train loss:0.6438294600465408\n",
      "train loss:0.38868918241909406\n",
      "train loss:0.9584797416787323\n",
      "train loss:0.30662566016163517\n",
      "train loss:0.6098023331046579\n",
      "train loss:0.41733322817351126\n",
      "train loss:0.7204126915746539\n",
      "train loss:0.6064670796776619\n",
      "train loss:0.3293532242507935\n",
      "train loss:0.7887933597342458\n",
      "train loss:0.4247330046383356\n",
      "train loss:0.8998974545609986\n",
      "train loss:0.6033991647019995\n",
      "train loss:0.6286144530362981\n",
      "train loss:0.6056528853029667\n",
      "train loss:0.535267978466844\n",
      "train loss:0.5263222375872902\n",
      "train loss:0.7566911364564058\n",
      "train loss:0.5961938199472993\n",
      "train loss:0.6134923692808376\n",
      "train loss:0.5317788569927013\n",
      "train loss:0.522487183585217\n",
      "train loss:0.6248053752368301\n",
      "train loss:0.44533232729482125\n",
      "train loss:0.6063282617748225\n",
      "train loss:0.44743118977611307\n",
      "train loss:0.27490837306228105\n",
      "train loss:0.5003177263782852\n",
      "train loss:0.5226803622605224\n",
      "train loss:0.6461861612237471\n",
      "train loss:0.9195112341590406\n",
      "train loss:0.3553521552795643\n",
      "train loss:0.8998232127740892\n",
      "train loss:0.8789246437670795\n",
      "train loss:0.9735469459498507\n",
      "train loss:0.7939435087009086\n",
      "train loss:0.35282674896107846\n",
      "train loss:0.5450068356714648\n",
      "train loss:0.7434382900651608\n",
      "train loss:0.49329825792607834\n",
      "train loss:0.6180689313240423\n",
      "train loss:0.568172877822996\n",
      "train loss:0.7896742344665448\n",
      "train loss:0.6226653237415218\n",
      "train loss:0.7692670743585752\n",
      "train loss:0.6259774905545802\n",
      "train loss:0.5822028608044713\n",
      "train loss:0.6268988804766316\n",
      "train loss:0.5843931278869899\n",
      "train loss:0.6315759650820636\n",
      "train loss:0.7337045982410945\n",
      "train loss:0.6773984768918814\n",
      "train loss:0.5199379988030992\n",
      "train loss:0.5594884931852464\n",
      "train loss:0.690262677166302\n",
      "train loss:0.5423349338469695\n",
      "train loss:0.6226227851198268\n",
      "train loss:0.7629907208924359\n",
      "train loss:0.4495527721661533\n",
      "train loss:0.5248659808434095\n",
      "train loss:0.5207991172054571\n",
      "train loss:0.7146497091700121\n",
      "train loss:0.7382431757690113\n",
      "train loss:0.5938335495014513\n",
      "train loss:0.6307560371636614\n",
      "train loss:0.5121914185798546\n",
      "train loss:0.2732646763535247\n",
      "train loss:0.5802224243010949\n",
      "train loss:0.5274090131743125\n",
      "train loss:0.6136306870514251\n",
      "train loss:0.7648045930566219\n",
      "train loss:0.7338690593534194\n",
      "train loss:0.5012996792061215\n",
      "train loss:0.6340630388102647\n",
      "train loss:0.806994548050357\n",
      "train loss:0.5189175612944104\n",
      "train loss:0.776438835094238\n",
      "train loss:0.5226370459313822\n",
      "train loss:0.5324486652178243\n",
      "train loss:0.6222296227979814\n",
      "train loss:0.6072672036069678\n",
      "train loss:0.3719712144597541\n",
      "train loss:0.8424961682378767\n",
      "train loss:0.7011449966476143\n",
      "train loss:0.6193374939483658\n",
      "train loss:0.6848539422828389\n",
      "train loss:0.5436489802208273\n",
      "train loss:0.47714913015650123\n",
      "train loss:0.6144093266160757\n",
      "train loss:0.531767000220307\n",
      "train loss:0.8504134428575684\n",
      "train loss:0.3641475093010052\n",
      "train loss:0.5264845296960321\n",
      "train loss:0.6210979448847138\n",
      "train loss:0.5159042205258684\n",
      "train loss:0.7289134895046266\n",
      "train loss:0.5106186067561922\n",
      "train loss:0.60783482106524\n",
      "train loss:0.505371096554302\n",
      "train loss:0.6074810166623178\n",
      "train loss:0.37767300300594403\n",
      "train loss:0.8435487651785658\n",
      "train loss:0.8322390385560512\n",
      "train loss:0.9345207104689498\n",
      "train loss:0.5249746901620282\n",
      "train loss:0.7888963934278654\n",
      "train loss:0.46056547172782497\n",
      "train loss:0.6821863531647462\n",
      "train loss:0.6709476394498836\n",
      "train loss:0.5482639232182944\n",
      "train loss:0.6088242303505927\n",
      "train loss:0.5638717108528051\n",
      "train loss:0.7294795499829154\n",
      "train loss:0.509499060693894\n",
      "train loss:0.6195448843536076\n",
      "train loss:0.6760113105322642\n",
      "train loss:0.556353342435039\n",
      "train loss:0.7524925326728099\n",
      "train loss:0.6161540726572423\n",
      "train loss:0.6220677051985903\n",
      "train loss:0.5509694886895337\n",
      "train loss:0.3783094624410336\n",
      "train loss:0.7857479894273349\n",
      "train loss:0.5312409408469385\n",
      "train loss:0.5219297172033459\n",
      "train loss:0.614528579113901\n",
      "train loss:0.5170456032817189\n",
      "train loss:0.4025792385398742\n",
      "train loss:0.6330481749451253\n",
      "train loss:0.5218853869125365\n",
      "train loss:0.37439930997473847\n",
      "train loss:0.5097872099671964\n",
      "train loss:0.47940546196421846\n",
      "train loss:0.7950181362542578\n",
      "train loss:0.34755183598103434\n",
      "train loss:0.5211936770644738\n",
      "train loss:0.791854139602034\n",
      "train loss:0.6472546253911169\n",
      "train loss:0.7678012872393241\n",
      "train loss:0.23702801266572995\n",
      "train loss:0.6260573405258555\n",
      "train loss:0.7194637690113144\n",
      "train loss:0.7189551056925001\n",
      "train loss:0.8937632379178515\n",
      "train loss:0.6129350199973549\n",
      "train loss:0.7615126029344078\n",
      "train loss:0.6250862157659727\n",
      "train loss:0.6206456816777435\n",
      "train loss:0.5953523134388533\n",
      "train loss:0.6346795478222322\n",
      "train loss:0.5954637777285953\n",
      "train loss:0.5537532747937475\n",
      "train loss:0.6817832094522877\n",
      "train loss:0.64142764415554\n",
      "train loss:0.5377019095517658\n",
      "train loss:0.5752189763733982\n",
      "train loss:0.6872393380135245\n",
      "train loss:0.6116984138099969\n",
      "train loss:0.551285218443309\n",
      "train loss:0.5451131848121119\n",
      "train loss:0.5230011090975666\n",
      "train loss:0.7012817080812871\n",
      "train loss:0.508118132586872\n",
      "train loss:0.515416722469936\n",
      "train loss:0.8424398106764821\n",
      "train loss:0.6301429952342595\n",
      "train loss:0.38575810298099844\n",
      "train loss:0.7218242963342238\n",
      "train loss:0.3691401203608907\n",
      "train loss:0.5058132290356393\n",
      "train loss:0.5033113232935286\n",
      "train loss:0.7910224070750445\n",
      "train loss:0.6229066017193938\n",
      "train loss:0.5006065491804377\n",
      "train loss:0.5075846200491629\n",
      "train loss:0.4919843099606213\n",
      "train loss:0.502737182757772\n",
      "train loss:0.5271571207587571\n",
      "train loss:0.7602397112629313\n",
      "train loss:0.4876816362420763\n",
      "train loss:0.4771525334034748\n",
      "train loss:0.3779146338578348\n",
      "train loss:0.7334580921699954\n",
      "train loss:0.6108536396043316\n",
      "train loss:0.7259479432907863\n",
      "train loss:0.5103226071384794\n",
      "train loss:0.5128250298416361\n",
      "train loss:0.5072778894436037\n",
      "train loss:0.8039424007205065\n",
      "train loss:0.506593481566551\n",
      "train loss:0.4144077981684367\n",
      "train loss:0.7216246763781689\n",
      "train loss:0.7163730994521669\n",
      "train loss:0.520324142033089\n",
      "train loss:0.5222205805681165\n",
      "train loss:0.4306058635969457\n",
      "train loss:0.5217623653082454\n",
      "train loss:0.5324624177972381\n",
      "train loss:0.7188585585313222\n",
      "train loss:0.29114617307357676\n",
      "train loss:0.5040510373572479\n",
      "train loss:0.49707263188968004\n",
      "train loss:0.382252988526077\n",
      "train loss:0.48030329547128864\n",
      "train loss:0.9209702863051599\n",
      "train loss:0.5114112948609046\n",
      "train loss:0.7670656884354649\n",
      "train loss:0.48250289499774857\n",
      "train loss:0.9023699768906519\n",
      "train loss:0.733798725671977\n",
      "train loss:0.6237245370327792\n",
      "train loss:0.7082505873487217\n",
      "train loss:0.6253523061705337\n",
      "train loss:0.45881454472710254\n",
      "train loss:0.6803387560250223\n",
      "train loss:0.5503088904604494\n",
      "train loss:0.8674411251180972\n",
      "train loss:0.5670017676267192\n",
      "train loss:0.6259530845274623\n",
      "train loss:0.5716970330451425\n",
      "train loss:0.6207732692612172\n",
      "train loss:0.5786459229543672\n",
      "train loss:0.62355535326777\n",
      "train loss:0.6271850741518684\n",
      "train loss:0.6194939020474256\n",
      "train loss:0.6145828360265955\n",
      "train loss:0.6889026113474656\n",
      "train loss:0.6813625965886685\n",
      "train loss:0.6861671171383442\n",
      "train loss:0.6863929258204314\n",
      "train loss:0.6861548264356943\n",
      "train loss:0.6772542386805254\n",
      "train loss:0.5477839096991269\n",
      "train loss:0.6815087780720372\n",
      "train loss:0.4093406989695751\n",
      "train loss:0.5523553515492869\n",
      "train loss:0.6110197612067031\n",
      "train loss:0.7716136759301029\n",
      "train loss:0.5422530011049336\n",
      "train loss:1.0156607591571871\n",
      "train loss:0.5334472991195239\n",
      "train loss:0.6118706371604474\n",
      "train loss:0.610904032008462\n",
      "train loss:0.6119668853409996\n",
      "train loss:0.5370601536153067\n",
      "train loss:0.5324571176091817\n",
      "train loss:0.6083750285367434\n",
      "train loss:0.5308866424376305\n",
      "train loss:0.5256595034416602\n",
      "train loss:0.6001216845751769\n",
      "train loss:0.7150911154564653\n",
      "train loss:0.7213519203287176\n",
      "train loss:0.5169563900928771\n",
      "train loss:0.5216815344548815\n",
      "train loss:0.7074248239602594\n",
      "train loss:0.7005634901572637\n",
      "train loss:0.6280256243872993\n",
      "train loss:0.7797554549522014\n",
      "train loss:0.6096997865165222\n",
      "train loss:0.5304273007601377\n",
      "train loss:0.6909686147970093\n",
      "train loss:0.6196653207989333\n",
      "train loss:0.7503596895321983\n",
      "train loss:0.4853616130908948\n",
      "train loss:0.612544679260952\n",
      "train loss:0.4111928154875451\n",
      "train loss:0.5355675993583358\n",
      "train loss:0.3673851099489176\n",
      "train loss:0.4275769584809995\n",
      "train loss:0.614821714061324\n",
      "train loss:0.4950160800796691\n",
      "train loss:0.506862705019274\n",
      "train loss:0.6410103203322389\n",
      "train loss:0.7771364721904627\n",
      "train loss:0.8860069198514842\n",
      "train loss:0.7421192915240511\n",
      "train loss:0.6184861026237755\n",
      "train loss:0.5004934440646395\n",
      "train loss:0.5051215951813118\n",
      "train loss:0.8216899875545629\n",
      "train loss:0.5296330502509968\n",
      "train loss:0.8644266226523566\n",
      "train loss:0.446086666112115\n",
      "train loss:0.6879874740716486\n",
      "train loss:0.7526975529380413\n",
      "train loss:0.6760335394054111\n",
      "train loss:0.7308215425338982\n",
      "train loss:0.6274373057461505\n",
      "train loss:0.6759494434199728\n",
      "train loss:0.5961739966147067\n",
      "train loss:0.6382085413111338\n",
      "train loss:0.5562289432917917\n",
      "train loss:0.7139162556557686\n",
      "train loss:0.7109748886466962\n",
      "train loss:0.5455186456182824\n",
      "train loss:0.676111699501559\n",
      "train loss:0.6340195879784091\n",
      "train loss:0.6257558215463446\n",
      "train loss:0.5723122447979229\n",
      "train loss:0.8408596711292766\n",
      "train loss:0.6270056691448465\n",
      "train loss:0.5565718269613475\n",
      "train loss:0.6200230856646056\n",
      "train loss:0.5412838749506925\n",
      "train loss:0.4616173722587737\n",
      "train loss:0.6197556076444035\n",
      "train loss:0.4243671770847636\n",
      "train loss:0.5072838980996675\n",
      "train loss:0.3947540055213897\n",
      "train loss:0.38354024922453867\n",
      "train loss:0.5323939721688931\n",
      "train loss:0.6636524423999791\n",
      "train loss:0.6408848995676351\n",
      "train loss:0.6975554546953519\n",
      "train loss:0.6760545674788231\n",
      "train loss:0.4981514243610956\n",
      "train loss:0.3450039045255856\n",
      "train loss:0.6508460991185623\n",
      "train loss:0.49715291058605054\n",
      "train loss:0.5250959231791338\n",
      "train loss:0.5127231088806233\n",
      "train loss:0.7611258755483463\n",
      "train loss:0.871559780612832\n",
      "train loss:0.3928214721930239\n",
      "train loss:0.815863266888256\n",
      "train loss:0.6065991265721158\n",
      "train loss:0.686812734893828\n",
      "train loss:0.606282264407611\n",
      "train loss:0.7457183029239026\n",
      "train loss:0.5055076819494324\n",
      "train loss:0.6743764701523588\n",
      "train loss:0.6255889129625112\n",
      "train loss:0.6795907349771804\n",
      "train loss:0.5814761803264509\n",
      "train loss:0.6338395492667939\n",
      "train loss:0.6252704352601018\n",
      "train loss:0.579555206973171\n",
      "train loss:0.5105736520530325\n",
      "train loss:0.5043579287442725\n",
      "train loss:0.5539292234030552\n",
      "train loss:0.6837585220095591\n",
      "train loss:0.6760023202380149\n",
      "train loss:0.6095296375355524\n",
      "train loss:0.5227050231336963\n",
      "train loss:0.7007442126927554\n",
      "train loss:0.7080724962409605\n",
      "train loss:0.5143917295794799\n",
      "train loss:0.5058271560135943\n",
      "train loss:0.6152109388445294\n",
      "train loss:0.719281824240032\n",
      "train loss:0.7160916816876928\n",
      "train loss:0.7258200020355086\n",
      "train loss:0.6037371186086269\n",
      "train loss:0.6253178080710888\n",
      "train loss:0.7011604002181195\n",
      "train loss:0.8521260196767692\n",
      "train loss:0.5456762571576561\n",
      "train loss:0.8805272723993575\n",
      "train loss:0.6162872927531124\n",
      "train loss:0.5655823104720014\n",
      "train loss:0.6252324897998918\n",
      "train loss:0.5300754875332305\n",
      "train loss:0.5823182291082292\n",
      "train loss:0.6231525250102445\n",
      "train loss:0.5692221362955081\n",
      "train loss:0.6780429028798383\n",
      "train loss:0.5589418201552074\n",
      "train loss:0.6763479556606099\n",
      "train loss:0.6795285223227381\n",
      "train loss:0.6149524892518295\n",
      "train loss:0.6155180676758728\n",
      "train loss:0.38191212482356507\n",
      "train loss:0.5341406504324537\n",
      "train loss:0.5318138313781751\n",
      "train loss:0.7127047503743913\n",
      "train loss:0.6212385185554088\n",
      "train loss:0.8120580557557056\n",
      "train loss:0.3948167907476111\n",
      "train loss:0.5034911761054977\n",
      "train loss:0.6159534565306926\n",
      "train loss:0.834880428451579\n",
      "train loss:0.5140859388408083\n",
      "train loss:0.38957817292628266\n",
      "train loss:0.6022522180244765\n",
      "train loss:0.3922008039689867\n",
      "train loss:0.5073008238049741\n",
      "train loss:0.4997833299863602\n",
      "train loss:0.617321941689781\n",
      "train loss:0.8817492997487314\n",
      "train loss:0.3731101301832562\n",
      "train loss:0.6185130167382462\n",
      "train loss:0.2549058935116143\n",
      "train loss:0.7539562350788465\n",
      "train loss:0.385614243330362\n",
      "train loss:0.5046154014991766\n",
      "train loss:0.6225788131175193\n",
      "train loss:0.2426227843247461\n",
      "train loss:0.3705975736803991\n",
      "train loss:0.3499842571594406\n",
      "train loss:0.7906800420718157\n",
      "train loss:0.7843806818946338\n",
      "train loss:0.9140693422344418\n",
      "train loss:0.7382101590625719\n",
      "train loss:0.4991206086355218\n",
      "train loss:0.5062987454810292\n",
      "train loss:0.7133334293224924\n",
      "train loss:0.6892718255345953\n",
      "train loss:0.5373189666731472\n",
      "train loss:0.5288461557391341\n",
      "train loss:0.5332330685569103\n",
      "train loss:0.7606119723144321\n",
      "train loss:0.681057168852706\n",
      "train loss:0.4724864177008894\n",
      "train loss:0.620498399417212\n",
      "train loss:0.481846211287138\n",
      "train loss:0.47179850275673124\n",
      "train loss:0.6127625523235476\n",
      "train loss:0.5264016207032651\n",
      "train loss:0.598628975119034\n",
      "train loss:0.519406013801906\n",
      "train loss:0.5053870896853555\n",
      "train loss:0.7220801604105429\n",
      "train loss:0.5152712505475824\n",
      "train loss:0.5071182898743569\n",
      "train loss:0.5145634429990522\n",
      "train loss:0.6126934586503605\n",
      "train loss:0.7634976446666097\n",
      "train loss:0.9519627691943502\n",
      "train loss:0.6158510907469273\n",
      "train loss:0.511997423712537\n",
      "train loss:0.3021563148750465\n",
      "train loss:0.7183882912439664\n",
      "train loss:1.0028612898975922\n",
      "train loss:0.6130667895626861\n",
      "train loss:0.6979714827097923\n",
      "train loss:0.4594277782450658\n",
      "train loss:0.6235117315086715\n",
      "train loss:0.5468472841902171\n",
      "train loss:0.6858045105705728\n",
      "train loss:0.476266677501984\n",
      "train loss:0.6172462453447876\n",
      "train loss:0.7439360493966289\n",
      "train loss:0.543399339412464\n",
      "train loss:0.554146387916199\n",
      "train loss:0.4697143993126109\n",
      "train loss:0.7731267174641996\n",
      "train loss:0.6063558931994029\n",
      "train loss:0.7612594134828482\n",
      "train loss:0.5290382268264529\n",
      "train loss:0.763395049153374\n",
      "train loss:0.4525828260725242\n",
      "train loss:0.6046038701949595\n",
      "train loss:0.7006546495286575\n",
      "train loss:0.6870042044758767\n",
      "train loss:0.615260697659874\n",
      "train loss:0.5271609959023886\n",
      "train loss:0.5253031086241429\n",
      "train loss:0.42413064528157607\n",
      "train loss:0.6015452676276569\n",
      "train loss:0.6160408766606709\n",
      "train loss:0.8100203208384625\n",
      "train loss:0.5173590354504565\n",
      "train loss:0.7989374314145568\n",
      "train loss:0.7962126177764886\n",
      "train loss:0.59571146872529\n",
      "train loss:0.5227847601756721\n",
      "train loss:0.6065139048824761\n",
      "train loss:0.7021747020172502\n",
      "train loss:0.750062600019367\n",
      "train loss:0.45995789001549026\n",
      "train loss:0.6076104435742506\n",
      "train loss:0.6855203613891001\n",
      "train loss:0.6142355930439274\n",
      "train loss:0.6797007547056169\n",
      "train loss:0.6120962960083206\n",
      "train loss:0.47574905913049453\n",
      "train loss:0.6217361048563091\n",
      "train loss:0.6830780226944724\n",
      "train loss:0.46828112987888104\n",
      "train loss:0.5353398003279295\n",
      "train loss:0.5162325227167351\n",
      "train loss:0.44082315249379017\n",
      "train loss:0.40377745563840167\n",
      "train loss:0.6221492489881715\n",
      "train loss:0.7509435951317942\n",
      "train loss:0.6308160389611973\n",
      "train loss:0.7470662970760392\n",
      "train loss:0.3868630520814143\n",
      "train loss:0.4979682268002203\n",
      "train loss:0.747452446426719\n",
      "train loss:0.488113468565422\n",
      "train loss:0.6213259766240304\n",
      "train loss:0.6352542912472426\n",
      "train loss:0.7114365663236131\n",
      "train loss:0.6144276172030374\n",
      "train loss:0.5200742084597676\n",
      "train loss:0.5214206691317252\n",
      "train loss:0.6276115190117049\n",
      "train loss:0.41505766785996484\n",
      "train loss:0.6023358329977325\n",
      "train loss:0.6213994343114092\n",
      "train loss:0.6002357866484234\n",
      "train loss:0.6987507858862816\n",
      "train loss:0.6182116132461796\n",
      "train loss:0.6053015473875133\n",
      "train loss:0.4318451544770937\n",
      "train loss:0.5186614646548758\n",
      "train loss:0.5124527846085516\n",
      "train loss:0.7133928123973623\n",
      "train loss:0.7960522666547242\n",
      "train loss:0.42513044082949036\n",
      "train loss:0.6988609955199286\n",
      "train loss:0.5332172206742369\n",
      "train loss:0.7003535384757285\n",
      "train loss:0.5254250913608238\n",
      "train loss:0.7044628582614831\n",
      "train loss:0.33049376525882523\n",
      "train loss:0.6189553035081996\n",
      "train loss:0.6190506634159637\n",
      "train loss:0.6122955519079091\n",
      "train loss:0.4130352317290609\n",
      "train loss:0.6999771733987429\n",
      "train loss:0.7010404944727708\n",
      "train loss:0.722803864553065\n",
      "train loss:0.6897975048959987\n",
      "train loss:0.7014055985898938\n",
      "train loss:0.5966414979021035\n",
      "train loss:0.45229947380015173\n",
      "train loss:0.5264787452442621\n",
      "train loss:0.5255899434413677\n",
      "train loss:0.7685822368185604\n",
      "train loss:0.5235888770605163\n",
      "train loss:0.6032206920720465\n",
      "train loss:0.5948543860446838\n",
      "train loss:0.5336816338668224\n",
      "train loss:0.6061668996969443\n",
      "train loss:0.7008177993480291\n",
      "train loss:0.5238916091387601\n",
      "train loss:0.5104208478525681\n",
      "train loss:0.611876129695423\n",
      "train loss:0.506674155948438\n",
      "train loss:0.512130668934768\n",
      "train loss:0.601201585207283\n",
      "train loss:0.6176659016227322\n",
      "train loss:0.7071751280146228\n",
      "train loss:0.7243814718702207\n",
      "train loss:0.7934533686045658\n",
      "train loss:0.6134386908262117\n",
      "train loss:0.6963934645508358\n",
      "train loss:0.5266882417018184\n",
      "train loss:0.6192613625369041\n",
      "train loss:0.4631410229112206\n",
      "train loss:0.6144852148414237\n",
      "train loss:0.7637872704962791\n",
      "train loss:0.6168000074023552\n",
      "train loss:0.6881043849576226\n",
      "train loss:0.7333230785239494\n",
      "train loss:0.4868996287770003\n",
      "train loss:0.6061682776013182\n",
      "train loss:0.6819656843670543\n",
      "train loss:0.6750594544034763\n",
      "train loss:0.6169482536050852\n",
      "train loss:0.7407910565297908\n",
      "train loss:0.5565097948429311\n",
      "train loss:0.6166802278521646\n",
      "train loss:0.5547482438903885\n",
      "train loss:0.6192412938057569\n",
      "train loss:0.4863218966529085\n",
      "train loss:0.6168140854529616\n",
      "train loss:0.6073635253487196\n",
      "train loss:0.6043545640820916\n",
      "train loss:0.6138031812192308\n",
      "train loss:0.7014159925640149\n",
      "train loss:0.5283071084696345\n",
      "train loss:0.7906848478203057\n",
      "train loss:1.0279570248954584\n",
      "train loss:0.6021834131239808\n",
      "train loss:0.6178229609269534\n",
      "train loss:0.6170453358154495\n",
      "train loss:0.6765849320703443\n",
      "train loss:0.5603016688985643\n",
      "train loss:0.6681871285741688\n",
      "train loss:0.4939703991348531\n",
      "train loss:0.6198805879994363\n",
      "train loss:0.5576832321753591\n",
      "train loss:0.4627429805041052\n",
      "train loss:0.4635762659087253\n",
      "train loss:0.7606837511275311\n",
      "train loss:0.6170407302119189\n",
      "train loss:0.6149960551505715\n",
      "train loss:0.7838491431863966\n",
      "train loss:0.5263232940905007\n",
      "train loss:0.6136122301342735\n",
      "train loss:0.6136209481603766\n",
      "train loss:0.6254911210052925\n",
      "train loss:0.4094309180376343\n",
      "train loss:0.4087095171658923\n",
      "train loss:0.39367922109484327\n",
      "train loss:0.5101639072137378\n",
      "train loss:0.7305520718186557\n",
      "train loss:0.9763352278492787\n",
      "train loss:0.6185755717620757\n",
      "train loss:0.6195787333996804\n",
      "train loss:0.7115460792755145\n",
      "train loss:0.7969568616023434\n",
      "train loss:0.5208963769865094\n",
      "train loss:0.5262917243996469\n",
      "train loss:0.6045691439409032\n",
      "train loss:0.6165736247004755\n",
      "train loss:0.5413780783331903\n",
      "train loss:0.6109569789210239\n",
      "train loss:0.37420690612512797\n",
      "train loss:0.5270656611314871\n",
      "train loss:0.4238221227932155\n",
      "train loss:0.4129232045296048\n",
      "train loss:0.8076745295101526\n",
      "train loss:0.5125517964941675\n",
      "train loss:0.620234596525864\n",
      "train loss:0.6282128335180234\n",
      "train loss:0.5035059003991236\n",
      "train loss:0.24158368131327843\n",
      "train loss:0.6556472890354044\n",
      "train loss:0.76367552813195\n",
      "train loss:0.6306142508920476\n",
      "train loss:0.508246844361825\n",
      "train loss:0.9755008587861601\n",
      "train loss:0.622549993335688\n",
      "train loss:0.5010192729462505\n",
      "train loss:0.6993242230965742\n",
      "train loss:0.601481544768552\n",
      "train loss:0.5222962540544552\n",
      "train loss:0.6929235257268654\n",
      "train loss:0.5394190951510522\n",
      "train loss:0.5240406921330226\n",
      "train loss:0.5421760431225132\n",
      "train loss:0.6261344111942899\n",
      "train loss:0.605915321909874\n",
      "train loss:0.5986421353728921\n",
      "train loss:0.45632363446624835\n",
      "train loss:0.7784305590973561\n",
      "train loss:0.6215128417138212\n",
      "train loss:0.6894149069694374\n",
      "train loss:0.6927904097308666\n",
      "train loss:0.8435252460760131\n",
      "train loss:0.8148757314304467\n",
      "train loss:0.7417846910205833\n",
      "train loss:0.685287619165215\n",
      "train loss:0.62970971045591\n",
      "train loss:0.5438293095904031\n",
      "train loss:0.5826655343835373\n",
      "train loss:0.6352690582222105\n",
      "train loss:0.5886672654574381\n",
      "train loss:0.6302630075497666\n",
      "train loss:0.5171985802024466\n",
      "train loss:0.5558246006059171\n",
      "train loss:0.609402222001141\n",
      "train loss:0.6161255959522115\n",
      "train loss:0.6748398999856812\n",
      "train loss:0.6886753791933635\n",
      "train loss:0.5322929653328303\n",
      "train loss:0.522551216108306\n",
      "train loss:0.8901318207401421\n",
      "train loss:0.7094824330178171\n",
      "train loss:0.6043453695321153\n",
      "train loss:0.6104185429560485\n",
      "train loss:0.6917927498219966\n",
      "train loss:0.6820841195258228\n",
      "train loss:0.7655246620474867\n",
      "train loss:0.6908253962467835\n",
      "train loss:0.6129902927639289\n",
      "train loss:0.6175846442080725\n",
      "train loss:0.6205980367238338\n",
      "train loss:0.5505585084293779\n",
      "train loss:0.5553456530393207\n",
      "train loss:0.5538896682359592\n",
      "train loss:0.4817093775643896\n",
      "train loss:0.7315357171523637\n",
      "train loss:0.5429509691521104\n",
      "train loss:0.44005894985732497\n",
      "train loss:0.6163343661004419\n",
      "train loss:0.4166840888796057\n",
      "train loss:0.3016387110162292\n",
      "train loss:0.48969381878287166\n",
      "train loss:0.5056890385283729\n",
      "train loss:0.3539492329314528\n",
      "train loss:0.48734465378087133\n",
      "train loss:0.8111714191537281\n",
      "train loss:0.3318336962259963\n",
      "train loss:0.6672631344101195\n",
      "train loss:0.674300068018254\n",
      "train loss:0.6963636885445876\n",
      "train loss:0.6621366688594849\n",
      "train loss:0.6370118129896515\n",
      "train loss:0.49875635452926526\n",
      "train loss:0.613089440754681\n",
      "train loss:0.49638005502153104\n",
      "train loss:0.3960998993502448\n",
      "train loss:0.8179908375463597\n",
      "train loss:0.5152377492783209\n",
      "train loss:0.7100878575846122\n",
      "train loss:0.6097567613659569\n",
      "train loss:0.6159645730340008\n",
      "train loss:0.6096591620579827\n",
      "train loss:0.4762067844635152\n",
      "train loss:0.5454237534088531\n",
      "train loss:0.4626047563365109\n",
      "train loss:0.6862837749204299\n",
      "train loss:0.6201640633364383\n",
      "train loss:0.525066919706392\n",
      "train loss:0.8778482749728056\n",
      "train loss:0.8517344786808445\n",
      "train loss:0.46362614296900395\n",
      "train loss:0.4509600824423199\n",
      "train loss:0.6216510983085186\n",
      "train loss:0.5315797205302044\n",
      "train loss:0.45227301832792033\n",
      "train loss:0.5228547192403545\n",
      "train loss:0.42231091720435865\n",
      "train loss:0.7209400600656781\n",
      "train loss:0.9130063565293002\n",
      "train loss:0.8208751961050025\n",
      "train loss:0.5194981788546553\n",
      "train loss:0.615185490123896\n",
      "train loss:0.5222927136629848\n",
      "train loss:0.5129004713178594\n",
      "train loss:0.6157936423855792\n",
      "train loss:0.5113572083714877\n",
      "train loss:0.7815527394721039\n",
      "train loss:0.7780159611998296\n",
      "train loss:0.5393015295683478\n",
      "train loss:0.6217708312513596\n",
      "train loss:0.6827814659220023\n",
      "train loss:0.533141301905722\n",
      "train loss:0.5226927336913464\n",
      "train loss:0.4536400483838533\n",
      "train loss:0.76085007110332\n",
      "train loss:0.4477981314202625\n",
      "train loss:0.6173215020967362\n",
      "train loss:0.606110692675774\n",
      "train loss:0.6155765978380596\n",
      "train loss:0.6086047805450381\n",
      "train loss:0.5212393107278167\n",
      "train loss:0.4155774899451982\n",
      "train loss:0.2943567639015409\n",
      "train loss:0.8174623176584458\n",
      "train loss:0.6054079446982742\n",
      "train loss:0.6009891852513657\n",
      "train loss:0.2518930662479361\n",
      "train loss:0.6378428299463088\n",
      "train loss:0.6311970307997904\n",
      "train loss:0.6268375679899778\n",
      "train loss:0.5023133544326563\n",
      "train loss:0.5088992041639481\n",
      "train loss:0.8694532780453443\n",
      "train loss:0.6327968580631192\n",
      "train loss:0.6208244306995491\n",
      "train loss:0.8174000706518033\n",
      "train loss:0.8028111311303672\n",
      "train loss:0.6135117736495219\n",
      "train loss:0.5392737008371493\n",
      "train loss:0.664904635339488\n",
      "train loss:0.42984983809521615\n",
      "train loss:0.6238667092596756\n",
      "train loss:0.43157095750886193\n",
      "train loss:0.4812591770845035\n",
      "train loss:0.60665029225082\n",
      "train loss:0.5431374836514702\n",
      "train loss:0.7576987104692499\n",
      "train loss:0.618442484323648\n",
      "train loss:0.5278995810469286\n",
      "train loss:0.4310394667507539\n",
      "train loss:0.32034193990597026\n",
      "train loss:0.6877874120774239\n",
      "train loss:0.3886172271750475\n",
      "train loss:0.24288978674460265\n",
      "train loss:0.9011315915809958\n",
      "train loss:0.9212768730327723\n",
      "train loss:0.3619696999865901\n",
      "train loss:0.636461950847125\n",
      "train loss:0.6252398083453661\n",
      "train loss:0.48728609383637\n",
      "train loss:0.7475952898292398\n",
      "train loss:0.5112326026137882\n",
      "train loss:0.6062317997380852\n",
      "train loss:0.615133417125145\n",
      "train loss:1.0250393970921077\n",
      "train loss:0.6003412497478375\n",
      "train loss:0.6234949119004416\n",
      "train loss:0.45458699149894144\n",
      "train loss:0.6185127975581487\n",
      "train loss:0.45389028677090454\n",
      "train loss:0.8288954887559374\n",
      "train loss:0.3996808485867732\n",
      "train loss:0.7359385271753777\n",
      "train loss:0.6081369854654236\n",
      "train loss:0.7468423171493035\n",
      "train loss:0.5549039792387864\n",
      "train loss:0.5330842404216135\n",
      "train loss:0.5451771844834534\n",
      "train loss:0.4625809493993671\n",
      "train loss:0.6907904461973611\n",
      "train loss:0.5294020232344891\n",
      "train loss:0.5215683220584071\n",
      "train loss:0.6182760446339957\n",
      "train loss:0.7149507986409219\n",
      "train loss:0.7164889232734735\n",
      "train loss:0.4027994603288958\n",
      "train loss:0.7040912170917873\n",
      "train loss:0.5083330229944591\n",
      "train loss:0.7248974897600335\n",
      "train loss:0.40479270805354073\n",
      "train loss:0.5033171233351721\n",
      "train loss:0.8305456674153687\n",
      "train loss:0.5153506860750705\n",
      "train loss:0.5169556994735588\n",
      "train loss:0.40267561190291223\n",
      "train loss:0.515569676970416\n",
      "train loss:0.7044170326221284\n",
      "train loss:0.5057289116801531\n",
      "train loss:0.609531059441778\n",
      "train loss:0.5103820432692951\n",
      "train loss:0.7323766112454224\n",
      "train loss:0.8147256458166947\n",
      "train loss:0.5146658988796753\n",
      "train loss:0.4216525287428315\n",
      "train loss:0.6079096448918209\n",
      "train loss:0.6163717139885734\n",
      "train loss:0.7009178058732979\n",
      "train loss:0.5253745440501536\n",
      "train loss:0.7058856133922387\n",
      "train loss:0.7845533915498438\n",
      "train loss:0.6840460738023044\n",
      "train loss:0.37527278074287335\n",
      "train loss:0.5372218759344048\n",
      "train loss:0.6866796176239737\n",
      "train loss:0.6023376545482279\n",
      "train loss:0.4070129812541202\n",
      "train loss:0.6807082110399233\n",
      "train loss:0.6097505546438425\n",
      "train loss:0.6950462015506048\n",
      "train loss:0.6115111732601617\n",
      "train loss:0.5225093729093044\n",
      "train loss:0.5292195619554458\n",
      "train loss:0.4135772865272916\n",
      "train loss:0.4069166064685651\n",
      "train loss:0.6094463188803267\n",
      "train loss:0.8333395544316611\n",
      "train loss:0.5122480742639592\n",
      "train loss:0.5085475372349426\n",
      "train loss:0.37587261660853927\n",
      "train loss:0.737951572975251\n",
      "train loss:0.4834365283225635\n",
      "train loss:0.5010478372304326\n",
      "train loss:0.4972125125625408\n",
      "train loss:0.9033976906985156\n",
      "train loss:0.8632449719099485\n",
      "train loss:0.5114533986521069\n",
      "train loss:0.7295027403717802\n",
      "train loss:0.5165917123869758\n",
      "train loss:0.5177146873856527\n",
      "train loss:0.5244161772594678\n",
      "train loss:0.6782637639294895\n",
      "train loss:0.446928053561553\n",
      "train loss:0.6132652971056102\n",
      "train loss:0.5273745866548548\n",
      "train loss:0.5199648320436268\n",
      "train loss:0.33455703578708035\n",
      "train loss:0.5060674101161615\n",
      "train loss:0.5070829233126773\n",
      "train loss:0.5061115070348841\n",
      "train loss:0.9411441618524211\n",
      "train loss:0.72211003640777\n",
      "train loss:0.515916879071581\n",
      "train loss:0.6107944437782056\n",
      "train loss:0.39089037391470727\n",
      "train loss:0.5001100678241466\n",
      "train loss:0.5050395242890351\n",
      "train loss:0.37073510360640066\n",
      "train loss:0.7473448315521772\n",
      "train loss:0.7478691643379676\n",
      "train loss:0.38063502336770194\n",
      "train loss:0.7282321526530023\n",
      "train loss:0.2569627960443323\n",
      "train loss:0.49784170502906183\n",
      "train loss:0.8513405077797565\n",
      "train loss:0.5075535538581688\n",
      "train loss:0.3868665319733867\n",
      "train loss:0.49945521712677915\n",
      "train loss:0.6275927595775599\n",
      "train loss:0.6393498099998174\n",
      "train loss:0.47464860015667687\n",
      "train loss:0.5063864950152928\n",
      "train loss:0.3880158559568191\n",
      "train loss:0.7112389502043914\n",
      "train loss:0.37941109033303666\n",
      "train loss:0.48381698821349\n",
      "train loss:0.47473183985449685\n",
      "train loss:0.7212008693781633\n",
      "train loss:0.7380338982651059\n",
      "train loss:0.39284061157696676\n",
      "train loss:0.37894176142301994\n",
      "train loss:0.7043822527114253\n",
      "train loss:0.6100755895379943\n",
      "train loss:0.7183135064891116\n",
      "train loss:0.6174409281727693\n",
      "train loss:0.4991184399835529\n",
      "train loss:0.4986420029798011\n",
      "train loss:0.509051603510543\n",
      "train loss:0.488912971362387\n",
      "train loss:0.5035478979438887\n",
      "train loss:0.7306790386570224\n",
      "train loss:0.8188527296917668\n",
      "train loss:0.6921551211959623\n",
      "train loss:0.7031004272286443\n",
      "train loss:0.5905737840316165\n",
      "train loss:0.6065490695898206\n",
      "train loss:0.6101036212059187\n",
      "train loss:0.6180234536776275\n",
      "train loss:0.5439593647955323\n",
      "train loss:0.530763294337489\n",
      "train loss:0.5389518324614339\n",
      "train loss:0.5293447033861596\n",
      "train loss:0.45786130675871456\n",
      "train loss:0.6936948585168798\n",
      "train loss:0.7662065032172836\n",
      "train loss:0.7721070954952071\n",
      "train loss:0.6769326973762407\n",
      "train loss:0.44403791801276665\n",
      "train loss:0.4593310307981935\n",
      "train loss:0.7088815069635032\n",
      "train loss:0.6012283778138587\n",
      "train loss:0.6103750671846185\n",
      "train loss:0.7072554363132129\n",
      "train loss:0.4364468685908608\n",
      "train loss:0.7739879641152061\n",
      "train loss:0.5215302850081127\n",
      "train loss:0.5182237508180605\n",
      "train loss:0.5123727537038005\n",
      "train loss:0.5843503621067015\n",
      "train loss:0.5969922279396327\n",
      "train loss:0.8849860596215722\n",
      "train loss:0.42218361031319773\n",
      "train loss:0.5016216553534953\n",
      "train loss:0.30446772748228085\n",
      "train loss:0.8338525969649362\n",
      "train loss:0.40398116845279713\n",
      "train loss:0.7308750431984121\n",
      "train loss:0.39163632646103874\n",
      "train loss:0.5093476245372186\n",
      "train loss:0.37697901620419494\n",
      "train loss:0.3745680150014558\n",
      "train loss:0.20911819353201472\n",
      "train loss:0.5104461982665545\n",
      "train loss:0.6383928950422612\n",
      "train loss:1.1115919170025357\n",
      "train loss:0.3615831426070414\n",
      "train loss:0.7902949319349457\n",
      "train loss:0.8770333784412696\n",
      "train loss:0.7125960228572904\n",
      "train loss:0.7951895076145593\n",
      "train loss:0.4244369135082467\n",
      "train loss:0.523170983981286\n",
      "train loss:0.6082316449894004\n",
      "train loss:0.6062578638723073\n",
      "train loss:0.6254017068752675\n",
      "train loss:0.6910623196650321\n",
      "train loss:0.6281491153265428\n",
      "train loss:0.7327923231916313\n",
      "train loss:0.624877771282647\n",
      "train loss:0.5727641173897376\n",
      "train loss:0.682940728324142\n",
      "train loss:0.6279277016868254\n",
      "train loss:0.6790418233609234\n",
      "train loss:0.5209283452983661\n",
      "train loss:0.611693933941689\n",
      "train loss:0.5011272716980816\n",
      "train loss:0.5277380978864813\n",
      "train loss:0.6181284070140001\n",
      "train loss:0.6126774045847135\n",
      "train loss:0.5282561939841903\n",
      "train loss:0.584353193162034\n",
      "train loss:0.39371175654358026\n",
      "train loss:0.7017363651956895\n",
      "train loss:1.0552349951847682\n",
      "train loss:0.39697284332734534\n",
      "train loss:0.5973302773772803\n",
      "train loss:0.632736824609951\n",
      "train loss:0.7139771431371762\n",
      "train loss:0.8182408289697781\n",
      "train loss:0.5083549304578648\n",
      "train loss:0.7041189889090298\n",
      "train loss:0.6920682890844827\n",
      "train loss:0.6104286036777528\n",
      "train loss:0.44394045397175635\n",
      "train loss:0.7676410730743316\n",
      "train loss:0.671024627460062\n",
      "train loss:0.6768930218239659\n",
      "train loss:0.48417582180684243\n",
      "train loss:0.7237100900759124\n",
      "train loss:0.6756202112896001\n",
      "train loss:0.6127895329795467\n",
      "train loss:0.7082527194243265\n",
      "train loss:0.7027405343555624\n",
      "train loss:0.557375841256306\n",
      "train loss:0.6188986975118242\n",
      "train loss:0.5619358280269758\n",
      "train loss:0.6677760623530763\n",
      "train loss:0.5666764546121476\n",
      "train loss:0.557146567328586\n",
      "train loss:0.46950206519120663\n",
      "train loss:0.5229246530116711\n",
      "train loss:0.7792742765036108\n",
      "train loss:0.6256668518101418\n",
      "train loss:0.6923723893802426\n",
      "train loss:0.6778163638473435\n",
      "train loss:0.4178926581789392\n",
      "train loss:0.5973268703201212\n",
      "train loss:0.6215681048451728\n",
      "train loss:0.5027089489267024\n",
      "train loss:0.8333826170730939\n",
      "train loss:0.39373931830623043\n",
      "train loss:0.8087534308094128\n",
      "train loss:0.5152543836946505\n",
      "train loss:0.7172223264736001\n",
      "train loss:0.7957987208814361\n",
      "train loss:0.6188390935066506\n",
      "train loss:0.4317438633177342\n",
      "train loss:0.43009818686430384\n",
      "train loss:0.6111150004644199\n",
      "train loss:0.7719288789567108\n",
      "train loss:0.6540400786506627\n",
      "train loss:0.7674919823193213\n",
      "train loss:0.6596932930127151\n",
      "train loss:0.5526125493945269\n",
      "train loss:0.672149240377259\n",
      "train loss:0.6602548055446646\n",
      "train loss:0.660550617080626\n",
      "train loss:0.7442927513965387\n",
      "train loss:0.5629523664433975\n",
      "train loss:0.6225481273680994\n",
      "train loss:0.556097508070594\n",
      "train loss:0.5636523603966158\n",
      "train loss:0.5811920997649331\n",
      "train loss:0.5958162536055792\n",
      "train loss:0.47372116209880766\n",
      "train loss:0.549773398606092\n",
      "train loss:0.7623010462878868\n",
      "train loss:0.43902795260181715\n",
      "train loss:0.6254962693859596\n",
      "train loss:0.6897426380039722\n",
      "train loss:0.6045034816059494\n",
      "train loss:0.5148697428192889\n",
      "train loss:0.7389843031757791\n",
      "train loss:0.7010851581470959\n",
      "train loss:0.6125164207773193\n",
      "train loss:0.7085449053570871\n",
      "train loss:0.7018240505929706\n",
      "train loss:0.49806664409134793\n",
      "train loss:0.6978385885374181\n",
      "train loss:0.5283665103666972\n",
      "train loss:0.4526945358376837\n",
      "train loss:0.7047745832796513\n",
      "train loss:0.615355789455162\n",
      "train loss:0.6787066321632371\n",
      "train loss:0.5334704966524625\n",
      "train loss:0.66381806616772\n",
      "train loss:0.5983528921264114\n",
      "train loss:0.700969694866888\n",
      "train loss:0.4612648109362696\n",
      "train loss:0.4479419368229104\n",
      "train loss:0.4331459454336379\n",
      "train loss:0.4273516246251717\n",
      "train loss:0.4045154531941046\n",
      "train loss:0.5327791101893669\n",
      "train loss:0.4904009725360227\n",
      "train loss:0.7720452017266678\n",
      "train loss:0.49782579290446305\n",
      "train loss:0.49291941720086313\n",
      "train loss:0.9041497897794564\n",
      "train loss:0.9644677322098575\n",
      "train loss:0.6393559667745962\n",
      "train loss:0.39118522821533863\n",
      "train loss:0.618002520285599\n",
      "train loss:0.9972434223489464\n",
      "train loss:0.6932330995667941\n",
      "train loss:0.5993619950496111\n",
      "train loss:0.6772876960912029\n",
      "train loss:0.5577137204017698\n",
      "train loss:0.7902175664833957\n",
      "train loss:0.5775641034522014\n",
      "train loss:0.5778702496487435\n",
      "train loss:0.6308681468421375\n",
      "train loss:0.7210678158207312\n",
      "train loss:0.5848459106606975\n",
      "train loss:0.5889825670349434\n",
      "train loss:0.528864086459727\n",
      "train loss:0.5728944163362222\n",
      "train loss:0.6312481061377515\n",
      "train loss:0.6763797551360186\n",
      "train loss:0.7327521358602247\n",
      "train loss:0.6191978994231877\n",
      "train loss:0.46144072353637056\n",
      "train loss:0.5158327367683396\n",
      "train loss:0.5932850486775789\n",
      "train loss:0.5875759014566373\n",
      "train loss:0.5993864678004016\n",
      "train loss:0.6184755447463417\n",
      "train loss:0.7225856962607513\n",
      "train loss:0.6107644158858376\n",
      "train loss:0.6994117248208396\n",
      "train loss:0.4755924511685358\n",
      "train loss:0.9010499233845802\n",
      "train loss:0.6153592374301617\n",
      "train loss:0.5895727466402879\n",
      "train loss:0.9672390902367016\n",
      "train loss:0.7668381868608569\n",
      "train loss:0.7263400006164091\n",
      "train loss:0.5490923672642833\n",
      "train loss:0.6745685049766907\n",
      "train loss:0.6236200457014636\n",
      "train loss:0.5778563885265793\n",
      "train loss:0.6130178420962631\n",
      "train loss:0.6874889656502263\n",
      "train loss:0.5911119416836446\n",
      "train loss:0.6344351301780188\n",
      "train loss:0.6393716214336181\n",
      "train loss:0.677241048220999\n",
      "train loss:0.5784916232354115\n",
      "train loss:0.65970222497661\n",
      "train loss:0.5714245737688547\n",
      "train loss:0.6294211685261196\n",
      "train loss:0.6030073573203054\n",
      "train loss:0.6066561604402707\n",
      "train loss:0.6860932015051391\n",
      "train loss:0.5431419112973086\n",
      "train loss:0.6993536726416488\n",
      "train loss:0.6181534090481302\n",
      "train loss:0.5084978828475564\n",
      "train loss:0.44459271894279395\n",
      "train loss:0.6723433632468148\n",
      "train loss:0.5870811336202165\n",
      "train loss:0.5020711606629467\n",
      "train loss:0.5176321514180096\n",
      "train loss:0.505940429554687\n",
      "train loss:0.6297828641833035\n",
      "train loss:0.8315062865611711\n",
      "train loss:0.5175936245973178\n",
      "train loss:0.6237121190224283\n",
      "train loss:0.4893572836490433\n",
      "train loss:0.4924137826861311\n",
      "train loss:0.6296380953112972\n",
      "train loss:0.8446834586324824\n",
      "train loss:0.628608394539774\n",
      "train loss:0.5727796887297171\n",
      "train loss:0.7932127039614688\n",
      "train loss:0.6906115791528259\n",
      "train loss:0.5128257375951659\n",
      "train loss:0.5871669077695724\n",
      "train loss:0.3984256580378131\n",
      "train loss:0.5921524787862531\n",
      "train loss:0.5375122293308919\n",
      "train loss:0.5434973259043244\n",
      "train loss:0.5982287856973451\n",
      "train loss:0.6227985986838969\n",
      "train loss:0.426338207305765\n",
      "train loss:0.6368587089617781\n",
      "train loss:0.7117614207367666\n",
      "train loss:0.32069156285835365\n",
      "train loss:0.26555554648165797\n",
      "train loss:0.7132122290880379\n",
      "train loss:0.5064704749006221\n",
      "train loss:0.6374706874846585\n",
      "train loss:0.48689729433831774\n",
      "train loss:0.64533399334443\n",
      "train loss:0.48278747093970403\n",
      "train loss:0.8960305905795029\n",
      "train loss:0.2376167902204483\n",
      "train loss:0.8944268436751942\n",
      "train loss:0.9407777707035587\n",
      "train loss:0.5109976317879671\n",
      "train loss:0.6078075217729485\n",
      "train loss:0.4452908354808631\n",
      "train loss:0.6850383667515373\n",
      "train loss:0.7729608330360616\n",
      "train loss:0.519852118824042\n",
      "train loss:0.7299239102908744\n",
      "train loss:0.6763129105967979\n",
      "train loss:0.6202036968307623\n",
      "train loss:0.5563140356483658\n",
      "train loss:0.6362702327832261\n",
      "train loss:0.6150006163810561\n",
      "train loss:0.6146530460543363\n",
      "train loss:0.6154943069211016\n",
      "train loss:0.7726811575896033\n",
      "train loss:0.5103964812922512\n",
      "train loss:0.4925570350355872\n",
      "train loss:0.7228218980645146\n",
      "train loss:0.5614558136675553\n",
      "train loss:0.5478059872678602\n",
      "train loss:0.38319331882204133\n",
      "train loss:0.5402538744322785\n",
      "train loss:0.5161811432925932\n",
      "train loss:0.7253085772811737\n",
      "train loss:0.5872636096223101\n",
      "train loss:0.3677170718083055\n",
      "train loss:0.5039831868212676\n",
      "train loss:0.8520640394699598\n",
      "train loss:0.6272109861692062\n",
      "train loss:0.428085931737304\n",
      "train loss:0.22258421278378054\n",
      "train loss:0.8488839706781632\n",
      "train loss:0.45593418789522655\n",
      "train loss:0.3768726064157144\n",
      "train loss:0.3527041127920244\n",
      "train loss:0.6479771614226186\n",
      "train loss:0.7786307848050793\n",
      "train loss:0.60243456592516\n",
      "train loss:0.5122646659193424\n",
      "train loss:0.6353990695776034\n",
      "train loss:0.4934961604801037\n",
      "train loss:0.753049563440585\n",
      "train loss:0.5163342333041927\n",
      "train loss:0.5974435094213918\n",
      "train loss:0.7008722845599177\n",
      "train loss:0.6671590589994102\n",
      "train loss:0.7706433546995555\n",
      "train loss:0.6057288442835307\n",
      "train loss:0.6703019737046889\n",
      "train loss:0.49784681003680004\n",
      "train loss:0.4232586526151551\n",
      "train loss:0.49821262696173\n",
      "train loss:0.4726633142751055\n",
      "train loss:0.6258842922963224\n",
      "train loss:0.4578642258610235\n",
      "train loss:0.528289717236955\n",
      "train loss:0.7066288666313295\n",
      "train loss:0.7080788154630001\n",
      "train loss:0.4946596642580562\n",
      "train loss:0.6188927174780078\n",
      "train loss:0.6177669595770222\n",
      "train loss:0.39591640347426693\n",
      "train loss:0.3897929488053028\n",
      "train loss:0.6147173912228202\n",
      "train loss:0.6309418554300652\n",
      "train loss:0.7451276286768601\n",
      "train loss:0.6227749905443342\n",
      "train loss:0.7039965759456337\n",
      "train loss:0.9598374876330482\n",
      "train loss:0.4965761968951716\n",
      "train loss:0.7002009739690578\n",
      "train loss:0.948092329753187\n",
      "train loss:0.609282906654354\n",
      "train loss:0.4047922556571394\n",
      "train loss:0.8690972655807382\n",
      "train loss:0.6127599952481171\n",
      "train loss:0.45772816568175745\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6617647058823529\n"
     ]
    }
   ],
   "source": [
    "from mymethod.neural_network import *\n",
    "from common.trainer import Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7da5f3e7-b13a-46b0-9714-9aebcfb2f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), #필터 수 조절해서 성능 조절\n",
    "                 conv_param_1 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=10):\n",
    "        # 가중치 초기화===========\n",
    "        # 각 층의 뉴런 하나당 앞 층의 몇 개 뉴런과 연결되는가（TODO: 자동 계산되게 바꿀 것）\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLU를 사용할 때의 권장 초깃값\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = wight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = wight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13c771a3-5cfb-4b73-b64a-77e4e611e9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.1926423634401764\n",
      "=== epoch:1, train acc:0.71, test acc:0.69 ===\n",
      "train loss:1.8036818621599462\n",
      "train loss:1.8409563547465617\n",
      "train loss:1.9970071750397653\n",
      "train loss:1.8478479147870235\n",
      "train loss:1.5068419171513934\n",
      "train loss:1.4590959227558167\n",
      "train loss:1.4983113626547984\n",
      "train loss:1.5991708063229875\n",
      "train loss:2.1358777962600706\n",
      "train loss:1.4895395286648687\n",
      "train loss:1.6358821248454933\n",
      "train loss:1.8811459797526893\n",
      "train loss:1.8741239605335551\n",
      "train loss:1.5558697258941554\n",
      "train loss:1.2376692131263007\n",
      "train loss:2.0758130744484222\n",
      "train loss:2.2944552448258713\n",
      "train loss:1.498889848887047\n",
      "train loss:1.703141017022388\n",
      "train loss:1.9034897453749633\n",
      "train loss:1.8248833220369574\n",
      "train loss:1.9287133931875435\n",
      "train loss:1.8595566898612774\n",
      "train loss:1.6282412238864477\n",
      "train loss:2.343416517445358\n",
      "train loss:1.5276564859299533\n",
      "train loss:1.6857717610790435\n",
      "train loss:1.5764725130626596\n",
      "train loss:1.7799459036732284\n",
      "train loss:1.546171409477338\n",
      "train loss:1.9315387987735264\n",
      "train loss:1.6523577991940446\n",
      "train loss:1.8110172418103676\n",
      "train loss:1.4223576376698537\n",
      "train loss:1.5114239914739789\n",
      "train loss:2.101880136571353\n",
      "train loss:1.4786444979178996\n",
      "train loss:2.090807969572473\n",
      "train loss:1.6085968588234387\n",
      "train loss:1.5128648773602245\n",
      "train loss:1.6336633564384073\n",
      "train loss:2.192860570172119\n",
      "train loss:1.8583465512035846\n",
      "train loss:2.105853061852689\n",
      "train loss:1.804146586418112\n",
      "train loss:1.4556989492606038\n",
      "train loss:1.250563398626956\n",
      "train loss:1.4160551788574787\n",
      "train loss:1.8707014272135971\n",
      "train loss:1.7289640927779277\n",
      "train loss:1.9038744982319113\n",
      "train loss:1.6878111279572583\n",
      "train loss:1.4572161880797805\n",
      "train loss:1.5488455565116168\n",
      "train loss:1.282243422290214\n",
      "train loss:1.5367410506259824\n",
      "train loss:1.9926922233959234\n",
      "train loss:1.8480329494778887\n",
      "train loss:1.2184837853667028\n",
      "train loss:1.4844579999281249\n",
      "train loss:1.8587179912002063\n",
      "train loss:1.5726409202536253\n",
      "train loss:1.5685541412400492\n",
      "train loss:1.4892982277846145\n",
      "train loss:1.6259812030409422\n",
      "train loss:1.910222876881265\n",
      "train loss:1.6125022726955927\n",
      "train loss:1.6515840604693506\n",
      "train loss:1.6470319875931185\n",
      "train loss:1.576674950259051\n",
      "train loss:1.6914515453443637\n",
      "train loss:1.0215410350420446\n",
      "train loss:1.30665449405081\n",
      "train loss:1.9161170455167944\n",
      "train loss:1.1600276106686378\n",
      "train loss:1.4189643449624865\n",
      "train loss:1.6155692378320192\n",
      "train loss:1.0761653134820102\n",
      "train loss:1.0405550706222697\n",
      "train loss:2.176038074158085\n",
      "train loss:1.1247476450327505\n",
      "train loss:1.4602011511289443\n",
      "train loss:2.005052579764101\n",
      "train loss:1.9260578568347693\n",
      "train loss:1.762341857352574\n",
      "train loss:1.8310094917770747\n",
      "train loss:1.1734072813408345\n",
      "train loss:1.3351433468979907\n",
      "train loss:1.552000042524884\n",
      "train loss:1.6771953259451298\n",
      "train loss:1.7209361813897668\n",
      "train loss:1.0622424909390817\n",
      "train loss:1.4729568239955553\n",
      "train loss:1.6104406009862813\n",
      "train loss:1.1417413837996968\n",
      "train loss:1.3791371527855347\n",
      "train loss:1.6803414639596916\n",
      "train loss:1.5758757184797028\n",
      "train loss:1.5912150525732602\n",
      "train loss:1.9311866209534891\n",
      "train loss:1.7876119894235085\n",
      "train loss:1.3133782547300976\n",
      "train loss:1.6713424027918875\n",
      "train loss:1.6806523594458802\n",
      "train loss:1.5174121577670097\n",
      "train loss:1.6358758242779334\n",
      "train loss:1.3854809989281576\n",
      "train loss:1.4677206614231735\n",
      "train loss:1.481368880532301\n",
      "train loss:1.235461573607857\n",
      "train loss:1.202191692860747\n",
      "train loss:2.1161117547701034\n",
      "train loss:1.7842043728172787\n",
      "train loss:1.6260052109242302\n",
      "train loss:1.5836921696267443\n",
      "train loss:1.2822645953647152\n",
      "train loss:1.6150344908246042\n",
      "train loss:0.6720625667794803\n",
      "train loss:1.1693865396544527\n",
      "train loss:1.4831363152172714\n",
      "train loss:1.5703257043984409\n",
      "train loss:1.8869072752421492\n",
      "train loss:1.1336320725942663\n",
      "train loss:2.2186513898519875\n",
      "train loss:1.065705001667447\n",
      "train loss:1.7751909282769436\n",
      "train loss:1.9569774095339731\n",
      "train loss:1.4409241725161928\n",
      "train loss:1.5285595027497103\n",
      "train loss:1.8725506828840472\n",
      "train loss:0.6015732625320434\n",
      "train loss:1.0977041902899922\n",
      "train loss:0.9270618177304973\n",
      "train loss:1.685911793353743\n",
      "train loss:1.712029397465042\n",
      "train loss:2.128010705223804\n",
      "train loss:1.500106144854162\n",
      "train loss:0.984022375060355\n",
      "train loss:1.0985051867997855\n",
      "train loss:1.4300430443342198\n",
      "train loss:1.2165571070019396\n",
      "train loss:1.66694384012493\n",
      "train loss:1.4204711506854268\n",
      "train loss:1.2639338269346214\n",
      "train loss:1.757685465352758\n",
      "train loss:1.8592763462347501\n",
      "train loss:1.4875259346369816\n",
      "train loss:1.6865749176274523\n",
      "train loss:1.8756433463056557\n",
      "train loss:1.8153650002693038\n",
      "train loss:1.4402806074804138\n",
      "train loss:2.03955624640795\n",
      "train loss:1.945813319017254\n",
      "train loss:1.725803522087378\n",
      "train loss:1.436456565241769\n",
      "train loss:2.1708369327264614\n",
      "train loss:1.0054658680335686\n",
      "train loss:1.3488379451085404\n",
      "train loss:2.248492663222522\n",
      "train loss:1.2050018623425764\n",
      "train loss:1.0391747911389162\n",
      "train loss:0.905134838533615\n",
      "train loss:1.4947457998220024\n",
      "train loss:1.6388061028894687\n",
      "train loss:1.3178684562951748\n",
      "train loss:1.6561469922977836\n",
      "train loss:1.8250171406284728\n",
      "train loss:1.5157160160795597\n",
      "train loss:1.5032603635877497\n",
      "train loss:1.3478443203901271\n",
      "train loss:1.417704304743807\n",
      "train loss:1.5000860048642126\n",
      "train loss:1.2151655384827522\n",
      "train loss:1.6309935052902893\n",
      "train loss:1.6934241258049934\n",
      "train loss:1.7450902995793691\n",
      "train loss:1.1434310730970871\n",
      "train loss:1.193325844483508\n",
      "train loss:1.695968147266339\n",
      "train loss:1.1393162922230335\n",
      "train loss:1.6585134453217187\n",
      "train loss:1.7052761646879322\n",
      "train loss:1.7105396948356961\n",
      "train loss:1.245567773631695\n",
      "train loss:1.5162288362235783\n",
      "train loss:1.9769411277888236\n",
      "train loss:1.6484399546076414\n",
      "train loss:1.514108261134191\n",
      "train loss:1.5808664057421367\n",
      "train loss:1.3085836469978194\n",
      "train loss:1.6615416936103256\n",
      "train loss:1.233490301501651\n",
      "train loss:1.0428998129369904\n",
      "train loss:2.065830152413689\n",
      "train loss:1.5366141747326754\n",
      "train loss:1.152813036657594\n",
      "train loss:1.4839364614474662\n",
      "train loss:1.5857003331541797\n",
      "train loss:1.4598659725773764\n",
      "train loss:1.9659892264971444\n",
      "train loss:1.4959073767458464\n",
      "train loss:1.5361396132184664\n",
      "train loss:1.5819304939931393\n",
      "train loss:1.2529311857136143\n",
      "train loss:1.6346383780780571\n",
      "train loss:1.2134606865486468\n",
      "train loss:2.245462750841906\n",
      "train loss:1.5156880941270656\n",
      "train loss:1.319572252346333\n",
      "train loss:1.8946965505376625\n",
      "train loss:1.7773590052853911\n",
      "train loss:1.4913525063255606\n",
      "train loss:0.9861804098960247\n",
      "train loss:1.4875532179749176\n",
      "train loss:1.6078049481442807\n",
      "train loss:1.033897369201744\n",
      "train loss:1.651665635839692\n",
      "train loss:1.7635626930056127\n",
      "train loss:1.5074944103352883\n",
      "train loss:1.294591265135346\n",
      "train loss:1.4707992098867197\n",
      "train loss:1.5902516761508394\n",
      "train loss:1.6925481400990652\n",
      "train loss:1.5115764156424258\n",
      "train loss:1.911588135809718\n",
      "train loss:1.6362418804016776\n",
      "train loss:1.9796215615079187\n",
      "train loss:1.5965180832911468\n",
      "train loss:1.5129912887956034\n",
      "train loss:1.4955389152852745\n",
      "train loss:1.3525852941884182\n",
      "train loss:1.2451527636479822\n",
      "train loss:1.490763395042164\n",
      "train loss:1.5319824121075887\n",
      "train loss:1.377459011336213\n",
      "train loss:1.515445305121084\n",
      "train loss:1.2168680325715289\n",
      "train loss:1.3820809718708051\n",
      "train loss:1.922399520258423\n",
      "train loss:1.4102297831991444\n",
      "train loss:1.2914403469490057\n",
      "train loss:1.704662935784631\n",
      "train loss:1.683467056350823\n",
      "train loss:1.4194443893812878\n",
      "train loss:1.2663993492525343\n",
      "train loss:1.8861300992875685\n",
      "train loss:2.117082236967616\n",
      "train loss:1.9968629539383786\n",
      "train loss:1.171166033001667\n",
      "train loss:1.4273564851590432\n",
      "train loss:1.906055016384603\n",
      "train loss:1.4858200477807268\n",
      "train loss:1.4248321692910069\n",
      "train loss:1.4196746609138726\n",
      "train loss:2.0616631138833195\n",
      "train loss:1.8777889413422577\n",
      "train loss:1.647959072507613\n",
      "train loss:2.0990394570496824\n",
      "train loss:1.1856272611201755\n",
      "train loss:1.660811237341121\n",
      "train loss:0.9761423036305809\n",
      "train loss:1.9145159836060102\n",
      "train loss:0.910063004900109\n",
      "train loss:1.120011153612721\n",
      "train loss:1.0482033691418127\n",
      "train loss:1.79026709897263\n",
      "train loss:1.3209905054402487\n",
      "train loss:1.066687390055487\n",
      "train loss:1.7239438691286768\n",
      "train loss:1.166774074604768\n",
      "train loss:1.5630729900888458\n",
      "train loss:1.589798362799424\n",
      "train loss:1.8044721518111029\n",
      "train loss:1.5589247617419613\n",
      "train loss:1.256679880265383\n",
      "train loss:0.7625418397088878\n",
      "train loss:1.7784937546603181\n",
      "train loss:1.9723604816627642\n",
      "train loss:1.090601563493868\n",
      "train loss:1.4048541205897978\n",
      "train loss:1.299638455661525\n",
      "train loss:1.5778626059020373\n",
      "train loss:1.7052976910853066\n",
      "train loss:1.4171557465228786\n",
      "train loss:1.5914577765896403\n",
      "train loss:1.1771867319502503\n",
      "train loss:1.4031369284581974\n",
      "train loss:1.4700142302112713\n",
      "train loss:1.5943139079699944\n",
      "train loss:1.4345954908014256\n",
      "train loss:1.5325506467548375\n",
      "train loss:1.4634067301885967\n",
      "train loss:1.1611987408977167\n",
      "train loss:1.291706154079656\n",
      "train loss:0.878128754468975\n",
      "train loss:1.6505803127647218\n",
      "train loss:1.4510806673988086\n",
      "train loss:1.3646601014637378\n",
      "train loss:1.526575603365873\n",
      "train loss:1.492774873169664\n",
      "train loss:1.4287870231677242\n",
      "train loss:1.5845691396071067\n",
      "train loss:1.4954032051036086\n",
      "train loss:1.4981103896125654\n",
      "train loss:0.9783016227422403\n",
      "train loss:1.3576368217052397\n",
      "train loss:1.739019911585998\n",
      "train loss:1.1719389478467426\n",
      "train loss:1.4324081927675476\n",
      "train loss:1.2651731956898296\n",
      "train loss:0.6674233745607422\n",
      "train loss:1.634177287269054\n",
      "train loss:1.2953020766405134\n",
      "train loss:1.4019973207465708\n",
      "train loss:1.6770329186242101\n",
      "train loss:1.3337227709141497\n",
      "train loss:1.5624865429344246\n",
      "train loss:1.3176439415462409\n",
      "train loss:1.3611313509205445\n",
      "train loss:1.1272522236878584\n",
      "train loss:0.9792135665392504\n",
      "train loss:1.4349896524843566\n",
      "train loss:1.6225006070997352\n",
      "train loss:1.5219443831079433\n",
      "train loss:2.0665853269856926\n",
      "train loss:1.2412109846577484\n",
      "train loss:1.5954616446604084\n",
      "train loss:1.3287165136428134\n",
      "train loss:1.186347233147458\n",
      "train loss:1.4155130814981327\n",
      "train loss:1.6806469183678208\n",
      "train loss:1.1417392778179538\n",
      "train loss:1.3965305680554825\n",
      "train loss:1.3411832515754092\n",
      "train loss:1.49606842931223\n",
      "train loss:1.4487796210572612\n",
      "train loss:1.4612767517592236\n",
      "train loss:1.3363966034177959\n",
      "train loss:1.3700065493467595\n",
      "train loss:1.772698825981994\n",
      "train loss:1.6012451887815373\n",
      "train loss:1.7865063070902405\n",
      "train loss:1.591072984965123\n",
      "train loss:1.9469013838318634\n",
      "train loss:1.1408266155960374\n",
      "train loss:1.5122004923467438\n",
      "train loss:1.1956792449866112\n",
      "train loss:1.4777425078325348\n",
      "train loss:1.3782669650693598\n",
      "train loss:1.6643987675569627\n",
      "train loss:1.4213562020133013\n",
      "train loss:1.2919964869074214\n",
      "train loss:1.701510914201997\n",
      "train loss:1.7027981995720591\n",
      "train loss:1.8100052452639037\n",
      "train loss:1.3756243945124247\n",
      "train loss:1.8140912475294186\n",
      "train loss:1.3129551818421608\n",
      "train loss:1.6081228950119066\n",
      "train loss:1.3330132395694887\n",
      "train loss:1.2697337970756934\n",
      "train loss:1.5387276027026258\n",
      "train loss:1.523962023990962\n",
      "train loss:1.2809205147651925\n",
      "train loss:1.3124355869134574\n",
      "train loss:1.922726089391388\n",
      "train loss:1.5301436304714726\n",
      "train loss:1.4486432026944336\n",
      "train loss:1.5902011716196442\n",
      "train loss:1.3574742359678411\n",
      "train loss:1.2490833680680058\n",
      "train loss:1.2428573435198058\n",
      "train loss:1.583022190589221\n",
      "train loss:1.1781069146166037\n",
      "train loss:1.7990119086592813\n",
      "train loss:1.7967221825264494\n",
      "train loss:1.4026187499989207\n",
      "train loss:1.735992849657983\n",
      "train loss:1.5599121346507943\n",
      "train loss:1.4798099412498864\n",
      "train loss:1.5823181125720374\n",
      "train loss:1.3094996008175577\n",
      "train loss:1.3118927015645718\n",
      "train loss:1.3765660004135436\n",
      "train loss:1.5677018827682967\n",
      "train loss:1.468042728115691\n",
      "train loss:1.4822431493294999\n",
      "train loss:1.5856259567557618\n",
      "train loss:1.4778095546946473\n",
      "train loss:1.1974064971422842\n",
      "train loss:1.2730533516834392\n",
      "train loss:0.7308546618045526\n",
      "train loss:1.758869324396149\n",
      "train loss:1.7347227027052377\n",
      "train loss:1.4963301017286301\n",
      "train loss:1.6659649396353218\n",
      "train loss:1.137085787928589\n",
      "train loss:1.7263167742986851\n",
      "train loss:1.7254643644748218\n",
      "train loss:1.3625215102720942\n",
      "train loss:1.3555484246452205\n",
      "train loss:1.4096153729971816\n",
      "train loss:1.2301997763570478\n",
      "train loss:1.4723011817575617\n",
      "train loss:1.3664925513389692\n",
      "train loss:1.1895544788151153\n",
      "train loss:1.393117579972479\n",
      "train loss:1.4612005732598203\n",
      "train loss:1.6433045856165094\n",
      "train loss:1.8231703841958518\n",
      "train loss:1.4552738529432443\n",
      "train loss:1.9371507876534948\n",
      "train loss:1.6312501080853612\n",
      "train loss:1.7692519482226643\n",
      "train loss:1.5922591853085413\n",
      "train loss:1.0475762360225986\n",
      "train loss:1.7263956194406784\n",
      "train loss:1.5938407178339162\n",
      "train loss:1.7092836865999737\n",
      "train loss:1.6882616637834666\n",
      "train loss:1.1612705003664658\n",
      "train loss:1.3158430729938047\n",
      "train loss:1.8316152665252878\n",
      "train loss:1.5118670118385231\n",
      "train loss:1.126140015894436\n",
      "train loss:1.534564110439693\n",
      "train loss:1.4115508291305676\n",
      "train loss:1.7691515646105203\n",
      "train loss:1.3712654187037698\n",
      "train loss:1.3568663323924688\n",
      "train loss:1.4535599741389926\n",
      "train loss:1.2752752985595726\n",
      "train loss:1.0799336968195488\n",
      "train loss:1.391614734789689\n",
      "train loss:1.4501686580329167\n",
      "train loss:1.3885770263902486\n",
      "train loss:1.5647146352119727\n",
      "train loss:1.7917549192752174\n",
      "train loss:1.6094686934386235\n",
      "train loss:2.025883660960662\n",
      "train loss:1.3593432531778629\n",
      "train loss:1.307807758842222\n",
      "train loss:1.3300164553081404\n",
      "train loss:1.049448341345394\n",
      "train loss:1.00248756462714\n",
      "train loss:1.4552461264692642\n",
      "train loss:1.9564974393890036\n",
      "train loss:1.6132584832123087\n",
      "train loss:1.5249965320852925\n",
      "train loss:1.2179344190679857\n",
      "train loss:1.4864647110554194\n",
      "train loss:1.7724716683328918\n",
      "train loss:1.6507197208189628\n",
      "train loss:1.3843851315011517\n",
      "train loss:1.546960385983096\n",
      "train loss:1.3419318380404384\n",
      "train loss:1.4129269021660773\n",
      "train loss:1.1638279803383993\n",
      "train loss:1.503962051061235\n",
      "train loss:1.2323411220258635\n",
      "train loss:1.7387948268278461\n",
      "train loss:1.146567487369581\n",
      "train loss:1.3902842999219858\n",
      "train loss:1.189598059544328\n",
      "train loss:1.5716100787430565\n",
      "train loss:1.1674838239299485\n",
      "train loss:1.4753930798381851\n",
      "train loss:1.8578616250639939\n",
      "train loss:1.3982362761706102\n",
      "train loss:1.654791168095236\n",
      "train loss:1.3591412537894025\n",
      "train loss:1.6618429235922876\n",
      "train loss:1.3125525441403552\n",
      "train loss:1.7485404666385105\n",
      "train loss:1.665096488796251\n",
      "train loss:1.4107191951515554\n",
      "train loss:1.573363426064968\n",
      "train loss:1.587138294181257\n",
      "train loss:1.3615390803261633\n",
      "train loss:1.5555317937671342\n",
      "train loss:1.190701139309514\n",
      "train loss:1.1229642576950485\n",
      "train loss:0.8917329534726044\n",
      "train loss:1.6833110601225336\n",
      "train loss:1.2084265727867685\n",
      "train loss:1.7620888691322403\n",
      "train loss:0.8404504523534758\n",
      "train loss:1.46504326668718\n",
      "train loss:1.6773360197165856\n",
      "train loss:1.327434424673091\n",
      "train loss:1.690800408081672\n",
      "train loss:1.4651839292808382\n",
      "train loss:1.6521671988638211\n",
      "train loss:1.5969087068290637\n",
      "train loss:0.7046309359312362\n",
      "train loss:1.8587453688152387\n",
      "train loss:1.7343971204213489\n",
      "train loss:1.1439552933784785\n",
      "train loss:1.3619661216228942\n",
      "train loss:1.7397777277087727\n",
      "train loss:1.5779866099979212\n",
      "train loss:1.6885639051125305\n",
      "train loss:1.606414981355399\n",
      "train loss:1.688026642985267\n",
      "train loss:1.5346755919995212\n",
      "train loss:1.5410060219414314\n",
      "train loss:1.333663454418813\n",
      "train loss:1.1583677133591674\n",
      "train loss:1.4613668170172465\n",
      "train loss:0.8099590412670385\n",
      "train loss:1.470375408619683\n",
      "train loss:1.4269443557392227\n",
      "train loss:1.5869515506266079\n",
      "train loss:1.58168607522463\n",
      "train loss:1.7549108006520797\n",
      "train loss:1.7100791110421973\n",
      "train loss:1.5090758401716278\n",
      "train loss:1.4488058576177938\n",
      "train loss:1.6734393356436623\n",
      "train loss:2.1310786259998484\n",
      "train loss:1.439262251794973\n",
      "train loss:1.335935393436033\n",
      "train loss:1.86732929325008\n",
      "train loss:1.219012044274617\n",
      "train loss:1.7992308166604185\n",
      "train loss:1.4357228374307145\n",
      "train loss:1.261508672824533\n",
      "train loss:1.5555516192058865\n",
      "train loss:1.527519898533591\n",
      "train loss:2.0993498792075176\n",
      "train loss:1.3545769603449478\n",
      "train loss:1.308641679033296\n",
      "train loss:1.5986808384532\n",
      "train loss:1.1717473056104617\n",
      "train loss:1.2855828581553004\n",
      "train loss:1.438109395332151\n",
      "train loss:1.060427351737621\n",
      "train loss:1.109508246980269\n",
      "train loss:1.4323995949623742\n",
      "train loss:2.221695190271599\n",
      "train loss:1.2404446138242864\n",
      "train loss:1.5079455809215805\n",
      "train loss:1.102478633445503\n",
      "train loss:1.006040417871369\n",
      "train loss:1.3492768703036344\n",
      "train loss:1.4287626768893629\n",
      "train loss:1.3489535630357012\n",
      "train loss:1.3057563685594016\n",
      "train loss:1.1573167255630876\n",
      "train loss:1.304714064852385\n",
      "train loss:1.1963072785408337\n",
      "train loss:2.094040425215153\n",
      "train loss:1.401894213431022\n",
      "train loss:1.2974068652668884\n",
      "train loss:1.4198631469619492\n",
      "train loss:1.7067114484941086\n",
      "train loss:0.9024064957668682\n",
      "train loss:1.6484952360817107\n",
      "train loss:1.3872217285181319\n",
      "train loss:1.3734760133042998\n",
      "train loss:1.4200959328155434\n",
      "train loss:1.5289576858088865\n",
      "train loss:1.4664448425751566\n",
      "train loss:1.5125714699689206\n",
      "train loss:1.8728596951675596\n",
      "train loss:2.1915858955351877\n",
      "train loss:1.7956412443467773\n",
      "train loss:1.6267070762482043\n",
      "train loss:1.276980983912425\n",
      "train loss:1.7343233331730175\n",
      "train loss:1.2629393470657249\n",
      "train loss:1.9233045451199384\n",
      "train loss:1.3441192283880903\n",
      "train loss:1.0637791403672776\n",
      "train loss:1.505154754850582\n",
      "train loss:1.407016786659326\n",
      "train loss:1.6881032199227604\n",
      "train loss:1.0374136135804268\n",
      "train loss:1.5552503979415806\n",
      "train loss:1.6792116089618312\n",
      "train loss:1.5672407464080833\n",
      "train loss:1.6691716895041881\n",
      "train loss:1.9034086235474514\n",
      "train loss:1.2419531081784634\n",
      "train loss:1.3956587282047637\n",
      "train loss:1.22654261302626\n",
      "train loss:1.5888781196418482\n",
      "train loss:1.4465652018028972\n",
      "train loss:0.7062827200717521\n",
      "train loss:1.1848294006589355\n",
      "train loss:1.6719134024184457\n",
      "train loss:1.3062279310080025\n",
      "train loss:1.6001154763179517\n",
      "train loss:1.3544459854802269\n",
      "train loss:1.251177826772337\n",
      "train loss:1.4247401090791425\n",
      "train loss:1.289594787955005\n",
      "train loss:1.450415126616463\n",
      "train loss:1.6615615039301879\n",
      "train loss:1.3737391578057092\n",
      "train loss:1.3794369964180435\n",
      "train loss:1.6544477357006095\n",
      "train loss:1.2730227540081314\n",
      "train loss:1.5196897221274537\n",
      "train loss:1.5391540948699811\n",
      "train loss:1.104209003941682\n",
      "train loss:1.7176978114488572\n",
      "train loss:1.3928414649299061\n",
      "train loss:1.6832277746478597\n",
      "train loss:1.3047430252662005\n",
      "train loss:1.810836172467146\n",
      "train loss:1.0842874590239353\n",
      "train loss:1.2609621346237267\n",
      "train loss:1.4554387045880577\n",
      "train loss:1.1507557899225707\n",
      "train loss:1.9150891922686202\n",
      "train loss:1.4251269080534998\n",
      "train loss:1.2871449475034091\n",
      "train loss:1.9541452779771178\n",
      "train loss:1.1422275222951894\n",
      "train loss:1.202147940302563\n",
      "train loss:1.0575760575349493\n",
      "train loss:1.3210028104613225\n",
      "train loss:1.4835647434880712\n",
      "train loss:1.1394340557058575\n",
      "train loss:1.413103014990765\n",
      "train loss:1.7870134035483354\n",
      "train loss:1.5638564595237396\n",
      "train loss:1.2493397326964124\n",
      "train loss:1.2433589816156683\n",
      "train loss:1.387542851996486\n",
      "train loss:1.5168548268547877\n",
      "train loss:1.4624862191284103\n",
      "train loss:1.2574279099235959\n",
      "train loss:1.531303951235737\n",
      "train loss:1.4339509544466282\n",
      "train loss:1.3867558809862306\n",
      "train loss:1.3255298183515665\n",
      "train loss:1.630309089858749\n",
      "train loss:1.5106766657000457\n",
      "train loss:1.500394178742181\n",
      "train loss:1.5079636858503498\n",
      "train loss:2.006073593853105\n",
      "train loss:1.8575630931223859\n",
      "train loss:1.0044743425830753\n",
      "train loss:1.5322553443963443\n",
      "train loss:1.934079880967547\n",
      "train loss:0.9964139626728065\n",
      "train loss:1.4536994042698321\n",
      "train loss:1.8496708605448937\n",
      "train loss:1.3566662092594133\n",
      "train loss:1.5655229099965617\n",
      "train loss:1.2226971475086412\n",
      "train loss:1.513324897340842\n",
      "train loss:1.613597879848884\n",
      "train loss:1.445415129233385\n",
      "train loss:1.9499468136530056\n",
      "train loss:1.7627668677546857\n",
      "train loss:1.848845152859559\n",
      "train loss:1.3634599858705065\n",
      "train loss:1.8802850235069226\n",
      "train loss:1.3907422841692392\n",
      "train loss:1.6823601420898147\n",
      "train loss:1.171395049491537\n",
      "train loss:1.5047729466877837\n",
      "train loss:1.7761449488003915\n",
      "train loss:1.4021982237739168\n",
      "train loss:1.2970919252898514\n",
      "train loss:1.3950859369645152\n",
      "train loss:1.094669230612618\n",
      "train loss:1.7994075125425497\n",
      "train loss:1.5731237401546792\n",
      "train loss:1.5470594667121582\n",
      "train loss:1.573412956014149\n",
      "train loss:1.6545804343046995\n",
      "train loss:1.089959961680632\n",
      "train loss:1.030582161380631\n",
      "train loss:1.26474675029825\n",
      "train loss:1.7733706452642561\n",
      "train loss:1.5907703423179893\n",
      "train loss:1.4653813688691926\n",
      "train loss:1.1703200164912853\n",
      "train loss:1.2949476756529243\n",
      "train loss:1.564667532096005\n",
      "train loss:1.1316895944451764\n",
      "train loss:1.3395331310599627\n",
      "train loss:1.5842338185626577\n",
      "train loss:1.744612255695794\n",
      "train loss:1.517101873829185\n",
      "train loss:1.467271667371126\n",
      "train loss:1.889389427967318\n",
      "train loss:1.643127212372376\n",
      "train loss:1.3270874579143213\n",
      "train loss:1.4908626241534777\n",
      "train loss:1.5348488175167248\n",
      "train loss:1.1170990065279451\n",
      "train loss:2.0129876198605614\n",
      "train loss:1.4893803177642488\n",
      "train loss:1.1091902495259842\n",
      "train loss:1.25513268850379\n",
      "train loss:1.2267040939431806\n",
      "train loss:1.2990647013389267\n",
      "train loss:1.3412920943643816\n",
      "train loss:1.524146423022148\n",
      "train loss:1.414366982915637\n",
      "train loss:1.925166172032304\n",
      "train loss:1.4703103041267378\n",
      "train loss:1.3600218540511724\n",
      "train loss:1.065484453134883\n",
      "train loss:1.836247505714777\n",
      "train loss:1.2900279257054943\n",
      "train loss:1.541205360624284\n",
      "train loss:1.1750283057210793\n",
      "train loss:1.706311444022429\n",
      "train loss:1.238320620341949\n",
      "train loss:1.756098109849932\n",
      "train loss:1.3374287180687787\n",
      "train loss:1.7328748132531875\n",
      "train loss:1.320145077097615\n",
      "train loss:1.0750038151437211\n",
      "train loss:1.4796693640580239\n",
      "train loss:1.695484751933869\n",
      "train loss:1.3576062929998929\n",
      "train loss:1.1381030422146987\n",
      "train loss:1.7299626618559227\n",
      "train loss:1.6607723740385976\n",
      "train loss:1.617261134905546\n",
      "train loss:1.244717597587527\n",
      "train loss:1.6252924478659256\n",
      "train loss:1.518443159172708\n",
      "train loss:1.1885891549250747\n",
      "train loss:1.522414208824388\n",
      "train loss:1.6549985033066672\n",
      "train loss:1.2212190498890296\n",
      "train loss:1.219164647056108\n",
      "train loss:1.2123736378158727\n",
      "train loss:1.5608060924828018\n",
      "train loss:1.4632274635545683\n",
      "train loss:1.8533427030395284\n",
      "train loss:1.3302972810564533\n",
      "train loss:1.410705754651527\n",
      "train loss:1.686295986188122\n",
      "train loss:1.519674201014724\n",
      "train loss:1.139551020059907\n",
      "train loss:1.3228012684305286\n",
      "train loss:1.4351177082388504\n",
      "train loss:1.728353594164242\n",
      "train loss:1.5532923782031474\n",
      "train loss:1.4431337332315575\n",
      "train loss:1.0344221772571458\n",
      "train loss:0.30086952947987233\n",
      "train loss:0.9292332984201925\n",
      "train loss:0.7202794649420559\n",
      "train loss:1.4180713261024576\n",
      "train loss:1.885547121924315\n",
      "train loss:1.3505184260642575\n",
      "train loss:1.5002367337188074\n",
      "train loss:1.1052876208592421\n",
      "train loss:1.3541357873374824\n",
      "train loss:1.5426342284414907\n",
      "train loss:1.3029149613182995\n",
      "train loss:1.1604424598248122\n",
      "train loss:1.4871910443851037\n",
      "train loss:0.8910324943970546\n",
      "train loss:0.8774996357030567\n",
      "train loss:1.244830560769571\n",
      "train loss:1.058975585319297\n",
      "train loss:1.1807859279394006\n",
      "train loss:1.2296491664329787\n",
      "train loss:1.1725264150876564\n",
      "train loss:1.7076524216615057\n",
      "train loss:1.7341027526001427\n",
      "train loss:1.4747945746467965\n",
      "train loss:1.1324683074351192\n",
      "train loss:1.4854372854251359\n",
      "train loss:1.5787717058429631\n",
      "train loss:1.7676226868542677\n",
      "train loss:1.7909395028384316\n",
      "train loss:0.929063226557094\n",
      "train loss:1.1936676937287305\n",
      "train loss:1.116323250982684\n",
      "train loss:1.8335325532476898\n",
      "train loss:1.1820699544464384\n",
      "train loss:1.2207613912256614\n",
      "train loss:1.6168519690989764\n",
      "train loss:1.432542247275189\n",
      "train loss:1.5620889678639922\n",
      "train loss:1.3791514289756048\n",
      "train loss:1.6575561182397247\n",
      "train loss:1.2912645514843335\n",
      "train loss:1.600276945850495\n",
      "train loss:1.49330941043443\n",
      "train loss:1.1918463665085635\n",
      "train loss:1.4118477513136818\n",
      "train loss:1.0810131991093601\n",
      "train loss:1.4709787208241174\n",
      "train loss:1.4553382246522948\n",
      "train loss:1.179597233259165\n",
      "train loss:1.1795404903523887\n",
      "train loss:0.892491472702068\n",
      "train loss:1.146209983507885\n",
      "train loss:1.1925445027315094\n",
      "train loss:1.4002563815370823\n",
      "train loss:1.6039183114175586\n",
      "train loss:1.5469307066202955\n",
      "train loss:1.1648577102357767\n",
      "train loss:1.6331949391022362\n",
      "train loss:1.1549756488576883\n",
      "train loss:1.221650153848243\n",
      "train loss:1.6843627042216007\n",
      "train loss:1.238195752042681\n",
      "train loss:1.9336595637406748\n",
      "train loss:1.4519098673359778\n",
      "train loss:1.2282944752875662\n",
      "train loss:1.8229240380239837\n",
      "train loss:1.4780417988607355\n",
      "train loss:1.3452710402272419\n",
      "train loss:1.3385305612133218\n",
      "train loss:1.479221690304306\n",
      "train loss:1.2588814237894297\n",
      "train loss:1.5461571823246598\n",
      "train loss:1.3568073709768702\n",
      "train loss:1.3809967970390968\n",
      "train loss:1.3206985177403614\n",
      "train loss:1.6678204352990775\n",
      "train loss:1.2883721879695789\n",
      "train loss:1.7097528461493017\n",
      "train loss:1.7430254138136587\n",
      "train loss:1.5822302947231872\n",
      "train loss:1.6692364803914976\n",
      "train loss:1.899360629001734\n",
      "train loss:1.4976610088350768\n",
      "train loss:1.3624821741599447\n",
      "train loss:1.14014796757924\n",
      "train loss:1.1416628597310106\n",
      "train loss:1.7033297789599267\n",
      "train loss:1.2241337717841263\n",
      "train loss:1.3986874289120903\n",
      "train loss:1.066812898638677\n",
      "train loss:1.756831056249117\n",
      "train loss:1.6358193664795124\n",
      "train loss:1.4972048606060748\n",
      "train loss:0.9160535828273628\n",
      "train loss:1.427582683684493\n",
      "train loss:0.8885951144330019\n",
      "train loss:1.0571831503691622\n",
      "train loss:1.0188089664920177\n",
      "train loss:1.1115137599027678\n",
      "train loss:1.9524293660190781\n",
      "train loss:1.228232269647602\n",
      "train loss:1.5752329500592206\n",
      "train loss:1.2295557233673267\n",
      "train loss:0.9994397223916328\n",
      "train loss:1.5831875446573012\n",
      "train loss:1.7662172056380423\n",
      "train loss:1.5985708973391766\n",
      "train loss:1.5610647086478893\n",
      "train loss:1.3842041455713554\n",
      "train loss:1.2108214635092944\n",
      "train loss:1.248681774527213\n",
      "train loss:1.0135981334029744\n",
      "train loss:1.5734599093028652\n",
      "train loss:1.3275279107842999\n",
      "train loss:1.7642314113927746\n",
      "train loss:1.3356590786562375\n",
      "train loss:1.4805707352995081\n",
      "train loss:1.409956070491654\n",
      "train loss:1.175956334437779\n",
      "train loss:0.823424503354304\n",
      "train loss:1.306960646839718\n",
      "train loss:1.43878862341542\n",
      "train loss:1.8303852900637305\n",
      "train loss:1.2598084790089352\n",
      "train loss:1.702160004957726\n",
      "train loss:1.940662897470315\n",
      "train loss:1.8361660827779474\n",
      "train loss:1.3115389663310917\n",
      "train loss:2.2750663334236108\n",
      "train loss:1.4164919910610745\n",
      "train loss:1.1727442155523804\n",
      "train loss:1.2328740285001278\n",
      "train loss:1.6685401552652468\n",
      "train loss:1.1927041304653323\n",
      "train loss:1.435749686077924\n",
      "train loss:1.4026140631234374\n",
      "train loss:1.6570585341455775\n",
      "train loss:0.8602643606874464\n",
      "train loss:1.0210398252325832\n",
      "train loss:1.3063430241536866\n",
      "train loss:1.5927262703170906\n",
      "train loss:1.9342175666533745\n",
      "train loss:1.6622875185751842\n",
      "train loss:1.1447815490909572\n",
      "train loss:0.9548865800656776\n",
      "train loss:1.4914524185933749\n",
      "train loss:1.8297558977865993\n",
      "train loss:1.032400202914547\n",
      "train loss:1.5828212358317872\n",
      "train loss:1.29965606689294\n",
      "train loss:1.5697109224581052\n",
      "train loss:1.7796651196411972\n",
      "train loss:1.4737892448296344\n",
      "train loss:1.6726418286149543\n",
      "train loss:1.5889866292403858\n",
      "train loss:1.1812994682755327\n",
      "train loss:1.4112195975041302\n",
      "train loss:1.6441214622667313\n",
      "train loss:1.605603721617852\n",
      "train loss:1.5708253860634356\n",
      "train loss:1.5066898239682538\n",
      "train loss:1.5969707476123236\n",
      "train loss:1.5619989114967006\n",
      "train loss:1.0071920695606553\n",
      "train loss:1.3786123958895757\n",
      "train loss:1.4369378598227311\n",
      "train loss:1.6329066396311451\n",
      "train loss:1.690153868993185\n",
      "train loss:1.4230367743831922\n",
      "train loss:1.3048788700769332\n",
      "train loss:1.3569516499121643\n",
      "train loss:1.1352108058191213\n",
      "train loss:1.9278205465121179\n",
      "train loss:1.4007439399280919\n",
      "train loss:1.606996118543265\n",
      "train loss:1.3244287004111077\n",
      "train loss:1.1443346846938032\n",
      "train loss:1.3360350771258738\n",
      "train loss:1.819513810828155\n",
      "train loss:1.5116530091378473\n",
      "train loss:1.385168212626766\n",
      "train loss:1.2727169604626543\n",
      "train loss:1.1478249055529246\n",
      "train loss:1.3059213462016395\n",
      "train loss:0.9018910328382825\n",
      "train loss:1.1544251321230548\n",
      "train loss:0.767382402828808\n",
      "train loss:1.117324505179451\n",
      "train loss:1.5781839441396612\n",
      "train loss:1.9106952029685345\n",
      "train loss:1.7091982562787031\n",
      "train loss:1.7196843886364879\n",
      "train loss:1.634395637507629\n",
      "train loss:1.295316451408831\n",
      "train loss:1.003800330718649\n",
      "train loss:1.3490313542781052\n",
      "train loss:1.5978668849767579\n",
      "train loss:1.276106555519051\n",
      "train loss:0.7943485211281038\n",
      "train loss:1.659085097291398\n",
      "train loss:1.65560842594626\n",
      "train loss:1.428759865987437\n",
      "train loss:1.372484835377717\n",
      "train loss:1.9157679068947147\n",
      "train loss:1.1218015708404363\n",
      "train loss:1.709801201707391\n",
      "train loss:1.8661667686192966\n",
      "train loss:0.9912787797226406\n",
      "train loss:1.1928046986817247\n",
      "train loss:1.5721148932308664\n",
      "train loss:1.6601145988198844\n",
      "train loss:1.6277706989564718\n",
      "train loss:1.3150149977341514\n",
      "train loss:1.432111714693121\n",
      "train loss:1.3220233219913853\n",
      "train loss:1.610728961108353\n",
      "train loss:1.3677967182750872\n",
      "train loss:1.5589162050001852\n",
      "train loss:1.1641718930206362\n",
      "train loss:1.0354234030405622\n",
      "train loss:1.2171061902123115\n",
      "train loss:1.0050746482543402\n",
      "train loss:1.864583654343709\n",
      "train loss:1.6842966696539545\n",
      "train loss:1.6079967590327926\n",
      "train loss:1.9996603917924483\n",
      "train loss:1.7490096337475314\n",
      "train loss:1.1792137206144429\n",
      "train loss:1.2205785168410732\n",
      "train loss:1.560344729811704\n",
      "train loss:1.1107127678636886\n",
      "train loss:1.9798275231246845\n",
      "train loss:1.660237593329838\n",
      "train loss:1.3436332749319402\n",
      "train loss:1.4274600830138364\n",
      "train loss:1.3307912666347086\n",
      "train loss:1.4065865339196322\n",
      "train loss:0.8931092219839766\n",
      "train loss:1.6882981478478176\n",
      "train loss:1.2514317327378033\n",
      "train loss:1.0704448084123528\n",
      "train loss:1.4959936760716654\n",
      "train loss:1.3984964513995495\n",
      "train loss:1.317669974425265\n",
      "train loss:1.6969538599141025\n",
      "train loss:1.2021734139294722\n",
      "train loss:1.3927095861417078\n",
      "train loss:0.9447856600986043\n",
      "train loss:1.5338183749130827\n",
      "train loss:1.486417220794746\n",
      "train loss:1.317260500572158\n",
      "train loss:1.359838086396341\n",
      "train loss:0.7655411099050851\n",
      "train loss:1.4101717744210673\n",
      "train loss:1.3216157216311077\n",
      "train loss:1.5750754044307076\n",
      "train loss:1.599139139784701\n",
      "train loss:1.7074943293679468\n",
      "train loss:0.901864940317191\n",
      "train loss:1.1638066673394678\n",
      "train loss:1.4430831953575896\n",
      "train loss:1.5773677919628923\n",
      "train loss:1.1526695151519397\n",
      "train loss:1.5483996834456037\n",
      "train loss:1.598549703284284\n",
      "train loss:1.5720975185009058\n",
      "train loss:1.4926824209319285\n",
      "train loss:1.3391333666639702\n",
      "train loss:1.5020966881674092\n",
      "train loss:1.4115186189578446\n",
      "train loss:1.1961059686768087\n",
      "train loss:1.4062419534447486\n",
      "train loss:1.4802552831048006\n",
      "train loss:1.320925253890827\n",
      "train loss:1.4671830223816071\n",
      "train loss:1.4620267449412503\n",
      "train loss:1.3662894752729895\n",
      "train loss:1.5809266410166356\n",
      "train loss:1.5505853154070126\n",
      "train loss:0.8447417362065652\n",
      "train loss:1.4948441492574165\n",
      "train loss:1.7620646673055411\n",
      "train loss:1.3997649602894506\n",
      "train loss:1.7664096991975626\n",
      "train loss:1.7102408273254455\n",
      "train loss:1.2096163668504254\n",
      "train loss:1.588759024549036\n",
      "train loss:1.3950697121426185\n",
      "train loss:1.4990357966560635\n",
      "train loss:1.0668264212494258\n",
      "train loss:1.3270534345686507\n",
      "train loss:1.4974858061986713\n",
      "train loss:1.3598984422095912\n",
      "train loss:1.7990253808436236\n",
      "train loss:1.5207681336409147\n",
      "train loss:1.4696433982653936\n",
      "train loss:1.2169487196379254\n",
      "train loss:1.3506132185991255\n",
      "train loss:2.1065328557708245\n",
      "train loss:1.2923458282538782\n",
      "train loss:1.3193083731785236\n",
      "train loss:1.427338039476845\n",
      "train loss:1.4727340999288328\n",
      "train loss:1.3303935735915555\n",
      "train loss:1.3638927172544038\n",
      "train loss:1.1099405791971586\n",
      "train loss:1.3952677171415233\n",
      "train loss:1.827476292316185\n",
      "train loss:1.0353717088731602\n",
      "train loss:1.2600467471138879\n",
      "train loss:1.0188255833032087\n",
      "train loss:1.5768998608712101\n",
      "train loss:1.3098073747241303\n",
      "train loss:0.970651934110764\n",
      "train loss:1.7981710263361115\n",
      "train loss:1.3493747149293758\n",
      "train loss:1.419074352143974\n",
      "train loss:1.6705737127231166\n",
      "train loss:1.1099949896013819\n",
      "train loss:1.9317597853732529\n",
      "train loss:1.3008346754583324\n",
      "train loss:1.672138348325905\n",
      "train loss:1.4190772458335126\n",
      "train loss:1.4733530115614681\n",
      "train loss:1.4696411895991424\n",
      "train loss:1.3169858593768209\n",
      "train loss:1.2445030247513524\n",
      "train loss:1.224626757450131\n",
      "train loss:1.8885931017493913\n",
      "train loss:1.4189452293854767\n",
      "train loss:1.5337361691788296\n",
      "train loss:1.10946544376865\n",
      "train loss:1.4582618707566823\n",
      "train loss:1.2285045649984734\n",
      "train loss:0.9035473150408251\n",
      "train loss:1.832274020470893\n",
      "train loss:1.3430725752056591\n",
      "train loss:0.9752565860702962\n",
      "train loss:1.5608233312862896\n",
      "train loss:1.5351452399027734\n",
      "train loss:1.8030149250885263\n",
      "train loss:1.4531047849718532\n",
      "train loss:1.058742049567307\n",
      "train loss:1.522308362674669\n",
      "train loss:1.7040353474200622\n",
      "train loss:1.8858892928754236\n",
      "train loss:1.0040433714684047\n",
      "train loss:1.2661559534326574\n",
      "train loss:1.425353538517181\n",
      "train loss:1.4422642677745507\n",
      "train loss:1.6009351203437496\n",
      "train loss:1.1771211865557432\n",
      "train loss:1.014015517325739\n",
      "train loss:1.497892800508407\n",
      "train loss:1.2050194002634196\n",
      "train loss:1.1206446050507357\n",
      "train loss:1.6807620290941745\n",
      "train loss:1.7474993922807776\n",
      "train loss:1.1815453918013206\n",
      "train loss:1.1807624642932182\n",
      "train loss:1.5535117042598698\n",
      "train loss:1.4796828935635382\n",
      "train loss:1.6276209717953023\n",
      "train loss:1.3749679444080283\n",
      "train loss:1.466841860534026\n",
      "train loss:1.4956141040928992\n",
      "train loss:1.2913974804639783\n",
      "train loss:1.5541708022467913\n",
      "train loss:1.2847695449686078\n",
      "train loss:1.3775111693710467\n",
      "train loss:1.3971055190605726\n",
      "train loss:1.8833960850641898\n",
      "train loss:1.5870214857813538\n",
      "train loss:1.412503550375797\n",
      "train loss:1.5492753317249452\n",
      "train loss:0.961671734722089\n",
      "train loss:1.3744375425532291\n",
      "train loss:0.9704135478233906\n",
      "train loss:0.7565468538048994\n",
      "train loss:1.408398428568246\n",
      "train loss:1.7287324310284162\n",
      "train loss:1.2074689381541748\n",
      "train loss:1.2929677947581557\n",
      "train loss:1.6962784622750544\n",
      "train loss:1.429118280866024\n",
      "train loss:0.8826907112498885\n",
      "train loss:1.001700490360646\n",
      "train loss:1.7731604244863597\n",
      "train loss:1.4079439609380664\n",
      "train loss:1.4060699012292546\n",
      "train loss:1.242100841552027\n",
      "train loss:0.8961281120678715\n",
      "train loss:1.764729284166971\n",
      "train loss:1.346900894014693\n",
      "train loss:1.1312493288115266\n",
      "train loss:1.6688194253039328\n",
      "train loss:1.2818218981740268\n",
      "train loss:1.3365852664871798\n",
      "train loss:1.2095758698609482\n",
      "train loss:1.5577496315758468\n",
      "train loss:1.302757220062486\n",
      "train loss:1.8081932564292533\n",
      "train loss:1.6019379616963725\n",
      "train loss:1.3111485988350693\n",
      "train loss:1.3986404383536488\n",
      "train loss:1.5035637640278479\n",
      "train loss:1.1996002269134682\n",
      "train loss:0.8827397258599519\n",
      "train loss:0.8126757326028615\n",
      "train loss:1.5205571728828489\n",
      "train loss:1.802565366989164\n",
      "train loss:1.136325692161742\n",
      "train loss:1.57306797236405\n",
      "train loss:1.2426936569969178\n",
      "train loss:1.6250931847639722\n",
      "train loss:1.4096668909026708\n",
      "train loss:1.134291142150462\n",
      "train loss:1.7343877445238143\n",
      "train loss:1.5946378031639328\n",
      "train loss:1.3358557234260928\n",
      "train loss:1.896497491322362\n",
      "train loss:0.9468402877005305\n",
      "train loss:1.0036613413073372\n",
      "train loss:1.4445015570235507\n",
      "train loss:1.3255405921936094\n",
      "train loss:1.577467339605377\n",
      "train loss:1.4263992914774553\n",
      "train loss:1.029652444982783\n",
      "train loss:1.5405171528771133\n",
      "train loss:1.377669380290415\n",
      "train loss:1.4459973375483883\n",
      "train loss:1.691789463196569\n",
      "train loss:0.8567324283637084\n",
      "train loss:1.727420246235368\n",
      "train loss:1.5893395409336493\n",
      "train loss:1.6693364103971455\n",
      "train loss:1.6917783195603249\n",
      "train loss:1.2974165308977588\n",
      "train loss:0.9400854551033806\n",
      "train loss:1.6428850658304743\n",
      "train loss:1.5592870788898017\n",
      "train loss:1.1658339684650882\n",
      "train loss:1.2271600790458568\n",
      "train loss:1.6141025083959213\n",
      "train loss:1.1003129678068553\n",
      "train loss:1.55244492295383\n",
      "train loss:1.3515916817883897\n",
      "train loss:1.492383455691546\n",
      "train loss:1.6085801709990253\n",
      "train loss:1.0883012111554848\n",
      "train loss:1.414600133218597\n",
      "train loss:1.3304841287388498\n",
      "train loss:1.257335019947486\n",
      "train loss:1.5817994998400857\n",
      "train loss:1.3956491346284352\n",
      "train loss:1.1832654606132909\n",
      "train loss:1.5987365962980693\n",
      "train loss:1.3733917731567558\n",
      "train loss:1.4401771142910247\n",
      "train loss:1.6134459936957932\n",
      "train loss:1.111036936813002\n",
      "train loss:1.3183400842254287\n",
      "train loss:1.4998956055809076\n",
      "train loss:1.2207282701462288\n",
      "train loss:1.237765675153362\n",
      "train loss:0.981378057824758\n",
      "train loss:1.610215679390374\n",
      "train loss:0.9356694176941452\n",
      "train loss:1.2652180289291235\n",
      "train loss:1.6330389158910836\n",
      "train loss:0.9849176252724987\n",
      "train loss:1.0922878741149655\n",
      "train loss:1.531493359142039\n",
      "train loss:1.248441557676048\n",
      "train loss:1.2642877115832394\n",
      "train loss:1.323405964987822\n",
      "train loss:1.4340917811007239\n",
      "train loss:1.4288876719112946\n",
      "train loss:2.0117046052468748\n",
      "train loss:0.9371916854327964\n",
      "train loss:1.6073222279820745\n",
      "train loss:0.9843305946093459\n",
      "train loss:1.6265384692879425\n",
      "train loss:1.1705912469962079\n",
      "train loss:1.574291641732858\n",
      "train loss:1.693248882819098\n",
      "train loss:1.0790633968890233\n",
      "train loss:1.3443302839363933\n",
      "train loss:1.9720660269498311\n",
      "train loss:1.2918153015640363\n",
      "train loss:1.271240993510585\n",
      "train loss:1.4420420875564035\n",
      "train loss:1.539467612186587\n",
      "train loss:1.383573472738361\n",
      "train loss:1.8913476777497351\n",
      "train loss:1.1393316491942698\n",
      "train loss:1.0949663529789848\n",
      "train loss:1.7539900829968402\n",
      "train loss:1.425873526846114\n",
      "train loss:1.4218057433688744\n",
      "train loss:1.6306307185150941\n",
      "train loss:0.765064238507487\n",
      "train loss:1.0914313653513326\n",
      "train loss:1.0865033253639156\n",
      "train loss:1.821646710806687\n",
      "train loss:1.1405934506132942\n",
      "train loss:1.579828999931334\n",
      "train loss:1.3354831867306356\n",
      "train loss:1.4125670384565585\n",
      "train loss:1.2781559583282383\n",
      "train loss:1.5054898597709223\n",
      "train loss:1.5697859970402068\n",
      "train loss:1.4233246496436784\n",
      "train loss:1.192489998291944\n",
      "train loss:1.2344492619945664\n",
      "train loss:1.2932462268265816\n",
      "train loss:1.5094618147043397\n",
      "train loss:1.2789205813592441\n",
      "train loss:1.6555876023332423\n",
      "train loss:1.4827612932055594\n",
      "train loss:1.1813944826944787\n",
      "train loss:1.7091720829805492\n",
      "train loss:1.5567447872843185\n",
      "train loss:1.4714189271943874\n",
      "train loss:1.6143934103586974\n",
      "train loss:1.7177416143362982\n",
      "train loss:1.0392126450265626\n",
      "train loss:1.5050774402729605\n",
      "train loss:1.2493801234944995\n",
      "train loss:1.2183521021213057\n",
      "train loss:1.5382046432120577\n",
      "train loss:1.2658456174127843\n",
      "train loss:1.7509884663145434\n",
      "train loss:1.0534849464403644\n",
      "train loss:1.522381855160678\n",
      "train loss:1.3890039419688118\n",
      "train loss:0.9510194095958011\n",
      "train loss:2.0515505903108564\n",
      "train loss:1.2323798498246903\n",
      "train loss:1.4916901242083165\n",
      "train loss:1.1021814615202885\n",
      "train loss:1.4773749581778444\n",
      "train loss:1.4460751381146915\n",
      "train loss:1.1297659968853808\n",
      "train loss:1.1949063055853342\n",
      "train loss:1.4352453308691058\n",
      "train loss:1.9499001882624025\n",
      "train loss:1.5635446094456928\n",
      "train loss:1.409494397315288\n",
      "train loss:1.5038723837756869\n",
      "train loss:1.3512964460161243\n",
      "train loss:0.7652288468279445\n",
      "train loss:1.4320806200794822\n",
      "train loss:0.8467805238763313\n",
      "train loss:1.4752831314823984\n",
      "train loss:1.6864423904068349\n",
      "train loss:1.22045228346768\n",
      "train loss:1.2799551152342044\n",
      "train loss:0.9533444234510113\n",
      "train loss:0.7324234466174648\n",
      "train loss:1.8692366997857384\n",
      "train loss:1.501769855542869\n",
      "train loss:1.6407431375599812\n",
      "train loss:1.5264376060024025\n",
      "train loss:1.2136831461348883\n",
      "train loss:1.5415891154116956\n",
      "train loss:1.2512945010600691\n",
      "train loss:1.652275305268784\n",
      "train loss:1.496464341090507\n",
      "train loss:1.2602797413067885\n",
      "train loss:1.0036054092863478\n",
      "train loss:0.9652328008624946\n",
      "train loss:1.131380217573898\n",
      "train loss:1.5411099326069653\n",
      "train loss:1.0505213756717202\n",
      "train loss:1.2448085664152024\n",
      "train loss:1.213720643674655\n",
      "train loss:1.434488688316262\n",
      "train loss:1.5345520436101712\n",
      "train loss:1.523374201273835\n",
      "train loss:1.3808849285204439\n",
      "train loss:1.339048824640211\n",
      "train loss:1.40153526746565\n",
      "train loss:2.050674172013303\n",
      "train loss:1.2200918928864382\n",
      "train loss:1.6830304103316496\n",
      "train loss:1.2324165442212278\n",
      "train loss:1.4419470903483886\n",
      "train loss:1.280556477736735\n",
      "train loss:1.029070822280191\n",
      "train loss:1.4385846337963735\n",
      "train loss:1.2110307343052518\n",
      "train loss:1.248077380204332\n",
      "train loss:1.235255812830124\n",
      "train loss:1.2560568966152115\n",
      "train loss:1.8044995280612366\n",
      "train loss:0.9419988583534031\n",
      "train loss:1.13695493616087\n",
      "train loss:1.2082348546812398\n",
      "train loss:1.2545237264099585\n",
      "train loss:1.2553634909811158\n",
      "train loss:1.4680812562961782\n",
      "train loss:1.2452433992357455\n",
      "train loss:1.4401491202149468\n",
      "train loss:1.663582818991958\n",
      "train loss:1.4729506490286606\n",
      "train loss:1.0646048618467807\n",
      "train loss:1.6657841741092536\n",
      "train loss:1.7428112992909228\n",
      "train loss:1.6688443174789893\n",
      "train loss:1.2652482969450263\n",
      "train loss:1.8808302899935794\n",
      "train loss:1.0249874987958456\n",
      "train loss:1.3036989739472395\n",
      "train loss:1.3842449488394244\n",
      "train loss:1.337695644401341\n",
      "train loss:1.05125329939626\n",
      "train loss:1.1699615278134576\n",
      "train loss:1.7131214064700437\n",
      "train loss:0.9774027384065533\n",
      "train loss:1.435448895247821\n",
      "train loss:1.5952687911293517\n",
      "train loss:1.7219304221901575\n",
      "train loss:1.4145675379626954\n",
      "train loss:1.4282250023859646\n",
      "train loss:1.7323941793357889\n",
      "train loss:1.2944366677242507\n",
      "train loss:1.7539218094145788\n",
      "train loss:1.290049521545622\n",
      "train loss:1.1457585462390805\n",
      "train loss:1.347391281360594\n",
      "train loss:1.1436603612497838\n",
      "train loss:1.7783795188294398\n",
      "train loss:1.5649623623486595\n",
      "train loss:1.3240072310577795\n",
      "train loss:1.3522635791006263\n",
      "train loss:1.1294891434061882\n",
      "train loss:1.1052153492868115\n",
      "train loss:1.6284349222323347\n",
      "train loss:1.8945410528584563\n",
      "train loss:1.2530701345012307\n",
      "train loss:1.6106620904005957\n",
      "train loss:1.4155602759751804\n",
      "train loss:1.452811259684648\n",
      "train loss:1.531742856928488\n",
      "train loss:1.2388996688125304\n",
      "train loss:2.107991627997618\n",
      "train loss:1.2998583861886421\n",
      "train loss:0.977497290065845\n",
      "train loss:1.7468699196639843\n",
      "train loss:1.4715353864395018\n",
      "train loss:1.4871923630271469\n",
      "train loss:1.4409528528481015\n",
      "train loss:1.457884139706824\n",
      "train loss:1.7648907278695005\n",
      "train loss:1.6108073480833671\n",
      "train loss:1.463841111643648\n",
      "train loss:1.1544038455721795\n",
      "train loss:1.6154452191365753\n",
      "train loss:1.3966211444170147\n",
      "train loss:1.4050881017218475\n",
      "train loss:1.4237966289272426\n",
      "train loss:1.3398917521315263\n",
      "train loss:1.199473965186673\n",
      "train loss:1.4972759300143168\n",
      "train loss:1.0789661107471618\n",
      "train loss:1.4812215120796455\n",
      "train loss:1.1489966916954237\n",
      "train loss:1.3500111300840048\n",
      "train loss:1.3928022276700764\n",
      "train loss:1.11127453153886\n",
      "train loss:1.313069300942912\n",
      "train loss:1.3768855193068117\n",
      "train loss:1.140519601592335\n",
      "train loss:1.4793389458892108\n",
      "train loss:1.4623612224624885\n",
      "train loss:1.163925856576391\n",
      "train loss:0.8165522913236914\n",
      "train loss:1.359719541199921\n",
      "train loss:1.7416989694845373\n",
      "train loss:1.788701757192958\n",
      "train loss:1.5950898388020485\n",
      "train loss:1.5813074756238388\n",
      "train loss:1.0414660100434023\n",
      "train loss:1.0339018476832194\n",
      "train loss:1.3109870336541631\n",
      "train loss:1.4336339747305502\n",
      "train loss:1.2466103860624018\n",
      "train loss:1.751681787414905\n",
      "train loss:1.5836745634174856\n",
      "train loss:1.6622944087322633\n",
      "train loss:1.54400852609245\n",
      "train loss:1.7204484310414554\n",
      "train loss:1.4475456558061088\n",
      "train loss:1.1988840101050446\n",
      "train loss:1.134485105891484\n",
      "train loss:1.6519948647311602\n",
      "train loss:0.9398053186194725\n",
      "train loss:0.9834694590121693\n",
      "train loss:0.9422376915889329\n",
      "train loss:1.5644824799058603\n",
      "train loss:1.6846483033113084\n",
      "train loss:1.6421618573003596\n",
      "train loss:1.4879472928564401\n",
      "train loss:1.3513576110921144\n",
      "train loss:1.4301129726224162\n",
      "train loss:1.3017847621483842\n",
      "train loss:1.5512542248366028\n",
      "train loss:1.2280079438297709\n",
      "train loss:1.6233979881681768\n",
      "train loss:1.3831594590652625\n",
      "train loss:1.3036788346656942\n",
      "train loss:1.2661564146905235\n",
      "train loss:1.4409943179942186\n",
      "train loss:1.6267816762768512\n",
      "train loss:1.84293505479751\n",
      "train loss:1.1951950782058531\n",
      "train loss:1.4080086807294883\n",
      "train loss:1.6363549294604556\n",
      "train loss:1.2891211559052735\n",
      "train loss:1.388676766433318\n",
      "train loss:1.3671056596354638\n",
      "train loss:1.2102512997019006\n",
      "train loss:1.806970449566712\n",
      "train loss:1.598591659273148\n",
      "train loss:1.261744739771211\n",
      "train loss:0.7652698457485985\n",
      "train loss:1.4153526089742965\n",
      "train loss:1.111348574359239\n",
      "train loss:1.5133764661551616\n",
      "train loss:1.353463038775596\n",
      "train loss:1.5155070902360581\n",
      "train loss:1.1426748245284826\n",
      "train loss:1.7739182841386962\n",
      "train loss:1.6269111535118728\n",
      "train loss:1.430335643110622\n",
      "train loss:1.6094714217020418\n",
      "train loss:1.421280428780968\n",
      "train loss:1.005167142893078\n",
      "train loss:1.7286948640915263\n",
      "train loss:1.248843001370297\n",
      "train loss:1.492247346495029\n",
      "train loss:1.9353486101924513\n",
      "train loss:1.3302248195190989\n",
      "train loss:1.5199953585246813\n",
      "train loss:1.468977453752993\n",
      "train loss:1.5216791440682\n",
      "train loss:1.523467954985077\n",
      "train loss:1.3049104493600678\n",
      "train loss:1.0736309858498778\n",
      "train loss:1.5853944474772788\n",
      "train loss:0.971110337373497\n",
      "train loss:1.2853350918380504\n",
      "train loss:1.1844408693147348\n",
      "train loss:1.8105247089300007\n",
      "train loss:1.4799372817263465\n",
      "train loss:1.4262113276488562\n",
      "train loss:1.7306691453376302\n",
      "train loss:1.5189963557902462\n",
      "train loss:1.4024462895340661\n",
      "train loss:1.4984476469072199\n",
      "train loss:1.306937212918837\n",
      "train loss:0.8555717744700276\n",
      "train loss:1.5797143891531342\n",
      "train loss:1.737815806377218\n",
      "train loss:1.4461658323806779\n",
      "train loss:1.134253311711716\n",
      "train loss:1.2038055880593674\n",
      "train loss:1.8959393924065087\n",
      "train loss:1.0981494683422863\n",
      "train loss:1.1587732143245604\n",
      "train loss:1.5455400120139167\n",
      "train loss:1.1673141223196906\n",
      "train loss:1.272803889449217\n",
      "train loss:1.2346098301114201\n",
      "train loss:1.8319927063877106\n",
      "train loss:0.9568251160832661\n",
      "train loss:1.7305070991667346\n",
      "train loss:0.6935652030642944\n",
      "train loss:0.8695667030386893\n",
      "train loss:1.019869212213655\n",
      "train loss:1.4719368581089352\n",
      "train loss:0.8714069390921695\n",
      "train loss:0.929544817552386\n",
      "train loss:1.6404714919907044\n",
      "train loss:1.4200502386301337\n",
      "train loss:1.7722016188257725\n",
      "train loss:1.36474026341129\n",
      "train loss:1.540027092807489\n",
      "train loss:1.3008559192279463\n",
      "train loss:2.157198446322718\n",
      "train loss:1.961605816968767\n",
      "train loss:1.482415350020417\n",
      "train loss:1.4399998082780845\n",
      "train loss:1.396892115857072\n",
      "train loss:1.695153030761253\n",
      "train loss:1.6682839764451376\n",
      "train loss:1.6163245899286458\n",
      "train loss:1.1365166890870744\n",
      "train loss:1.1289662795203417\n",
      "train loss:1.6131393833301424\n",
      "train loss:1.0698546605470711\n",
      "train loss:1.1310423815144621\n",
      "train loss:1.8179430371091327\n",
      "train loss:1.9033767542817237\n",
      "train loss:1.311732258351009\n",
      "train loss:1.0774925761199043\n",
      "train loss:1.2858411725632293\n",
      "train loss:1.6411888870544984\n",
      "train loss:1.470408774567704\n",
      "train loss:1.4358826924001478\n",
      "train loss:0.9778393241117683\n",
      "train loss:1.1203432164544753\n",
      "train loss:1.5441417099194596\n",
      "train loss:1.299773110198561\n",
      "train loss:0.9305250677545566\n",
      "train loss:1.2554032943653017\n",
      "train loss:1.5095673758783523\n",
      "train loss:1.1352394792954035\n",
      "train loss:0.7174233516253905\n",
      "train loss:1.2444398385711968\n",
      "train loss:1.519055060845869\n",
      "train loss:1.2151303509929874\n",
      "train loss:1.5846957817012013\n",
      "train loss:0.892747554231294\n",
      "train loss:1.0670516746974958\n",
      "train loss:1.3589703721319817\n",
      "train loss:1.6580143753704952\n",
      "train loss:1.6221334902449633\n",
      "train loss:1.548055670849466\n",
      "train loss:1.33828376967961\n",
      "train loss:1.7364165489181016\n",
      "train loss:1.3596440029628991\n",
      "train loss:1.6876273831059236\n",
      "train loss:1.330491828007853\n",
      "train loss:1.5307849290428106\n",
      "train loss:1.6207981050451323\n",
      "train loss:1.455171227519612\n",
      "train loss:1.619054464582583\n",
      "train loss:1.692256503129513\n",
      "train loss:1.7191754839017137\n",
      "train loss:1.0907266320992093\n",
      "train loss:1.516360562069933\n",
      "train loss:1.5834639388007674\n",
      "train loss:1.3249962645897089\n",
      "train loss:1.3384198957972209\n",
      "train loss:1.6290676018177745\n",
      "train loss:0.9321124592847123\n",
      "train loss:0.7526611693407709\n",
      "train loss:1.339004371167172\n",
      "train loss:1.3620023379588935\n",
      "train loss:1.2672754562924629\n",
      "train loss:1.2361580809833188\n",
      "train loss:0.7456931110637587\n",
      "train loss:1.7724438456346168\n",
      "train loss:1.5229886857563268\n",
      "train loss:1.4372324961627274\n",
      "train loss:1.5052929865766038\n",
      "train loss:0.8949313848124583\n",
      "train loss:1.0049707741461942\n",
      "train loss:1.4293988609916717\n",
      "train loss:1.2991102577160565\n",
      "train loss:0.9943305268084492\n",
      "train loss:1.5194474644748368\n",
      "train loss:1.472821058054072\n",
      "train loss:1.4482397615893172\n",
      "train loss:1.4986847941226178\n",
      "train loss:1.576581354417158\n",
      "train loss:1.4058290840288976\n",
      "train loss:1.1922032157442684\n",
      "train loss:1.1994230529355163\n",
      "train loss:1.5463847941702542\n",
      "train loss:1.2684306779514258\n",
      "train loss:1.64023755487266\n",
      "train loss:1.2550046788279174\n",
      "train loss:1.2433506359720399\n",
      "train loss:1.5560681834062466\n",
      "train loss:1.798494627005089\n",
      "train loss:1.0022790110209627\n",
      "train loss:1.4586308986670478\n",
      "train loss:0.6975951341963575\n",
      "train loss:1.148576116139283\n",
      "train loss:1.1862189992952952\n",
      "train loss:0.9958681897834138\n",
      "train loss:1.1612857256807456\n",
      "train loss:1.833270098305664\n",
      "train loss:1.8173061041278533\n",
      "train loss:1.5416696886307664\n",
      "train loss:1.5389334721556884\n",
      "train loss:1.9685858708794235\n",
      "train loss:1.2681962313370172\n",
      "train loss:1.3426714999822693\n",
      "train loss:1.5695440414971107\n",
      "train loss:1.1172803413690076\n",
      "train loss:1.346415702999257\n",
      "train loss:1.342366196014828\n",
      "train loss:1.3963706868750025\n",
      "train loss:1.219546187273554\n",
      "train loss:1.182252274196034\n",
      "train loss:1.8744581196329702\n",
      "train loss:1.2950302150679165\n",
      "train loss:1.4398188613891558\n",
      "train loss:1.4469488564726605\n",
      "train loss:1.2095541502383644\n",
      "train loss:1.2676771301681875\n",
      "train loss:0.9863424180024897\n",
      "train loss:1.0172675281071561\n",
      "train loss:1.0869180607193785\n",
      "train loss:1.38934541850937\n",
      "train loss:1.949099863345839\n",
      "train loss:1.2935332698069213\n",
      "train loss:1.561067810227643\n",
      "train loss:2.0460216145409036\n",
      "train loss:1.7136888036388658\n",
      "train loss:1.1788444269300529\n",
      "train loss:1.4490580189710038\n",
      "train loss:1.7602845069948412\n",
      "train loss:0.9353209237160647\n",
      "train loss:1.3294588544257944\n",
      "train loss:1.944828245532502\n",
      "train loss:1.1554378037033053\n",
      "train loss:1.5151152123532075\n",
      "train loss:0.996910063235874\n",
      "train loss:1.4272660152557624\n",
      "train loss:1.4053295121164369\n",
      "train loss:1.9466055118617382\n",
      "train loss:1.490667801498191\n",
      "train loss:0.9774331718665309\n",
      "train loss:1.6583174200336788\n",
      "train loss:1.7219839473246412\n",
      "train loss:1.4322681586171762\n",
      "train loss:1.2032289709361896\n",
      "train loss:0.9045828460651759\n",
      "train loss:1.3123571170532864\n",
      "train loss:0.9207240913212364\n",
      "train loss:1.2220856360786723\n",
      "train loss:1.2542557469582492\n",
      "train loss:2.0351685963165904\n",
      "train loss:1.5683311707821108\n",
      "train loss:1.047170588324388\n",
      "train loss:1.684337342941511\n",
      "train loss:1.7794365109947106\n",
      "train loss:1.1558898829326945\n",
      "train loss:1.6057458747314715\n",
      "train loss:1.6973712266644536\n",
      "train loss:1.3415150547604073\n",
      "train loss:0.8452008009366635\n",
      "train loss:1.3685853891525936\n",
      "train loss:1.363009742579116\n",
      "train loss:1.172248730299776\n",
      "train loss:1.22556683581755\n",
      "train loss:1.52949190993207\n",
      "train loss:1.5577483477285972\n",
      "train loss:1.5724190335022106\n",
      "train loss:1.223099690400392\n",
      "train loss:1.4712210670690014\n",
      "train loss:1.5886792821202838\n",
      "train loss:1.5625858379582378\n",
      "train loss:1.5488754606590238\n",
      "train loss:2.0024870557733854\n",
      "train loss:1.377444456078921\n",
      "train loss:1.3928587726621262\n",
      "train loss:1.3180282331097601\n",
      "train loss:1.3228330030616797\n",
      "train loss:1.3212636487696208\n",
      "train loss:1.227298816564767\n",
      "train loss:0.9801473638539615\n",
      "train loss:1.7346495916774753\n",
      "train loss:1.517457223526365\n",
      "train loss:1.147851574994913\n",
      "train loss:1.222364986642618\n",
      "train loss:1.5922679668772894\n",
      "train loss:1.1856273031336395\n",
      "train loss:1.4553448126434598\n",
      "train loss:1.4167372088442298\n",
      "train loss:1.0281007675117237\n",
      "train loss:1.1790505789134627\n",
      "train loss:1.2127944093611398\n",
      "train loss:1.4217510272960054\n",
      "train loss:1.5033580315954655\n",
      "train loss:1.510074779864591\n",
      "train loss:1.143870342528927\n",
      "train loss:1.354679979339341\n",
      "train loss:1.6765758950614522\n",
      "train loss:1.2989105189798587\n",
      "train loss:1.4420890353913967\n",
      "train loss:1.309864787264369\n",
      "train loss:1.8313472067260697\n",
      "train loss:1.392073418006834\n",
      "train loss:1.5661545256689229\n",
      "train loss:1.230075894510872\n",
      "train loss:1.157308530001495\n",
      "train loss:1.322041774183195\n",
      "train loss:1.683781899619881\n",
      "train loss:1.4997721544798959\n",
      "train loss:1.5322883461816637\n",
      "train loss:1.3673465806282468\n",
      "train loss:1.3847593295702798\n",
      "train loss:1.4959073196375583\n",
      "train loss:1.9266548344648613\n",
      "train loss:1.4672815250472366\n",
      "train loss:1.1195415272375415\n",
      "train loss:1.5925688278853216\n",
      "train loss:1.390225168423886\n",
      "train loss:1.2737974471957731\n",
      "train loss:1.4019309992348865\n",
      "train loss:1.510357484733265\n",
      "train loss:0.977393821838708\n",
      "train loss:1.6799408084323528\n",
      "train loss:1.3566811784581272\n",
      "train loss:1.2626887989180353\n",
      "train loss:1.25250901650872\n",
      "train loss:1.3599865707808214\n",
      "train loss:1.4473870480419797\n",
      "train loss:1.7471512489943593\n",
      "train loss:1.2032699953073416\n",
      "train loss:1.258636713123882\n",
      "train loss:1.5796214241602728\n",
      "train loss:0.9392999471118415\n",
      "train loss:1.2537550149285255\n",
      "train loss:1.666614745289991\n",
      "train loss:1.5788851829937864\n",
      "train loss:2.110271294441267\n",
      "train loss:1.4394169912834849\n",
      "train loss:1.4998815787539146\n",
      "train loss:1.4526789721273454\n",
      "train loss:1.2154457291159053\n",
      "train loss:1.245783502614739\n",
      "train loss:1.2601062776422876\n",
      "train loss:1.148697281815791\n",
      "train loss:1.3150527006057504\n",
      "train loss:1.4343451990499811\n",
      "train loss:1.077752858556714\n",
      "train loss:1.6669965710609158\n",
      "train loss:1.5146821245950477\n",
      "train loss:1.9318400307732897\n",
      "train loss:0.8084343040473551\n",
      "train loss:1.3293750186695779\n",
      "train loss:1.2357524719153932\n",
      "train loss:0.5904635061162116\n",
      "train loss:1.6462308994462398\n",
      "train loss:1.6409744163852564\n",
      "train loss:1.4600087647787945\n",
      "train loss:1.4767107390417327\n",
      "train loss:1.7242342402501034\n",
      "train loss:1.9115229381323289\n",
      "train loss:1.2815051021234585\n",
      "train loss:1.218291927599023\n",
      "train loss:1.8575516330602397\n",
      "train loss:1.4288192928295835\n",
      "train loss:1.2105024840038614\n",
      "train loss:1.2300386541341366\n",
      "train loss:1.2480540439416805\n",
      "train loss:1.5140579728599168\n",
      "train loss:1.175610601937821\n",
      "train loss:1.279766648286625\n",
      "train loss:0.8556445062697664\n",
      "train loss:0.7870170853428212\n",
      "train loss:1.0626755747247434\n",
      "train loss:1.2319511253133983\n",
      "train loss:1.6678536791986116\n",
      "train loss:1.359835805394209\n",
      "train loss:1.4596680488702969\n",
      "train loss:1.3438085850005526\n",
      "train loss:1.2927952500673894\n",
      "train loss:1.541824081173646\n",
      "train loss:1.4694919377821853\n",
      "train loss:1.438443217968079\n",
      "train loss:1.3717504652802552\n",
      "train loss:1.611774446645645\n",
      "train loss:0.604161261172844\n",
      "train loss:1.383061312942655\n",
      "train loss:1.28649675688356\n",
      "train loss:0.9456079208893773\n",
      "train loss:1.1462491148786325\n",
      "train loss:0.8054125543769732\n",
      "train loss:1.345737813770262\n",
      "train loss:1.117119337514185\n",
      "train loss:1.0930798917188242\n",
      "train loss:1.0259743898797782\n",
      "train loss:0.9621157964287226\n",
      "train loss:1.097312545470119\n",
      "train loss:1.3306880501698688\n",
      "train loss:1.6150052554947245\n",
      "train loss:1.2791221683728602\n",
      "train loss:1.0198129493661037\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6764705882352942\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.1, random_state=40)\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = DeepConvNet()\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92ec7ac8-05fa-4aec-8c62-2108e518d69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.5196078431372549\n",
      "혼동 행렬:\n",
      " [[84 54]\n",
      " [44 22]]\n",
      "분류 보고서:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.61      0.63       138\n",
      "           1       0.29      0.33      0.31        66\n",
      "\n",
      "    accuracy                           0.52       204\n",
      "   macro avg       0.47      0.47      0.47       204\n",
      "weighted avg       0.54      0.52      0.53       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mymethod.functions import *\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "x_train_reshaped = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], -1)\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train_reshaped, y_train)\n",
    "y_pred = model.predict(x_test_reshaped)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"정확도:\", accuracy)\n",
    "print(\"혼동 행렬:\\n\", conf_matrix)\n",
    "print(\"분류 보고서:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e92a4ef-5c5d-4697-a337-8206468fa99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.5490196078431373\n",
      "혼동 행렬:\n",
      " [[88 50]\n",
      " [42 24]]\n",
      "분류 보고서:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       138\n",
      "           1       0.32      0.36      0.34        66\n",
      "\n",
      "    accuracy                           0.55       204\n",
      "   macro avg       0.50      0.50      0.50       204\n",
      "weighted avg       0.56      0.55      0.56       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_reshaped = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], -1)\n",
    "model = LinearDiscriminantAnalysis()\n",
    "model.fit(x_train_reshaped, y_train)\n",
    "y_pred = model.predict(x_test_reshaped)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"정확도:\", accuracy)\n",
    "print(\"혼동 행렬:\\n\", conf_matrix)\n",
    "print(\"분류 보고서:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "250de2db-df94-406b-922b-7ce798726a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6764705882352942\n",
      "혼동 행렬:\n",
      " [[138   0]\n",
      " [ 66   0]]\n",
      "분류 보고서:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      1.00      0.81       138\n",
      "           1       0.00      0.00      0.00        66\n",
      "\n",
      "    accuracy                           0.68       204\n",
      "   macro avg       0.34      0.50      0.40       204\n",
      "weighted avg       0.46      0.68      0.55       204\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hotmi\\OneDrive\\Desktop\\4학년 2학기\\딥러닝\\과제\\mymethod\\functions.py:87: RuntimeWarning: divide by zero encountered in log\n",
      "  class_conditional = -0.5 * np.sum(np.log(2. * np.pi * self.variances[cls])) - 0.5 * np.sum(((x - self.means[cls]) ** 2) / (self.variances[cls]))\n",
      "C:\\Users\\hotmi\\OneDrive\\Desktop\\4학년 2학기\\딥러닝\\과제\\mymethod\\functions.py:87: RuntimeWarning: invalid value encountered in divide\n",
      "  class_conditional = -0.5 * np.sum(np.log(2. * np.pi * self.variances[cls])) - 0.5 * np.sum(((x - self.means[cls]) ** 2) / (self.variances[cls]))\n",
      "C:\\Users\\hotmi\\OneDrive\\Desktop\\4학년 2학기\\딥러닝\\과제\\mymethod\\functions.py:87: RuntimeWarning: divide by zero encountered in divide\n",
      "  class_conditional = -0.5 * np.sum(np.log(2. * np.pi * self.variances[cls])) - 0.5 * np.sum(((x - self.means[cls]) ** 2) / (self.variances[cls]))\n",
      "C:\\Users\\hotmi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\hotmi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\hotmi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "x_train_reshaped = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], -1)\n",
    "model = GaussianNaiveBayes()\n",
    "model.fit(x_train_reshaped, y_train)\n",
    "y_pred = model.predict(x_test_reshaped)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"정확도:\", accuracy)\n",
    "print(\"혼동 행렬:\\n\", conf_matrix)\n",
    "print(\"분류 보고서:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0900e48-24f5-48c8-8707-45b47dcb1228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6961\n",
      "LDA Accuracy: 0.5980\n",
      "Gaussian Classifier Accuracy: 0.3725\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "x_train_reshaped = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "# Logistic Regression model\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(x_train_reshaped, y_train)\n",
    "y_pred_logistic = logistic_model.predict(x_test_reshaped)\n",
    "logistic_accuracy = accuracy_score(y_test, y_pred_logistic)\n",
    "print(f\"Logistic Regression Accuracy: {logistic_accuracy:.4f}\")\n",
    "\n",
    "# Linear Discriminant Analysis (LDA) model\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(x_train_reshaped, y_train)\n",
    "y_pred_lda = lda_model.predict(x_test_reshaped)\n",
    "lda_accuracy = accuracy_score(y_test, y_pred_lda)\n",
    "print(f\"LDA Accuracy: {lda_accuracy:.4f}\")\n",
    "\n",
    "# Gaussian Naive Bayes classifier\n",
    "gaussian_model = GaussianNB()\n",
    "gaussian_model.fit(x_train_reshaped, y_train)\n",
    "y_pred_gaussian = gaussian_model.predict(x_test_reshaped)\n",
    "gaussian_accuracy = accuracy_score(y_test, y_pred_gaussian)\n",
    "print(f\"Gaussian Classifier Accuracy: {gaussian_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7932aff5-591e-4853-b1eb-c4e60d31dfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hotmi\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.4832 - loss: 0.7661 - val_accuracy: 0.6033 - val_loss: 0.6933\n",
      "Epoch 2/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7212 - loss: 0.5863 - val_accuracy: 0.6087 - val_loss: 0.6702\n",
      "Epoch 3/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7457 - loss: 0.5374 - val_accuracy: 0.6359 - val_loss: 0.6966\n",
      "Epoch 4/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7763 - loss: 0.5051 - val_accuracy: 0.6250 - val_loss: 0.6872\n",
      "Epoch 5/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7669 - loss: 0.4868 - val_accuracy: 0.6685 - val_loss: 0.6810\n",
      "Epoch 6/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8036 - loss: 0.4519 - val_accuracy: 0.6413 - val_loss: 0.7029\n",
      "Epoch 7/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7977 - loss: 0.4298 - val_accuracy: 0.6576 - val_loss: 0.7347\n",
      "Epoch 8/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8201 - loss: 0.4104 - val_accuracy: 0.6359 - val_loss: 0.7586\n",
      "Epoch 9/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8487 - loss: 0.3595 - val_accuracy: 0.6304 - val_loss: 0.7589\n",
      "Epoch 10/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8753 - loss: 0.3387 - val_accuracy: 0.6250 - val_loss: 0.7863\n",
      "Epoch 11/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8924 - loss: 0.2964 - val_accuracy: 0.5978 - val_loss: 0.7953\n",
      "Epoch 12/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9101 - loss: 0.2884 - val_accuracy: 0.6141 - val_loss: 0.9121\n",
      "Epoch 13/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9092 - loss: 0.2740 - val_accuracy: 0.6413 - val_loss: 0.9045\n",
      "Epoch 14/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9339 - loss: 0.2258 - val_accuracy: 0.5435 - val_loss: 0.9568\n",
      "Epoch 15/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9553 - loss: 0.2064 - val_accuracy: 0.6141 - val_loss: 0.9536\n",
      "Epoch 16/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9624 - loss: 0.1753 - val_accuracy: 0.6359 - val_loss: 1.0224\n",
      "Epoch 17/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9678 - loss: 0.1404 - val_accuracy: 0.6359 - val_loss: 1.0830\n",
      "Epoch 18/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9728 - loss: 0.1302 - val_accuracy: 0.5870 - val_loss: 1.1104\n",
      "Epoch 19/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9687 - loss: 0.1318 - val_accuracy: 0.6033 - val_loss: 1.1454\n",
      "Epoch 20/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9897 - loss: 0.0957 - val_accuracy: 0.5761 - val_loss: 1.2452\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "TensorFlow Model Accuracy: 0.6373\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(x_train_reshaped )\n",
    "X_test = scaler.transform(x_test_reshaped)\n",
    "\n",
    "# Build a simple TensorFlow model for binary classification\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"TensorFlow Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22533e5e-efc9-4bce-98dc-73299e44182e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hotmi\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 71ms/step - accuracy: 0.7001 - loss: 0.6431 - val_accuracy: 0.6848 - val_loss: 0.6569\n",
      "Epoch 2/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.7053 - loss: 0.6178 - val_accuracy: 0.6848 - val_loss: 0.6377\n",
      "Epoch 3/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.6909 - loss: 0.5939 - val_accuracy: 0.6848 - val_loss: 0.6465\n",
      "Epoch 4/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.7117 - loss: 0.5636 - val_accuracy: 0.6630 - val_loss: 0.6736\n",
      "Epoch 5/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.7465 - loss: 0.4968 - val_accuracy: 0.6848 - val_loss: 0.8285\n",
      "Epoch 6/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8227 - loss: 0.4349 - val_accuracy: 0.6141 - val_loss: 0.8646\n",
      "Epoch 7/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8621 - loss: 0.3218 - val_accuracy: 0.6576 - val_loss: 0.9545\n",
      "Epoch 8/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9438 - loss: 0.2027 - val_accuracy: 0.5924 - val_loss: 1.3421\n",
      "Epoch 9/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9518 - loss: 0.1212 - val_accuracy: 0.6793 - val_loss: 1.5798\n",
      "Epoch 10/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.9877 - loss: 0.0746 - val_accuracy: 0.5978 - val_loss: 1.7852\n",
      "Epoch 11/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9981 - loss: 0.0231 - val_accuracy: 0.6304 - val_loss: 2.1009\n",
      "Epoch 12/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.0092 - val_accuracy: 0.6359 - val_loss: 2.3785\n",
      "Epoch 13/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.6250 - val_loss: 2.4311\n",
      "Epoch 14/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.6359 - val_loss: 2.6308\n",
      "Epoch 15/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.6304 - val_loss: 2.6779\n",
      "Epoch 16/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 6.2167e-04 - val_accuracy: 0.6196 - val_loss: 2.7089\n",
      "Epoch 17/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 5.2313e-04 - val_accuracy: 0.6250 - val_loss: 2.7927\n",
      "Epoch 18/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 4.7716e-04 - val_accuracy: 0.6304 - val_loss: 2.8394\n",
      "Epoch 19/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 3.6996e-04 - val_accuracy: 0.6196 - val_loss: 2.8760\n",
      "Epoch 20/20\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 3.1697e-04 - val_accuracy: 0.6304 - val_loss: 2.9381\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "TensorFlow Model Accuracy: 0.7059\n"
     ]
    }
   ],
   "source": [
    "# Build a deeper TensorFlow model for binary classification with CNN layers\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(X_train.shape[1], 1)),\n",
    "    tf.keras.layers.Reshape((X_train.shape[1], 1)),\n",
    "    tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"TensorFlow Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fd6fe03-f64d-4a76-9c8d-d1e1416e5c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.298641071418149\n",
      "=== epoch:1, train acc:0.71, test acc:0.69 ===\n",
      "train loss:2.2934049303714685\n",
      "train loss:2.282243063616174\n",
      "train loss:2.259613356627758\n",
      "train loss:2.233853661981151\n",
      "train loss:2.2009860553546714\n",
      "train loss:2.1759477147285744\n",
      "train loss:2.0433458215459535\n",
      "train loss:2.015516936703876\n",
      "train loss:1.9008936879723872\n",
      "train loss:1.716120890933917\n",
      "train loss:1.562397487422223\n",
      "train loss:1.372472096284913\n",
      "train loss:1.300841303911374\n",
      "train loss:1.1302382074537105\n",
      "train loss:0.8868701717967694\n",
      "train loss:0.7398032063622868\n",
      "train loss:0.5935327162326302\n",
      "train loss:0.661110506768716\n",
      "train loss:0.6556226353730414\n",
      "train loss:0.7536658480500266\n",
      "train loss:1.2373977764165622\n",
      "train loss:0.6579699239721591\n",
      "train loss:0.7406314113926293\n",
      "train loss:0.6412668718210162\n",
      "train loss:0.6788394406997923\n",
      "train loss:0.6315239888181188\n",
      "train loss:0.651816009968834\n",
      "train loss:0.677653267772727\n",
      "train loss:0.6933139494854753\n",
      "train loss:0.7031473731990496\n",
      "train loss:0.6595696902782204\n",
      "train loss:0.7133394827503537\n",
      "train loss:0.6461698191554401\n",
      "train loss:0.5852483419206951\n",
      "train loss:0.7973242782621833\n",
      "train loss:0.4563721756867203\n",
      "train loss:0.41964778297379823\n",
      "train loss:0.35654719833083054\n",
      "train loss:0.7162454201215931\n",
      "train loss:0.5288097430939119\n",
      "train loss:0.9954512372720604\n",
      "train loss:0.33281370383618536\n",
      "train loss:0.7081694136192197\n",
      "train loss:0.5038507668948939\n",
      "train loss:0.5259846004413493\n",
      "train loss:0.3613940438482598\n",
      "train loss:0.6661867897739217\n",
      "train loss:0.9698517939060046\n",
      "train loss:0.42430679806679494\n",
      "train loss:0.6221239225779319\n",
      "train loss:0.6759347119820855\n",
      "train loss:0.5787274783442878\n",
      "train loss:0.6670594271181048\n",
      "train loss:0.6988775662552955\n",
      "train loss:0.6493528806002458\n",
      "train loss:0.646686996900756\n",
      "train loss:0.6717768110983462\n",
      "train loss:0.6119835086367428\n",
      "train loss:0.6706243784767356\n",
      "train loss:0.6130129817043862\n",
      "train loss:0.658150773712831\n",
      "train loss:0.623705246399422\n",
      "train loss:0.517620047177063\n",
      "train loss:0.6110859276038142\n",
      "train loss:0.4882721037233515\n",
      "train loss:0.49131415667169637\n",
      "train loss:0.9157618953653188\n",
      "train loss:0.5792854851129627\n",
      "train loss:0.6415219659429918\n",
      "train loss:0.6141433928869084\n",
      "train loss:0.49870999740907546\n",
      "train loss:0.5139382306114005\n",
      "train loss:0.598587896621346\n",
      "train loss:0.7230089299040773\n",
      "train loss:0.4099634876678747\n",
      "train loss:0.49857048446288454\n",
      "train loss:0.7094096727256893\n",
      "train loss:0.2766867634721595\n",
      "train loss:0.6353376554587815\n",
      "train loss:0.6979844396501798\n",
      "train loss:0.6429695494408844\n",
      "train loss:0.702954957665773\n",
      "train loss:0.5453406041658304\n",
      "train loss:0.7553177847877524\n",
      "train loss:0.7459500465773681\n",
      "train loss:0.6395215391782337\n",
      "train loss:0.6738188631502182\n",
      "train loss:0.6313488847739005\n",
      "train loss:0.6791506071197175\n",
      "train loss:0.6490179101308637\n",
      "train loss:0.6840110463711657\n",
      "train loss:0.5775012077435793\n",
      "train loss:0.5133264284369239\n",
      "train loss:0.6233026570929561\n",
      "train loss:0.5320102640728054\n",
      "train loss:0.4211649351376726\n",
      "train loss:0.6351583176641723\n",
      "train loss:0.7980665386770749\n",
      "train loss:0.5132459816045113\n",
      "train loss:0.7975688224790345\n",
      "train loss:0.4635752288953662\n",
      "train loss:0.37612438615512606\n",
      "train loss:0.4643516153369194\n",
      "train loss:0.9565909532776404\n",
      "train loss:0.62217628771456\n",
      "train loss:0.638701032826006\n",
      "train loss:0.7387335424064913\n",
      "train loss:0.3960863957801519\n",
      "train loss:0.618368789959951\n",
      "train loss:0.5225959010788995\n",
      "train loss:0.6964031539193202\n",
      "train loss:0.6027857893705475\n",
      "train loss:0.6960401932076125\n",
      "train loss:0.6547043630559872\n",
      "train loss:0.5915831836637105\n",
      "train loss:0.6872323696507713\n",
      "train loss:0.7107924259167948\n",
      "train loss:0.592737552090569\n",
      "train loss:0.5738677111129218\n",
      "train loss:0.6341476409247229\n",
      "train loss:0.5037149413313026\n",
      "train loss:0.7411173905832112\n",
      "train loss:0.6689481900953917\n",
      "train loss:0.6071921719418848\n",
      "train loss:0.5227182121361069\n",
      "train loss:0.6289990604081266\n",
      "train loss:0.5956623397270246\n",
      "train loss:0.5775429881382312\n",
      "train loss:0.49836661699964174\n",
      "train loss:0.27679927218678857\n",
      "train loss:0.49086051740724335\n",
      "train loss:0.7744663046269531\n",
      "train loss:0.6528899982419568\n",
      "train loss:0.7986787868584565\n",
      "train loss:0.8697400388625528\n",
      "train loss:0.3790045668278628\n",
      "train loss:0.8282931103652725\n",
      "train loss:0.8304760623905463\n",
      "train loss:0.7021834213719087\n",
      "train loss:0.619689766739538\n",
      "train loss:0.6712943782899579\n",
      "train loss:0.5329574445426779\n",
      "train loss:0.637854838269817\n",
      "train loss:0.5533582162126229\n",
      "train loss:0.6415767553631009\n",
      "train loss:0.5410825153095026\n",
      "train loss:0.5614671667398989\n",
      "train loss:0.5666912314835144\n",
      "train loss:0.5402322457563586\n",
      "train loss:0.5314910941728442\n",
      "train loss:0.8222256407950775\n",
      "train loss:0.8311376171739493\n",
      "train loss:0.5236698887831963\n",
      "train loss:0.2977415792029993\n",
      "train loss:0.502751379999604\n",
      "train loss:0.5316438019981287\n",
      "train loss:0.5093952604511464\n",
      "train loss:0.36356262487632074\n",
      "train loss:0.3583817685214101\n",
      "train loss:0.6663139314756943\n",
      "train loss:0.531098420161285\n",
      "train loss:0.6855563671035702\n",
      "train loss:0.5644598686209368\n",
      "train loss:0.5362398490798113\n",
      "train loss:0.3530988533486652\n",
      "train loss:0.6737548753503023\n",
      "train loss:0.9723154809434129\n",
      "train loss:0.9287390352600824\n",
      "train loss:0.7212372609641347\n",
      "train loss:0.6251126022049445\n",
      "train loss:0.6114752466310907\n",
      "train loss:0.6893473983847516\n",
      "train loss:0.7303298523741415\n",
      "train loss:0.6101307570264131\n",
      "train loss:0.6834300498634892\n",
      "train loss:0.6834960423863532\n",
      "train loss:0.6803079780728004\n",
      "train loss:0.6524022029204701\n",
      "train loss:0.6747960715428197\n",
      "train loss:0.6467657424433428\n",
      "train loss:0.6400414663923782\n",
      "train loss:0.6115298298214111\n",
      "train loss:0.6570142710797444\n",
      "train loss:0.5368938392243762\n",
      "train loss:0.6362752210947461\n",
      "train loss:0.5254622443771728\n",
      "train loss:0.6131851772447442\n",
      "train loss:0.32658894772593233\n",
      "train loss:1.0211813877032538\n",
      "train loss:0.7475666880365412\n",
      "train loss:0.6165576538880916\n",
      "train loss:0.9792846178775776\n",
      "train loss:0.626448223018425\n",
      "train loss:0.5812438969961684\n",
      "train loss:0.723015911211818\n",
      "train loss:0.5184194431091631\n",
      "train loss:0.3986975366690512\n",
      "train loss:0.4209753361757489\n",
      "train loss:0.8336069670331051\n",
      "train loss:0.7061600625025432\n",
      "train loss:0.6154633695689575\n",
      "train loss:0.5286526927401031\n",
      "train loss:0.5062972799529686\n",
      "train loss:0.6153211079156751\n",
      "train loss:0.7198117288191443\n",
      "train loss:0.7827026053428734\n",
      "train loss:0.5245946612491779\n",
      "train loss:0.6791431682076429\n",
      "train loss:0.620483181509802\n",
      "train loss:0.5498784440258481\n",
      "train loss:0.5367970803156548\n",
      "train loss:0.59147177588614\n",
      "train loss:0.42426886710761214\n",
      "train loss:0.3437891764852452\n",
      "train loss:0.7978452814720157\n",
      "train loss:0.4192964072372596\n",
      "train loss:0.6006693521417311\n",
      "train loss:0.3798124007687034\n",
      "train loss:0.4860963036538194\n",
      "train loss:0.36933839015961656\n",
      "train loss:0.5017120919628392\n",
      "train loss:0.48122621198563487\n",
      "train loss:0.3328259299307148\n",
      "train loss:0.693884891663483\n",
      "train loss:0.875118523031962\n",
      "train loss:0.6337053098198335\n",
      "train loss:0.7650371215396954\n",
      "train loss:0.4623806761051957\n",
      "train loss:0.4855188185846143\n",
      "train loss:0.521093554269985\n",
      "train loss:0.5112498209550578\n",
      "train loss:0.489234324370228\n",
      "train loss:0.3842189517837844\n",
      "train loss:0.5067411790597907\n",
      "train loss:0.7468018342295555\n",
      "train loss:0.5246205704488414\n",
      "train loss:0.73313999302422\n",
      "train loss:0.5187756623305135\n",
      "train loss:0.8808717806917958\n",
      "train loss:0.5724951296921318\n",
      "train loss:0.6710184563867673\n",
      "train loss:0.5497828461567886\n",
      "train loss:0.5617431447957302\n",
      "train loss:0.6728776285902257\n",
      "train loss:0.6289227135999027\n",
      "train loss:0.6319849343332231\n",
      "train loss:0.7943716824544846\n",
      "train loss:0.5708111855850605\n",
      "train loss:0.680759779665086\n",
      "train loss:0.6744601349145858\n",
      "train loss:0.531114542955679\n",
      "train loss:0.6240607632469838\n",
      "train loss:0.5633096297200366\n",
      "train loss:0.6772071191321886\n",
      "train loss:0.6234036314913359\n",
      "train loss:0.48025643110589966\n",
      "train loss:0.45244203570146524\n",
      "train loss:0.7982776979333747\n",
      "train loss:0.6281493259692728\n",
      "train loss:0.6118326936569789\n",
      "train loss:0.7130555209630357\n",
      "train loss:0.42363276963213314\n",
      "train loss:0.8137821059662704\n",
      "train loss:0.5315685778922101\n",
      "train loss:0.7025962046275346\n",
      "train loss:0.82485629811809\n",
      "train loss:0.6980304951791396\n",
      "train loss:0.6843274812957969\n",
      "train loss:0.3745980594262737\n",
      "train loss:0.6276405400778318\n",
      "train loss:0.6173134058334373\n",
      "train loss:0.5312394921994527\n",
      "train loss:0.5237037918652069\n",
      "train loss:0.6037098560725913\n",
      "train loss:0.5179036788241536\n",
      "train loss:0.6208316047243935\n",
      "train loss:0.6357102627957159\n",
      "train loss:0.7929615217621746\n",
      "train loss:0.4344254093155759\n",
      "train loss:0.5216655369157075\n",
      "train loss:0.40179662885745265\n",
      "train loss:0.5035386150194141\n",
      "train loss:0.7657794041674928\n",
      "train loss:0.49484952853773134\n",
      "train loss:0.5110772271051964\n",
      "train loss:0.59343747709013\n",
      "train loss:0.9792065652127915\n",
      "train loss:0.4996807176851047\n",
      "train loss:0.5123323795068414\n",
      "train loss:0.5130941750717934\n",
      "train loss:0.4023053782132786\n",
      "train loss:0.8634413686464038\n",
      "train loss:0.3873144594658908\n",
      "train loss:0.5102031525491946\n",
      "train loss:0.5194109540749386\n",
      "train loss:0.5262433508643626\n",
      "train loss:0.6205532547275122\n",
      "train loss:0.634044903223087\n",
      "train loss:0.5006418670760163\n",
      "train loss:0.9280897740589295\n",
      "train loss:0.4985624801647693\n",
      "train loss:0.4223914945193375\n",
      "train loss:0.7921211408175591\n",
      "train loss:0.4212669823105314\n",
      "train loss:0.5171823824049724\n",
      "train loss:0.7218282244149374\n",
      "train loss:0.6955442233967711\n",
      "train loss:0.6085763747611148\n",
      "train loss:0.6042321162855004\n",
      "train loss:0.5353583855316304\n",
      "train loss:0.6232747933128481\n",
      "train loss:0.7652602398263009\n",
      "train loss:0.7341370145504644\n",
      "train loss:0.5507228318400378\n",
      "train loss:0.5522121421349963\n",
      "train loss:0.6312695972333644\n",
      "train loss:0.5370133888485795\n",
      "train loss:0.5469420953832271\n",
      "train loss:0.6021827221562926\n",
      "train loss:0.6308572274884219\n",
      "train loss:0.6124453149354251\n",
      "train loss:0.6113043045960354\n",
      "train loss:0.610042889191045\n",
      "train loss:0.6569848178432516\n",
      "train loss:0.5287499876894464\n",
      "train loss:0.3177305788159389\n",
      "train loss:0.6305602476191925\n",
      "train loss:0.5065976684186613\n",
      "train loss:0.5990149818424102\n",
      "train loss:0.6429218750774699\n",
      "train loss:0.6273481253060601\n",
      "train loss:0.7406139067855404\n",
      "train loss:0.6155181027258358\n",
      "train loss:0.8449108721857108\n",
      "train loss:0.41608197867787367\n",
      "train loss:0.49921760676667837\n",
      "train loss:0.7788485121709292\n",
      "train loss:0.5258347209124853\n",
      "train loss:0.46506200406764775\n",
      "train loss:0.6866563130266025\n",
      "train loss:0.45723238545934375\n",
      "train loss:0.6230868991824446\n",
      "train loss:0.4364856653554331\n",
      "train loss:0.5007336405621974\n",
      "train loss:0.4182382664960076\n",
      "train loss:0.7113405863951611\n",
      "train loss:0.6123121964032406\n",
      "train loss:0.4817113350276484\n",
      "train loss:0.6157760363706493\n",
      "train loss:0.46553603322198295\n",
      "train loss:0.37252715127852576\n",
      "train loss:0.7480288868592937\n",
      "train loss:0.6431979833373533\n",
      "train loss:0.657982333841111\n",
      "train loss:0.6192758480179947\n",
      "train loss:0.5013186986058112\n",
      "train loss:0.5094581656940873\n",
      "train loss:0.7453299481075494\n",
      "train loss:0.2997714012677778\n",
      "train loss:1.02795110383812\n",
      "train loss:0.8547524899063357\n",
      "train loss:0.3809455504149525\n",
      "train loss:0.5672651640118701\n",
      "train loss:0.5542816286366069\n",
      "train loss:0.6247584993954659\n",
      "train loss:0.4926240647195594\n",
      "train loss:0.4799445041764553\n",
      "train loss:0.5241224902404602\n",
      "train loss:0.5288496931450144\n",
      "train loss:0.7021440772682703\n",
      "train loss:0.7130922759963825\n",
      "train loss:0.5952063338757567\n",
      "train loss:0.49254801141695603\n",
      "train loss:0.5249227058009286\n",
      "train loss:0.5247011203783833\n",
      "train loss:0.7252198077673288\n",
      "train loss:1.0238287803784383\n",
      "train loss:0.8094291034048865\n",
      "train loss:0.6479324615670187\n",
      "train loss:0.4535955204781996\n",
      "train loss:0.5362931812684841\n",
      "train loss:0.708782245679366\n",
      "train loss:0.6761612532180813\n",
      "train loss:0.6298914395873071\n",
      "train loss:0.5629171101211474\n",
      "train loss:0.620128412006834\n",
      "train loss:0.7855973914466651\n",
      "train loss:0.6330175904539703\n",
      "train loss:0.5602666265581414\n",
      "train loss:0.6763450850080749\n",
      "train loss:0.5633783034762956\n",
      "train loss:0.7308717036430663\n",
      "train loss:0.6223872115834992\n",
      "train loss:0.50164300669071\n",
      "train loss:0.6807278391737472\n",
      "train loss:0.6136985000431016\n",
      "train loss:0.8766149862291364\n",
      "train loss:0.6088182087149068\n",
      "train loss:0.5521870520982997\n",
      "train loss:0.6513202644728222\n",
      "train loss:0.5509337857873533\n",
      "train loss:0.6699597563670061\n",
      "train loss:0.6189091448268474\n",
      "train loss:0.4559352200713649\n",
      "train loss:0.5954418955596723\n",
      "train loss:0.43662632905245446\n",
      "train loss:0.4077748513253228\n",
      "train loss:0.7444597037123196\n",
      "train loss:0.49973738959960584\n",
      "train loss:0.7241895741473856\n",
      "train loss:0.631172993402042\n",
      "train loss:0.7399599165816613\n",
      "train loss:0.509324827514201\n",
      "train loss:0.7490781848402569\n",
      "train loss:0.5202969107902102\n",
      "train loss:0.6094650642586547\n",
      "train loss:0.7070088305604403\n",
      "train loss:0.7200773933633583\n",
      "train loss:0.5328681598086659\n",
      "train loss:0.5441578060655037\n",
      "train loss:0.6051496478205485\n",
      "train loss:0.5990426797853866\n",
      "train loss:0.6795353049246795\n",
      "train loss:0.731051107598428\n",
      "train loss:0.47651392451238356\n",
      "train loss:0.6830889267860425\n",
      "train loss:0.8453002480662425\n",
      "train loss:0.4452334781158756\n",
      "train loss:0.6178082335752431\n",
      "train loss:0.7384065935847797\n",
      "train loss:0.5788182802311692\n",
      "train loss:0.5616894469402862\n",
      "train loss:0.6798516985030597\n",
      "train loss:0.7427408355806175\n",
      "train loss:0.5484561254085552\n",
      "train loss:0.5467437386458165\n",
      "train loss:0.39004025905931633\n",
      "train loss:0.5045444865216325\n",
      "train loss:0.614945238499141\n",
      "train loss:0.6258536573819544\n",
      "train loss:0.7165984092270901\n",
      "train loss:0.4843588524633999\n",
      "train loss:0.7458232423158135\n",
      "train loss:0.3799497721367905\n",
      "train loss:0.6255903350721843\n",
      "train loss:0.37309912376141874\n",
      "train loss:0.5236465594368017\n",
      "train loss:0.47134438680206187\n",
      "train loss:0.6575279183750637\n",
      "train loss:0.3570893529812496\n",
      "train loss:0.3478171167125567\n",
      "train loss:1.0840604438249253\n",
      "train loss:0.6486824813740534\n",
      "train loss:0.9049533622497037\n",
      "train loss:0.6294674235835658\n",
      "train loss:0.7863586521062754\n",
      "train loss:0.374767683606686\n",
      "train loss:0.8825949612305873\n",
      "train loss:0.5007190001470897\n",
      "train loss:0.6762351676275801\n",
      "train loss:0.6681373503932055\n",
      "train loss:0.6341519292217053\n",
      "train loss:0.6758089784746085\n",
      "train loss:0.6874920919235133\n",
      "train loss:0.5863161209316042\n",
      "train loss:0.6387027766315446\n",
      "train loss:0.6839876480804663\n",
      "train loss:0.644861435940521\n",
      "train loss:0.6746593680479038\n",
      "train loss:0.6804264138557451\n",
      "train loss:0.6352637131565935\n",
      "train loss:0.5825370710341703\n",
      "train loss:0.5064867451714822\n",
      "train loss:0.7873041652078085\n",
      "train loss:0.5390013273790052\n",
      "train loss:0.6906520168839616\n",
      "train loss:0.5361162517107841\n",
      "train loss:0.6144352078533024\n",
      "train loss:0.5146991030426219\n",
      "train loss:0.9657355675060353\n",
      "train loss:0.6949514710038194\n",
      "train loss:0.4317106472202973\n",
      "train loss:0.5062784920749909\n",
      "train loss:0.810540768030218\n",
      "train loss:0.6158054747960294\n",
      "train loss:0.7029713849522757\n",
      "train loss:0.7032113918074774\n",
      "train loss:0.5349748250260682\n",
      "train loss:0.5344384020674905\n",
      "train loss:0.6115796831399983\n",
      "train loss:0.7818195416450303\n",
      "train loss:0.606834535544033\n",
      "train loss:0.46390123078337686\n",
      "train loss:0.6785352843012638\n",
      "train loss:0.36876559509081897\n",
      "train loss:0.7679157323800456\n",
      "train loss:0.5149623068732172\n",
      "train loss:0.6065280641732472\n",
      "train loss:0.6280646172724608\n",
      "train loss:0.6935677383654026\n",
      "train loss:0.5188157089990092\n",
      "train loss:0.7041364511208823\n",
      "train loss:0.5230321341910708\n",
      "train loss:0.8792517003476975\n",
      "train loss:0.45466867195619065\n",
      "train loss:0.33775021117073023\n",
      "train loss:0.7039815995370425\n",
      "train loss:0.6058837216904169\n",
      "train loss:0.7001160892173306\n",
      "train loss:0.6930248465025308\n",
      "train loss:0.513123950241873\n",
      "train loss:0.6051711120286172\n",
      "train loss:0.6064413561452446\n",
      "train loss:0.7198539544318299\n",
      "train loss:0.6814948463042183\n",
      "train loss:0.5375745568429078\n",
      "train loss:0.4544327848429092\n",
      "train loss:0.6209063032885576\n",
      "train loss:0.6214426829248318\n",
      "train loss:0.792224400187225\n",
      "train loss:0.6931425643104966\n",
      "train loss:0.5894150040037476\n",
      "train loss:0.6229553805430592\n",
      "train loss:0.6135195438810264\n",
      "train loss:0.5360971884376803\n",
      "train loss:0.5387504173353692\n",
      "train loss:0.6036695223145966\n",
      "train loss:0.6085111445089167\n",
      "train loss:0.6046149285415247\n",
      "train loss:0.6258844877125682\n",
      "train loss:0.8570830572265418\n",
      "train loss:0.6986934154498549\n",
      "train loss:0.6186172125666738\n",
      "train loss:0.6834007130948911\n",
      "train loss:0.6140832211363699\n",
      "train loss:0.7906934105296104\n",
      "train loss:0.6772136211707085\n",
      "train loss:0.6728932424783054\n",
      "train loss:0.6292444261878292\n",
      "train loss:0.6303322257993555\n",
      "train loss:0.5550042911608078\n",
      "train loss:0.7122061290127648\n",
      "train loss:0.6265594873625151\n",
      "train loss:0.7142156432258344\n",
      "train loss:0.6614864448631392\n",
      "train loss:0.6318337030642749\n",
      "train loss:0.7587578419291421\n",
      "train loss:0.7913076738751563\n",
      "train loss:0.7649811559135166\n",
      "train loss:0.7065075793771287\n",
      "train loss:0.6655198448437005\n",
      "train loss:0.6532371625247387\n",
      "train loss:0.6834062526924829\n",
      "train loss:0.6674591049554925\n",
      "train loss:0.6812520375224658\n",
      "train loss:0.680976368025696\n",
      "train loss:0.6034308223972197\n",
      "train loss:0.6118858702970708\n",
      "train loss:0.6253258906206105\n",
      "train loss:0.5174224207393789\n",
      "train loss:0.6228229580355612\n",
      "train loss:0.7579524698648274\n",
      "train loss:0.776971477754655\n",
      "train loss:0.6074208948422422\n",
      "train loss:0.8679031755494331\n",
      "train loss:0.6150484408787947\n",
      "train loss:0.5344196455120955\n",
      "train loss:0.6127348046979522\n",
      "train loss:0.7734208390907613\n",
      "train loss:0.5204998831196174\n",
      "train loss:0.5176266452142189\n",
      "train loss:0.7078606420039042\n",
      "train loss:0.5060630405317337\n",
      "train loss:0.4274940681812656\n",
      "train loss:0.5264986416023981\n",
      "train loss:0.6310972404214892\n",
      "train loss:0.7310786515112665\n",
      "train loss:0.508429928957691\n",
      "train loss:0.6131039512966191\n",
      "train loss:0.9297583624057355\n",
      "train loss:0.5099953066247527\n",
      "train loss:0.6202404383699637\n",
      "train loss:0.5179806925264611\n",
      "train loss:0.6020256139981003\n",
      "train loss:0.8680081231471378\n",
      "train loss:0.45043338839953406\n",
      "train loss:0.6225592544534863\n",
      "train loss:0.6899566093070864\n",
      "train loss:0.4656412021183204\n",
      "train loss:0.7591416241263351\n",
      "train loss:0.5413802717524352\n",
      "train loss:0.5387376896988554\n",
      "train loss:0.5987950581530745\n",
      "train loss:0.44983822064793166\n",
      "train loss:0.6102388122853863\n",
      "train loss:0.6257742244946974\n",
      "train loss:0.7180023771227615\n",
      "train loss:0.41214076306877956\n",
      "train loss:0.30630501694010076\n",
      "train loss:0.6098599280297062\n",
      "train loss:0.6329863210692179\n",
      "train loss:0.6269888036207248\n",
      "train loss:0.639369373969278\n",
      "train loss:0.7358036492309055\n",
      "train loss:0.6441532085962672\n",
      "train loss:0.6182405671112763\n",
      "train loss:0.5891120657649184\n",
      "train loss:0.2912739168937764\n",
      "train loss:0.39040391931407875\n",
      "train loss:0.7393695065176092\n",
      "train loss:0.7185776206366018\n",
      "train loss:0.6184460809029456\n",
      "train loss:0.6235690266343991\n",
      "train loss:0.6144615663419832\n",
      "train loss:0.6164357531407466\n",
      "train loss:0.6819036016007256\n",
      "train loss:0.6114834038640271\n",
      "train loss:0.5464149629841558\n",
      "train loss:0.6822247660264169\n",
      "train loss:0.5331122689621595\n",
      "train loss:0.6141978968651575\n",
      "train loss:0.6182829584286478\n",
      "train loss:0.6814410538863966\n",
      "train loss:0.40411462809256316\n",
      "train loss:0.680682667594668\n",
      "train loss:0.5346374156442486\n",
      "train loss:0.839734691143439\n",
      "train loss:0.6222958580669145\n",
      "train loss:0.5325191020933737\n",
      "train loss:0.6923623964857113\n",
      "train loss:0.6116881102475029\n",
      "train loss:0.5318158544872502\n",
      "train loss:0.353840233983771\n",
      "train loss:0.32449116882403956\n",
      "train loss:0.9114823779016135\n",
      "train loss:0.61381233616845\n",
      "train loss:0.6352942533809932\n",
      "train loss:0.7105473596390358\n",
      "train loss:0.7232346350162409\n",
      "train loss:0.4111580830426395\n",
      "train loss:0.5114184348591013\n",
      "train loss:0.7194212115114476\n",
      "train loss:0.3018424862039205\n",
      "train loss:0.8293595211398126\n",
      "train loss:0.7263574470392038\n",
      "train loss:0.4223926393006246\n",
      "train loss:0.5969640875977811\n",
      "train loss:0.7132143823637412\n",
      "train loss:0.6152358270923178\n",
      "train loss:0.6933331523300789\n",
      "train loss:0.7728557148830287\n",
      "train loss:0.6210185551166287\n",
      "train loss:0.5380895952245833\n",
      "train loss:0.6901226762419694\n",
      "train loss:0.4948256518485394\n",
      "train loss:0.6882343283637085\n",
      "train loss:0.5566531595009738\n",
      "train loss:0.616991954136853\n",
      "train loss:0.47761929543665493\n",
      "train loss:0.5407322225445171\n",
      "train loss:0.7614809026398056\n",
      "train loss:0.60558324063355\n",
      "train loss:0.43130245612875073\n",
      "train loss:0.42035083365203557\n",
      "train loss:0.7430714579009127\n",
      "train loss:0.605964652130078\n",
      "train loss:0.5083821548876211\n",
      "train loss:0.8271045359558761\n",
      "train loss:0.6234372116955014\n",
      "train loss:0.38354758350386975\n",
      "train loss:0.6080953453163805\n",
      "train loss:0.39798993814459294\n",
      "train loss:0.3838478730854038\n",
      "train loss:0.6300273619470325\n",
      "train loss:0.2237152147129604\n",
      "train loss:0.6509124080544069\n",
      "train loss:0.9459553093038539\n",
      "train loss:0.6363775609742137\n",
      "train loss:1.1071254596116509\n",
      "train loss:0.517069278569045\n",
      "train loss:0.5184877206293106\n",
      "train loss:0.4484303357760404\n",
      "train loss:0.45012193860233696\n",
      "train loss:0.6069574587475195\n",
      "train loss:0.7640867252607644\n",
      "train loss:0.6819821982957504\n",
      "train loss:0.6820837866244251\n",
      "train loss:0.856746822339408\n",
      "train loss:0.7197194001219167\n",
      "train loss:0.6369606154776288\n",
      "train loss:0.7543361384051662\n",
      "train loss:0.579008102296706\n",
      "train loss:0.5447370088739052\n",
      "train loss:0.6786518090193712\n",
      "train loss:0.7709687901882626\n",
      "train loss:0.6468557852691585\n",
      "train loss:0.6164427271023886\n",
      "train loss:0.7121446144933838\n",
      "train loss:0.5769588782136912\n",
      "train loss:0.7103255819395795\n",
      "train loss:0.5549538035471258\n",
      "train loss:0.6745148441155442\n",
      "train loss:0.8117612135197276\n",
      "train loss:0.6258581793317756\n",
      "train loss:0.7208744137709175\n",
      "train loss:0.6278299234323925\n",
      "train loss:0.6231517912269146\n",
      "train loss:0.7261947642981955\n",
      "train loss:0.6144616915455807\n",
      "train loss:0.5633207713118724\n",
      "train loss:0.5507804250901323\n",
      "train loss:0.6178534110772136\n",
      "train loss:0.5258601704112686\n",
      "train loss:0.6845351504139784\n",
      "train loss:0.5326827359444408\n",
      "train loss:0.6920167143634766\n",
      "train loss:0.609881388669295\n",
      "train loss:0.7179057006248442\n",
      "train loss:0.5052531150743331\n",
      "train loss:0.6232323626603931\n",
      "train loss:0.6108616236367626\n",
      "train loss:0.7085274297859605\n",
      "train loss:0.40385666636135503\n",
      "train loss:0.720997662733681\n",
      "train loss:0.27970483129771795\n",
      "train loss:0.3833311162349332\n",
      "train loss:0.5018837704679673\n",
      "train loss:0.8659188841421959\n",
      "train loss:0.7349532185075378\n",
      "train loss:0.5229082782771801\n",
      "train loss:0.500633909885884\n",
      "train loss:0.7275109725171439\n",
      "train loss:0.41412909886662097\n",
      "train loss:0.7135234941074439\n",
      "train loss:0.510177051250319\n",
      "train loss:0.7108393618977005\n",
      "train loss:0.7171021154404469\n",
      "train loss:0.6160252573336896\n",
      "train loss:0.6299178925120285\n",
      "train loss:0.7601805410386836\n",
      "train loss:0.6730968913923421\n",
      "train loss:0.7256757557092621\n",
      "train loss:0.6295612934383337\n",
      "train loss:0.6772360335695788\n",
      "train loss:0.6351459445081863\n",
      "train loss:0.7125039991434269\n",
      "train loss:0.6156724159905054\n",
      "train loss:0.644986824636437\n",
      "train loss:0.6771371167885015\n",
      "train loss:0.5767911034217954\n",
      "train loss:0.6352649392952449\n",
      "train loss:0.5944742516568677\n",
      "train loss:0.583624614971755\n",
      "train loss:0.5103401497281798\n",
      "train loss:0.8051380122635912\n",
      "train loss:0.6161606561422616\n",
      "train loss:0.5320817812731049\n",
      "train loss:0.3455829736436717\n",
      "train loss:0.8002538382366501\n",
      "train loss:0.8945707160004194\n",
      "train loss:0.9903285124085549\n",
      "train loss:0.700430275499473\n",
      "train loss:0.6918947156468865\n",
      "train loss:0.5326684341778058\n",
      "train loss:0.551016595938597\n",
      "train loss:0.672028661939428\n",
      "train loss:0.6160210558581066\n",
      "train loss:0.6799383486567232\n",
      "train loss:0.6173351064633847\n",
      "train loss:0.5503789946771632\n",
      "train loss:0.6178013713062065\n",
      "train loss:0.4774456456500502\n",
      "train loss:0.5437261522838336\n",
      "train loss:0.6192076725949128\n",
      "train loss:0.703408382832424\n",
      "train loss:0.6994830265924316\n",
      "train loss:0.6056539361585508\n",
      "train loss:0.5991308220501287\n",
      "train loss:0.6139226387704335\n",
      "train loss:0.4308898360260855\n",
      "train loss:0.6960638290202541\n",
      "train loss:0.7087515096190982\n",
      "train loss:0.5135632341328351\n",
      "train loss:0.40415044824427326\n",
      "train loss:0.7179383227261207\n",
      "train loss:0.5036758812665351\n",
      "train loss:0.519793745922331\n",
      "train loss:0.27214248978717503\n",
      "train loss:0.6475816589307554\n",
      "train loss:0.7270799238189132\n",
      "train loss:0.49516574033115884\n",
      "train loss:0.49565330673061825\n",
      "train loss:0.49046038711549633\n",
      "train loss:0.49944766896023174\n",
      "train loss:0.7784587518025987\n",
      "train loss:0.5010199281369385\n",
      "train loss:0.358795639805876\n",
      "train loss:0.50920475318809\n",
      "train loss:0.7591094933262411\n",
      "train loss:0.6624125370133955\n",
      "train loss:0.512333216511199\n",
      "train loss:0.7295933821736675\n",
      "train loss:0.6218946526382194\n",
      "train loss:0.6293232579845326\n",
      "train loss:0.6193916332340115\n",
      "train loss:0.4485242908309696\n",
      "train loss:0.44261070903936356\n",
      "train loss:0.6164723763824501\n",
      "train loss:0.7009314335033856\n",
      "train loss:0.6116913942500117\n",
      "train loss:0.35707480072934406\n",
      "train loss:0.5288049695198457\n",
      "train loss:0.609020801934413\n",
      "train loss:0.42439467453165003\n",
      "train loss:0.6180282236193572\n",
      "train loss:0.7276119945724349\n",
      "train loss:0.8124412403670489\n",
      "train loss:0.6149159305977451\n",
      "train loss:0.8069003231177808\n",
      "train loss:0.7017495737336608\n",
      "train loss:0.6176115126109599\n",
      "train loss:0.7586124900306023\n",
      "train loss:0.6215567941428966\n",
      "train loss:0.5535529703532428\n",
      "train loss:0.7355831561626639\n",
      "train loss:0.6245053829231401\n",
      "train loss:0.7736422681035355\n",
      "train loss:0.720765632736845\n",
      "train loss:0.6774041486631921\n",
      "train loss:0.6440250639291576\n",
      "train loss:0.5739513681924313\n",
      "train loss:0.7437791855698829\n",
      "train loss:0.6778572984881555\n",
      "train loss:0.6507466735829835\n",
      "train loss:0.5804282586175388\n",
      "train loss:0.6405216244174377\n",
      "train loss:0.5983965838624072\n",
      "train loss:0.6330479611328459\n",
      "train loss:0.5690549671270427\n",
      "train loss:0.7754197728082397\n",
      "train loss:0.5601346077570577\n",
      "train loss:0.6798553099664377\n",
      "train loss:0.5448568403113636\n",
      "train loss:0.5291893432723664\n",
      "train loss:0.6989573872236312\n",
      "train loss:0.43317011583857257\n",
      "train loss:0.7095029182705204\n",
      "train loss:0.5154218431476654\n",
      "train loss:0.3903262596792122\n",
      "train loss:0.7397689958454066\n",
      "train loss:0.6245111915076916\n",
      "train loss:0.7590719672612163\n",
      "train loss:0.3717037510805185\n",
      "train loss:0.8673200791676979\n",
      "train loss:0.632472092066477\n",
      "train loss:0.8294883762193169\n",
      "train loss:0.7072319505453065\n",
      "train loss:0.6996556434307688\n",
      "train loss:0.6143763877239035\n",
      "train loss:0.5464675296352737\n",
      "train loss:0.6155830338396253\n",
      "train loss:0.6177682349021228\n",
      "train loss:0.43317739232022856\n",
      "train loss:0.6799489452001688\n",
      "train loss:0.5517419876332379\n",
      "train loss:0.5524915615511117\n",
      "train loss:0.680270987583715\n",
      "train loss:0.5495675106945338\n",
      "train loss:0.7719861969829048\n",
      "train loss:0.8831263809616694\n",
      "train loss:0.6123177777001125\n",
      "train loss:0.6223463899626706\n",
      "train loss:0.7961528665446556\n",
      "train loss:0.7875620199386478\n",
      "train loss:0.5756264986634655\n",
      "train loss:0.6780793666412817\n",
      "train loss:0.6271348601257444\n",
      "train loss:0.5375752877840448\n",
      "train loss:0.5297537621061581\n",
      "train loss:0.7234726629895698\n",
      "train loss:0.6251079343579102\n",
      "train loss:0.6832479486612233\n",
      "train loss:0.7326307257870344\n",
      "train loss:0.7314739245057226\n",
      "train loss:0.7802352697361646\n",
      "train loss:0.6207993682315653\n",
      "train loss:0.6763551260027424\n",
      "train loss:0.7181167796769122\n",
      "train loss:0.5882703770866813\n",
      "train loss:0.5837489894105184\n",
      "train loss:0.6251003928624295\n",
      "train loss:0.6240665298822204\n",
      "train loss:0.6237045233699051\n",
      "train loss:0.6220255516016121\n",
      "train loss:0.4916604353253417\n",
      "train loss:0.6117941091295638\n",
      "train loss:0.6911208531261865\n",
      "train loss:0.6117859072019136\n",
      "train loss:0.6087665670476163\n",
      "train loss:0.8588461150566375\n",
      "train loss:0.5258146384874626\n",
      "train loss:0.6072102156860197\n",
      "train loss:0.6232411237811123\n",
      "train loss:0.6170675360216189\n",
      "train loss:0.7796586328367817\n",
      "train loss:0.42748422247004514\n",
      "train loss:0.6969701272634218\n",
      "train loss:0.92453865896217\n",
      "train loss:0.6086770963023848\n",
      "train loss:0.5379525977804832\n",
      "train loss:0.3967276065520253\n",
      "train loss:0.5342133530050284\n",
      "train loss:0.691301851088413\n",
      "train loss:0.5370395920894222\n",
      "train loss:0.5184651926139103\n",
      "train loss:0.422909787839568\n",
      "train loss:0.6126804707016504\n",
      "train loss:0.5151068929982532\n",
      "train loss:0.7258410698553469\n",
      "train loss:0.7375473836374422\n",
      "train loss:0.7301187291809208\n",
      "train loss:0.5052814419936403\n",
      "train loss:0.7129252927044488\n",
      "train loss:0.6130784445002562\n",
      "train loss:0.618315101486987\n",
      "train loss:0.6987388078062011\n",
      "train loss:0.6108781259268273\n",
      "train loss:0.766425755241935\n",
      "train loss:0.47266631006655124\n",
      "train loss:0.697010301052863\n",
      "train loss:0.47894316098455303\n",
      "train loss:0.6814059246198291\n",
      "train loss:0.6808770237359286\n",
      "train loss:0.6151596129190465\n",
      "train loss:0.6187722372507902\n",
      "train loss:0.6236098518320566\n",
      "train loss:0.6140909651450908\n",
      "train loss:0.6733220800880519\n",
      "train loss:0.6146206908740474\n",
      "train loss:0.6860460147718006\n",
      "train loss:0.5539857727893\n",
      "train loss:0.538247994370852\n",
      "train loss:0.748787049525238\n",
      "train loss:0.46765249882222626\n",
      "train loss:0.61713261152577\n",
      "train loss:0.44468010726068685\n",
      "train loss:0.6157328143661035\n",
      "train loss:0.5075660246429845\n",
      "train loss:0.7309338974548023\n",
      "train loss:0.8166023470614361\n",
      "train loss:0.8033936172833342\n",
      "train loss:0.6151703383676377\n",
      "train loss:0.776197121407417\n",
      "train loss:0.6858576566682666\n",
      "train loss:0.4684012927970704\n",
      "train loss:0.694848412425465\n",
      "train loss:0.7411815548552739\n",
      "train loss:0.4913936462482531\n",
      "train loss:0.6233618242409391\n",
      "train loss:0.6808342033351694\n",
      "train loss:0.4989858518665452\n",
      "train loss:0.6136871428543744\n",
      "train loss:0.6197910472653688\n",
      "train loss:0.5488778858020488\n",
      "train loss:0.5438624856063254\n",
      "train loss:0.5347237438979924\n",
      "train loss:0.6058991122052316\n",
      "train loss:0.7026911884039173\n",
      "train loss:0.3241187444381175\n",
      "train loss:0.8166538903536272\n",
      "train loss:0.5051264669998138\n",
      "train loss:0.6161385468854357\n",
      "train loss:0.9379298401297589\n",
      "train loss:0.5047515959979079\n",
      "train loss:0.6195786648219992\n",
      "train loss:0.7997821399542306\n",
      "train loss:0.415504541146415\n",
      "train loss:0.5198778038688378\n",
      "train loss:0.7005292113737359\n",
      "train loss:0.5173201814170872\n",
      "train loss:0.7756435791994328\n",
      "train loss:0.6063325886123101\n",
      "train loss:0.4431992909441296\n",
      "train loss:0.6097146447360966\n",
      "train loss:0.5226406417314065\n",
      "train loss:0.8624278559527856\n",
      "train loss:0.5244746807490571\n",
      "train loss:0.6153985892440504\n",
      "train loss:0.6134327699249869\n",
      "train loss:0.5375398504859865\n",
      "train loss:0.6091003362027669\n",
      "train loss:0.6936909587639981\n",
      "train loss:0.5304233227790668\n",
      "train loss:0.6122803327819589\n",
      "train loss:0.6141000096756579\n",
      "train loss:0.6958224837473972\n",
      "train loss:0.6096848050800412\n",
      "train loss:0.9193643610395782\n",
      "train loss:0.6100721049219446\n",
      "train loss:0.6122870698723923\n",
      "train loss:0.739855803264746\n",
      "train loss:0.618086574808185\n",
      "train loss:0.5644991384849382\n",
      "train loss:0.6195069875816964\n",
      "train loss:0.5016383197973375\n",
      "train loss:0.48916923488525443\n",
      "train loss:0.547317771460125\n",
      "train loss:0.6138699628775148\n",
      "train loss:0.5299631684433262\n",
      "train loss:0.42987338774922856\n",
      "train loss:0.7113407472569094\n",
      "train loss:0.6147777531137255\n",
      "train loss:0.6189007871506063\n",
      "train loss:0.2709476760664279\n",
      "train loss:0.5025622572292907\n",
      "train loss:0.890467767072046\n",
      "train loss:0.635987277675069\n",
      "train loss:0.8837300501896148\n",
      "train loss:0.5073571953298222\n",
      "train loss:0.7426667098311827\n",
      "train loss:0.6217505434742211\n",
      "train loss:0.5960464265548969\n",
      "train loss:0.5134409260030768\n",
      "train loss:0.6157505744628996\n",
      "train loss:0.6051855537393649\n",
      "train loss:0.36048830032924395\n",
      "train loss:0.7746915196522475\n",
      "train loss:0.6936166970678144\n",
      "train loss:0.45620290264582974\n",
      "train loss:0.7626549061725254\n",
      "train loss:0.534134107161191\n",
      "train loss:0.45846920406533875\n",
      "train loss:0.6912684819440349\n",
      "train loss:0.5318292527297965\n",
      "train loss:0.43589673000285944\n",
      "train loss:0.4270167038333012\n",
      "train loss:0.40416534550641525\n",
      "train loss:0.8377583035589368\n",
      "train loss:0.8540750533174315\n",
      "train loss:0.6241822920245508\n",
      "train loss:0.2918490103999673\n",
      "train loss:0.7478119123131977\n",
      "train loss:0.7249245941530644\n",
      "train loss:0.5191026283668752\n",
      "train loss:0.7020456176601333\n",
      "train loss:0.7099819226251997\n",
      "train loss:0.6258316174340636\n",
      "train loss:0.7676183126261467\n",
      "train loss:0.7645300446784264\n",
      "train loss:0.6190277937481807\n",
      "train loss:0.680721118694831\n",
      "train loss:0.7812514813813541\n",
      "train loss:0.632248945043027\n",
      "train loss:0.6785919125504437\n",
      "train loss:0.642160603956234\n",
      "train loss:0.6432734970937555\n",
      "train loss:0.680211236315085\n",
      "train loss:0.60944528193008\n",
      "train loss:0.6064356074222773\n",
      "train loss:0.5613246617403463\n",
      "train loss:0.6329909037106819\n",
      "train loss:0.7655909071227692\n",
      "train loss:0.7200132004074906\n",
      "train loss:0.6298130567223053\n",
      "train loss:0.5733802297877689\n",
      "train loss:0.67529166500921\n",
      "train loss:0.6762804143940168\n",
      "train loss:0.557845936229967\n",
      "train loss:0.5519296591754093\n",
      "train loss:0.6101179785888612\n",
      "train loss:0.7627926301202747\n",
      "train loss:0.532543541210008\n",
      "train loss:0.6100098071829682\n",
      "train loss:0.7006658139546953\n",
      "train loss:0.612578409772458\n",
      "train loss:0.5172836452404967\n",
      "train loss:0.5143991204886118\n",
      "train loss:0.6131430635495271\n",
      "train loss:0.7223609775533513\n",
      "train loss:0.4104507429985878\n",
      "train loss:0.2798453525643371\n",
      "train loss:0.5093940154169668\n",
      "train loss:0.6248066640354838\n",
      "train loss:0.6447961328628418\n",
      "train loss:0.3654688141446281\n",
      "train loss:0.6394060109685972\n",
      "train loss:0.3819204593909217\n",
      "train loss:0.3465260880981347\n",
      "train loss:0.8100290327222475\n",
      "train loss:0.34366891658784915\n",
      "train loss:0.34087188811181696\n",
      "train loss:0.6579207674690584\n",
      "train loss:0.5156776491058922\n",
      "train loss:0.5030498557013379\n",
      "train loss:0.6498100345846256\n",
      "train loss:0.3535768107676469\n",
      "train loss:0.7787786171547604\n",
      "train loss:0.9877159571217901\n",
      "train loss:0.28586223073557904\n",
      "train loss:0.9009311362654058\n",
      "train loss:0.7703587896471511\n",
      "train loss:0.4701128068079705\n",
      "train loss:0.550335661039852\n",
      "train loss:0.5610971260430813\n",
      "train loss:0.6760162101867512\n",
      "train loss:0.5681776055625332\n",
      "train loss:0.6242770863665872\n",
      "train loss:0.51257633699225\n",
      "train loss:0.678588856188825\n",
      "train loss:0.5620434901031645\n",
      "train loss:0.556791524642134\n",
      "train loss:0.5496664217859482\n",
      "train loss:0.5394156085770502\n",
      "train loss:0.6169417057116922\n",
      "train loss:0.6954120018880362\n",
      "train loss:0.5249963594979514\n",
      "train loss:0.4280838543771554\n",
      "train loss:0.612756906074702\n",
      "train loss:0.6243576050642491\n",
      "train loss:0.5060100902553322\n",
      "train loss:0.8424607612248607\n",
      "train loss:0.6210403896829\n",
      "train loss:0.5061884556236895\n",
      "train loss:0.4995767163889628\n",
      "train loss:0.3785127017834532\n",
      "train loss:0.6299837737139764\n",
      "train loss:0.625814132950113\n",
      "train loss:0.6236454877209676\n",
      "train loss:0.5092198492971228\n",
      "train loss:0.7406169610835427\n",
      "train loss:0.5031441902420346\n",
      "train loss:0.5007164296435118\n",
      "train loss:0.5067285578151551\n",
      "train loss:0.5039254175416022\n",
      "train loss:0.4937047794949633\n",
      "train loss:0.5072608343262631\n",
      "train loss:0.9517305149601054\n",
      "train loss:0.5090864099787429\n",
      "train loss:0.5083261892261343\n",
      "train loss:0.7147964834011817\n",
      "train loss:0.70540624584507\n",
      "train loss:0.5284206277727955\n",
      "train loss:0.6143128116995517\n",
      "train loss:0.4380933829772277\n",
      "train loss:0.6102345821338782\n",
      "train loss:0.341919693526559\n",
      "train loss:0.6138814251513379\n",
      "train loss:0.7025569757978996\n",
      "train loss:0.3173205717799585\n",
      "train loss:0.7188107639641019\n",
      "train loss:0.6145061080930493\n",
      "train loss:0.6112573599342304\n",
      "train loss:0.5138582982895672\n",
      "train loss:0.5052428783316562\n",
      "train loss:0.6189563055200539\n",
      "train loss:0.3878975459248368\n",
      "train loss:0.6202097931219801\n",
      "train loss:0.6191537449344733\n",
      "train loss:0.6360236995411707\n",
      "train loss:0.38169562630658305\n",
      "train loss:0.7393551515010199\n",
      "train loss:0.8527438079667661\n",
      "train loss:0.6183133689924972\n",
      "train loss:0.8087789745144086\n",
      "train loss:0.6133267073311272\n",
      "train loss:0.5278711367700686\n",
      "train loss:0.6883883441028372\n",
      "train loss:0.467441664110714\n",
      "train loss:0.6149473362644481\n",
      "train loss:0.4711960303205004\n",
      "train loss:0.6198114238436715\n",
      "train loss:0.5418584207464696\n",
      "train loss:0.6116249495180636\n",
      "train loss:0.53435906780786\n",
      "train loss:0.6112977081019053\n",
      "train loss:0.5249482064199276\n",
      "train loss:0.6145479752163674\n",
      "train loss:0.7024680636419054\n",
      "train loss:0.6106066307885382\n",
      "train loss:0.6118964000732754\n",
      "train loss:0.8002247796902097\n",
      "train loss:0.5170808407038093\n",
      "train loss:0.7905511375507444\n",
      "train loss:0.697841749975819\n",
      "train loss:0.7680229623868626\n",
      "train loss:0.5373421247504674\n",
      "train loss:0.7533758119508894\n",
      "train loss:0.681825940699455\n",
      "train loss:0.6795200941432944\n",
      "train loss:0.6215126626063336\n",
      "train loss:0.6775785595213608\n",
      "train loss:0.6760770528599394\n",
      "train loss:0.6763304815769996\n",
      "train loss:0.6339286750464013\n",
      "train loss:0.5893642499949394\n",
      "train loss:0.7192906750519511\n",
      "train loss:0.5920231789467063\n",
      "train loss:0.6321155868571409\n",
      "train loss:0.6756100341871797\n",
      "train loss:0.6751728751297372\n",
      "train loss:0.5785033928500755\n",
      "train loss:0.675062805877715\n",
      "train loss:0.5700937880404651\n",
      "train loss:0.49985170789592576\n",
      "train loss:0.5490840422464427\n",
      "train loss:0.6101755399024168\n",
      "train loss:0.6922467711584265\n",
      "train loss:0.6111348240044747\n",
      "train loss:0.7058076893103971\n",
      "train loss:0.6075216730383646\n",
      "train loss:0.4143704822583888\n",
      "train loss:0.5105348172465258\n",
      "train loss:0.508253299282211\n",
      "train loss:0.6224432653370091\n",
      "train loss:0.5042334192451242\n",
      "train loss:0.6262879609725396\n",
      "train loss:0.6391210592171637\n",
      "train loss:0.6337723827547496\n",
      "train loss:0.3645544805467902\n",
      "train loss:0.7566001600566592\n",
      "train loss:0.6334968474088194\n",
      "train loss:0.5081113833695534\n",
      "train loss:0.4969249686157343\n",
      "train loss:0.5040035843729753\n",
      "train loss:0.4987426073348633\n",
      "train loss:0.7304356055864037\n",
      "train loss:0.3935491522404463\n",
      "train loss:0.6162703282158077\n",
      "train loss:0.7191168358579431\n",
      "train loss:0.7114593486178348\n",
      "train loss:0.611468307004752\n",
      "train loss:0.42446313940122693\n",
      "train loss:0.42746685914494664\n",
      "train loss:0.6956860244580042\n",
      "train loss:0.5187909588454849\n",
      "train loss:0.5183714308248224\n",
      "train loss:0.5096285167419449\n",
      "train loss:0.5024003313088833\n",
      "train loss:0.5032668180761739\n",
      "train loss:0.5046648939214966\n",
      "train loss:0.5094020437056066\n",
      "train loss:0.6303210483805132\n",
      "train loss:0.49986736247571795\n",
      "train loss:0.21908630408140145\n",
      "train loss:0.6339699447687235\n",
      "train loss:0.7825544934318656\n",
      "train loss:0.9125087697076534\n",
      "train loss:0.6289578582028269\n",
      "train loss:0.6169104165005557\n",
      "train loss:0.39925481221394765\n",
      "train loss:0.7184183865417717\n",
      "train loss:0.5152449756284562\n",
      "train loss:0.5151955874010267\n",
      "train loss:0.612488896931467\n",
      "train loss:0.5266979223427204\n",
      "train loss:0.5224128212416312\n",
      "train loss:0.6052127644800293\n",
      "train loss:0.6985995325631156\n",
      "train loss:0.6940993629469275\n",
      "train loss:0.6124260745051643\n",
      "train loss:0.6068964492541447\n",
      "train loss:0.6114835581218928\n",
      "train loss:0.6164222994968436\n",
      "train loss:0.6145525447462513\n",
      "train loss:0.6143449944524704\n",
      "train loss:0.4603604986410136\n",
      "train loss:0.37104844473517234\n",
      "train loss:0.5263848165406565\n",
      "train loss:0.31680977684532585\n",
      "train loss:0.617315338771778\n",
      "train loss:0.5037046695208123\n",
      "train loss:0.3766825905366093\n",
      "train loss:0.20929904122729312\n",
      "train loss:0.7998382184256552\n",
      "train loss:0.8158748505843396\n",
      "train loss:0.17018504244340216\n",
      "train loss:0.49696729867660394\n",
      "train loss:0.6739779178252479\n",
      "train loss:0.8773623005594716\n",
      "train loss:0.67063209668552\n",
      "train loss:0.6291107582393556\n",
      "train loss:0.3638120053047343\n",
      "train loss:0.6300507450477596\n",
      "train loss:0.6168426070069368\n",
      "train loss:0.6226572345696502\n",
      "train loss:0.5132023633351046\n",
      "train loss:0.7005453928210154\n",
      "train loss:0.5277111087101276\n",
      "train loss:0.6945780979640702\n",
      "train loss:0.7601103936563955\n",
      "train loss:0.6138462403402472\n",
      "train loss:0.6868484469922966\n",
      "train loss:0.6259687793784859\n",
      "train loss:0.6763149438411133\n",
      "train loss:0.630633610548045\n",
      "train loss:0.5772719048985075\n",
      "train loss:0.5747437851277174\n",
      "train loss:0.6264953779471748\n",
      "train loss:0.6780693392224565\n",
      "train loss:0.732379025485856\n",
      "train loss:0.7307390280233717\n",
      "train loss:0.5680221351338652\n",
      "train loss:0.7827388167452181\n",
      "train loss:0.6742132162314223\n",
      "train loss:0.6259582159920559\n",
      "train loss:0.6748035110654721\n",
      "train loss:0.626018371826885\n",
      "train loss:0.6221637255034859\n",
      "train loss:0.6731404689009761\n",
      "train loss:0.620583635260666\n",
      "train loss:0.6179304812982248\n",
      "train loss:0.7310628016688068\n",
      "train loss:0.5622732321549055\n",
      "train loss:0.6189933725230932\n",
      "train loss:0.6822459778501919\n",
      "train loss:0.6153742571560756\n",
      "train loss:0.6107901188182174\n",
      "train loss:0.47052338708824887\n",
      "train loss:0.5308765854094408\n",
      "train loss:0.696074657374385\n",
      "train loss:0.694126934760225\n",
      "train loss:0.43954735716155546\n",
      "train loss:0.4149348490343206\n",
      "train loss:0.6284720779703907\n",
      "train loss:0.6366711245310708\n",
      "train loss:0.6166615649832277\n",
      "train loss:0.7312485316407236\n",
      "train loss:0.827448861223751\n",
      "train loss:0.3988930400933824\n",
      "train loss:0.6262292902133384\n",
      "train loss:0.5047006930125673\n",
      "train loss:0.39765802814517925\n",
      "train loss:0.7378492389660323\n",
      "train loss:0.6147443024438577\n",
      "train loss:0.6181452887822136\n",
      "train loss:0.41216876585741113\n",
      "train loss:0.5135722168507113\n",
      "train loss:0.7117911316863659\n",
      "train loss:0.4113956699320337\n",
      "train loss:0.6232715608516326\n",
      "train loss:0.8055673237197191\n",
      "train loss:0.611593435344064\n",
      "train loss:0.7947284985118013\n",
      "train loss:0.6943347323940597\n",
      "train loss:0.8295863694347696\n",
      "train loss:0.68191180013337\n",
      "train loss:0.5026552760252363\n",
      "train loss:0.620438496871843\n",
      "train loss:0.7205160815528987\n",
      "train loss:0.674435486653685\n",
      "train loss:0.6733704532894026\n",
      "train loss:0.5531372307420936\n",
      "train loss:0.5922547264306719\n",
      "train loss:0.5438249833613074\n",
      "train loss:0.626804378177536\n",
      "train loss:0.7228436254697082\n",
      "train loss:0.6740791577107565\n",
      "train loss:0.5692696739913766\n",
      "train loss:0.7321015110827332\n",
      "train loss:0.6172581360153861\n",
      "train loss:0.6774109744514603\n",
      "train loss:0.6786534684067247\n",
      "train loss:0.7998552969831103\n",
      "train loss:0.5567292796679892\n",
      "train loss:0.8510685835450819\n",
      "train loss:0.6229851522444472\n",
      "train loss:0.6758800732101647\n",
      "train loss:0.6218470014190207\n",
      "train loss:0.6240492899521385\n",
      "train loss:0.6243228147052398\n",
      "train loss:0.6735051163303675\n",
      "train loss:0.5146129398147492\n",
      "train loss:0.6189812419196306\n",
      "train loss:0.6184910954068195\n",
      "train loss:0.551123549712111\n",
      "train loss:0.6141255352798403\n",
      "train loss:0.6114582177760133\n",
      "train loss:0.6089831635088403\n",
      "train loss:0.5272743051141816\n",
      "train loss:0.698736155057835\n",
      "train loss:0.5182451983014156\n",
      "train loss:0.4062910916572736\n",
      "train loss:0.5042398929664984\n",
      "train loss:0.6178359132815301\n",
      "train loss:0.9691792257506391\n",
      "train loss:0.3878842982587033\n",
      "train loss:0.3878858343458769\n",
      "train loss:0.5050703901641713\n",
      "train loss:0.758338743724165\n",
      "train loss:0.7466549235038091\n",
      "train loss:0.37421322476460006\n",
      "train loss:0.5036075393994074\n",
      "train loss:0.5088649109524043\n",
      "train loss:0.6146178901143464\n",
      "train loss:0.7340140158419224\n",
      "train loss:0.26399688669127197\n",
      "train loss:0.6228239099204075\n",
      "train loss:0.3868182551645171\n",
      "train loss:0.37990712241900515\n",
      "train loss:0.3661491337128085\n",
      "train loss:0.5024340931210258\n",
      "train loss:0.34531261404766883\n",
      "train loss:0.5028612556255634\n",
      "train loss:0.6575910432663817\n",
      "train loss:0.49680109514440873\n",
      "train loss:0.6728329691224466\n",
      "train loss:0.8046633930145605\n",
      "train loss:0.9261295980186937\n",
      "train loss:0.37782319969900535\n",
      "train loss:0.6232822218350382\n",
      "train loss:0.7152605547319635\n",
      "train loss:0.4265631460877383\n",
      "train loss:0.5258908162757939\n",
      "train loss:0.6130086280651201\n",
      "train loss:0.44826091665040924\n",
      "train loss:0.6120590847922862\n",
      "train loss:0.6167574985615596\n",
      "train loss:0.6149363852645001\n",
      "train loss:0.4560962438170352\n",
      "train loss:0.4450709788619129\n",
      "train loss:0.7796444493112696\n",
      "train loss:0.5232979047278132\n",
      "train loss:0.43044780520257514\n",
      "train loss:0.7990178830020224\n",
      "train loss:0.609949847869631\n",
      "train loss:0.5118760485991884\n",
      "train loss:0.6169761909954381\n",
      "train loss:0.6131752306613916\n",
      "train loss:0.4121610998423203\n",
      "train loss:0.5105488185290538\n",
      "train loss:0.27439584608686646\n",
      "train loss:0.49498609591433923\n",
      "train loss:0.8808968291969261\n",
      "train loss:0.5022553574559019\n",
      "train loss:0.5021137676315557\n",
      "train loss:0.3518263730213721\n",
      "train loss:0.3528717309322903\n",
      "train loss:0.6456304031534889\n",
      "train loss:0.6668575119253839\n",
      "train loss:0.3458528429297905\n",
      "train loss:0.3463615673053245\n",
      "train loss:1.1000959579535803\n",
      "train loss:0.49537257864683654\n",
      "train loss:0.36060402696199023\n",
      "train loss:0.6382670658927667\n",
      "train loss:0.4992675635402688\n",
      "train loss:0.8519593413344205\n",
      "train loss:0.39357121294700825\n",
      "train loss:0.8265302134159448\n",
      "train loss:0.5163122135074083\n",
      "train loss:0.42876004326975503\n",
      "train loss:0.8689702751390603\n",
      "train loss:0.7669522046763831\n",
      "train loss:0.5419388668500037\n",
      "train loss:0.47711338887664106\n",
      "train loss:0.613848990251502\n",
      "train loss:0.5489633908663494\n",
      "train loss:0.617394780088679\n",
      "train loss:0.7501279271679356\n",
      "train loss:0.9206578442328148\n",
      "train loss:0.788986061705369\n",
      "train loss:0.5229286504166863\n",
      "train loss:0.6777606192534452\n",
      "train loss:0.6311285750283421\n",
      "train loss:0.6310933060415368\n",
      "train loss:0.7220145031285504\n",
      "train loss:0.6341527341738199\n",
      "train loss:0.6331435483651919\n",
      "train loss:0.588437885100122\n",
      "train loss:0.6751998231382335\n",
      "train loss:0.6278310512485488\n",
      "train loss:0.6755254280894067\n",
      "train loss:0.4665707514860369\n",
      "train loss:0.6198292532589992\n",
      "train loss:0.6783633908364909\n",
      "train loss:0.48289578055192955\n",
      "train loss:0.5389975613265215\n",
      "train loss:0.6907501567410559\n",
      "train loss:0.6105199786345064\n",
      "train loss:0.6207669960580652\n",
      "train loss:0.31435511411682276\n",
      "train loss:0.5035481400058681\n",
      "train loss:0.5067654225444586\n",
      "train loss:0.8745148173544823\n",
      "train loss:0.4968777659866574\n",
      "train loss:0.6345402190346565\n",
      "train loss:0.5080476291256698\n",
      "train loss:0.5087172772439114\n",
      "train loss:0.8790336368726926\n",
      "train loss:0.7406360767321074\n",
      "train loss:0.619124854999565\n",
      "train loss:0.7150743398972168\n",
      "train loss:0.5145775130871536\n",
      "train loss:0.42876833530156244\n",
      "train loss:0.5187316646199073\n",
      "train loss:0.5240336304774843\n",
      "train loss:0.6967832082656122\n",
      "train loss:0.34482284516750383\n",
      "train loss:0.5193237470437394\n",
      "train loss:0.514423174316872\n",
      "train loss:0.6133448846364628\n",
      "train loss:0.5067265115921473\n",
      "train loss:0.7183403753277002\n",
      "train loss:0.7192055008105893\n",
      "train loss:0.8122352135813651\n",
      "train loss:0.6120065370044767\n",
      "train loss:0.6131750680370583\n",
      "train loss:0.611972091627262\n",
      "train loss:0.5238684625918212\n",
      "train loss:0.5232061679261932\n",
      "train loss:0.5262997011328958\n",
      "train loss:0.4327848959947794\n",
      "train loss:0.32196873355286243\n",
      "train loss:0.8179612737439028\n",
      "train loss:0.5103098799517568\n",
      "train loss:0.5080576564114881\n",
      "train loss:0.5136101499795196\n",
      "train loss:0.25913218141855776\n",
      "train loss:0.635898702767735\n",
      "train loss:0.6270641241622704\n",
      "train loss:0.6300468250970337\n",
      "train loss:0.500976940810933\n",
      "train loss:0.4976516514546507\n",
      "train loss:0.637942377291015\n",
      "train loss:0.5049711803754029\n",
      "train loss:0.6280105455581124\n",
      "train loss:0.6291709423837843\n",
      "train loss:0.6275535284165474\n",
      "train loss:0.5053702912550455\n",
      "train loss:0.5026925011035717\n",
      "train loss:0.6192341585404584\n",
      "train loss:0.7286024400746689\n",
      "train loss:0.6109916631551269\n",
      "train loss:0.7075990871166448\n",
      "train loss:0.6146148150751924\n",
      "train loss:0.5260548422617141\n",
      "train loss:0.6127184394100453\n",
      "train loss:0.6889953737777273\n",
      "train loss:0.38634202653158967\n",
      "train loss:0.4569390567278983\n",
      "train loss:0.44900448619685196\n",
      "train loss:0.5260696749403987\n",
      "train loss:0.7873484308393295\n",
      "train loss:0.5182741272479466\n",
      "train loss:0.5176375360579908\n",
      "train loss:0.30379438885825766\n",
      "train loss:0.5085423483599985\n",
      "train loss:0.6180972133652118\n",
      "train loss:0.5020054289600899\n",
      "train loss:0.7692033286496941\n",
      "train loss:0.2292187423609692\n",
      "train loss:0.2066997830903894\n",
      "train loss:0.6659663481641978\n",
      "train loss:0.9550487571671885\n",
      "train loss:0.49579504841444066\n",
      "train loss:0.5026239124549193\n",
      "train loss:0.5088882584836414\n",
      "train loss:0.6413409257764806\n",
      "train loss:0.893276885611542\n",
      "train loss:0.5002739007123853\n",
      "train loss:0.724685270669747\n",
      "train loss:0.7094308409289068\n",
      "train loss:0.4271792255410235\n",
      "train loss:0.6121428193312115\n",
      "train loss:0.6093381117809998\n",
      "train loss:0.5362392088059186\n",
      "train loss:0.6095473393761272\n",
      "train loss:0.4638760482263839\n",
      "train loss:0.6902599385892492\n",
      "train loss:0.6140841249509734\n",
      "train loss:0.5394849883753332\n",
      "train loss:0.6120954086494975\n",
      "train loss:0.611633209252129\n",
      "train loss:0.36857390251248107\n",
      "train loss:0.6105432470040537\n",
      "train loss:0.5219572422192632\n",
      "train loss:0.6176071987392662\n",
      "train loss:0.5165356189894065\n",
      "train loss:0.5101386633593166\n",
      "train loss:0.6185286043617418\n",
      "train loss:0.503822350764966\n",
      "train loss:0.6185910446982383\n",
      "train loss:0.7383244918950387\n",
      "train loss:0.6195477392584731\n",
      "train loss:0.737969717360134\n",
      "train loss:0.5044049456759453\n",
      "train loss:0.27490100111544724\n",
      "train loss:0.25774995887131014\n",
      "train loss:0.5042900874975433\n",
      "train loss:0.6335413621354703\n",
      "train loss:0.35936010153770404\n",
      "train loss:0.6394133711392219\n",
      "train loss:0.7834323833952317\n",
      "train loss:0.49493094018017814\n",
      "train loss:0.36055386123796285\n",
      "train loss:0.3621565726087948\n",
      "train loss:0.6424800806377907\n",
      "train loss:0.49703821862572506\n",
      "train loss:0.4943949706921552\n",
      "train loss:0.9062373298162834\n",
      "train loss:0.4969450129197859\n",
      "train loss:0.6231418278873092\n",
      "train loss:0.6163475488013767\n",
      "train loss:0.6169787966708989\n",
      "train loss:0.5062102738175686\n",
      "train loss:0.8064171960150388\n",
      "train loss:0.6986459982974602\n",
      "train loss:0.6889470673053767\n",
      "train loss:0.6836302236861242\n",
      "train loss:0.48052683432925375\n",
      "train loss:0.48708406262272347\n",
      "train loss:0.552021423883399\n",
      "train loss:0.6175786328371244\n",
      "train loss:0.5496112967304743\n",
      "train loss:0.6830031210803627\n",
      "train loss:0.5449073696722356\n",
      "train loss:0.6870494905939257\n",
      "train loss:0.7555806677379926\n",
      "train loss:0.6837436117774522\n",
      "train loss:0.6153947060222749\n",
      "train loss:0.5440726799393178\n",
      "train loss:0.6130003299933755\n",
      "train loss:0.6112198182251299\n",
      "train loss:0.5376269330367425\n",
      "train loss:0.6109880987682417\n",
      "train loss:0.530320500488263\n",
      "train loss:0.5267761947618517\n",
      "train loss:0.4296991264508442\n",
      "train loss:0.6103124474163037\n",
      "train loss:0.40595581916681167\n",
      "train loss:0.3876374268936751\n",
      "train loss:0.8648663637040137\n",
      "train loss:0.6250320584715945\n",
      "train loss:0.4987616578668764\n",
      "train loss:0.6297086622853009\n",
      "train loss:0.49782495493508466\n",
      "train loss:0.5043868095250956\n",
      "train loss:0.36215189192285147\n",
      "train loss:0.5012278616262059\n",
      "train loss:0.35372484396441933\n",
      "train loss:0.3438834911743707\n",
      "train loss:0.6671383658288683\n",
      "train loss:0.5096605508799289\n",
      "train loss:0.8136831098096053\n",
      "train loss:0.9291628820268045\n",
      "train loss:0.3594130352966459\n",
      "train loss:0.8750830830873513\n",
      "train loss:0.721707900420067\n",
      "train loss:0.7040454496841572\n",
      "train loss:0.4439864736833986\n",
      "train loss:0.5347217128863522\n",
      "train loss:0.4673358147055664\n",
      "train loss:0.5435427439777679\n",
      "train loss:0.8187733556642602\n",
      "train loss:0.5544687543403601\n",
      "train loss:0.48600193453468155\n",
      "train loss:0.6197161792481468\n",
      "train loss:0.4802580150683336\n",
      "train loss:0.5432716560832778\n",
      "train loss:0.4600936341273597\n",
      "train loss:0.6064516325707717\n",
      "train loss:0.6113936791861884\n",
      "train loss:0.8743885497906374\n",
      "train loss:0.6974776992867198\n",
      "train loss:0.6090127916380761\n",
      "train loss:0.7823029589627162\n",
      "train loss:0.6966459660268812\n",
      "train loss:0.7700375855044436\n",
      "train loss:0.5359616999437911\n",
      "train loss:0.5422371465318584\n",
      "train loss:0.7523087463411035\n",
      "train loss:0.5416768940731826\n",
      "train loss:0.5454708285246638\n",
      "train loss:0.682716893598179\n",
      "train loss:0.5434695213561345\n",
      "train loss:0.4690391234163294\n",
      "train loss:0.6164343447798366\n",
      "train loss:0.5344410729493093\n",
      "train loss:0.6097116144940224\n",
      "train loss:0.608702116661991\n",
      "train loss:0.7835359664644777\n",
      "train loss:0.611856604766741\n",
      "train loss:0.6110773715117404\n",
      "train loss:0.5212766031748697\n",
      "train loss:0.5163233238963781\n",
      "train loss:0.41451748669050537\n",
      "train loss:0.4051672317651544\n",
      "train loss:0.8348295074557853\n",
      "train loss:0.7249709240276635\n",
      "train loss:0.6155533385473877\n",
      "train loss:0.6129418459215127\n",
      "train loss:0.28871420605836534\n",
      "train loss:0.6157250323270743\n",
      "train loss:0.5051845882698562\n",
      "train loss:0.7317196116790478\n",
      "train loss:0.6182953291038643\n",
      "train loss:0.7237920081239769\n",
      "train loss:0.8178645834477377\n",
      "train loss:0.5154368139237868\n",
      "train loss:0.61185427807418\n",
      "train loss:0.6949861910246172\n",
      "train loss:0.5252754737254399\n",
      "train loss:0.44651874726796\n",
      "train loss:0.44387228471676804\n",
      "train loss:0.5244094067479804\n",
      "train loss:0.4308220277060261\n",
      "train loss:0.6100794289725526\n",
      "train loss:0.4075806973523619\n",
      "train loss:0.718925119329505\n",
      "train loss:0.28172308933647533\n",
      "train loss:0.5022878567184439\n",
      "train loss:0.6271600419332133\n",
      "train loss:0.49832212154139227\n",
      "train loss:0.6314210341925166\n",
      "train loss:0.6345455063079328\n",
      "train loss:0.4992910635278201\n",
      "train loss:0.6390073839706125\n",
      "train loss:0.7656329871626386\n",
      "train loss:0.5032933963262074\n",
      "train loss:0.8627768670855416\n",
      "train loss:0.6160048201199058\n",
      "train loss:0.5110367877773492\n",
      "train loss:0.42123595106137407\n",
      "train loss:0.3123208046208701\n",
      "train loss:0.5127191134451713\n",
      "train loss:0.6139321582419451\n",
      "train loss:0.509357956388046\n",
      "train loss:0.7187851189747364\n",
      "train loss:0.6135448317707712\n",
      "train loss:0.7134800361329087\n",
      "train loss:0.5143941389015703\n",
      "train loss:0.7099207128142572\n",
      "train loss:0.42166364300488607\n",
      "train loss:0.608418109354619\n",
      "train loss:0.5188356627001438\n",
      "train loss:0.6122872843978662\n",
      "train loss:0.5148243597814918\n",
      "train loss:0.41464225575747937\n",
      "train loss:0.6150087669532435\n",
      "train loss:0.613862677265775\n",
      "train loss:0.9192320659520734\n",
      "train loss:0.4100693375135875\n",
      "train loss:0.6187597878221162\n",
      "train loss:0.406962775364245\n",
      "train loss:0.2905739508869381\n",
      "train loss:0.5050382551082055\n",
      "train loss:0.38196126876888375\n",
      "train loss:0.6230924501377963\n",
      "train loss:0.6317209611458166\n",
      "train loss:0.36145114000982365\n",
      "train loss:0.35807689707797097\n",
      "train loss:0.6494563916367646\n",
      "train loss:1.2006968369980708\n",
      "train loss:0.6321450279378318\n",
      "train loss:0.37335668579378767\n",
      "train loss:0.6246313982798897\n",
      "train loss:0.8399104736469496\n",
      "train loss:0.8997866050959369\n",
      "train loss:0.6112049585345831\n",
      "train loss:0.6101772003936559\n",
      "train loss:0.6138230796041197\n",
      "train loss:0.6193053381538187\n",
      "train loss:0.6264699217827959\n",
      "train loss:0.7410974384982971\n",
      "train loss:0.6280818997430947\n",
      "train loss:0.6353568956818069\n",
      "train loss:0.6302452514518292\n",
      "train loss:0.7275852176856146\n",
      "train loss:0.5848593537294132\n",
      "train loss:0.6814064788847379\n",
      "train loss:0.6775693982949356\n",
      "train loss:0.6752291627330338\n",
      "train loss:0.675155061910399\n",
      "train loss:0.6735382235639422\n",
      "train loss:0.5839921901839568\n",
      "train loss:0.6273069427048625\n",
      "train loss:0.5751210202868338\n",
      "train loss:0.725770753938274\n",
      "train loss:0.5685634396616994\n",
      "train loss:0.5028068423688936\n",
      "train loss:0.7967724550983426\n",
      "train loss:0.5513444771766498\n",
      "train loss:0.7435084910323008\n",
      "train loss:0.5441783910536799\n",
      "train loss:0.7501912477528514\n",
      "train loss:0.6105734478297731\n",
      "train loss:0.5359207717138703\n",
      "train loss:0.4510627248358115\n",
      "train loss:0.6105879512951239\n",
      "train loss:0.5209036030568391\n",
      "train loss:0.7033430479860355\n",
      "train loss:0.7029508719728962\n",
      "train loss:0.6112639391667527\n",
      "train loss:0.511997989235158\n",
      "train loss:0.5087297905848972\n",
      "train loss:0.6150706048381697\n",
      "train loss:0.7211794375517706\n",
      "train loss:0.6143346766968638\n",
      "train loss:0.4036261510541225\n",
      "train loss:0.6121848097504557\n",
      "train loss:0.7145121918382484\n",
      "train loss:0.6111303106957815\n",
      "train loss:0.6095206883770797\n",
      "train loss:0.513257501907815\n",
      "train loss:0.8056146838935472\n",
      "train loss:0.42142271035462864\n",
      "train loss:0.87844610908742\n",
      "train loss:0.5257794897802118\n",
      "train loss:0.44165647674080766\n",
      "train loss:0.6090996135260018\n",
      "train loss:0.7714894438587967\n",
      "train loss:0.6850526493307756\n",
      "train loss:0.6064809055148933\n",
      "train loss:0.6130639186777802\n",
      "train loss:0.5433888454314881\n",
      "train loss:0.7484650232156417\n",
      "train loss:0.6818441497703489\n",
      "train loss:0.6798241949257202\n",
      "train loss:0.7297885787944616\n",
      "train loss:0.7820001094881471\n",
      "train loss:0.6257030417702588\n",
      "train loss:0.6729115225241358\n",
      "train loss:0.6298474664579788\n",
      "train loss:0.6769657880180879\n",
      "train loss:0.5526032246824987\n",
      "train loss:0.6332901628019786\n",
      "train loss:0.6316645785048218\n",
      "train loss:0.6298205406994946\n",
      "train loss:0.5325100216577123\n",
      "train loss:0.6207754425899475\n",
      "train loss:0.6189098467236565\n",
      "train loss:0.49228670850591183\n",
      "train loss:0.613743779444382\n",
      "train loss:0.751874394967626\n",
      "train loss:0.7593976849155702\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6764705882352942\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.1, random_state=40)\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "005801ba-ffd9-4553-90e5-bba5ff889fd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2990463639710312\n",
      "=== epoch:1, train acc:0.74, test acc:0.69 ===\n",
      "train loss:2.2931606412915424\n",
      "train loss:2.2831535488952452\n",
      "train loss:2.264097885406605\n",
      "train loss:2.2412893443293394\n",
      "train loss:2.2070330730571204\n",
      "train loss:2.1579869179786284\n",
      "train loss:2.0661783195460206\n",
      "train loss:1.9656802823041446\n",
      "train loss:1.8513945484946401\n",
      "train loss:1.6824599242419738\n",
      "train loss:1.6223888143774272\n",
      "train loss:1.3641721621130376\n",
      "train loss:1.1314413185738554\n",
      "train loss:0.8907968708746443\n",
      "train loss:0.7774217483573294\n",
      "train loss:0.6372293449424643\n",
      "train loss:0.5559398899683597\n",
      "train loss:0.8551802400748777\n",
      "train loss:0.6835673883147017\n",
      "train loss:1.2344437082968143\n",
      "train loss:0.6968525385775584\n",
      "train loss:0.5164321342520914\n",
      "train loss:0.7993875383652477\n",
      "train loss:0.8830566855862185\n",
      "train loss:0.4091587321499824\n",
      "train loss:0.7466832738609752\n",
      "train loss:0.538933012067659\n",
      "train loss:0.6082298174838331\n",
      "train loss:0.5714990711727572\n",
      "train loss:0.6349129575485944\n",
      "train loss:0.5222895661949412\n",
      "train loss:0.6105114032003115\n",
      "train loss:0.4494434441484126\n",
      "train loss:0.3948738250070458\n",
      "train loss:0.37582772015052923\n",
      "train loss:0.49556170510680503\n",
      "train loss:1.0418185101133224\n",
      "train loss:0.6681913335720102\n",
      "train loss:0.6495928867857313\n",
      "train loss:0.3444245198975927\n",
      "train loss:0.5037771212381188\n",
      "train loss:0.8241828768182309\n",
      "train loss:0.8693660936535894\n",
      "train loss:0.9391483868171189\n",
      "train loss:0.6439749215020598\n",
      "train loss:0.5999533115453892\n",
      "train loss:0.6211538759575576\n",
      "train loss:0.6039734721856439\n",
      "train loss:0.6487539115075759\n",
      "train loss:0.6480474359182928\n",
      "train loss:0.5956183787682101\n",
      "train loss:0.6226898522706774\n",
      "train loss:0.7491370751316565\n",
      "train loss:0.5922727857471036\n",
      "train loss:0.6068163268640063\n",
      "train loss:0.5126937858151037\n",
      "train loss:0.47795064549621086\n",
      "train loss:0.7494935913280352\n",
      "train loss:0.7574995271072733\n",
      "train loss:0.7387874634502329\n",
      "train loss:0.7945691403233969\n",
      "train loss:0.5130894691874526\n",
      "train loss:0.5823028810779398\n",
      "train loss:0.7613577076446616\n",
      "train loss:0.6186635680555653\n",
      "train loss:0.6831405622737126\n",
      "train loss:0.7637558403474044\n",
      "train loss:0.6921195472524172\n",
      "train loss:0.6464746248298315\n",
      "train loss:0.5954756303064209\n",
      "train loss:0.6140587006971381\n",
      "train loss:0.5858404341082648\n",
      "train loss:0.6428317078167882\n",
      "train loss:0.5474772414103098\n",
      "train loss:0.5081544551720678\n",
      "train loss:0.4936384822171796\n",
      "train loss:0.38290901436234326\n",
      "train loss:0.9071078068726587\n",
      "train loss:0.6645634443098942\n",
      "train loss:0.3663212681606029\n",
      "train loss:0.6492839880616158\n",
      "train loss:0.33878338693696\n",
      "train loss:0.31942100415044805\n",
      "train loss:0.14829275187783747\n",
      "train loss:0.12449179333543867\n",
      "train loss:0.9809026560576923\n",
      "train loss:0.510657611664707\n",
      "train loss:0.6907040185135298\n",
      "train loss:0.6093167356786173\n",
      "train loss:0.3396332527593427\n",
      "train loss:1.020969331671234\n",
      "train loss:0.8027713016384299\n",
      "train loss:0.37174112199323817\n",
      "train loss:0.5200908602227081\n",
      "train loss:0.5216605830398451\n",
      "train loss:0.6080457484746615\n",
      "train loss:0.5042156754660312\n",
      "train loss:0.31121366903328146\n",
      "train loss:0.5394974078002344\n",
      "train loss:0.5287533194633247\n",
      "train loss:0.5270606232697909\n",
      "train loss:0.39513464970681006\n",
      "train loss:0.27618229959215845\n",
      "train loss:0.7124307784528238\n",
      "train loss:0.872241457081782\n",
      "train loss:0.6074055637540698\n",
      "train loss:0.3828319670235511\n",
      "train loss:0.4994802388129713\n",
      "train loss:0.4992228113053126\n",
      "train loss:0.5145058938613924\n",
      "train loss:0.6233750178171785\n",
      "train loss:0.5043741199131876\n",
      "train loss:0.6637488844437842\n",
      "train loss:0.7793309399963981\n",
      "train loss:0.27137298585326375\n",
      "train loss:0.42394846282721266\n",
      "train loss:0.6328144818310516\n",
      "train loss:0.7322889460018793\n",
      "train loss:0.83335353715016\n",
      "train loss:0.7403624159512632\n",
      "train loss:0.7855622868725172\n",
      "train loss:0.46329956636569936\n",
      "train loss:0.8241923212660384\n",
      "train loss:0.73715890775399\n",
      "train loss:0.631294726362871\n",
      "train loss:0.612401063503327\n",
      "train loss:0.62171654397769\n",
      "train loss:0.7009025290165203\n",
      "train loss:0.6803660399035587\n",
      "train loss:0.59164181155799\n",
      "train loss:0.6743168955655399\n",
      "train loss:0.6012508853610166\n",
      "train loss:0.6249739079392008\n",
      "train loss:0.7636390939734014\n",
      "train loss:0.5159457353625176\n",
      "train loss:0.5755619183236156\n",
      "train loss:0.7126884217228089\n",
      "train loss:0.6117806891871159\n",
      "train loss:0.5177273600384844\n",
      "train loss:0.7015978567698268\n",
      "train loss:0.8021531582972476\n",
      "train loss:0.5078377053900623\n",
      "train loss:0.7110045631117357\n",
      "train loss:0.39596584573103877\n",
      "train loss:0.49121737783968433\n",
      "train loss:0.7355990442808732\n",
      "train loss:0.7041128210771268\n",
      "train loss:0.7345347126171033\n",
      "train loss:0.5093876407891347\n",
      "train loss:0.8709587658042632\n",
      "train loss:0.5314411453780437\n",
      "train loss:0.36432325501372914\n",
      "train loss:0.7167819536706921\n",
      "train loss:0.7696143203320089\n",
      "train loss:0.5225060846062506\n",
      "train loss:0.6896096811245707\n",
      "train loss:0.4687463241869644\n",
      "train loss:0.7451841658988339\n",
      "train loss:0.6968305957982074\n",
      "train loss:0.4693802181204923\n",
      "train loss:0.6278268402499604\n",
      "train loss:0.47026383501440455\n",
      "train loss:0.5449819497172246\n",
      "train loss:0.531191702361196\n",
      "train loss:0.6101984169405862\n",
      "train loss:0.531401330422551\n",
      "train loss:0.6608871286960427\n",
      "train loss:0.47663309342104176\n",
      "train loss:0.7401911305670755\n",
      "train loss:0.7288923375654991\n",
      "train loss:0.5326427728575557\n",
      "train loss:0.714434897410906\n",
      "train loss:0.7470924780778034\n",
      "train loss:0.5353301435133732\n",
      "train loss:0.5069634699548808\n",
      "train loss:0.6176791819348488\n",
      "train loss:0.6953613909901943\n",
      "train loss:0.7830502745462052\n",
      "train loss:0.6794582739527322\n",
      "train loss:0.5532717001012402\n",
      "train loss:0.5790871204880765\n",
      "train loss:0.6745668517601939\n",
      "train loss:0.6040550909683444\n",
      "train loss:0.6079680321631582\n",
      "train loss:0.6188282985236138\n",
      "train loss:0.6154123749968371\n",
      "train loss:0.4831947772471422\n",
      "train loss:0.6186058458573769\n",
      "train loss:0.6281147354536986\n",
      "train loss:0.6146782775247328\n",
      "train loss:0.6661994065866441\n",
      "train loss:0.5372893455124796\n",
      "train loss:0.5897082353426882\n",
      "train loss:0.6014418334619948\n",
      "train loss:0.7959410180711315\n",
      "train loss:0.6910522039389883\n",
      "train loss:0.6144127844687077\n",
      "train loss:0.540016563292083\n",
      "train loss:0.5260073024938057\n",
      "train loss:0.6377688834481648\n",
      "train loss:0.7910550435488569\n",
      "train loss:0.5282269446933299\n",
      "train loss:0.5920934831893456\n",
      "train loss:0.3636825539338505\n",
      "train loss:0.6206728297383999\n",
      "train loss:0.6837713638869269\n",
      "train loss:0.8606238390130592\n",
      "train loss:0.7103103104102665\n",
      "train loss:0.44923937927716323\n",
      "train loss:0.4570517865068201\n",
      "train loss:0.6932587286960927\n",
      "train loss:0.618396238913264\n",
      "train loss:0.521619347189585\n",
      "train loss:0.7059174352373339\n",
      "train loss:0.6047147543835656\n",
      "train loss:0.7014702499208959\n",
      "train loss:0.5527297942748987\n",
      "train loss:0.6245789428620692\n",
      "train loss:0.5213603836284942\n",
      "train loss:0.6470971540125152\n",
      "train loss:0.5177708646437631\n",
      "train loss:0.4365683501108048\n",
      "train loss:0.8971987154453094\n",
      "train loss:0.7111815794225753\n",
      "train loss:0.7012390308825849\n",
      "train loss:0.4284974764661741\n",
      "train loss:0.6896333411821896\n",
      "train loss:0.6161918723391537\n",
      "train loss:0.5286712355486907\n",
      "train loss:0.5341036655557734\n",
      "train loss:0.6002417645805589\n",
      "train loss:0.5097473831853836\n",
      "train loss:0.6102947585278043\n",
      "train loss:0.41342633690372554\n",
      "train loss:0.6167406154695023\n",
      "train loss:0.618591926413582\n",
      "train loss:0.6036668055173849\n",
      "train loss:0.7137284504379038\n",
      "train loss:0.5935043497258594\n",
      "train loss:0.5003955758705652\n",
      "train loss:0.5245216889921656\n",
      "train loss:0.7995595606297471\n",
      "train loss:0.5123675048474998\n",
      "train loss:0.6150547651399461\n",
      "train loss:0.2898696401736289\n",
      "train loss:0.7080231096143154\n",
      "train loss:0.7252041570842712\n",
      "train loss:0.5097332045864389\n",
      "train loss:0.7132592090645478\n",
      "train loss:0.6200244401278783\n",
      "train loss:0.753915634128563\n",
      "train loss:0.5210069446872534\n",
      "train loss:0.6095303390606015\n",
      "train loss:0.6095313112577111\n",
      "train loss:0.5240507467820212\n",
      "train loss:0.630573210417368\n",
      "train loss:0.7072010227313739\n",
      "train loss:0.5546229870670636\n",
      "train loss:0.6984147652866469\n",
      "train loss:0.7669227026930949\n",
      "train loss:0.6258655480810564\n",
      "train loss:0.6360561131659702\n",
      "train loss:0.717443904477782\n",
      "train loss:0.5564783563792608\n",
      "train loss:0.6696335554479306\n",
      "train loss:0.6161672131975326\n",
      "train loss:0.5536655266437028\n",
      "train loss:0.6866124826557051\n",
      "train loss:0.6216233206671841\n",
      "train loss:0.7531402286466433\n",
      "train loss:0.6186835613596153\n",
      "train loss:0.6246797844035875\n",
      "train loss:0.4584020858149909\n",
      "train loss:0.4512853995827431\n",
      "train loss:0.6852772094466066\n",
      "train loss:0.42344583890743925\n",
      "train loss:0.38614731917479145\n",
      "train loss:0.5004976607130216\n",
      "train loss:0.5233846652947103\n",
      "train loss:0.37439663683972996\n",
      "train loss:0.36964415678999896\n",
      "train loss:0.662878827945401\n",
      "train loss:0.3629254704658315\n",
      "train loss:0.34248657998933524\n",
      "train loss:1.0108048338334021\n",
      "train loss:0.7243674539633134\n",
      "train loss:0.15947271059171614\n",
      "train loss:0.6888325658363836\n",
      "train loss:1.076900995229337\n",
      "train loss:0.5104762848601079\n",
      "train loss:0.6358451918119988\n",
      "train loss:0.7170828005136881\n",
      "train loss:0.7741868785458346\n",
      "train loss:0.691003496937223\n",
      "train loss:0.5836345887617824\n",
      "train loss:0.6312347167664552\n",
      "train loss:0.6762208660912641\n",
      "train loss:0.6222838001798209\n",
      "train loss:0.6350868841229775\n",
      "train loss:0.6920118597874857\n",
      "train loss:0.5991980968882746\n",
      "train loss:0.6471060506765463\n",
      "train loss:0.6860720270868212\n",
      "train loss:0.5585710883261931\n",
      "train loss:0.5836876188897743\n",
      "train loss:0.6088207925851962\n",
      "train loss:0.5464123235557092\n",
      "train loss:0.6892808221109685\n",
      "train loss:0.4426978821825517\n",
      "train loss:0.8191009408985568\n",
      "train loss:0.609704097722125\n",
      "train loss:0.5264935924109079\n",
      "train loss:0.49161324254834227\n",
      "train loss:0.7261741923306002\n",
      "train loss:0.5135742514623864\n",
      "train loss:0.7376548032175605\n",
      "train loss:0.6556113254248191\n",
      "train loss:0.4867898684510008\n",
      "train loss:0.3783760114737501\n",
      "train loss:0.7713744225767211\n",
      "train loss:0.6107163916296121\n",
      "train loss:0.6215559797877057\n",
      "train loss:0.5348305138047793\n",
      "train loss:0.7361862087288965\n",
      "train loss:0.4907826879406401\n",
      "train loss:0.6882812547015604\n",
      "train loss:0.5238855228064521\n",
      "train loss:0.5333055247793944\n",
      "train loss:0.6197335119934764\n",
      "train loss:0.6033071494246356\n",
      "train loss:0.5450628461974925\n",
      "train loss:0.42387875761776883\n",
      "train loss:0.4342152611898894\n",
      "train loss:0.6122050458671274\n",
      "train loss:0.817485527872219\n",
      "train loss:0.5951489402549857\n",
      "train loss:0.2979902748047533\n",
      "train loss:0.3980774700291276\n",
      "train loss:0.6137882948020643\n",
      "train loss:0.3692792337147719\n",
      "train loss:0.6607218822280525\n",
      "train loss:0.5035428085726777\n",
      "train loss:0.7644504404141794\n",
      "train loss:0.7534055968429156\n",
      "train loss:0.6483873221618234\n",
      "train loss:0.5149850866776134\n",
      "train loss:0.7524660629835722\n",
      "train loss:0.6137394081652806\n",
      "train loss:0.7031031885776382\n",
      "train loss:0.5250208538201984\n",
      "train loss:0.34231702387291585\n",
      "train loss:0.5291004405510603\n",
      "train loss:0.692254423040441\n",
      "train loss:0.46244139152705077\n",
      "train loss:0.6864604654468707\n",
      "train loss:0.6150997301175949\n",
      "train loss:0.619935674465094\n",
      "train loss:0.40859320693614387\n",
      "train loss:0.7039970972779122\n",
      "train loss:0.8695036073283852\n",
      "train loss:0.5068561090568314\n",
      "train loss:0.5255981698214349\n",
      "train loss:0.4351053846156354\n",
      "train loss:0.783235936408271\n",
      "train loss:0.614127167235968\n",
      "train loss:0.4588876938883658\n",
      "train loss:0.5332288995953585\n",
      "train loss:0.7888784238696572\n",
      "train loss:0.6171019833201512\n",
      "train loss:0.6779574566017055\n",
      "train loss:0.8364666181794617\n",
      "train loss:0.6858571916804703\n",
      "train loss:0.4761362421481394\n",
      "train loss:0.691751779722812\n",
      "train loss:0.6112042748826086\n",
      "train loss:0.6277527552670059\n",
      "train loss:0.7292076498979151\n",
      "train loss:0.7300631067976303\n",
      "train loss:0.5280355045415838\n",
      "train loss:0.5048260698780507\n",
      "train loss:0.5523565930714048\n",
      "train loss:0.400753331349225\n",
      "train loss:0.530048388111344\n",
      "train loss:0.5222200204131625\n",
      "train loss:0.39009711998911845\n",
      "train loss:0.49935740559957387\n",
      "train loss:0.7358663242896688\n",
      "train loss:0.384202392406995\n",
      "train loss:0.5143082550248628\n",
      "train loss:0.7866894825917015\n",
      "train loss:0.804157746377687\n",
      "train loss:0.6609473645059657\n",
      "train loss:0.8092119929149121\n",
      "train loss:0.6131870411780411\n",
      "train loss:0.505361712530194\n",
      "train loss:0.7072592969575334\n",
      "train loss:0.5055197096570768\n",
      "train loss:0.5188945354679718\n",
      "train loss:0.5215716993511836\n",
      "train loss:0.5228561923538114\n",
      "train loss:0.43734135973064775\n",
      "train loss:0.5733219970588299\n",
      "train loss:0.610946409214381\n",
      "train loss:0.5147736899675371\n",
      "train loss:0.6941041583681088\n",
      "train loss:0.3043029572926841\n",
      "train loss:0.6143957268620147\n",
      "train loss:0.8158427049386292\n",
      "train loss:0.8043144815243968\n",
      "train loss:0.8119824818000685\n",
      "train loss:0.6886781401801101\n",
      "train loss:0.6301825780829767\n",
      "train loss:0.4776378694700189\n",
      "train loss:0.7573697431813983\n",
      "train loss:0.47502125100779224\n",
      "train loss:0.46548616511593266\n",
      "train loss:0.6178128308872569\n",
      "train loss:0.6780385930468857\n",
      "train loss:0.7452028076135152\n",
      "train loss:0.46704919639352305\n",
      "train loss:0.8235243274956794\n",
      "train loss:0.4715854058234189\n",
      "train loss:0.7581053908639721\n",
      "train loss:0.45656324450805885\n",
      "train loss:0.7048755771779366\n",
      "train loss:0.7850885639947796\n",
      "train loss:0.7046183164117499\n",
      "train loss:0.604421339513086\n",
      "train loss:0.40593645084236724\n",
      "train loss:0.5351643700248478\n",
      "train loss:0.624742068650904\n",
      "train loss:0.7049921819590267\n",
      "train loss:0.6078634913272429\n",
      "train loss:0.6307767478778463\n",
      "train loss:0.4320473212983943\n",
      "train loss:0.6024040915464106\n",
      "train loss:0.6288793428658751\n",
      "train loss:0.5090495752078641\n",
      "train loss:0.5145200225719677\n",
      "train loss:0.6179320051565942\n",
      "train loss:0.5984121662093869\n",
      "train loss:0.5043832846333032\n",
      "train loss:0.7627829342785922\n",
      "train loss:0.8288570194023382\n",
      "train loss:0.8137724471054234\n",
      "train loss:0.5070972005536885\n",
      "train loss:0.6031151373034346\n",
      "train loss:0.851505073451597\n",
      "train loss:0.47050352710286136\n",
      "train loss:0.6755583757849873\n",
      "train loss:0.6098780952772536\n",
      "train loss:0.5562047998489563\n",
      "train loss:0.6273439125433953\n",
      "train loss:0.6203503912656496\n",
      "train loss:0.6301764149251047\n",
      "train loss:0.4937189054651573\n",
      "train loss:0.6186917840684005\n",
      "train loss:0.7376594010562995\n",
      "train loss:0.752957391304278\n",
      "train loss:0.6197801392288509\n",
      "train loss:0.6065563397533164\n",
      "train loss:0.7483843268392527\n",
      "train loss:0.614534719972104\n",
      "train loss:0.5511223871423903\n",
      "train loss:0.6754522765155968\n",
      "train loss:0.48312708328883075\n",
      "train loss:0.46098638906161427\n",
      "train loss:0.6259446091468052\n",
      "train loss:0.531389518057395\n",
      "train loss:0.5309581390827273\n",
      "train loss:0.6156508730783511\n",
      "train loss:0.8116976151926405\n",
      "train loss:0.7350554173122331\n",
      "train loss:0.6050027522442973\n",
      "train loss:0.6205776455936209\n",
      "train loss:0.5124582364524726\n",
      "train loss:0.5058065189635491\n",
      "train loss:0.64874779491866\n",
      "train loss:0.6054977775300985\n",
      "train loss:0.7348622890900383\n",
      "train loss:0.6070652444716617\n",
      "train loss:0.5185261065022575\n",
      "train loss:0.6085595595717697\n",
      "train loss:0.6714793848450242\n",
      "train loss:0.6153029788345871\n",
      "train loss:0.43732990855546455\n",
      "train loss:0.602024230439223\n",
      "train loss:0.6145497728911581\n",
      "train loss:0.7700547248294988\n",
      "train loss:0.6012249389520001\n",
      "train loss:0.5260415310177092\n",
      "train loss:0.5123352368078908\n",
      "train loss:0.4348506292343511\n",
      "train loss:0.6054631913361754\n",
      "train loss:0.814920620026912\n",
      "train loss:0.5064498795531784\n",
      "train loss:0.5345867676390292\n",
      "train loss:0.5149558108493018\n",
      "train loss:0.6244731485170657\n",
      "train loss:0.636849011357094\n",
      "train loss:0.506617083199595\n",
      "train loss:0.5083187946595106\n",
      "train loss:0.9112801381840724\n",
      "train loss:0.6209767416852026\n",
      "train loss:0.7162873157668749\n",
      "train loss:0.402368289647603\n",
      "train loss:0.6126183645393831\n",
      "train loss:0.3218637019197851\n",
      "train loss:0.6161936432679147\n",
      "train loss:0.7005066584026134\n",
      "train loss:0.6200019393407776\n",
      "train loss:0.8068997510744562\n",
      "train loss:0.5144422203582855\n",
      "train loss:0.6205156760882999\n",
      "train loss:0.8548632663395515\n",
      "train loss:0.37014358392140034\n",
      "train loss:0.7641825125261519\n",
      "train loss:0.6903731469003409\n",
      "train loss:0.6890655031763109\n",
      "train loss:0.5413007717566647\n",
      "train loss:0.7523812407726937\n",
      "train loss:0.5678061759698219\n",
      "train loss:0.6738351424408805\n",
      "train loss:0.5659202635200478\n",
      "train loss:0.5066477331325898\n",
      "train loss:0.5518662375377913\n",
      "train loss:0.73081653822434\n",
      "train loss:0.5424560985274967\n",
      "train loss:0.7675480796801551\n",
      "train loss:0.5302612647721183\n",
      "train loss:0.6091739751666296\n",
      "train loss:0.3594197055607059\n",
      "train loss:0.6945578101559466\n",
      "train loss:0.6137075668293737\n",
      "train loss:0.41298050256131147\n",
      "train loss:0.6258699418625588\n",
      "train loss:0.3771629852412787\n",
      "train loss:0.6362493905304364\n",
      "train loss:0.5055680620288465\n",
      "train loss:0.4985314774063655\n",
      "train loss:0.7661812947045561\n",
      "train loss:0.3634643950995074\n",
      "train loss:0.8980298458158122\n",
      "train loss:0.6195860730202426\n",
      "train loss:0.4971920499074732\n",
      "train loss:0.495884788164092\n",
      "train loss:0.5114972251562354\n",
      "train loss:0.6227407280720509\n",
      "train loss:0.6263722160853772\n",
      "train loss:0.6219025256988195\n",
      "train loss:0.7070140824171626\n",
      "train loss:0.41011442845600643\n",
      "train loss:0.6828860469745437\n",
      "train loss:0.42915060549861\n",
      "train loss:0.5163209680621932\n",
      "train loss:0.7821480647216977\n",
      "train loss:0.695249586660603\n",
      "train loss:0.4494408915940692\n",
      "train loss:0.6098052611379237\n",
      "train loss:0.8520233629341873\n",
      "train loss:0.5998091359548148\n",
      "train loss:0.7607004578034617\n",
      "train loss:0.47713487396396237\n",
      "train loss:0.6863502087403189\n",
      "train loss:0.7532639576108512\n",
      "train loss:0.5680303610577677\n",
      "train loss:0.5045018505425912\n",
      "train loss:0.5638746794115049\n",
      "train loss:0.6678625770212321\n",
      "train loss:0.5643335715398192\n",
      "train loss:0.6840978812054312\n",
      "train loss:0.47468168822424317\n",
      "train loss:0.5307466871074811\n",
      "train loss:0.519997896006303\n",
      "train loss:0.6283141741780573\n",
      "train loss:0.5244657505650648\n",
      "train loss:0.5128759316305301\n",
      "train loss:0.3806354732939802\n",
      "train loss:0.38033691465002945\n",
      "train loss:0.3639180814633432\n",
      "train loss:0.6662674809177821\n",
      "train loss:0.9606178412519867\n",
      "train loss:0.3391939661196546\n",
      "train loss:0.4894557421946808\n",
      "train loss:0.6658870890923719\n",
      "train loss:0.34740475881898075\n",
      "train loss:0.9503884834653386\n",
      "train loss:0.5121909313360095\n",
      "train loss:0.36286191731076956\n",
      "train loss:0.8786094722493324\n",
      "train loss:0.7264530722510877\n",
      "train loss:0.8187993822994113\n",
      "train loss:0.4232270298418278\n",
      "train loss:0.5204816324684465\n",
      "train loss:0.6883532269914303\n",
      "train loss:0.4092466840748837\n",
      "train loss:0.6223741616902775\n",
      "train loss:0.541427005293478\n",
      "train loss:0.5459005958745529\n",
      "train loss:0.47993227481067213\n",
      "train loss:0.36894395941217983\n",
      "train loss:0.5348806733268573\n",
      "train loss:0.6987594299908919\n",
      "train loss:0.6080831437318511\n",
      "train loss:0.5144171298698643\n",
      "train loss:0.6151883335464551\n",
      "train loss:0.5319608683685347\n",
      "train loss:0.5154387024128922\n",
      "train loss:0.6240894778168244\n",
      "train loss:0.6062691813202477\n",
      "train loss:0.6345523052489968\n",
      "train loss:0.6040077273292095\n",
      "train loss:0.6084109508962335\n",
      "train loss:0.7260862484487183\n",
      "train loss:0.5165765984967177\n",
      "train loss:0.9297234657250201\n",
      "train loss:0.41088048902542595\n",
      "train loss:0.3003056984261779\n",
      "train loss:0.4025764206450454\n",
      "train loss:0.7082779762949811\n",
      "train loss:0.5117891012343894\n",
      "train loss:0.49466096720921\n",
      "train loss:0.6279304157255299\n",
      "train loss:0.7518528295755412\n",
      "train loss:0.7200602653314396\n",
      "train loss:0.7198458258399112\n",
      "train loss:0.44249460789964623\n",
      "train loss:0.6881262337765958\n",
      "train loss:0.4300909156368623\n",
      "train loss:0.6110199532542487\n",
      "train loss:0.8065864251727678\n",
      "train loss:0.7705130433882943\n",
      "train loss:0.47215178110289757\n",
      "train loss:0.5521125483259924\n",
      "train loss:0.6121750888941595\n",
      "train loss:0.681847433634154\n",
      "train loss:0.6115481594763715\n",
      "train loss:0.4638359501971362\n",
      "train loss:0.7551213034998534\n",
      "train loss:0.6840926861358814\n",
      "train loss:0.5359446538538568\n",
      "train loss:0.7002022439581808\n",
      "train loss:0.689189329391843\n",
      "train loss:0.7455651060553921\n",
      "train loss:0.5450327293440648\n",
      "train loss:0.8027950669122479\n",
      "train loss:0.6779265311280943\n",
      "train loss:0.6188557248845351\n",
      "train loss:0.673890879230043\n",
      "train loss:0.5610064823212804\n",
      "train loss:0.5591464786392754\n",
      "train loss:0.6163411169727698\n",
      "train loss:0.6308454154552559\n",
      "train loss:0.731998958502781\n",
      "train loss:0.5538595632294017\n",
      "train loss:0.5442952012218545\n",
      "train loss:0.5957503249389506\n",
      "train loss:0.6948690577793306\n",
      "train loss:0.46112281520721954\n",
      "train loss:0.6137713210625142\n",
      "train loss:0.6117853980825982\n",
      "train loss:0.5183464234524895\n",
      "train loss:0.6125599821753049\n",
      "train loss:0.500462611329414\n",
      "train loss:0.7162162275201537\n",
      "train loss:0.747183118443429\n",
      "train loss:0.4021266151820785\n",
      "train loss:0.731742756197873\n",
      "train loss:0.7122305390516388\n",
      "train loss:0.5370216507901071\n",
      "train loss:0.7184353274843637\n",
      "train loss:0.5189382148079997\n",
      "train loss:0.6057306393352788\n",
      "train loss:0.6956262524976031\n",
      "train loss:0.8625473694591375\n",
      "train loss:0.3705590975321347\n",
      "train loss:0.5241218427144572\n",
      "train loss:0.7611095958079893\n",
      "train loss:0.4531085179016511\n",
      "train loss:0.4515054473160431\n",
      "train loss:0.44358411678549753\n",
      "train loss:0.5286881907882386\n",
      "train loss:0.3141179268687772\n",
      "train loss:0.7206799041480783\n",
      "train loss:0.5107005534523619\n",
      "train loss:0.6194443304597284\n",
      "train loss:0.38203351480773595\n",
      "train loss:0.4971448052891675\n",
      "train loss:0.7886356922950951\n",
      "train loss:0.775133921556161\n",
      "train loss:1.0088671533112283\n",
      "train loss:0.5088103830584113\n",
      "train loss:0.49731157792663083\n",
      "train loss:0.6021829496313919\n",
      "train loss:0.39758140424584903\n",
      "train loss:0.5324554577516954\n",
      "train loss:0.8021309450158179\n",
      "train loss:0.6926504231942839\n",
      "train loss:0.7642199542952485\n",
      "train loss:0.4402220363783108\n",
      "train loss:0.5355417035697279\n",
      "train loss:0.6791968231319492\n",
      "train loss:0.6067044732281263\n",
      "train loss:0.5566334575592556\n",
      "train loss:0.5469573299348985\n",
      "train loss:0.682609341894701\n",
      "train loss:0.7685320286276693\n",
      "train loss:0.6831958161397328\n",
      "train loss:0.6753261412932801\n",
      "train loss:0.6017980231963346\n",
      "train loss:0.6844471781462165\n",
      "train loss:0.6144681707908669\n",
      "train loss:0.6776351351405195\n",
      "train loss:0.4992220401685891\n",
      "train loss:0.42950698753389693\n",
      "train loss:0.5510400339151372\n",
      "train loss:0.6131049387668749\n",
      "train loss:0.45705331516408254\n",
      "train loss:0.6139590471643854\n",
      "train loss:0.8122170412946176\n",
      "train loss:0.4165227161692555\n",
      "train loss:0.3061495377575424\n",
      "train loss:0.8505276520251984\n",
      "train loss:0.3917250631030316\n",
      "train loss:0.8427723629970476\n",
      "train loss:0.6343253320170736\n",
      "train loss:0.7343111893585973\n",
      "train loss:0.6107651095541761\n",
      "train loss:0.7205130803008966\n",
      "train loss:0.5035566671057807\n",
      "train loss:0.40906921979931055\n",
      "train loss:0.5069611170677216\n",
      "train loss:0.6104850008621953\n",
      "train loss:0.622203322330051\n",
      "train loss:0.5033088460251557\n",
      "train loss:0.7190192398655298\n",
      "train loss:0.7097903781703703\n",
      "train loss:0.6881871937706108\n",
      "train loss:0.4311786302819396\n",
      "train loss:0.4298686604187389\n",
      "train loss:0.6321050441893543\n",
      "train loss:0.5238944399577738\n",
      "train loss:0.8030618377320238\n",
      "train loss:0.6998591808010326\n",
      "train loss:0.7088847024253803\n",
      "train loss:0.5297368453791849\n",
      "train loss:0.6868731991376874\n",
      "train loss:0.690833895131717\n",
      "train loss:0.8260625917889868\n",
      "train loss:0.6342011788253099\n",
      "train loss:0.682758058276607\n",
      "train loss:0.6140888750077599\n",
      "train loss:0.6164721390315476\n",
      "train loss:0.6236414017689871\n",
      "train loss:0.6309979555281768\n",
      "train loss:0.6233574363470572\n",
      "train loss:0.6261248233170983\n",
      "train loss:0.6203305003559474\n",
      "train loss:0.5704142993019733\n",
      "train loss:0.5094654142750716\n",
      "train loss:0.7398784783666029\n",
      "train loss:0.6803439219548231\n",
      "train loss:0.6763110584821158\n",
      "train loss:0.6138746725123049\n",
      "train loss:0.6777957103181081\n",
      "train loss:0.6122365539803323\n",
      "train loss:0.6069342965772139\n",
      "train loss:0.6817344382188619\n",
      "train loss:0.3933220597036735\n",
      "train loss:0.36278784598665625\n",
      "train loss:0.4269448690691993\n",
      "train loss:0.6171670068549397\n",
      "train loss:0.810977072331607\n",
      "train loss:0.5032924429648983\n",
      "train loss:0.8276588851222646\n",
      "train loss:0.3825831925398391\n",
      "train loss:0.7382771371868708\n",
      "train loss:0.7258337706824036\n",
      "train loss:0.3783586261443176\n",
      "train loss:0.5166408725291194\n",
      "train loss:0.49619722902113417\n",
      "train loss:0.7258428454876464\n",
      "train loss:0.9462149727693345\n",
      "train loss:0.9152522780840998\n",
      "train loss:0.520538916470422\n",
      "train loss:0.5977894489802287\n",
      "train loss:0.45615091806128094\n",
      "train loss:0.8352189093995749\n",
      "train loss:0.681169093359067\n",
      "train loss:0.6226471806332942\n",
      "train loss:0.6784949987313247\n",
      "train loss:0.6807528550856101\n",
      "train loss:0.6737558154840793\n",
      "train loss:0.6690019001332226\n",
      "train loss:0.5871539811476161\n",
      "train loss:0.6750714573079136\n",
      "train loss:0.6704395615183902\n",
      "train loss:0.6288771868065597\n",
      "train loss:0.7135633992831347\n",
      "train loss:0.5576772479786083\n",
      "train loss:0.7178800811057364\n",
      "train loss:0.5569519095060289\n",
      "train loss:0.581689202515352\n",
      "train loss:0.5356499838382411\n",
      "train loss:0.46362650424207724\n",
      "train loss:0.5594857526902887\n",
      "train loss:0.6746441970324318\n",
      "train loss:0.7547258994768898\n",
      "train loss:0.6103663368239629\n",
      "train loss:0.8414046415147565\n",
      "train loss:0.6136879726004538\n",
      "train loss:0.3582129339636308\n",
      "train loss:0.5200817481954522\n",
      "train loss:0.503058434960973\n",
      "train loss:0.5212916206288895\n",
      "train loss:0.8416316332394894\n",
      "train loss:0.6035578050871788\n",
      "train loss:0.726391454675693\n",
      "train loss:0.503956828302156\n",
      "train loss:0.71628132590359\n",
      "train loss:0.5116358067489107\n",
      "train loss:0.7056099465422474\n",
      "train loss:0.5173886795060206\n",
      "train loss:0.6112082736987491\n",
      "train loss:0.7927116160617145\n",
      "train loss:0.7156382633909933\n",
      "train loss:0.5444704713188091\n",
      "train loss:0.7634245006946287\n",
      "train loss:0.537615811677053\n",
      "train loss:0.7421051959331875\n",
      "train loss:0.5522122879532252\n",
      "train loss:0.61674145453886\n",
      "train loss:0.6110095222744125\n",
      "train loss:0.49536378681896165\n",
      "train loss:0.5491140550632121\n",
      "train loss:0.6158630035690409\n",
      "train loss:0.737498818225048\n",
      "train loss:0.5468151697593111\n",
      "train loss:0.6808625184114601\n",
      "train loss:0.5390869334040681\n",
      "train loss:0.7500783132134221\n",
      "train loss:0.6791780205951659\n",
      "train loss:0.4629349088553969\n",
      "train loss:0.8149666441723576\n",
      "train loss:0.53379588933221\n",
      "train loss:0.60480341986187\n",
      "train loss:0.6017987059544759\n",
      "train loss:0.5367894233194634\n",
      "train loss:0.6927853680467355\n",
      "train loss:0.835355145820666\n",
      "train loss:0.4569818217839704\n",
      "train loss:0.5431782744912741\n",
      "train loss:0.5198806775753051\n",
      "train loss:0.522089713215385\n",
      "train loss:0.6991246929044033\n",
      "train loss:0.6212985835203926\n",
      "train loss:0.4552042114210095\n",
      "train loss:0.5205625921273141\n",
      "train loss:0.5177999619849312\n",
      "train loss:0.5080749211924106\n",
      "train loss:0.8528427013701663\n",
      "train loss:0.7072528915159734\n",
      "train loss:0.5168098911162307\n",
      "train loss:0.39522589397698626\n",
      "train loss:0.3959574412498403\n",
      "train loss:0.6486605115140096\n",
      "train loss:0.4984303008482014\n",
      "train loss:0.506614186055476\n",
      "train loss:0.5036506622120958\n",
      "train loss:0.7596625040949498\n",
      "train loss:0.4917576079414272\n",
      "train loss:0.49366441541814277\n",
      "train loss:0.7325111390060581\n",
      "train loss:0.5006926174707417\n",
      "train loss:0.7317897089678349\n",
      "train loss:0.6242793958129883\n",
      "train loss:0.5274444821395441\n",
      "train loss:0.39741981199779297\n",
      "train loss:0.7169488952751304\n",
      "train loss:0.705728331501024\n",
      "train loss:0.5105033470728011\n",
      "train loss:0.7756878478659994\n",
      "train loss:0.765436199266867\n",
      "train loss:0.5414905774915948\n",
      "train loss:0.6863744087131508\n",
      "train loss:0.6753768760500594\n",
      "train loss:0.7529293016015071\n",
      "train loss:0.5597103786798934\n",
      "train loss:0.6788125538479053\n",
      "train loss:0.5212649151432861\n",
      "train loss:0.6779751646860918\n",
      "train loss:0.6780444246983484\n",
      "train loss:0.633410861738166\n",
      "train loss:0.7241734002732504\n",
      "train loss:0.6311683437433423\n",
      "train loss:0.6228915903852552\n",
      "train loss:0.6789168518784816\n",
      "train loss:0.5309379324530028\n",
      "train loss:0.6728542380402147\n",
      "train loss:0.625325432059508\n",
      "train loss:0.6622425951134908\n",
      "train loss:0.5595942290717377\n",
      "train loss:0.66498511273826\n",
      "train loss:0.6148851949578706\n",
      "train loss:0.5560592116045582\n",
      "train loss:0.5501828809300143\n",
      "train loss:0.6083475729905485\n",
      "train loss:0.5361160100078324\n",
      "train loss:0.6014336612661496\n",
      "train loss:0.7686252834618349\n",
      "train loss:0.7001769006036467\n",
      "train loss:0.8910430543893293\n",
      "train loss:0.5248006872031614\n",
      "train loss:0.436055063675003\n",
      "train loss:0.6881954520750904\n",
      "train loss:0.604875896581925\n",
      "train loss:0.5262330255684389\n",
      "train loss:0.6045652370752584\n",
      "train loss:0.43310529350552923\n",
      "train loss:0.7936308191075018\n",
      "train loss:0.7096824229448524\n",
      "train loss:0.5281850441923527\n",
      "train loss:0.6022292380603835\n",
      "train loss:0.6068157472595131\n",
      "train loss:0.5203786121426079\n",
      "train loss:0.6113276602691793\n",
      "train loss:0.7010162985035102\n",
      "train loss:0.4261860353136845\n",
      "train loss:0.693305309705633\n",
      "train loss:0.41636424118200244\n",
      "train loss:0.8062563572778011\n",
      "train loss:0.7886565883254545\n",
      "train loss:0.4310640450168739\n",
      "train loss:0.5166100339251282\n",
      "train loss:0.5171678811782081\n",
      "train loss:0.6969147932673087\n",
      "train loss:0.6936623430156709\n",
      "train loss:0.5249110145899487\n",
      "train loss:0.6016995084808776\n",
      "train loss:0.42188718805360476\n",
      "train loss:0.4002011896616734\n",
      "train loss:0.6007553963351591\n",
      "train loss:0.6169929248665965\n",
      "train loss:0.607958836292902\n",
      "train loss:0.7259137026479161\n",
      "train loss:0.38877365123405727\n",
      "train loss:0.6152624777184384\n",
      "train loss:0.6194606099276746\n",
      "train loss:0.37632204994774276\n",
      "train loss:0.840464948016051\n",
      "train loss:0.3864800185981558\n",
      "train loss:0.6159430044750378\n",
      "train loss:0.610111177940906\n",
      "train loss:0.6289713845767418\n",
      "train loss:0.5078595785420582\n",
      "train loss:0.62436659256316\n",
      "train loss:0.5286841714516648\n",
      "train loss:1.0266124265257761\n",
      "train loss:0.5157946449481127\n",
      "train loss:0.4296788177770618\n",
      "train loss:0.6047644162956431\n",
      "train loss:0.43576944954849883\n",
      "train loss:0.7030481332005575\n",
      "train loss:0.6195607953878621\n",
      "train loss:0.44676705512285275\n",
      "train loss:0.6069929242202774\n",
      "train loss:0.5285700492952725\n",
      "train loss:0.6354315323999836\n",
      "train loss:0.78791920175566\n",
      "train loss:0.6015996399321399\n",
      "train loss:0.5136482783989483\n",
      "train loss:0.5171281130017238\n",
      "train loss:0.6080029897178629\n",
      "train loss:0.6950646913350937\n",
      "train loss:0.8617698676871957\n",
      "train loss:0.5294185416578983\n",
      "train loss:0.830391595214585\n",
      "train loss:0.6122394958668151\n",
      "train loss:0.7550343139464836\n",
      "train loss:0.40987236403619215\n",
      "train loss:0.39961488229933906\n",
      "train loss:0.6807939664716789\n",
      "train loss:0.8268493871000956\n",
      "train loss:0.6650165602485641\n",
      "train loss:0.6697656366469553\n",
      "train loss:0.5499634533191473\n",
      "train loss:0.6194022373195374\n",
      "train loss:0.6754422574479234\n",
      "train loss:0.48163190348741214\n",
      "train loss:0.8092440662558464\n",
      "train loss:0.6846640550114967\n",
      "train loss:0.8390521729123723\n",
      "train loss:0.5577893516797704\n",
      "train loss:0.679639102355841\n",
      "train loss:0.6751688534174987\n",
      "train loss:0.6255125409019004\n",
      "train loss:0.6649281228881093\n",
      "train loss:0.6682331844536791\n",
      "train loss:0.5745373802116518\n",
      "train loss:0.7226860305088504\n",
      "train loss:0.5709115332290395\n",
      "train loss:0.5177250828205391\n",
      "train loss:0.6237236368394274\n",
      "train loss:0.5623010894463849\n",
      "train loss:0.6740185376246661\n",
      "train loss:0.6134718523925303\n",
      "train loss:0.5410003818075897\n",
      "train loss:0.5451046260929943\n",
      "train loss:0.5435195652412187\n",
      "train loss:0.6745129467552992\n",
      "train loss:0.7779478476126094\n",
      "train loss:0.6077880159607421\n",
      "train loss:0.520827757962522\n",
      "train loss:0.4204524393135676\n",
      "train loss:0.5988564669974242\n",
      "train loss:0.7150856869263768\n",
      "train loss:0.6956606775535765\n",
      "train loss:0.7170064813419648\n",
      "train loss:0.7031733353234253\n",
      "train loss:0.7732028790271965\n",
      "train loss:0.8931312062099863\n",
      "train loss:0.4608936226167618\n",
      "train loss:0.6870688302159282\n",
      "train loss:0.487526799389043\n",
      "train loss:0.7258097124510609\n",
      "train loss:0.6156981409119646\n",
      "train loss:0.5533806366443562\n",
      "train loss:0.4931025661933172\n",
      "train loss:0.4933512027881178\n",
      "train loss:0.6145637299020599\n",
      "train loss:0.5484818780893581\n",
      "train loss:0.5513680827373176\n",
      "train loss:0.6846481340098513\n",
      "train loss:0.5941529392183753\n",
      "train loss:0.7789439979382798\n",
      "train loss:0.3620404455806607\n",
      "train loss:0.43055565972289844\n",
      "train loss:0.5132038154588477\n",
      "train loss:0.4010845316481938\n",
      "train loss:0.9386678381440243\n",
      "train loss:0.8232701442438255\n",
      "train loss:0.5062563612015676\n",
      "train loss:0.7255203121530513\n",
      "train loss:0.6971271464420333\n",
      "train loss:0.4132902579819985\n",
      "train loss:0.7996204756675057\n",
      "train loss:0.5282126648554601\n",
      "train loss:0.7923201217707099\n",
      "train loss:0.6070333986141969\n",
      "train loss:0.6951109094066945\n",
      "train loss:0.5792039152695088\n",
      "train loss:0.6134453293937915\n",
      "train loss:0.539370824568992\n",
      "train loss:0.6824544951867845\n",
      "train loss:0.46962148402186055\n",
      "train loss:0.5250873550501021\n",
      "train loss:0.6044832145652534\n",
      "train loss:0.6744652413421635\n",
      "train loss:0.5369494998937332\n",
      "train loss:0.6858830773423982\n",
      "train loss:0.4553883455714372\n",
      "train loss:0.9108236438230801\n",
      "train loss:0.5215337918350749\n",
      "train loss:0.4303760467753647\n",
      "train loss:0.6874833065171005\n",
      "train loss:0.7589590362934322\n",
      "train loss:0.7078004544823384\n",
      "train loss:0.6195422377913326\n",
      "train loss:0.5432534795049472\n",
      "train loss:0.6143002015819368\n",
      "train loss:0.6971531466598495\n",
      "train loss:0.6128974192542679\n",
      "train loss:0.5916311710960641\n",
      "train loss:0.6023547618573177\n",
      "train loss:0.6835659045300922\n",
      "train loss:0.5287019215279641\n",
      "train loss:0.832589365997876\n",
      "train loss:0.681935076253989\n",
      "train loss:0.46163896224550294\n",
      "train loss:0.6725824711311532\n",
      "train loss:0.6843316142900056\n",
      "train loss:0.4736373916023194\n",
      "train loss:0.6629579080457495\n",
      "train loss:0.8139907977594362\n",
      "train loss:0.5578403638045419\n",
      "train loss:0.6787720633141872\n",
      "train loss:0.6698205564731319\n",
      "train loss:0.6798882478795909\n",
      "train loss:0.6075017176533729\n",
      "train loss:0.5030357091231952\n",
      "train loss:0.48746202968034913\n",
      "train loss:0.47236951307443453\n",
      "train loss:0.5403705821255425\n",
      "train loss:0.4558004257275208\n",
      "train loss:0.5970748144224529\n",
      "train loss:0.5033943715972192\n",
      "train loss:0.4069654365897001\n",
      "train loss:0.39589794593895306\n",
      "train loss:0.6453973921330253\n",
      "train loss:0.8159257909475061\n",
      "train loss:0.7625028070340014\n",
      "train loss:0.3693539925726391\n",
      "train loss:0.6519741846557316\n",
      "train loss:0.8679725485489251\n",
      "train loss:0.3836242168756049\n",
      "train loss:0.8663115254612471\n",
      "train loss:0.612143790574415\n",
      "train loss:0.5072423958385226\n",
      "train loss:0.517534389796672\n",
      "train loss:0.5840559248435151\n",
      "train loss:0.7011351784393918\n",
      "train loss:0.68731625094411\n",
      "train loss:0.6964769341087662\n",
      "train loss:0.6814731080529478\n",
      "train loss:0.676741742872793\n",
      "train loss:0.5497099435097365\n",
      "train loss:0.7908200079546408\n",
      "train loss:0.4998080855130257\n",
      "train loss:0.6182400901272657\n",
      "train loss:0.6226885381219727\n",
      "train loss:0.5496749808143462\n",
      "train loss:0.5534620630022415\n",
      "train loss:0.6824540387979853\n",
      "train loss:0.5580811007718582\n",
      "train loss:0.7966714363643804\n",
      "train loss:0.5433972266617092\n",
      "train loss:0.6161805750181112\n",
      "train loss:0.5508106954231303\n",
      "train loss:0.5470373382356324\n",
      "train loss:0.684902268088694\n",
      "train loss:0.7586978532435246\n",
      "train loss:0.6117189008136508\n",
      "train loss:0.721933099906101\n",
      "train loss:0.5394133638244648\n",
      "train loss:0.761021599523324\n",
      "train loss:0.4632712723352033\n",
      "train loss:0.5299621408301587\n",
      "train loss:0.6055910322986582\n",
      "train loss:0.6818163319895753\n",
      "train loss:0.7007395260729672\n",
      "train loss:0.614181755525131\n",
      "train loss:0.6070610492071082\n",
      "train loss:0.7675359716783033\n",
      "train loss:0.6738461102095713\n",
      "train loss:0.6774562072050275\n",
      "train loss:0.532459490031742\n",
      "train loss:0.6125274065028353\n",
      "train loss:0.6663533676367973\n",
      "train loss:0.5422864290323067\n",
      "train loss:0.4585097602357635\n",
      "train loss:0.5336925678489991\n",
      "train loss:0.7587248079599425\n",
      "train loss:0.6045141379366272\n",
      "train loss:0.6024874594066615\n",
      "train loss:0.6773398025895866\n",
      "train loss:0.6170070915779755\n",
      "train loss:0.6192642273487815\n",
      "train loss:0.36319713248464913\n",
      "train loss:0.6045733787718941\n",
      "train loss:0.6136438533908422\n",
      "train loss:0.6168444162275857\n",
      "train loss:0.5093157799196538\n",
      "train loss:0.6988710742081832\n",
      "train loss:0.31021656293166167\n",
      "train loss:0.7295632629108202\n",
      "train loss:0.28416492083834755\n",
      "train loss:0.60174610119935\n",
      "train loss:0.25215101288201114\n",
      "train loss:0.7600802673109568\n",
      "train loss:0.7286119683830776\n",
      "train loss:0.631524253836734\n",
      "train loss:0.6070009885084324\n",
      "train loss:0.739304206390231\n",
      "train loss:0.5017643569423574\n",
      "train loss:0.8300145665361727\n",
      "train loss:0.4970069664049238\n",
      "train loss:0.6099624202520268\n",
      "train loss:0.40498882062892816\n",
      "train loss:0.49971711419991216\n",
      "train loss:0.516470741152913\n",
      "train loss:0.6725179760536221\n",
      "train loss:0.6134633060344544\n",
      "train loss:0.49665142852746147\n",
      "train loss:0.4196929749283124\n",
      "train loss:0.3094445928547087\n",
      "train loss:0.4935835010218044\n",
      "train loss:0.2686017762930327\n",
      "train loss:0.7237934758794277\n",
      "train loss:0.630137443897737\n",
      "train loss:0.6214280270376922\n",
      "train loss:0.3436525847106988\n",
      "train loss:0.78426542927188\n",
      "train loss:0.5203562436857453\n",
      "train loss:0.3524050632436696\n",
      "train loss:1.0462561810028579\n",
      "train loss:0.6486330491491156\n",
      "train loss:0.6302598855575463\n",
      "train loss:0.39760169877293083\n",
      "train loss:0.8243458797053311\n",
      "train loss:0.6249445417530047\n",
      "train loss:0.4787376474472908\n",
      "train loss:0.7755566725090832\n",
      "train loss:0.5271171119087081\n",
      "train loss:0.5343500476529306\n",
      "train loss:0.5406177783230148\n",
      "train loss:0.6162146818183862\n",
      "train loss:0.526488163452525\n",
      "train loss:0.6041707722142305\n",
      "train loss:0.5096507872608858\n",
      "train loss:0.6889241050000693\n",
      "train loss:0.6028577555146101\n",
      "train loss:0.5356314850384118\n",
      "train loss:0.5128030763368369\n",
      "train loss:0.6818006626837376\n",
      "train loss:0.5372542713257439\n",
      "train loss:0.7001215129106524\n",
      "train loss:0.3416225929454876\n",
      "train loss:0.7868023719055899\n",
      "train loss:0.5177436772549087\n",
      "train loss:0.31730189283513305\n",
      "train loss:0.39764301855945167\n",
      "train loss:0.613893293885066\n",
      "train loss:0.6263463452313848\n",
      "train loss:0.4973190737235168\n",
      "train loss:0.9955540701551998\n",
      "train loss:0.8447870678759107\n",
      "train loss:0.5308049972394403\n",
      "train loss:0.5129179891736821\n",
      "train loss:0.48914738690248205\n",
      "train loss:0.5231993610317717\n",
      "train loss:0.49201149316797366\n",
      "train loss:0.6193296596554629\n",
      "train loss:0.5126376833364394\n",
      "train loss:0.8405011567385475\n",
      "train loss:0.7000402060601904\n",
      "train loss:0.6153691882375351\n",
      "train loss:0.5085855737766554\n",
      "train loss:0.6062633241654793\n",
      "train loss:0.43137563437343\n",
      "train loss:0.7905993789432664\n",
      "train loss:0.3492741450752378\n",
      "train loss:0.6024254810254074\n",
      "train loss:0.6305298002641695\n",
      "train loss:0.5167898397277935\n",
      "train loss:0.6185170130965938\n",
      "train loss:0.4965265808008673\n",
      "train loss:0.5256437173307805\n",
      "train loss:0.7780480829316907\n",
      "train loss:0.31475807097051584\n",
      "train loss:0.5105740774665243\n",
      "train loss:0.6136423566735179\n",
      "train loss:0.46973853465842186\n",
      "train loss:0.8440554401918042\n",
      "train loss:0.5138146898403944\n",
      "train loss:0.5922124567730201\n",
      "train loss:0.5046691326534051\n",
      "train loss:0.6891598572853194\n",
      "train loss:0.49749043067184634\n",
      "train loss:0.8099913549614002\n",
      "train loss:0.6082109170863907\n",
      "train loss:0.7210539548445024\n",
      "train loss:0.6135940587632838\n",
      "train loss:0.6190706588577457\n",
      "train loss:0.5321010401406074\n",
      "train loss:0.5728843632324245\n",
      "train loss:0.5326100922126846\n",
      "train loss:0.6177451883074385\n",
      "train loss:0.6834615572808406\n",
      "train loss:0.4516909280066207\n",
      "train loss:0.539211816440476\n",
      "train loss:0.597525856299183\n",
      "train loss:0.7067840194746801\n",
      "train loss:0.5910147293375198\n",
      "train loss:0.6964316071653024\n",
      "train loss:0.35381336002169517\n",
      "train loss:0.5183095346123224\n",
      "train loss:0.9695236483696933\n",
      "train loss:0.7710858371482151\n",
      "train loss:0.4542270819818072\n",
      "train loss:0.5718568314219143\n",
      "train loss:0.461515108102856\n",
      "train loss:0.5187015000714063\n",
      "train loss:0.5265547643291868\n",
      "train loss:0.511844936087636\n",
      "train loss:0.7665992361521531\n",
      "train loss:0.5882232811088837\n",
      "train loss:0.49450917373884834\n",
      "train loss:0.723887133251915\n",
      "train loss:0.7165266153933922\n",
      "train loss:0.5121170762949874\n",
      "train loss:0.42096952142425703\n",
      "train loss:0.512654795062217\n",
      "train loss:0.37115164102862425\n",
      "train loss:0.9475228330896488\n",
      "train loss:0.7234015673206092\n",
      "train loss:0.6159956624474108\n",
      "train loss:0.6880230612655835\n",
      "train loss:0.5156017341036306\n",
      "train loss:0.33108041375487657\n",
      "train loss:0.6609072814189924\n",
      "train loss:0.41616880509578785\n",
      "train loss:0.6510529796104977\n",
      "train loss:0.705997394104657\n",
      "train loss:0.8195359539819602\n",
      "train loss:0.7024577090998949\n",
      "train loss:0.8563771614434366\n",
      "train loss:0.599894271557269\n",
      "train loss:0.4047512498547494\n",
      "train loss:0.6779878021671161\n",
      "train loss:0.6281794236339081\n",
      "train loss:0.7264864109679103\n",
      "train loss:0.5071695854948565\n",
      "train loss:0.6753992014027317\n",
      "train loss:0.5477971570563739\n",
      "train loss:0.6772904243934663\n",
      "train loss:0.593656854981209\n",
      "train loss:0.42293237795883815\n",
      "train loss:0.44953421978025554\n",
      "train loss:0.4535903075542856\n",
      "train loss:0.6825245357404277\n",
      "train loss:0.5141507202066895\n",
      "train loss:0.5939896783103187\n",
      "train loss:0.4205368600895887\n",
      "train loss:0.6071438921665351\n",
      "train loss:0.3822806934075302\n",
      "train loss:0.8349377036046965\n",
      "train loss:0.5978581606856407\n",
      "train loss:0.6149401300646515\n",
      "train loss:0.7602976476680741\n",
      "train loss:0.5139715694296669\n",
      "train loss:0.6326436043331103\n",
      "train loss:0.48673880095732985\n",
      "train loss:0.2589912953746739\n",
      "train loss:0.8253687978603249\n",
      "train loss:0.7264748003362224\n",
      "train loss:0.6384217896419908\n",
      "train loss:0.7301813052293579\n",
      "train loss:0.6256288647139524\n",
      "train loss:0.7026567277596921\n",
      "train loss:0.5127591494100178\n",
      "train loss:0.5304411409988532\n",
      "train loss:0.6930103075656847\n",
      "train loss:0.7387692619907116\n",
      "train loss:0.7421284908633183\n",
      "train loss:0.47432132663715076\n",
      "train loss:0.5382287481942896\n",
      "train loss:0.5977318122413227\n",
      "train loss:0.6679503944142444\n",
      "train loss:0.5617842180681322\n",
      "train loss:0.6433137988082207\n",
      "train loss:0.5472442004995488\n",
      "train loss:0.6163110783581034\n",
      "train loss:0.5459148195847587\n",
      "train loss:0.6053087871469168\n",
      "train loss:0.4764447448471061\n",
      "train loss:0.5487485569300525\n",
      "train loss:0.684496226670374\n",
      "train loss:0.5214559994819451\n",
      "train loss:0.6157825232044969\n",
      "train loss:0.4235731040629142\n",
      "train loss:0.39811294549451853\n",
      "train loss:0.7443015314837862\n",
      "train loss:0.7250264572144525\n",
      "train loss:0.701726674596482\n",
      "train loss:0.2667066113779729\n",
      "train loss:0.458833829946457\n",
      "train loss:0.37565039413390966\n",
      "train loss:0.6007560634251641\n",
      "train loss:0.6049451912856749\n",
      "train loss:0.8645294903710937\n",
      "train loss:0.4947380083960359\n",
      "train loss:0.5003037141544912\n",
      "train loss:0.37174101017119854\n",
      "train loss:0.7083275303051247\n",
      "train loss:0.6965281608757725\n",
      "train loss:0.7688319334572873\n",
      "train loss:0.9183094403500857\n",
      "train loss:0.692455747936226\n",
      "train loss:0.7403757293075742\n",
      "train loss:0.5877818492372051\n",
      "train loss:0.5623818444523401\n",
      "train loss:0.7268212272487008\n",
      "train loss:0.6075695858711646\n",
      "train loss:0.6427601291788535\n",
      "train loss:0.6239555617229298\n",
      "train loss:0.5369993189056667\n",
      "train loss:0.5754475594901884\n",
      "train loss:0.5752650840747834\n",
      "train loss:0.6522874921730517\n",
      "train loss:0.5800954318546258\n",
      "train loss:0.6272104853445436\n",
      "train loss:0.7278692486593162\n",
      "train loss:0.6301669696934893\n",
      "train loss:0.6109049319431016\n",
      "train loss:0.44697541797823337\n",
      "train loss:0.7194402346036859\n",
      "train loss:0.47708077083957445\n",
      "train loss:0.5786195254878478\n",
      "train loss:0.6442159336396169\n",
      "train loss:0.6719961047488857\n",
      "train loss:0.34421760409360275\n",
      "train loss:0.5201212328621763\n",
      "train loss:0.6121641076536554\n",
      "train loss:0.6028133062333996\n",
      "train loss:0.35039445505190886\n",
      "train loss:0.6827778410533341\n",
      "train loss:0.8627884911086026\n",
      "train loss:0.3374601908629711\n",
      "train loss:0.6331985401225042\n",
      "train loss:0.7351229739987495\n",
      "train loss:0.887912749616796\n",
      "train loss:0.7813550936809224\n",
      "train loss:0.7803771958615802\n",
      "train loss:0.7015231805401356\n",
      "train loss:0.6069589824532353\n",
      "train loss:0.7262526564389913\n",
      "train loss:0.44098844011599614\n",
      "train loss:0.6432244383815278\n",
      "train loss:0.5028099061173259\n",
      "train loss:0.6691229593168166\n",
      "train loss:0.6984134864509856\n",
      "train loss:0.6241102717866702\n",
      "train loss:0.7608699743592666\n",
      "train loss:0.6276790053778496\n",
      "train loss:0.6194765975844828\n",
      "train loss:0.6980845734745824\n",
      "train loss:0.6170602073219495\n",
      "train loss:0.5831031813012123\n",
      "train loss:0.5281961994933383\n",
      "train loss:0.7133514568677206\n",
      "train loss:0.6316458603470974\n",
      "train loss:0.7057684008592979\n",
      "train loss:0.6549406952513747\n",
      "train loss:0.6400360901921303\n",
      "train loss:0.7285608066662959\n",
      "train loss:0.5723949308244913\n",
      "train loss:0.6119824361784697\n",
      "train loss:0.5532816683186543\n",
      "train loss:0.5593383755246071\n",
      "train loss:0.5806511757497128\n",
      "train loss:0.6567726114949128\n",
      "train loss:0.7627128250699723\n",
      "train loss:0.6673342436006156\n",
      "train loss:0.6744275390520765\n",
      "train loss:0.45936809616685254\n",
      "train loss:0.5145511096842889\n",
      "train loss:0.6107679675377444\n",
      "train loss:0.6944280008462351\n",
      "train loss:0.6204905765954071\n",
      "train loss:0.5257959298892939\n",
      "train loss:0.5718683035283267\n",
      "train loss:0.6189656348352004\n",
      "train loss:0.7208004881709981\n",
      "train loss:0.44033147266331135\n",
      "train loss:0.6010911192801179\n",
      "train loss:0.47899932802974304\n",
      "train loss:0.282482298773711\n",
      "train loss:0.7793353941490319\n",
      "train loss:0.8277637775166571\n",
      "train loss:0.515145241285819\n",
      "train loss:0.4565233169403834\n",
      "train loss:0.39247091680703167\n",
      "train loss:0.6065308339122658\n",
      "train loss:0.5057016261487688\n",
      "train loss:0.36869528777462257\n",
      "train loss:0.5428638121138404\n",
      "train loss:0.3504969222542006\n",
      "train loss:0.3594883048992808\n",
      "train loss:0.8950428294584029\n",
      "train loss:0.430783510492606\n",
      "train loss:0.6444008034055562\n",
      "train loss:0.44470617665071616\n",
      "train loss:0.7592710921451514\n",
      "train loss:0.8951938376045895\n",
      "train loss:0.7101071557216531\n",
      "train loss:0.6156029457777998\n",
      "train loss:0.557321377332141\n",
      "train loss:0.581584264232856\n",
      "train loss:0.7595635299755015\n",
      "train loss:0.5143632307418635\n",
      "train loss:0.4817521953058099\n",
      "train loss:0.6119526422176972\n",
      "train loss:0.6062273301345834\n",
      "train loss:0.6061207592592286\n",
      "train loss:0.546944164173033\n",
      "train loss:0.6351080063677038\n",
      "train loss:0.6273810490133933\n",
      "train loss:0.7148284712339477\n",
      "train loss:0.524796325057237\n",
      "train loss:0.6051113346100483\n",
      "train loss:0.6664972967687663\n",
      "train loss:0.46784811994804754\n",
      "train loss:0.6927806343784907\n",
      "train loss:0.47399270221628875\n",
      "train loss:0.7532390150726691\n",
      "train loss:0.5083548990925782\n",
      "train loss:0.6153658674160859\n",
      "train loss:0.6127556758446024\n",
      "train loss:0.4924103869445823\n",
      "train loss:0.43588182010662424\n",
      "train loss:0.6663627833173952\n",
      "train loss:0.6100253209623445\n",
      "train loss:0.7144262093650815\n",
      "train loss:0.40374734407055246\n",
      "train loss:0.4207727031829007\n",
      "train loss:0.5108946235849958\n",
      "train loss:0.26727384798953285\n",
      "train loss:0.8721034236165093\n",
      "train loss:0.36638276645135215\n",
      "train loss:0.5181252410659704\n",
      "train loss:0.45871419684139203\n",
      "train loss:0.6505858232589511\n",
      "train loss:0.6852771759824019\n",
      "train loss:0.31305391230738927\n",
      "train loss:0.5497670259141103\n",
      "train loss:1.1527350987334568\n",
      "train loss:0.5126852798981278\n",
      "train loss:0.24001381180561765\n",
      "train loss:0.5501212459575006\n",
      "train loss:0.6207143556918407\n",
      "train loss:0.5705611591442622\n",
      "train loss:0.5093973364714931\n",
      "train loss:0.6141745174153637\n",
      "train loss:0.7527784858943366\n",
      "train loss:0.8559407001954913\n",
      "train loss:0.3982389993834007\n",
      "train loss:0.5101197620007436\n",
      "train loss:0.6569431563969094\n",
      "train loss:0.5342053082564593\n",
      "train loss:0.5763161674131198\n",
      "train loss:0.7017438335307001\n",
      "train loss:0.6231437924325094\n",
      "train loss:0.40268904454637244\n",
      "train loss:0.4428581408729343\n",
      "train loss:0.7356618517681588\n",
      "train loss:0.6062119048673873\n",
      "train loss:0.519287594413013\n",
      "train loss:0.4996190126311726\n",
      "train loss:0.48166383963051296\n",
      "train loss:0.6059560860681454\n",
      "train loss:0.6186515560377581\n",
      "train loss:0.5106598521883818\n",
      "train loss:0.6409924270100726\n",
      "train loss:0.6349858977726408\n",
      "train loss:0.6288247484039254\n",
      "train loss:1.047056384338847\n",
      "train loss:0.8260620625187848\n",
      "train loss:0.5848265496174655\n",
      "train loss:0.582777720508316\n",
      "train loss:0.5966019412124731\n",
      "train loss:0.8144801685485377\n",
      "train loss:0.622740667152961\n",
      "train loss:0.6095452034000973\n",
      "train loss:0.6495949565238917\n",
      "train loss:0.6888323994734489\n",
      "train loss:0.7152736451286962\n",
      "train loss:0.5814193350613903\n",
      "train loss:0.6144580410094876\n",
      "train loss:0.5772545825741423\n",
      "train loss:0.6302618896427798\n",
      "train loss:0.6227530680536757\n",
      "train loss:0.6437281976473356\n",
      "train loss:0.588465980730352\n",
      "train loss:0.5164367856932104\n",
      "train loss:0.5330096758299356\n",
      "train loss:0.6086180708750202\n",
      "train loss:0.6016544719341066\n",
      "train loss:0.4816513912189784\n",
      "train loss:0.8296465155608667\n",
      "train loss:0.5106667604567022\n",
      "train loss:0.6043114354574735\n",
      "train loss:0.7112112497425925\n",
      "train loss:0.43038545533418554\n",
      "train loss:0.4918324171383365\n",
      "train loss:0.5091073588793991\n",
      "train loss:0.497675601678672\n",
      "train loss:0.8499849046866569\n",
      "train loss:0.7106528932937326\n",
      "train loss:1.014827733574938\n",
      "train loss:0.6773996673357764\n",
      "train loss:0.44947903816954815\n",
      "train loss:0.45215699029495304\n",
      "train loss:0.8694026014343426\n",
      "train loss:0.5202310001771357\n",
      "train loss:0.49126636071003\n",
      "train loss:0.6189216278256428\n",
      "train loss:0.7685806762693359\n",
      "train loss:0.44454086293921424\n",
      "train loss:0.690587720950097\n",
      "train loss:0.45293194043601676\n",
      "train loss:0.5169317711293802\n",
      "train loss:0.4665657474554215\n",
      "train loss:0.4173910179169866\n",
      "train loss:0.6022336852879139\n",
      "train loss:0.4929713923484492\n",
      "train loss:0.699831751624493\n",
      "train loss:0.6190823878806099\n",
      "train loss:0.8574610096086767\n",
      "train loss:0.5239477075980472\n",
      "train loss:0.6805688898844926\n",
      "train loss:0.7610722465529507\n",
      "train loss:0.693928733015145\n",
      "train loss:0.6772816106195717\n",
      "train loss:0.7207774273238805\n",
      "train loss:0.5420988936191292\n",
      "train loss:0.6112927201833293\n",
      "train loss:0.688775025443311\n",
      "train loss:0.603870212917201\n",
      "train loss:0.6124635834626037\n",
      "train loss:0.6225787714420601\n",
      "train loss:0.41126440045286794\n",
      "train loss:0.7000024232263995\n",
      "train loss:0.5435957321757687\n",
      "train loss:0.7554914572784239\n",
      "train loss:0.6847572695886532\n",
      "train loss:0.5332536734568578\n",
      "train loss:0.7480534368275795\n",
      "train loss:0.6615185198827958\n",
      "train loss:0.647907430420772\n",
      "train loss:0.5381129262164398\n",
      "train loss:0.6851263369686815\n",
      "train loss:0.7413054547534733\n",
      "train loss:0.3901553810770609\n",
      "train loss:0.5458343191715531\n",
      "train loss:0.39764129276391624\n",
      "train loss:0.6780669651624217\n",
      "train loss:0.3455911605390154\n",
      "train loss:0.6901067045000928\n",
      "train loss:0.7120657464642278\n",
      "train loss:0.5188499619360485\n",
      "train loss:0.6419872855305717\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6862745098039216\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=40)\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af604770-232e-41a9-91d0-ecf3108522fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.29939753712038\n",
      "=== epoch:1, train acc:0.73, test acc:0.69 ===\n",
      "train loss:2.2938691925828003\n",
      "train loss:2.28605819805522\n",
      "train loss:2.2737493479634634\n",
      "train loss:2.2453058439709386\n",
      "train loss:2.215235026566189\n",
      "train loss:2.1644670050933064\n",
      "train loss:2.117339595125234\n",
      "train loss:2.0427171555840835\n",
      "train loss:1.9073825831151048\n",
      "train loss:1.856211316311138\n",
      "train loss:1.6357375501655909\n",
      "train loss:1.4782890419952968\n",
      "train loss:1.2367037062501176\n",
      "train loss:1.1187517224910508\n",
      "train loss:1.0987064120436432\n",
      "train loss:0.7632450204243462\n",
      "train loss:0.5553110150845421\n",
      "train loss:0.6959434439186544\n",
      "train loss:0.5561921727960205\n",
      "train loss:0.3070820095681229\n",
      "train loss:0.9605470387185017\n",
      "train loss:0.5022397283820682\n",
      "train loss:0.5298032058203372\n",
      "train loss:1.013526158103717\n",
      "train loss:0.7408590186816693\n",
      "train loss:0.5511494544140287\n",
      "train loss:0.34003734282609377\n",
      "train loss:0.7861658960225808\n",
      "train loss:0.5144292331088963\n",
      "train loss:0.5913415731927154\n",
      "train loss:0.552202705882644\n",
      "train loss:0.6090264240722434\n",
      "train loss:0.7128694257527836\n",
      "train loss:0.6736473682080761\n",
      "train loss:0.6880794329089803\n",
      "train loss:0.6355666203278918\n",
      "train loss:0.6834673602574484\n",
      "train loss:0.6386196614580373\n",
      "train loss:0.484161895050985\n",
      "train loss:0.7051069475384807\n",
      "train loss:0.8245052929308565\n",
      "train loss:0.40965234862549166\n",
      "train loss:0.7720547378223567\n",
      "train loss:0.6077463630171926\n",
      "train loss:0.3798041847282308\n",
      "train loss:0.6445402100194026\n",
      "train loss:0.487383349002401\n",
      "train loss:0.7618321105605854\n",
      "train loss:0.5025056668205514\n",
      "train loss:0.5887340487641566\n",
      "train loss:0.716221274881516\n",
      "train loss:0.6248032279763691\n",
      "train loss:0.7468643631557671\n",
      "train loss:0.4899264417445684\n",
      "train loss:0.5855103574990473\n",
      "train loss:0.5935490192858548\n",
      "train loss:0.6124665448938664\n",
      "train loss:0.7410853377284016\n",
      "train loss:0.5439132846502774\n",
      "train loss:0.7364119570810576\n",
      "train loss:0.6300150532630749\n",
      "train loss:0.4883570553232917\n",
      "train loss:0.548137446001473\n",
      "train loss:0.6764370201433978\n",
      "train loss:0.7049394140634401\n",
      "train loss:0.7069670835483062\n",
      "train loss:0.4038073376798816\n",
      "train loss:0.6261215435052614\n",
      "train loss:0.5069910310146757\n",
      "train loss:0.7142051158477372\n",
      "train loss:0.8693455174651069\n",
      "train loss:0.7127371196218368\n",
      "train loss:0.7071643219806703\n",
      "train loss:0.6242315734444496\n",
      "train loss:0.6262894546589994\n",
      "train loss:0.6370897987129915\n",
      "train loss:0.6375295786338018\n",
      "train loss:0.74977827943462\n",
      "train loss:0.663637531082558\n",
      "train loss:0.6766379301138998\n",
      "train loss:0.5922647684589766\n",
      "train loss:0.6703024023560733\n",
      "train loss:0.682233652710253\n",
      "train loss:0.7801448247069576\n",
      "train loss:0.626719401488412\n",
      "train loss:0.5517877147998\n",
      "train loss:0.6642172384889058\n",
      "train loss:0.6234072143374643\n",
      "train loss:0.6140189002612151\n",
      "train loss:0.5062906069591472\n",
      "train loss:0.49448815288644077\n",
      "train loss:0.632002199880295\n",
      "train loss:0.37069066044761756\n",
      "train loss:0.46542718239046604\n",
      "train loss:0.6844790942112362\n",
      "train loss:0.6875419909516309\n",
      "train loss:0.6743424444560497\n",
      "train loss:0.3616638754846513\n",
      "train loss:0.5139817070011518\n",
      "train loss:0.8202440079389103\n",
      "train loss:0.655883566042568\n",
      "train loss:0.7073583731949131\n",
      "train loss:0.7783522026281675\n",
      "train loss:0.6147728872864232\n",
      "train loss:0.5066742509850778\n",
      "train loss:0.5228616878236622\n",
      "train loss:0.5740808884084753\n",
      "train loss:0.5352056235700863\n",
      "train loss:0.4601867819124214\n",
      "train loss:0.7469717311419315\n",
      "train loss:0.5198160624511022\n",
      "train loss:0.5077578676010148\n",
      "train loss:0.2687704321126124\n",
      "train loss:1.012788834390491\n",
      "train loss:0.4869250544043477\n",
      "train loss:0.6187358141861317\n",
      "train loss:0.4883632638242368\n",
      "train loss:0.9262189149877436\n",
      "train loss:0.8227904715555413\n",
      "train loss:0.5002295043161679\n",
      "train loss:0.6421397619231832\n",
      "train loss:0.5144056706463112\n",
      "train loss:0.6171616189649144\n",
      "train loss:0.5305791010582482\n",
      "train loss:0.6786389906631705\n",
      "train loss:0.5622962741141826\n",
      "train loss:0.6252917085113296\n",
      "train loss:0.6062530131115971\n",
      "train loss:0.5972119588246336\n",
      "train loss:0.46846444368226053\n",
      "train loss:0.7531385849075234\n",
      "train loss:0.7557703007993632\n",
      "train loss:0.7679001743317666\n",
      "train loss:0.5609131262022606\n",
      "train loss:0.6789621804936846\n",
      "train loss:0.6292570920721526\n",
      "train loss:0.6100462645030474\n",
      "train loss:0.7899816559851416\n",
      "train loss:0.5285426895417864\n",
      "train loss:0.8110373538874363\n",
      "train loss:0.7070126166127212\n",
      "train loss:0.5189586433584665\n",
      "train loss:0.5961159321724074\n",
      "train loss:0.523315973777142\n",
      "train loss:0.548517954028319\n",
      "train loss:0.623128072406956\n",
      "train loss:0.7738113772931019\n",
      "train loss:0.6006910785207332\n",
      "train loss:0.5257886535907556\n",
      "train loss:0.6520692423051315\n",
      "train loss:0.8472580851145644\n",
      "train loss:0.7224347087944027\n",
      "train loss:0.521764864727632\n",
      "train loss:0.6286765693583803\n",
      "train loss:0.7686003587807261\n",
      "train loss:0.5242977360977749\n",
      "train loss:0.5140946107168427\n",
      "train loss:0.4460749283544869\n",
      "train loss:0.7758055583734575\n",
      "train loss:0.532217698916706\n",
      "train loss:0.616342620938911\n",
      "train loss:0.5257558458410994\n",
      "train loss:0.6943344580817497\n",
      "train loss:0.5274588105538567\n",
      "train loss:0.6201253977707772\n",
      "train loss:0.725237363223205\n",
      "train loss:0.6449094146528742\n",
      "train loss:0.4984002181204076\n",
      "train loss:0.43603829600911126\n",
      "train loss:0.5788818829766484\n",
      "train loss:0.6233226958150506\n",
      "train loss:0.5127801403399118\n",
      "train loss:0.7150602544278957\n",
      "train loss:0.3936407914705493\n",
      "train loss:0.26790404485885877\n",
      "train loss:0.8908761812202016\n",
      "train loss:0.9215849624380767\n",
      "train loss:0.7174230641326423\n",
      "train loss:0.623009334685903\n",
      "train loss:0.7648469710362942\n",
      "train loss:0.5971700170229709\n",
      "train loss:0.600735449429701\n",
      "train loss:0.5606204294806413\n",
      "train loss:0.5603702630951541\n",
      "train loss:0.6285262154284852\n",
      "train loss:0.7337776079502472\n",
      "train loss:0.565953253909587\n",
      "train loss:0.6340363705612583\n",
      "train loss:0.7898738068203038\n",
      "train loss:0.44546615332325423\n",
      "train loss:0.5519688551736908\n",
      "train loss:0.46515032009522495\n",
      "train loss:0.6982310867146019\n",
      "train loss:0.44661843191177414\n",
      "train loss:0.6304645678727923\n",
      "train loss:0.6169400992287097\n",
      "train loss:0.8489059471070306\n",
      "train loss:0.7060741793583146\n",
      "train loss:0.5106881910202556\n",
      "train loss:0.7078590901370367\n",
      "train loss:0.7223263895185441\n",
      "train loss:0.5232489651114973\n",
      "train loss:0.5323505549486636\n",
      "train loss:0.7738341743091695\n",
      "train loss:0.6942926748327067\n",
      "train loss:0.5280754675975948\n",
      "train loss:0.45502151139853353\n",
      "train loss:0.6174049966685036\n",
      "train loss:0.6271343373212666\n",
      "train loss:0.6179595739512248\n",
      "train loss:0.8274359888436411\n",
      "train loss:0.5609444129988345\n",
      "train loss:0.7398076724852278\n",
      "train loss:0.49043943542956436\n",
      "train loss:0.6174423899031791\n",
      "train loss:0.8049036638840557\n",
      "train loss:0.6728828835515718\n",
      "train loss:0.6297480945442402\n",
      "train loss:0.448887826855906\n",
      "train loss:0.548194192274115\n",
      "train loss:0.6027628939445389\n",
      "train loss:0.4796923740090592\n",
      "train loss:0.526024897910414\n",
      "train loss:0.5031223972864801\n",
      "train loss:0.8600650489445784\n",
      "train loss:0.6160221312682836\n",
      "train loss:0.8508268787935268\n",
      "train loss:0.8595031380279086\n",
      "train loss:0.5936522085716303\n",
      "train loss:0.51015445747739\n",
      "train loss:0.693655000077691\n",
      "train loss:0.5345953284067165\n",
      "train loss:0.5098061028393683\n",
      "train loss:0.6881754792162462\n",
      "train loss:0.6242546596158945\n",
      "train loss:0.6053446792769127\n",
      "train loss:0.6022128596454224\n",
      "train loss:0.6002877903104116\n",
      "train loss:0.6900837406725817\n",
      "train loss:0.38853790765928686\n",
      "train loss:0.6191742052200537\n",
      "train loss:0.504658062192712\n",
      "train loss:0.5953914858256005\n",
      "train loss:0.6911489064270415\n",
      "train loss:0.7207154578446079\n",
      "train loss:0.6946911736081256\n",
      "train loss:0.879338930732888\n",
      "train loss:0.44455515054083417\n",
      "train loss:0.43382141858533557\n",
      "train loss:0.4258817782429922\n",
      "train loss:0.49512094022136155\n",
      "train loss:0.5246737721388899\n",
      "train loss:0.8598069484958681\n",
      "train loss:0.6240891347375499\n",
      "train loss:0.5341750728662366\n",
      "train loss:0.6044556759222262\n",
      "train loss:0.4113866643495892\n",
      "train loss:0.49931300335030837\n",
      "train loss:0.48977930063877356\n",
      "train loss:0.7485160082016815\n",
      "train loss:0.7123387112982892\n",
      "train loss:0.6184310633372851\n",
      "train loss:0.8333250303349186\n",
      "train loss:0.6259354723818307\n",
      "train loss:0.6441194405667681\n",
      "train loss:0.5288050011386399\n",
      "train loss:0.7480465983361418\n",
      "train loss:0.5781771831738197\n",
      "train loss:0.6612243776110482\n",
      "train loss:0.5746249193877185\n",
      "train loss:0.6335686291853774\n",
      "train loss:0.5176315102515769\n",
      "train loss:0.6162300916836746\n",
      "train loss:0.6136358729502093\n",
      "train loss:0.5358806148816676\n",
      "train loss:0.700777819248211\n",
      "train loss:0.6784253338227029\n",
      "train loss:0.34162460440668696\n",
      "train loss:0.6210438533230108\n",
      "train loss:0.5453368977618399\n",
      "train loss:0.48876673177838315\n",
      "train loss:0.6347619736474043\n",
      "train loss:0.3774188768141423\n",
      "train loss:0.6527069726859491\n",
      "train loss:0.8647604671848489\n",
      "train loss:0.4901799397147701\n",
      "train loss:0.6369557725864773\n",
      "train loss:0.626540197007917\n",
      "train loss:0.3699355335561605\n",
      "train loss:0.6518703281749229\n",
      "train loss:0.5196249169320937\n",
      "train loss:0.6712619785410987\n",
      "train loss:0.7069073436319344\n",
      "train loss:0.510697956580691\n",
      "train loss:0.7154290816261714\n",
      "train loss:0.5262279802236643\n",
      "train loss:0.6735064980687282\n",
      "train loss:0.5454058399566628\n",
      "train loss:0.6655554995905841\n",
      "train loss:0.545703580277167\n",
      "train loss:0.5656018389662745\n",
      "train loss:0.5338102845280865\n",
      "train loss:0.7181688007369512\n",
      "train loss:0.7267240263250356\n",
      "train loss:0.6628443762732737\n",
      "train loss:0.7381830747698105\n",
      "train loss:0.6159165936395303\n",
      "train loss:0.6793106509877497\n",
      "train loss:0.7129660282478001\n",
      "train loss:0.6262588938670762\n",
      "train loss:0.6715781588227319\n",
      "train loss:0.5757932001808995\n",
      "train loss:0.5316882613970446\n",
      "train loss:0.547073133229992\n",
      "train loss:0.6847629525750885\n",
      "train loss:0.7472757309314717\n",
      "train loss:0.6157598812915656\n",
      "train loss:0.4659165020237638\n",
      "train loss:0.6093042760833196\n",
      "train loss:0.7626883851466564\n",
      "train loss:0.49897070169039476\n",
      "train loss:0.7074760575083229\n",
      "train loss:0.3102855823811713\n",
      "train loss:0.9372627185066588\n",
      "train loss:0.9223053525467428\n",
      "train loss:0.7114895541998816\n",
      "train loss:0.6010528947597689\n",
      "train loss:0.6953254982923143\n",
      "train loss:0.7675292479309987\n",
      "train loss:0.6350291215190708\n",
      "train loss:0.7009754379460829\n",
      "train loss:0.6507906473888105\n",
      "train loss:0.6817593975697822\n",
      "train loss:0.6546756135508873\n",
      "train loss:0.7155550199618842\n",
      "train loss:0.64915532603639\n",
      "train loss:0.6676859316163244\n",
      "train loss:0.6174527755252136\n",
      "train loss:0.6154266698508518\n",
      "train loss:0.6347355984072575\n",
      "train loss:0.63063417706721\n",
      "train loss:0.6820832818554493\n",
      "train loss:0.5517547750127464\n",
      "train loss:0.5405421647796064\n",
      "train loss:0.4227883713384859\n",
      "train loss:0.7213513385036581\n",
      "train loss:0.5268307571809501\n",
      "train loss:0.4675575408713793\n",
      "train loss:0.4911849721812021\n",
      "train loss:0.6330145386160948\n",
      "train loss:0.815455372806906\n",
      "train loss:0.5075830206716924\n",
      "train loss:0.4847002769856633\n",
      "train loss:0.833895265663009\n",
      "train loss:0.34773018979612924\n",
      "train loss:0.7582088189810501\n",
      "train loss:0.4795338745479267\n",
      "train loss:0.6460281579435284\n",
      "train loss:0.7340322527223059\n",
      "train loss:0.734784888586572\n",
      "train loss:0.9035832084865018\n",
      "train loss:0.6022388298852465\n",
      "train loss:0.7023949079116194\n",
      "train loss:0.49438538335255827\n",
      "train loss:0.619002876641338\n",
      "train loss:0.6263890787024791\n",
      "train loss:0.6885128728115051\n",
      "train loss:0.5068923431686411\n",
      "train loss:0.6892628382707751\n",
      "train loss:0.5795915153081604\n",
      "train loss:0.629677536789347\n",
      "train loss:0.6154000873829071\n",
      "train loss:0.6348204753017651\n",
      "train loss:0.417977490015425\n",
      "train loss:0.687086762896035\n",
      "train loss:0.623521787304546\n",
      "train loss:0.37067067325095626\n",
      "train loss:0.6214249646288759\n",
      "train loss:0.6112981283082484\n",
      "train loss:0.6996578120837599\n",
      "train loss:0.7101087471760289\n",
      "train loss:0.3880447638874992\n",
      "train loss:0.37652789752346305\n",
      "train loss:0.8421291137248368\n",
      "train loss:0.8069376339140464\n",
      "train loss:0.5051591323302851\n",
      "train loss:0.39708983264975617\n",
      "train loss:0.7303099511337312\n",
      "train loss:0.3997908143254453\n",
      "train loss:0.6009005534630301\n",
      "train loss:0.6190795840021941\n",
      "train loss:0.5050723680266611\n",
      "train loss:0.6162723196430577\n",
      "train loss:0.3906295736278371\n",
      "train loss:0.8340925060952902\n",
      "train loss:0.7481922813051753\n",
      "train loss:1.0010258136062673\n",
      "train loss:0.5290011227439427\n",
      "train loss:0.6886446579982805\n",
      "train loss:0.6211286561428364\n",
      "train loss:0.6213835585911353\n",
      "train loss:0.7290297964707901\n",
      "train loss:0.6754159005685132\n",
      "train loss:0.5533465531768959\n",
      "train loss:0.7424105440149512\n",
      "train loss:0.6133905788630992\n",
      "train loss:0.7158156154674647\n",
      "train loss:0.6121552650161661\n",
      "train loss:0.6431182220621774\n",
      "train loss:0.5746957440377282\n",
      "train loss:0.6301420296778671\n",
      "train loss:0.6004323968791718\n",
      "train loss:0.48187992083689607\n",
      "train loss:0.5558247507005374\n",
      "train loss:0.47498575229760337\n",
      "train loss:0.6821668600449161\n",
      "train loss:0.6910321931308034\n",
      "train loss:0.5223372752678841\n",
      "train loss:0.7306601269579149\n",
      "train loss:0.7257829403311813\n",
      "train loss:0.38496251773456974\n",
      "train loss:0.8429899173408524\n",
      "train loss:0.9343829040624673\n",
      "train loss:0.29910659888775387\n",
      "train loss:0.5322756135974646\n",
      "train loss:0.5087635710591621\n",
      "train loss:0.625777352117921\n",
      "train loss:0.5121697278292976\n",
      "train loss:0.4965441632421392\n",
      "train loss:0.6179366344327617\n",
      "train loss:0.5035378071589436\n",
      "train loss:0.48270953953839424\n",
      "train loss:0.5025185883191992\n",
      "train loss:0.6496603406280587\n",
      "train loss:0.6113194610904029\n",
      "train loss:0.38394428223874055\n",
      "train loss:0.49686296383949846\n",
      "train loss:0.48774216929884096\n",
      "train loss:0.6013196498239789\n",
      "train loss:0.9058869881165966\n",
      "train loss:0.6300339277406799\n",
      "train loss:0.6191711716610905\n",
      "train loss:0.7465246411306434\n",
      "train loss:0.5026827241240687\n",
      "train loss:0.8749546352607174\n",
      "train loss:0.4404327842145575\n",
      "train loss:0.6105211370182672\n",
      "train loss:0.6181210787294522\n",
      "train loss:0.53576961198476\n",
      "train loss:0.7613631164296265\n",
      "train loss:0.39285203767772814\n",
      "train loss:0.604175218119219\n",
      "train loss:0.6184273110182567\n",
      "train loss:0.5381902676659465\n",
      "train loss:0.5289277873564288\n",
      "train loss:0.6090462368899787\n",
      "train loss:0.8390809376032105\n",
      "train loss:0.6842756508794297\n",
      "train loss:0.6237984568148509\n",
      "train loss:0.6084618119411462\n",
      "train loss:0.6772725729167272\n",
      "train loss:0.7511686080890188\n",
      "train loss:0.5393199948287842\n",
      "train loss:0.6873499592582013\n",
      "train loss:0.4721734557616591\n",
      "train loss:0.44959487396888465\n",
      "train loss:0.6905996890395869\n",
      "train loss:0.6307532491691153\n",
      "train loss:0.5302851565396592\n",
      "train loss:0.7664076120589648\n",
      "train loss:0.3499955729376819\n",
      "train loss:0.6313111036816049\n",
      "train loss:0.774763419669982\n",
      "train loss:0.7074785514693733\n",
      "train loss:0.5229167480091669\n",
      "train loss:0.614832568256483\n",
      "train loss:0.6016166436915362\n",
      "train loss:0.7865920909456993\n",
      "train loss:0.433622303156869\n",
      "train loss:0.6184315330436747\n",
      "train loss:0.5188207653207414\n",
      "train loss:0.5239029355844605\n",
      "train loss:0.43404754499464876\n",
      "train loss:0.6073704270863398\n",
      "train loss:0.8093838268425817\n",
      "train loss:0.5021236022863436\n",
      "train loss:0.731613992627665\n",
      "train loss:0.5247438754811022\n",
      "train loss:0.8023154398182013\n",
      "train loss:0.44528284991150446\n",
      "train loss:0.5085713850367454\n",
      "train loss:0.7320013586652574\n",
      "train loss:0.6156234407517481\n",
      "train loss:0.615823063159054\n",
      "train loss:0.5205492321537044\n",
      "train loss:0.5203434445039593\n",
      "train loss:0.5313781596602918\n",
      "train loss:0.7005055716713715\n",
      "train loss:0.6088860451116845\n",
      "train loss:0.6166649473480087\n",
      "train loss:0.6049437608955193\n",
      "train loss:0.7091773076219842\n",
      "train loss:0.5186443429006511\n",
      "train loss:0.6980322903440402\n",
      "train loss:0.7141121407850107\n",
      "train loss:0.6188966847322155\n",
      "train loss:0.6053309211211867\n",
      "train loss:0.7511227330132246\n",
      "train loss:0.607877262739481\n",
      "train loss:0.6800705647963662\n",
      "train loss:0.6122441370917993\n",
      "train loss:0.5532462040645933\n",
      "train loss:0.610381400653978\n",
      "train loss:0.6765070352046413\n",
      "train loss:0.8081971542205079\n",
      "train loss:0.6663745048672294\n",
      "train loss:0.5089320180814769\n",
      "train loss:0.830042376155246\n",
      "train loss:0.514829649060166\n",
      "train loss:0.5737286794828552\n",
      "train loss:0.5058042020794875\n",
      "train loss:0.6250977563311451\n",
      "train loss:0.7323914396525866\n",
      "train loss:0.5424483495518742\n",
      "train loss:0.6860016804970755\n",
      "train loss:0.6875239048201697\n",
      "train loss:0.5388029798849309\n",
      "train loss:0.8231747967737226\n",
      "train loss:0.7464631957354386\n",
      "train loss:0.6731942400940077\n",
      "train loss:0.6167764140017293\n",
      "train loss:0.6165883246144598\n",
      "train loss:0.4795357173311327\n",
      "train loss:0.7464220488178006\n",
      "train loss:0.6833280975892313\n",
      "train loss:0.5615917249077297\n",
      "train loss:0.5376729445361105\n",
      "train loss:0.6876378911676285\n",
      "train loss:0.6858394499492688\n",
      "train loss:0.6098774315387929\n",
      "train loss:0.673301549445158\n",
      "train loss:0.677412433661647\n",
      "train loss:0.5403878134598369\n",
      "train loss:0.7073154991512153\n",
      "train loss:0.4613670979745569\n",
      "train loss:0.5392365546770332\n",
      "train loss:0.6048026113919656\n",
      "train loss:0.5228161233033533\n",
      "train loss:0.6018309440683238\n",
      "train loss:0.4106504446815274\n",
      "train loss:0.5098372035975828\n",
      "train loss:0.48659934250284415\n",
      "train loss:0.486238742098643\n",
      "train loss:0.5464295339497178\n",
      "train loss:0.5019885804012898\n",
      "train loss:0.5332259615046501\n",
      "train loss:0.6628657866147011\n",
      "train loss:0.5038463893698745\n",
      "train loss:0.8292322020153854\n",
      "train loss:0.5253786070805414\n",
      "train loss:0.7831907990351286\n",
      "train loss:0.7593212212585796\n",
      "train loss:0.7311369722688503\n",
      "train loss:0.5144545200089189\n",
      "train loss:0.5304739280095916\n",
      "train loss:0.5181168925201797\n",
      "train loss:0.543042277160623\n",
      "train loss:0.7779693756441312\n",
      "train loss:0.6189574876083994\n",
      "train loss:0.7451352987081787\n",
      "train loss:0.6770816548955444\n",
      "train loss:0.5612474103787931\n",
      "train loss:0.5137436901188377\n",
      "train loss:0.6802746967403854\n",
      "train loss:0.5719043792944625\n",
      "train loss:0.5655390263824772\n",
      "train loss:0.7351157760645967\n",
      "train loss:0.7316916347826948\n",
      "train loss:0.7823321239150475\n",
      "train loss:0.6281593518772192\n",
      "train loss:0.46271576831806727\n",
      "train loss:0.5694624588604468\n",
      "train loss:0.5636050771834775\n",
      "train loss:0.5506062717717441\n",
      "train loss:0.6703000662500542\n",
      "train loss:0.5430473970590807\n",
      "train loss:0.5358047090023396\n",
      "train loss:0.60493951999775\n",
      "train loss:0.8626222393045022\n",
      "train loss:0.6012956427326229\n",
      "train loss:0.6138631403551261\n",
      "train loss:0.5945006250168637\n",
      "train loss:0.7840015464748042\n",
      "train loss:0.6103126099930072\n",
      "train loss:0.518899879893046\n",
      "train loss:0.5428059585231598\n",
      "train loss:0.6034434929051944\n",
      "train loss:0.6341547322742602\n",
      "train loss:0.3295371758240857\n",
      "train loss:0.6109821698088777\n",
      "train loss:0.7241826583219237\n",
      "train loss:0.4076531095355212\n",
      "train loss:0.7316344626530438\n",
      "train loss:0.3061494079559236\n",
      "train loss:0.5095726214926798\n",
      "train loss:0.8339497998308799\n",
      "train loss:0.7181347810055771\n",
      "train loss:0.511819305360645\n",
      "train loss:0.6316470086043388\n",
      "train loss:0.6075841595067353\n",
      "train loss:0.5049335775121193\n",
      "train loss:0.9138955926021722\n",
      "train loss:0.41846615953902494\n",
      "train loss:0.322768748360635\n",
      "train loss:0.8088600678716528\n",
      "train loss:0.6075736810350404\n",
      "train loss:0.5286540833583577\n",
      "train loss:0.4397703459520324\n",
      "train loss:0.506076252907576\n",
      "train loss:0.5074369724767669\n",
      "train loss:0.40681587170353534\n",
      "train loss:0.40046237159244447\n",
      "train loss:0.37334349888798846\n",
      "train loss:0.8861108615282305\n",
      "train loss:0.7586329317029457\n",
      "train loss:0.7626226269119822\n",
      "train loss:0.8608857888883469\n",
      "train loss:0.6125728886304901\n",
      "train loss:0.7301449515773581\n",
      "train loss:0.7011558098470327\n",
      "train loss:0.6104039799027797\n",
      "train loss:0.6860737058406335\n",
      "train loss:0.6830809685809724\n",
      "train loss:0.6743443592961188\n",
      "train loss:0.6730431218364489\n",
      "train loss:0.5818668788188828\n",
      "train loss:0.5828944426576886\n",
      "train loss:0.6435999997177865\n",
      "train loss:0.5868808907572538\n",
      "train loss:0.5861973399456996\n",
      "train loss:0.6283079542935005\n",
      "train loss:0.5816052580786197\n",
      "train loss:0.5215542383313678\n",
      "train loss:0.6731684208727429\n",
      "train loss:0.6748401412423584\n",
      "train loss:0.4877697741364333\n",
      "train loss:0.38967213782581983\n",
      "train loss:0.5310984176800966\n",
      "train loss:0.5176436274357739\n",
      "train loss:0.5041579152569641\n",
      "train loss:0.8437606618409909\n",
      "train loss:0.39695889590568206\n",
      "train loss:0.5018716927722582\n",
      "train loss:0.8690297523673276\n",
      "train loss:0.500197737712193\n",
      "train loss:0.8764208085016069\n",
      "train loss:0.9866959311575785\n",
      "train loss:0.5983296450898308\n",
      "train loss:0.29360884963173567\n",
      "train loss:0.837586221249923\n",
      "train loss:0.6216496550641505\n",
      "train loss:0.603039536395708\n",
      "train loss:0.7738368618971567\n",
      "train loss:0.45788629234591516\n",
      "train loss:0.6088553118030703\n",
      "train loss:0.5467645109221404\n",
      "train loss:0.6095997585488717\n",
      "train loss:0.4721671284703617\n",
      "train loss:0.6885962093919505\n",
      "train loss:0.5438593259731334\n",
      "train loss:0.686661485053269\n",
      "train loss:0.6790180506015584\n",
      "train loss:0.5534001939293545\n",
      "train loss:0.5327735929178988\n",
      "train loss:0.5423609756077316\n",
      "train loss:0.5304849037137233\n",
      "train loss:0.6122473570998894\n",
      "train loss:0.4276489543317704\n",
      "train loss:0.7958614470188216\n",
      "train loss:0.8814184071601355\n",
      "train loss:0.6079599167970258\n",
      "train loss:0.8141629074985073\n",
      "train loss:0.689739281248993\n",
      "train loss:0.5321942749797595\n",
      "train loss:0.5444168110034331\n",
      "train loss:0.5334514035403005\n",
      "train loss:0.6089131391758409\n",
      "train loss:0.7561999193093445\n",
      "train loss:0.6170923964489579\n",
      "train loss:0.7510809541292038\n",
      "train loss:0.6162999810244807\n",
      "train loss:0.547033489694542\n",
      "train loss:0.6137946513369433\n",
      "train loss:0.5413303658580755\n",
      "train loss:0.6895636353775532\n",
      "train loss:0.6755126839092667\n",
      "train loss:0.7449741011928998\n",
      "train loss:0.7571047398066072\n",
      "train loss:0.617691988633211\n",
      "train loss:0.621103909770486\n",
      "train loss:0.560092203618107\n",
      "train loss:0.7368954285186856\n",
      "train loss:0.4366175807472589\n",
      "train loss:0.6763750738908747\n",
      "train loss:0.5596819278209846\n",
      "train loss:0.6879777606442081\n",
      "train loss:0.5517164711332494\n",
      "train loss:0.8168298209181966\n",
      "train loss:0.6219967067845227\n",
      "train loss:0.6119681952507297\n",
      "train loss:0.6105886665974494\n",
      "train loss:0.5391655750924456\n",
      "train loss:0.6738397991450837\n",
      "train loss:0.7546672569340037\n",
      "train loss:0.747026374836621\n",
      "train loss:0.681112330553066\n",
      "train loss:0.5440489339411297\n",
      "train loss:0.6129121844108761\n",
      "train loss:0.542652886406438\n",
      "train loss:0.6782602550666464\n",
      "train loss:0.6762236874211754\n",
      "train loss:0.8164828963387395\n",
      "train loss:0.6809292345412806\n",
      "train loss:0.6705795756701189\n",
      "train loss:0.6166664549937398\n",
      "train loss:0.7917631745906661\n",
      "train loss:0.5609692625365414\n",
      "train loss:0.6721417753258343\n",
      "train loss:0.5137004547171041\n",
      "train loss:0.7746518625227115\n",
      "train loss:0.4601014300831355\n",
      "train loss:0.610075095131428\n",
      "train loss:0.5021281425314996\n",
      "train loss:0.684552380345363\n",
      "train loss:0.6136019850403596\n",
      "train loss:0.6051128372381497\n",
      "train loss:0.6869890319643737\n",
      "train loss:0.5267977487306618\n",
      "train loss:0.631565105974667\n",
      "train loss:0.4450799591343878\n",
      "train loss:0.6068510602171857\n",
      "train loss:0.42044463150135264\n",
      "train loss:0.408635051019658\n",
      "train loss:0.5159337041887566\n",
      "train loss:0.750449357370907\n",
      "train loss:0.9891720651957916\n",
      "train loss:0.6145332782634231\n",
      "train loss:0.6104153518190754\n",
      "train loss:0.5042406741845595\n",
      "train loss:0.6249471304553363\n",
      "train loss:0.6235791868446613\n",
      "train loss:0.7237958977365566\n",
      "train loss:0.5089722819850964\n",
      "train loss:0.8030266911711099\n",
      "train loss:0.52406431828613\n",
      "train loss:0.5342565942418278\n",
      "train loss:0.3591485735933003\n",
      "train loss:0.5203945226129519\n",
      "train loss:0.4291821812474358\n",
      "train loss:0.788196558528018\n",
      "train loss:0.6042799554544236\n",
      "train loss:0.42726291020875823\n",
      "train loss:0.6050813549789101\n",
      "train loss:0.7452433151608652\n",
      "train loss:0.6258494634872763\n",
      "train loss:0.7094012443951836\n",
      "train loss:0.5184941648580659\n",
      "train loss:0.7936975056026646\n",
      "train loss:0.695318551233662\n",
      "train loss:0.353712683816228\n",
      "train loss:0.7766744713559656\n",
      "train loss:0.6194591659204014\n",
      "train loss:0.6133786849073123\n",
      "train loss:0.8300387920990433\n",
      "train loss:0.47351335733045274\n",
      "train loss:0.4728583763227344\n",
      "train loss:0.46734450082083834\n",
      "train loss:0.4603989138199349\n",
      "train loss:0.4448510574648489\n",
      "train loss:0.6113758069073901\n",
      "train loss:0.5162370581189226\n",
      "train loss:0.6126838886441348\n",
      "train loss:0.8175019194390662\n",
      "train loss:0.5094945480741496\n",
      "train loss:0.6135960660055402\n",
      "train loss:0.5087360479792579\n",
      "train loss:0.829167820772127\n",
      "train loss:0.514263051208502\n",
      "train loss:0.7241944107837722\n",
      "train loss:0.818712180409084\n",
      "train loss:0.604803947115701\n",
      "train loss:0.5275848346707994\n",
      "train loss:0.7036723488131946\n",
      "train loss:0.8491276907684453\n",
      "train loss:0.676489835808509\n",
      "train loss:0.5442369439516662\n",
      "train loss:0.5505011457142011\n",
      "train loss:0.48848447216119456\n",
      "train loss:0.615332564542635\n",
      "train loss:0.6194876007810747\n",
      "train loss:0.6278924329052301\n",
      "train loss:0.6190722962686179\n",
      "train loss:0.6172754881720321\n",
      "train loss:0.6836736278121183\n",
      "train loss:0.6818212184346188\n",
      "train loss:0.6152887061964247\n",
      "train loss:0.6143964783920683\n",
      "train loss:0.548184798639439\n",
      "train loss:0.6116971823773821\n",
      "train loss:0.6883664223168731\n",
      "train loss:0.4699106652963702\n",
      "train loss:0.7562588224780419\n",
      "train loss:0.5403943562632743\n",
      "train loss:0.5336794000458572\n",
      "train loss:0.6045543342495286\n",
      "train loss:0.6118372431988249\n",
      "train loss:0.5227477951544252\n",
      "train loss:0.7113151619592107\n",
      "train loss:0.7089638449609471\n",
      "train loss:0.4224119274057264\n",
      "train loss:0.5211389101992296\n",
      "train loss:0.6166779952828684\n",
      "train loss:0.9886433040288545\n",
      "train loss:0.6128355952589838\n",
      "train loss:0.6174608948937816\n",
      "train loss:0.697053310431851\n",
      "train loss:0.4426253238622101\n",
      "train loss:0.439765401360949\n",
      "train loss:0.696939074234074\n",
      "train loss:0.6094342691559911\n",
      "train loss:0.7753771334069881\n",
      "train loss:0.6942069537473522\n",
      "train loss:0.9090388306878019\n",
      "train loss:0.47279474544661004\n",
      "train loss:0.68215167784818\n",
      "train loss:0.48448754421842793\n",
      "train loss:0.6793899651741917\n",
      "train loss:0.7992826918813308\n",
      "train loss:0.5585704974140759\n",
      "train loss:0.503116443354168\n",
      "train loss:0.617265861245029\n",
      "train loss:0.6195772225848486\n",
      "train loss:0.6796741830079253\n",
      "train loss:0.5551012191980542\n",
      "train loss:0.4205112400573457\n",
      "train loss:0.615594262966913\n",
      "train loss:0.4628663157591144\n",
      "train loss:0.533306915763256\n",
      "train loss:0.5202196455245416\n",
      "train loss:0.6059668756188759\n",
      "train loss:0.6150064292520505\n",
      "train loss:0.39334692869871424\n",
      "train loss:0.6187924959673743\n",
      "train loss:0.3689920472654136\n",
      "train loss:0.7723688830011812\n",
      "train loss:0.36491790664818613\n",
      "train loss:0.7828760049983586\n",
      "train loss:0.6318455701404498\n",
      "train loss:0.7616592041682793\n",
      "train loss:0.4895794512292019\n",
      "train loss:0.37189703358974213\n",
      "train loss:0.6319251981690905\n",
      "train loss:0.36303858309322123\n",
      "train loss:0.6267320856366472\n",
      "train loss:0.7455078168235163\n",
      "train loss:0.7290261924369452\n",
      "train loss:0.5118986412370852\n",
      "train loss:0.6235704054329588\n",
      "train loss:0.8045982706948298\n",
      "train loss:0.6069464573109935\n",
      "train loss:0.6978802101677433\n",
      "train loss:0.6176147852553263\n",
      "train loss:0.6153917587202542\n",
      "train loss:0.7436501360338142\n",
      "train loss:0.6165958283726712\n",
      "train loss:0.5607704364395706\n",
      "train loss:0.5628603830249468\n",
      "train loss:0.6212412861884857\n",
      "train loss:0.5072973914178659\n",
      "train loss:0.7315807330761696\n",
      "train loss:0.5615749040815675\n",
      "train loss:0.49823838201296755\n",
      "train loss:0.7384902798425882\n",
      "train loss:0.6807930939998106\n",
      "train loss:0.675482168832416\n",
      "train loss:0.6172777890883843\n",
      "train loss:0.6789885839035799\n",
      "train loss:0.6792172063936432\n",
      "train loss:0.6779677421783917\n",
      "train loss:0.619406891234688\n",
      "train loss:0.6814433184763794\n",
      "train loss:0.799022083795911\n",
      "train loss:0.7336943816949711\n",
      "train loss:0.6742291644276551\n",
      "train loss:0.7283247690622396\n",
      "train loss:0.6737000215171705\n",
      "train loss:0.7195035360333188\n",
      "train loss:0.6747672640886648\n",
      "train loss:0.5889432527812376\n",
      "train loss:0.5443988421287979\n",
      "train loss:0.7151457157814318\n",
      "train loss:0.541234921583355\n",
      "train loss:0.7125622854251255\n",
      "train loss:0.5829756229101088\n",
      "train loss:0.621973005042524\n",
      "train loss:0.5738224290042975\n",
      "train loss:0.7297035215388606\n",
      "train loss:0.6730756473132887\n",
      "train loss:0.5669232326426503\n",
      "train loss:0.5579205721472192\n",
      "train loss:0.6151191440062844\n",
      "train loss:0.48053874862118445\n",
      "train loss:0.5397770360115314\n",
      "train loss:0.45197580615973054\n",
      "train loss:0.8637515836545526\n",
      "train loss:0.5238587246498831\n",
      "train loss:0.6123842915349954\n",
      "train loss:0.6115993575956624\n",
      "train loss:0.7153135053827688\n",
      "train loss:0.7162686828142727\n",
      "train loss:0.8940020937817451\n",
      "train loss:0.7008102704464786\n",
      "train loss:0.5234234027035787\n",
      "train loss:0.4401386945431958\n",
      "train loss:0.7714273529522555\n",
      "train loss:0.6114694032522262\n",
      "train loss:0.5279692470718643\n",
      "train loss:0.5320072591889261\n",
      "train loss:0.5332018736735423\n",
      "train loss:0.5279789135812669\n",
      "train loss:0.6096088845797702\n",
      "train loss:0.3456710947069654\n",
      "train loss:0.6120493943259001\n",
      "train loss:0.41436708009041984\n",
      "train loss:0.40213773913630896\n",
      "train loss:0.7407865071178711\n",
      "train loss:0.7358713793425576\n",
      "train loss:0.6314927043202415\n",
      "train loss:0.4047706280933624\n",
      "train loss:0.5080555677093024\n",
      "train loss:0.49867690031122464\n",
      "train loss:0.3633185296880065\n",
      "train loss:0.8950914044075333\n",
      "train loss:0.8742949247227095\n",
      "train loss:0.7293566511757538\n",
      "train loss:0.8424222093840626\n",
      "train loss:0.796958133518207\n",
      "train loss:0.6855598960300339\n",
      "train loss:0.46348226861306613\n",
      "train loss:0.5386756463124763\n",
      "train loss:0.5519835529680021\n",
      "train loss:0.675888328207741\n",
      "train loss:0.7345200936873758\n",
      "train loss:0.5727669076055639\n",
      "train loss:0.5164757827092855\n",
      "train loss:0.7736319175789527\n",
      "train loss:0.5722770961251202\n",
      "train loss:0.5207809845302657\n",
      "train loss:0.5154227775303346\n",
      "train loss:0.6164372303281763\n",
      "train loss:0.7360220288286758\n",
      "train loss:0.7962212478596706\n",
      "train loss:0.6172224481489239\n",
      "train loss:0.7368353635543714\n",
      "train loss:0.6779909156186359\n",
      "train loss:0.67797621457888\n",
      "train loss:0.6200886542275061\n",
      "train loss:0.7275718924213495\n",
      "train loss:0.6709509227621658\n",
      "train loss:0.5710144070546728\n",
      "train loss:0.6097200734800652\n",
      "train loss:0.6179291218011946\n",
      "train loss:0.6185044751918556\n",
      "train loss:0.5009328263305116\n",
      "train loss:0.6805336455297109\n",
      "train loss:0.7421578091418997\n",
      "train loss:0.6744832585735094\n",
      "train loss:0.6176008870577008\n",
      "train loss:0.6818984008990822\n",
      "train loss:0.615392204548135\n",
      "train loss:0.6155039817335245\n",
      "train loss:0.6736579341902409\n",
      "train loss:0.4747592832235177\n",
      "train loss:0.6099633370797238\n",
      "train loss:0.75725790667513\n",
      "train loss:0.7555176976911786\n",
      "train loss:0.5410343584195753\n",
      "train loss:0.537469668391747\n",
      "train loss:0.611617298741659\n",
      "train loss:0.5304578146036586\n",
      "train loss:0.6110165716826751\n",
      "train loss:0.4373660311429705\n",
      "train loss:0.5135080070184197\n",
      "train loss:0.6084554067498213\n",
      "train loss:0.7087058203083563\n",
      "train loss:0.7160829418192349\n",
      "train loss:0.40769198046415245\n",
      "train loss:0.7950976616102615\n",
      "train loss:0.4062080336808339\n",
      "train loss:0.8162651042080299\n",
      "train loss:0.7063989545080338\n",
      "train loss:0.7916638830353615\n",
      "train loss:0.5184079742248975\n",
      "train loss:0.6957208236568242\n",
      "train loss:0.609707107787375\n",
      "train loss:0.6089603587200318\n",
      "train loss:0.6122826869068063\n",
      "train loss:0.6865405884950307\n",
      "train loss:0.6120278670903583\n",
      "train loss:0.7520378052708997\n",
      "train loss:0.4807618222368507\n",
      "train loss:0.5501855367880779\n",
      "train loss:0.6141447429669927\n",
      "train loss:0.47846233170711655\n",
      "train loss:0.5433939011822161\n",
      "train loss:0.6836346780017216\n",
      "train loss:0.6061882536573434\n",
      "train loss:0.45778620509487594\n",
      "train loss:0.5243265819351032\n",
      "train loss:0.8531583317712366\n",
      "train loss:0.4393388244856141\n",
      "train loss:0.6071096434466494\n",
      "train loss:1.0416047292254964\n",
      "train loss:0.5273617833255477\n",
      "train loss:0.6081834454571629\n",
      "train loss:0.7732814481917261\n",
      "train loss:0.6041956209558031\n",
      "train loss:0.6095981865937169\n",
      "train loss:0.6044323122648644\n",
      "train loss:0.688925918264119\n",
      "train loss:0.6137452040905081\n",
      "train loss:0.6132327701446876\n",
      "train loss:0.7421395840450262\n",
      "train loss:0.6690777884840515\n",
      "train loss:0.48491992871130557\n",
      "train loss:0.7421949558861924\n",
      "train loss:0.5560077552180421\n",
      "train loss:0.6798775526999858\n",
      "train loss:0.5531124462111505\n",
      "train loss:0.42357825249632297\n",
      "train loss:0.7468055133444428\n",
      "train loss:0.47164738288635233\n",
      "train loss:0.6813631214559914\n",
      "train loss:0.5361929293395346\n",
      "train loss:0.9055511499577775\n",
      "train loss:0.5986351692241538\n",
      "train loss:0.5348026532419562\n",
      "train loss:0.5352518527784867\n",
      "train loss:0.6051698591112802\n",
      "train loss:0.6869656049254116\n",
      "train loss:0.36474497486713037\n",
      "train loss:0.43484364046187396\n",
      "train loss:0.7062412357481734\n",
      "train loss:0.6122506349145643\n",
      "train loss:0.6111633304917158\n",
      "train loss:0.70866700357754\n",
      "train loss:0.705316332926594\n",
      "train loss:0.40737301563459327\n",
      "train loss:0.8857434780834005\n",
      "train loss:0.516691069173859\n",
      "train loss:0.6071688951413207\n",
      "train loss:0.6042391002165146\n",
      "train loss:0.7052094548764327\n",
      "train loss:0.6983418863226386\n",
      "train loss:0.7758644104553444\n",
      "train loss:0.6858241432741163\n",
      "train loss:0.8862309232518996\n",
      "train loss:0.6174634401481413\n",
      "train loss:0.6653278388205764\n",
      "train loss:0.626181254863143\n",
      "train loss:0.5231890579845551\n",
      "train loss:0.6197204666014664\n",
      "train loss:0.6244593527699224\n",
      "train loss:0.4793832248061027\n",
      "train loss:0.6197800243801956\n",
      "train loss:0.5637329031283959\n",
      "train loss:0.5583379218795292\n",
      "train loss:0.5539621122993269\n",
      "train loss:0.6851921813635277\n",
      "train loss:0.6782147317517981\n",
      "train loss:0.5447829145951575\n",
      "train loss:0.61158486116493\n",
      "train loss:0.6838259961028583\n",
      "train loss:0.7602139270017376\n",
      "train loss:0.6073319080981818\n",
      "train loss:0.6859268461314952\n",
      "train loss:0.6164239116453211\n",
      "train loss:0.5349085238533732\n",
      "train loss:0.6910171388497267\n",
      "train loss:0.4339106185525794\n",
      "train loss:0.6947916225032603\n",
      "train loss:0.4407502947740535\n",
      "train loss:0.6137034837433608\n",
      "train loss:0.4280757877157838\n",
      "train loss:0.6018741534977328\n",
      "train loss:0.6151547804286277\n",
      "train loss:0.5072134838597108\n",
      "train loss:0.6158428934562444\n",
      "train loss:0.3852189903933111\n",
      "train loss:0.5034392620957022\n",
      "train loss:0.6197534076266107\n",
      "train loss:0.48576675841299044\n",
      "train loss:0.6098812911259119\n",
      "train loss:0.3633106328705545\n",
      "train loss:0.6377950281716649\n",
      "train loss:1.0176840585779063\n",
      "train loss:0.37283613394158166\n",
      "train loss:0.6318612630223368\n",
      "train loss:0.5088311800244922\n",
      "train loss:0.5034116584215509\n",
      "train loss:0.6196832317827285\n",
      "train loss:0.7157168781747982\n",
      "train loss:0.5103189278088189\n",
      "train loss:0.6171463008748991\n",
      "train loss:0.6106148632201617\n",
      "train loss:0.41490354435594246\n",
      "train loss:0.4133542926289791\n",
      "train loss:0.6033095448469024\n",
      "train loss:0.887346092154943\n",
      "train loss:0.7031959323644743\n",
      "train loss:0.788327834385704\n",
      "train loss:0.6102518260190968\n",
      "train loss:0.44858667348510606\n",
      "train loss:0.6885417855630402\n",
      "train loss:0.7437702178794723\n",
      "train loss:0.5514708405756277\n",
      "train loss:0.6150763358382271\n",
      "train loss:0.6156403010105375\n",
      "train loss:0.6820860676276925\n",
      "train loss:0.6834704281267255\n",
      "train loss:0.6151552673251695\n",
      "train loss:0.4981592518550545\n",
      "train loss:0.6026835640734141\n",
      "train loss:0.5589158559296498\n",
      "train loss:0.6644092657428051\n",
      "train loss:0.6249521440746655\n",
      "train loss:0.7395938765151298\n",
      "train loss:0.47735115606321654\n",
      "train loss:0.5584574583570917\n",
      "train loss:0.6240075951015152\n",
      "train loss:0.5392560049091022\n",
      "train loss:0.6065936175756048\n",
      "train loss:0.5792237027179599\n",
      "train loss:0.6065582452302397\n",
      "train loss:0.7055476268739045\n",
      "train loss:0.6782090701193825\n",
      "train loss:0.5982063606921413\n",
      "train loss:0.42303769571717814\n",
      "train loss:0.6129083869722887\n",
      "train loss:0.7041417385599458\n",
      "train loss:0.5116548386103427\n",
      "train loss:0.4038828156093287\n",
      "train loss:0.7086788815727462\n",
      "train loss:0.8030802626834254\n",
      "train loss:0.505536337101345\n",
      "train loss:0.6080793063644125\n",
      "train loss:0.6081679252637255\n",
      "train loss:0.5226320122013813\n",
      "train loss:0.7116871471317234\n",
      "train loss:0.5894290499395718\n",
      "train loss:0.6112347859712396\n",
      "train loss:0.6155388894335971\n",
      "train loss:0.6846032434833196\n",
      "train loss:0.6996669200697545\n",
      "train loss:0.5312362230210537\n",
      "train loss:0.44342091449644405\n",
      "train loss:0.6750909255564757\n",
      "train loss:0.5292905102881356\n",
      "train loss:0.516187204793865\n",
      "train loss:0.7603890654592662\n",
      "train loss:0.5114341521999612\n",
      "train loss:0.5909864036739314\n",
      "train loss:0.5177948733338645\n",
      "train loss:0.5973865595189518\n",
      "train loss:0.766557128563329\n",
      "train loss:0.7679605488995753\n",
      "train loss:0.6220454058165458\n",
      "train loss:0.7489504841233044\n",
      "train loss:0.6069580032928699\n",
      "train loss:0.6645255831578353\n",
      "train loss:0.6206089707162231\n",
      "train loss:0.6274258074264942\n",
      "train loss:0.6138289998938988\n",
      "train loss:0.4196498973611299\n",
      "train loss:0.6804421486237469\n",
      "train loss:0.7491955198685247\n",
      "train loss:0.5270514021897578\n",
      "train loss:0.4803965177168215\n",
      "train loss:0.38395660908348245\n",
      "train loss:0.45875544099310905\n",
      "train loss:0.6094773214798583\n",
      "train loss:0.6069401016256213\n",
      "train loss:0.3821553042868754\n",
      "train loss:0.3909071742538092\n",
      "train loss:0.8449448322734726\n",
      "train loss:0.6259433076292884\n",
      "train loss:0.7505449906904748\n",
      "train loss:0.5046432157988381\n",
      "train loss:0.5979530012043696\n",
      "train loss:0.6199302931323754\n",
      "train loss:0.7410803417417633\n",
      "train loss:0.7180442922524993\n",
      "train loss:0.48982115358671513\n",
      "train loss:0.7142363942184168\n",
      "train loss:0.5210614187793312\n",
      "train loss:0.5932944629654955\n",
      "train loss:0.5326614228231303\n",
      "train loss:0.6088687670345065\n",
      "train loss:0.43284241532263784\n",
      "train loss:0.5276987045086508\n",
      "train loss:0.583555495079125\n",
      "train loss:0.43513616232328395\n",
      "train loss:0.7769051852164892\n",
      "train loss:0.5109352503445128\n",
      "train loss:0.6864919421013024\n",
      "train loss:0.6132801281691844\n",
      "train loss:0.503040045063073\n",
      "train loss:0.6095710078367634\n",
      "train loss:0.5960709169025439\n",
      "train loss:0.6193490393440247\n",
      "train loss:0.5177604721020419\n",
      "train loss:0.6905437753967305\n",
      "train loss:0.6100698450194246\n",
      "train loss:0.6656257277107664\n",
      "train loss:0.6984248710874776\n",
      "train loss:0.611783234548622\n",
      "train loss:0.5107951493452966\n",
      "train loss:0.5140367982852381\n",
      "train loss:0.5231348529510339\n",
      "train loss:0.5773544976956229\n",
      "train loss:0.5017036189578916\n",
      "train loss:0.7009159012231977\n",
      "train loss:0.4236460542014416\n",
      "train loss:0.5202398067954788\n",
      "train loss:0.6167082194031919\n",
      "train loss:0.380808944297259\n",
      "train loss:0.5094050732249766\n",
      "train loss:0.7349179699424591\n",
      "train loss:0.8061151013654936\n",
      "train loss:0.546512638303476\n",
      "train loss:0.5098269811456881\n",
      "train loss:0.47590702668448603\n",
      "train loss:0.5117506290633214\n",
      "train loss:0.37755044611439953\n",
      "train loss:0.727681650489346\n",
      "train loss:0.7375064896171698\n",
      "train loss:0.5017485238896612\n",
      "train loss:0.7796280736249358\n",
      "train loss:0.5807560873998536\n",
      "train loss:0.7069415977431767\n",
      "train loss:0.41449562932918227\n",
      "train loss:0.6079570200573242\n",
      "train loss:0.42456210883937723\n",
      "train loss:0.5916551324493128\n",
      "train loss:0.6518198583998533\n",
      "train loss:0.5136199152633809\n",
      "train loss:0.512035548059628\n",
      "train loss:0.6707803203859153\n",
      "train loss:0.6011172585222971\n",
      "train loss:0.5162817806480147\n",
      "train loss:0.40847877687032896\n",
      "train loss:0.5080930519441851\n",
      "train loss:0.503954812054548\n",
      "train loss:0.47054724436575024\n",
      "train loss:0.6134234824203131\n",
      "train loss:0.5768186156321827\n",
      "train loss:0.4950710767080414\n",
      "train loss:0.813581985458641\n",
      "train loss:0.4953621906025002\n",
      "train loss:0.5195181045833375\n",
      "train loss:0.5986497150293145\n",
      "train loss:0.6655120357982522\n",
      "train loss:0.5284087516562662\n",
      "train loss:0.7202818183368404\n",
      "train loss:0.5844720079496015\n",
      "train loss:0.7720194245984551\n",
      "train loss:0.9208006748113821\n",
      "train loss:0.43158035713830056\n",
      "train loss:0.6122261878592722\n",
      "train loss:0.6978974991592786\n",
      "train loss:0.6129860418200799\n",
      "train loss:0.5591060123959214\n",
      "train loss:0.5180536604496057\n",
      "train loss:0.6196059164369077\n",
      "train loss:0.7218747307315406\n",
      "train loss:0.6742061551822113\n",
      "train loss:0.6760509156479941\n",
      "train loss:0.6197458655246149\n",
      "train loss:0.8361605665827764\n",
      "train loss:0.6319961118011594\n",
      "train loss:0.7229930466058144\n",
      "train loss:0.621407277935023\n",
      "train loss:0.5378914180986623\n",
      "train loss:0.6688886600889484\n",
      "train loss:0.6784637706998037\n",
      "train loss:0.582720183837354\n",
      "train loss:0.6359369680414376\n",
      "train loss:0.671400674317538\n",
      "train loss:0.6407526911857041\n",
      "train loss:0.8234850060427231\n",
      "train loss:0.5711956356674943\n",
      "train loss:0.5616507189061932\n",
      "train loss:0.5571120129045275\n",
      "train loss:0.66954892481454\n",
      "train loss:0.5568777078903325\n",
      "train loss:0.6854172387773091\n",
      "train loss:0.6326538663172119\n",
      "train loss:0.6071596271444151\n",
      "train loss:0.43660982310788965\n",
      "train loss:0.6240597908831987\n",
      "train loss:0.512431330342175\n",
      "train loss:0.6250311588337715\n",
      "train loss:0.8335622282430363\n",
      "train loss:0.39859812330720784\n",
      "train loss:0.8213404531503438\n",
      "train loss:0.49978314004651414\n",
      "train loss:0.41660197350572076\n",
      "train loss:0.47631714879714837\n",
      "train loss:0.6172317623150841\n",
      "train loss:0.3770207115190845\n",
      "train loss:0.49209967426244206\n",
      "train loss:0.7267061662993001\n",
      "train loss:0.2523183829447264\n",
      "train loss:0.5009045086269828\n",
      "train loss:0.6367351554285496\n",
      "train loss:0.6078901803162046\n",
      "train loss:0.7737411701549239\n",
      "train loss:0.3727225559978854\n",
      "train loss:0.8565708772380999\n",
      "train loss:0.9261064460592054\n",
      "train loss:0.6441780196429917\n",
      "train loss:0.6099204369884996\n",
      "train loss:0.3509837338275991\n",
      "train loss:0.4541374035047657\n",
      "train loss:0.5949378839356149\n",
      "train loss:0.4975307455794552\n",
      "train loss:0.7915443853107383\n",
      "train loss:0.5362546976848126\n",
      "train loss:0.527877425650273\n",
      "train loss:0.5427376957479447\n",
      "train loss:0.8020539092495806\n",
      "train loss:0.5234608462479633\n",
      "train loss:0.6550710063321066\n",
      "train loss:0.5915044540498078\n",
      "train loss:0.39845290919387627\n",
      "train loss:0.6059433125886124\n",
      "train loss:0.515196140415692\n",
      "train loss:0.4242742035265133\n",
      "train loss:0.41512372374561446\n",
      "train loss:0.5844244704790411\n",
      "train loss:0.6347020655252694\n",
      "train loss:0.7517198247781675\n",
      "train loss:0.633583230686905\n",
      "train loss:0.6272182508275497\n",
      "train loss:0.5904994169286567\n",
      "train loss:0.61220600414603\n",
      "train loss:0.3792536020672621\n",
      "train loss:0.3891889568973268\n",
      "train loss:0.5291228553907182\n",
      "train loss:1.0088594683262766\n",
      "train loss:0.618038960710342\n",
      "train loss:0.9761233536693084\n",
      "train loss:0.6787230969998734\n",
      "train loss:0.6812285791706776\n",
      "train loss:0.4721191344168895\n",
      "train loss:0.5445273758183128\n",
      "train loss:0.5512176545256997\n",
      "train loss:0.594234495194528\n",
      "train loss:0.6593051160854346\n",
      "train loss:0.5063039974794112\n",
      "train loss:0.6789960476042006\n",
      "train loss:0.675811896574969\n",
      "train loss:0.500523299021318\n",
      "train loss:0.7628820644340506\n",
      "train loss:0.6186992626762171\n",
      "train loss:0.6153860676925866\n",
      "train loss:0.6694840698899197\n",
      "train loss:0.4997533443883266\n",
      "train loss:0.5462543074655221\n",
      "train loss:0.44693794625989447\n",
      "train loss:0.6688314374537606\n",
      "train loss:0.5415954667686889\n",
      "train loss:0.5010774047359655\n",
      "train loss:0.6097850709579105\n",
      "train loss:0.8680832518463181\n",
      "train loss:0.5586572591401031\n",
      "train loss:0.8187011397859747\n",
      "train loss:0.43424010809727187\n",
      "train loss:0.617648144211786\n",
      "train loss:0.7231061317805362\n",
      "train loss:0.7056442711826462\n",
      "train loss:0.42269943394201936\n",
      "train loss:0.6689114642835099\n",
      "train loss:0.5828464760989354\n",
      "train loss:0.4358766096434416\n",
      "train loss:0.6110879681957029\n",
      "train loss:0.6194341396840573\n",
      "train loss:0.41705434759259097\n",
      "train loss:0.5710022856831152\n",
      "train loss:0.6180846612213254\n",
      "train loss:0.7668063692587795\n",
      "train loss:0.4050296314087838\n",
      "train loss:0.6830852327106078\n",
      "train loss:0.7329917606116781\n",
      "train loss:0.5193570781289425\n",
      "train loss:0.6816328391163379\n",
      "train loss:0.7080761363307857\n",
      "train loss:0.5352038956795269\n",
      "train loss:0.35691609810107383\n",
      "train loss:0.6220083134700625\n",
      "train loss:0.5147571049037752\n",
      "train loss:0.6091655524512911\n",
      "train loss:0.4306788105284339\n",
      "train loss:0.5244324747730846\n",
      "train loss:0.6221418738387314\n",
      "train loss:0.9138298232567419\n",
      "train loss:0.7292668089021846\n",
      "train loss:0.7180401076101063\n",
      "train loss:0.33772608748176425\n",
      "train loss:0.5197225360485227\n",
      "train loss:0.6142399295910733\n",
      "train loss:0.5130304224689917\n",
      "train loss:0.3711116394114933\n",
      "train loss:0.40819529864597176\n",
      "train loss:0.7516185721101558\n",
      "train loss:0.41231259852958235\n",
      "train loss:0.39263354710227855\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.7026143790849673\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.3, random_state=40)\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43744da3-f359-41ad-8347-a67d233d76f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2995702905129334\n",
      "=== epoch:1, train acc:0.73, test acc:0.68 ===\n",
      "train loss:2.294557820508376\n",
      "train loss:2.2845460869434575\n",
      "train loss:2.2728519994385183\n",
      "train loss:2.2559535094637964\n",
      "train loss:2.2198222527615368\n",
      "train loss:2.1871627068430155\n",
      "train loss:2.1126492830075922\n",
      "train loss:2.050646907537175\n",
      "train loss:1.936180211797102\n",
      "train loss:1.783214368020479\n",
      "train loss:1.6193452161633388\n",
      "train loss:1.5007299264436864\n",
      "train loss:1.2721101760464424\n",
      "train loss:1.0080850869340519\n",
      "train loss:0.8293732626781388\n",
      "train loss:0.6353125785972122\n",
      "train loss:0.6240133162630641\n",
      "train loss:0.5334274638184483\n",
      "train loss:0.6416578689519067\n",
      "train loss:0.5564138955965716\n",
      "train loss:0.5735889558075218\n",
      "train loss:1.2284138757674803\n",
      "train loss:0.5272662745573669\n",
      "train loss:0.7053030672113172\n",
      "train loss:0.9260463276447559\n",
      "train loss:0.6102931459809585\n",
      "train loss:0.6093150455365736\n",
      "train loss:0.5041067090287449\n",
      "train loss:0.5088211940888798\n",
      "train loss:0.664644351272147\n",
      "train loss:0.5320297548585267\n",
      "train loss:0.6298575859644812\n",
      "train loss:0.5228267883548124\n",
      "train loss:0.5647583426795221\n",
      "train loss:0.5207241556613671\n",
      "train loss:0.7113534297244964\n",
      "train loss:0.7055057318975162\n",
      "train loss:0.5275851339373548\n",
      "train loss:0.9257973913876884\n",
      "train loss:0.5103864915973707\n",
      "train loss:0.31815940907568235\n",
      "train loss:0.7111241328986256\n",
      "train loss:0.3840176945066326\n",
      "train loss:0.473719954644516\n",
      "train loss:0.37047833847961154\n",
      "train loss:0.5102339474768749\n",
      "train loss:0.7702538808836017\n",
      "train loss:0.3576145687101321\n",
      "train loss:0.5193268105388543\n",
      "train loss:0.5251333427599304\n",
      "train loss:0.47256180387782687\n",
      "train loss:0.6261044951404279\n",
      "train loss:1.0678159413426593\n",
      "train loss:0.5120743692119412\n",
      "train loss:0.4849455277810992\n",
      "train loss:0.5135915008089011\n",
      "train loss:0.6911515628027377\n",
      "train loss:0.37686148317981727\n",
      "train loss:0.530186636299602\n",
      "train loss:0.9222672394140627\n",
      "train loss:0.6122163565793273\n",
      "train loss:0.6217256260797772\n",
      "train loss:0.5662450590983432\n",
      "train loss:0.6806028298862754\n",
      "train loss:0.5539269817572933\n",
      "train loss:0.36607760380052584\n",
      "train loss:0.3119381828946206\n",
      "train loss:0.6250389664967938\n",
      "train loss:0.5232621046075165\n",
      "train loss:0.6629472225592556\n",
      "train loss:1.1285934131201625\n",
      "train loss:0.6517420682525936\n",
      "train loss:0.6171594812680479\n",
      "train loss:0.4570240648506088\n",
      "train loss:0.49152800843346406\n",
      "train loss:0.8688743345288781\n",
      "train loss:0.39472560477937363\n",
      "train loss:0.39748662208848173\n",
      "train loss:0.6127280776658418\n",
      "train loss:0.7107603325726621\n",
      "train loss:0.6917362191784389\n",
      "train loss:0.44778129056720006\n",
      "train loss:0.7022746708077674\n",
      "train loss:0.8598125310565011\n",
      "train loss:0.5526655876259358\n",
      "train loss:0.5129798636727563\n",
      "train loss:0.455925573768174\n",
      "train loss:0.6210631973419639\n",
      "train loss:0.3666317450073698\n",
      "train loss:0.7978101034791696\n",
      "train loss:0.5056355576793629\n",
      "train loss:0.6410574386486716\n",
      "train loss:0.39302721137850927\n",
      "train loss:0.9305918352125537\n",
      "train loss:0.6215480719024045\n",
      "train loss:0.8281406230108104\n",
      "train loss:0.5894811696339001\n",
      "train loss:0.8713697520944438\n",
      "train loss:0.6019455725361815\n",
      "train loss:0.4648056248712593\n",
      "train loss:0.542525643477996\n",
      "train loss:0.7897929973730202\n",
      "train loss:0.6132261805458168\n",
      "train loss:0.5687775198461434\n",
      "train loss:0.6296248953643214\n",
      "train loss:0.722933388051144\n",
      "train loss:0.5637671634620933\n",
      "train loss:0.5631388590685252\n",
      "train loss:0.557229822923951\n",
      "train loss:0.6248106687447622\n",
      "train loss:0.4291283930291686\n",
      "train loss:0.6141063751320972\n",
      "train loss:0.8826180228450035\n",
      "train loss:0.6939053985551589\n",
      "train loss:0.49563313514891644\n",
      "train loss:0.40323112778115844\n",
      "train loss:0.5096524668814392\n",
      "train loss:0.8375291176594771\n",
      "train loss:0.3945298843715247\n",
      "train loss:0.6252379091011953\n",
      "train loss:0.8795854956125094\n",
      "train loss:0.6756896589934861\n",
      "train loss:0.41678573743055514\n",
      "train loss:0.2791739401917449\n",
      "train loss:0.5219131135192747\n",
      "train loss:0.49058788470097775\n",
      "train loss:0.7382725089065705\n",
      "train loss:0.3773145397948416\n",
      "train loss:0.7281850382977239\n",
      "train loss:0.7361213706942278\n",
      "train loss:0.7492436826995662\n",
      "train loss:0.4830068980947734\n",
      "train loss:0.6682030336553689\n",
      "train loss:0.6424572305088151\n",
      "train loss:0.6023561674181348\n",
      "train loss:0.5618216057892048\n",
      "train loss:0.6150789144361156\n",
      "train loss:0.5483388281508416\n",
      "train loss:0.6151699952303245\n",
      "train loss:0.6176017559801756\n",
      "train loss:0.6120792478309944\n",
      "train loss:0.5387483117390796\n",
      "train loss:0.5354782336533636\n",
      "train loss:0.5903575044303271\n",
      "train loss:0.520872597135631\n",
      "train loss:0.3007118749198052\n",
      "train loss:0.708593670718923\n",
      "train loss:0.5031367352118685\n",
      "train loss:0.38946855012917087\n",
      "train loss:0.47784181714886464\n",
      "train loss:0.5313182829260681\n",
      "train loss:0.7798952365888432\n",
      "train loss:0.8426815442213634\n",
      "train loss:0.7635661528472195\n",
      "train loss:0.4976338728865482\n",
      "train loss:0.5138684167498184\n",
      "train loss:0.3734827913571226\n",
      "train loss:0.7426720291396045\n",
      "train loss:0.655656305372811\n",
      "train loss:0.8135165140645737\n",
      "train loss:0.6132402555418156\n",
      "train loss:0.7677382139501006\n",
      "train loss:0.6226881898426102\n",
      "train loss:0.6365799358728903\n",
      "train loss:0.5880381886120196\n",
      "train loss:0.7035524498615146\n",
      "train loss:0.5702828758624043\n",
      "train loss:0.6032763577885866\n",
      "train loss:0.6417609221844933\n",
      "train loss:0.6535725593094949\n",
      "train loss:0.5696220980153986\n",
      "train loss:0.5596677521475755\n",
      "train loss:0.623951612191415\n",
      "train loss:0.5863351459385095\n",
      "train loss:0.6829030021962075\n",
      "train loss:0.6905454577809752\n",
      "train loss:0.7241462894131414\n",
      "train loss:0.42295301757779996\n",
      "train loss:0.7231436055452919\n",
      "train loss:0.7278377911572241\n",
      "train loss:0.8037871810198697\n",
      "train loss:0.5334364509243712\n",
      "train loss:0.6346192501710954\n",
      "train loss:0.6075232736681533\n",
      "train loss:0.6971115501173589\n",
      "train loss:0.6366552886215783\n",
      "train loss:0.6751484002383623\n",
      "train loss:0.5505080081476019\n",
      "train loss:0.49444662104878834\n",
      "train loss:0.48135661695511855\n",
      "train loss:0.5280689581562286\n",
      "train loss:0.7030739888798451\n",
      "train loss:0.5435700896336495\n",
      "train loss:0.4479676315259245\n",
      "train loss:0.8284356943432816\n",
      "train loss:0.8179772310120447\n",
      "train loss:0.5352842779441145\n",
      "train loss:0.6165623241271321\n",
      "train loss:0.6103081326470836\n",
      "train loss:0.5162090965522566\n",
      "train loss:0.7783961732801684\n",
      "train loss:0.8872215091604106\n",
      "train loss:0.5303858741624212\n",
      "train loss:0.45904284647686533\n",
      "train loss:0.5202155725236974\n",
      "train loss:0.5275573693085014\n",
      "train loss:0.6160350430464112\n",
      "train loss:0.6165044914814741\n",
      "train loss:0.7834276202435341\n",
      "train loss:0.6915894930073515\n",
      "train loss:0.5359705181782551\n",
      "train loss:0.692685682961119\n",
      "train loss:0.7110202310720769\n",
      "train loss:0.4584977822238819\n",
      "train loss:0.6103903110496811\n",
      "train loss:0.5277830713640118\n",
      "train loss:0.45254292139220276\n",
      "train loss:0.5215255263086608\n",
      "train loss:0.5078868349842749\n",
      "train loss:0.504949986184478\n",
      "train loss:0.7272429926569186\n",
      "train loss:0.6529909411707724\n",
      "train loss:0.5283497400730186\n",
      "train loss:0.6098431296633801\n",
      "train loss:0.3883009725337091\n",
      "train loss:0.6322070811958622\n",
      "train loss:0.6447835122959708\n",
      "train loss:0.3827242559988064\n",
      "train loss:0.6847178206193054\n",
      "train loss:0.7874333694092863\n",
      "train loss:0.40006954995935207\n",
      "train loss:0.721470206521536\n",
      "train loss:0.9028751288557885\n",
      "train loss:0.707853411560548\n",
      "train loss:0.47676042945673\n",
      "train loss:0.6269555655285959\n",
      "train loss:0.6163620461942754\n",
      "train loss:0.5707173549991684\n",
      "train loss:0.622584011830797\n",
      "train loss:0.5711342496774006\n",
      "train loss:0.6807388823827937\n",
      "train loss:0.6782518663424216\n",
      "train loss:0.8236763051045657\n",
      "train loss:0.7903544699477844\n",
      "train loss:0.6077892497102688\n",
      "train loss:0.5580248114744077\n",
      "train loss:0.7111524114797255\n",
      "train loss:0.603239801542484\n",
      "train loss:0.5966269825393581\n",
      "train loss:0.6258866171619035\n",
      "train loss:0.5109014724448768\n",
      "train loss:0.5426939117953183\n",
      "train loss:0.6773785687751721\n",
      "train loss:0.6699318748755362\n",
      "train loss:0.512862223402912\n",
      "train loss:0.6168004896992489\n",
      "train loss:0.5033232606797899\n",
      "train loss:0.4653041930294327\n",
      "train loss:0.47884950035151375\n",
      "train loss:0.6911930980263111\n",
      "train loss:1.0460006954876644\n",
      "train loss:0.5128532322524959\n",
      "train loss:0.6278164988613911\n",
      "train loss:0.7541576598768577\n",
      "train loss:0.6260576960123239\n",
      "train loss:0.7938347310016314\n",
      "train loss:0.6292215217457551\n",
      "train loss:0.5425724374885885\n",
      "train loss:0.4593354551515493\n",
      "train loss:0.6884621384185337\n",
      "train loss:0.6854551386463463\n",
      "train loss:0.843614304869666\n",
      "train loss:0.583787122495777\n",
      "train loss:0.6731158437347845\n",
      "train loss:0.6421143628820818\n",
      "train loss:0.5986702391997241\n",
      "train loss:0.5909056192873454\n",
      "train loss:0.5921066196181353\n",
      "train loss:0.6230594772778592\n",
      "train loss:0.5033509653880193\n",
      "train loss:0.6897922066162888\n",
      "train loss:0.6342964321087609\n",
      "train loss:0.7578596589081406\n",
      "train loss:0.6174429397267349\n",
      "train loss:0.4373720349406704\n",
      "train loss:0.5276195705189355\n",
      "train loss:0.5023052308813454\n",
      "train loss:0.28864814767683455\n",
      "train loss:0.6278183943614936\n",
      "train loss:0.40008652727097144\n",
      "train loss:0.7769636952846619\n",
      "train loss:0.6687696456775005\n",
      "train loss:0.6844196852952538\n",
      "train loss:0.3497226834507878\n",
      "train loss:0.4981988621058154\n",
      "train loss:0.3314704285910572\n",
      "train loss:0.7968940453540704\n",
      "train loss:0.17278527963572116\n",
      "train loss:1.248372247335066\n",
      "train loss:0.7599452515370644\n",
      "train loss:0.6021156344778481\n",
      "train loss:0.2919207195221057\n",
      "train loss:0.7020298862203671\n",
      "train loss:0.5265755831496205\n",
      "train loss:0.3256094159892974\n",
      "train loss:0.4408884390808222\n",
      "train loss:0.7208033498901019\n",
      "train loss:0.4230724971447697\n",
      "train loss:0.40386156231810644\n",
      "train loss:0.3871123348636428\n",
      "train loss:0.4911497770278757\n",
      "train loss:0.6267022556759796\n",
      "train loss:0.6196350298941098\n",
      "train loss:0.501526950944015\n",
      "train loss:0.671206493424289\n",
      "train loss:0.6660857053080823\n",
      "train loss:0.6396255768538358\n",
      "train loss:0.5055902811682954\n",
      "train loss:0.6123055100408386\n",
      "train loss:0.6099494681036468\n",
      "train loss:0.377529384347416\n",
      "train loss:0.4949869571243413\n",
      "train loss:0.7762199079006196\n",
      "train loss:0.8074684263337246\n",
      "train loss:0.41303637094322837\n",
      "train loss:0.620916844187741\n",
      "train loss:0.632056109109257\n",
      "train loss:0.52503707300388\n",
      "train loss:0.4352308241384105\n",
      "train loss:0.6146257453441061\n",
      "train loss:0.8286582343572222\n",
      "train loss:0.6094793376281518\n",
      "train loss:0.6065480685875464\n",
      "train loss:0.6042842178412406\n",
      "train loss:0.7651676606926316\n",
      "train loss:0.4754702460429173\n",
      "train loss:0.6743328200594111\n",
      "train loss:0.6116916721813096\n",
      "train loss:0.6980343272019486\n",
      "train loss:0.671431900907492\n",
      "train loss:0.6167117447286402\n",
      "train loss:0.6702286648351314\n",
      "train loss:0.5633986959708503\n",
      "train loss:0.6258967780010792\n",
      "train loss:0.7352216861189215\n",
      "train loss:0.6939364834222015\n",
      "train loss:0.668115051750182\n",
      "train loss:0.5672167660857043\n",
      "train loss:0.6766741015780218\n",
      "train loss:0.6690364181415112\n",
      "train loss:0.7276993543084309\n",
      "train loss:0.510182713881409\n",
      "train loss:0.6184583273134379\n",
      "train loss:0.7412297771789753\n",
      "train loss:0.6124909447754214\n",
      "train loss:0.6849627934168716\n",
      "train loss:0.6826269614809615\n",
      "train loss:0.6928079820256812\n",
      "train loss:0.7341101362528198\n",
      "train loss:0.5654837675012322\n",
      "train loss:0.6233736698427452\n",
      "train loss:0.7353997771449997\n",
      "train loss:0.5763086041349503\n",
      "train loss:0.6248021897742415\n",
      "train loss:0.6810494322759393\n",
      "train loss:0.6844451070377435\n",
      "train loss:0.596984911828924\n",
      "train loss:0.7348798848103499\n",
      "train loss:0.542606186875205\n",
      "train loss:0.6945463042905693\n",
      "train loss:0.48048761512334687\n",
      "train loss:0.47291651189948086\n",
      "train loss:0.33593502494957966\n",
      "train loss:0.4193477616705728\n",
      "train loss:0.36732546204305094\n",
      "train loss:0.6438760005860767\n",
      "train loss:0.9570459338592828\n",
      "train loss:0.6696971820815619\n",
      "train loss:1.0741105205081234\n",
      "train loss:0.3547832652299217\n",
      "train loss:0.3656653829047949\n",
      "train loss:0.5069241628315557\n",
      "train loss:0.49425703592831044\n",
      "train loss:0.48817526435498265\n",
      "train loss:0.638196730410511\n",
      "train loss:0.4944091167941699\n",
      "train loss:0.7323222849485516\n",
      "train loss:0.4973207868157005\n",
      "train loss:0.7222726372842452\n",
      "train loss:0.7250692507034329\n",
      "train loss:0.7726759080401774\n",
      "train loss:0.3709051734222875\n",
      "train loss:0.5352943770137781\n",
      "train loss:0.5318516000030866\n",
      "train loss:0.3591906844803753\n",
      "train loss:0.6793536549036565\n",
      "train loss:0.8528470074200658\n",
      "train loss:0.4545370931640883\n",
      "train loss:0.514973427400542\n",
      "train loss:0.34037538893084873\n",
      "train loss:0.7310937377935656\n",
      "train loss:0.6048843466296917\n",
      "train loss:0.6174203696100432\n",
      "train loss:0.5103258540715027\n",
      "train loss:0.600170634665105\n",
      "train loss:0.7501636016146489\n",
      "train loss:0.5067473291934126\n",
      "train loss:0.6152094986666022\n",
      "train loss:0.5150313384511785\n",
      "train loss:0.5011963733804201\n",
      "train loss:0.5057528082497414\n",
      "train loss:0.5886877945960267\n",
      "train loss:0.618485863349053\n",
      "train loss:0.5325355591300244\n",
      "train loss:0.7300527097125258\n",
      "train loss:0.38050416348141286\n",
      "train loss:0.4016645139575007\n",
      "train loss:0.7222133142554371\n",
      "train loss:0.864281384201861\n",
      "train loss:0.6314870953988297\n",
      "train loss:0.6342359740162345\n",
      "train loss:0.4151933331114277\n",
      "train loss:0.41936316648945127\n",
      "train loss:0.7308411674388918\n",
      "train loss:0.7283477763852766\n",
      "train loss:0.5271509152437407\n",
      "train loss:0.7807802510860609\n",
      "train loss:0.4483672093331146\n",
      "train loss:0.5321362882430108\n",
      "train loss:0.6106062943613806\n",
      "train loss:0.6787056606465145\n",
      "train loss:0.5374554534420165\n",
      "train loss:0.5361951575042088\n",
      "train loss:0.6221199657029522\n",
      "train loss:0.6178058711128551\n",
      "train loss:0.6187445599777928\n",
      "train loss:0.5193743990950802\n",
      "train loss:0.6196262764578417\n",
      "train loss:0.7070434153382623\n",
      "train loss:0.42714557054254876\n",
      "train loss:0.7021273656952666\n",
      "train loss:0.4156003983126853\n",
      "train loss:0.8060615475492217\n",
      "train loss:0.8844087483401181\n",
      "train loss:0.7760701075217946\n",
      "train loss:0.7001703752193695\n",
      "train loss:0.4849871882451131\n",
      "train loss:0.6887532448742679\n",
      "train loss:0.6177006648514467\n",
      "train loss:0.5744583177849873\n",
      "train loss:0.6189200116805589\n",
      "train loss:0.6814284642920212\n",
      "train loss:0.7196180698100589\n",
      "train loss:0.7692316894951068\n",
      "train loss:0.627802939129083\n",
      "train loss:0.6397327260351763\n",
      "train loss:0.5916097332637983\n",
      "train loss:0.6332192316182523\n",
      "train loss:0.7560193099973922\n",
      "train loss:0.7139442461993128\n",
      "train loss:0.7105288970975694\n",
      "train loss:0.6285667581689927\n",
      "train loss:0.641164407539889\n",
      "train loss:0.6363513025659403\n",
      "train loss:0.7071297712807552\n",
      "train loss:0.633691271939281\n",
      "train loss:0.5887113882213946\n",
      "train loss:0.6237791159442055\n",
      "train loss:0.5604157582096894\n",
      "train loss:0.6077475440152343\n",
      "train loss:0.4573531785487934\n",
      "train loss:0.6338125278143074\n",
      "train loss:0.7954447657853695\n",
      "train loss:0.5908194698560466\n",
      "train loss:0.7081652595130732\n",
      "train loss:0.3789687051502174\n",
      "train loss:0.6071484165071465\n",
      "train loss:0.3797099240359133\n",
      "train loss:0.8708019309552538\n",
      "train loss:0.6374687275386979\n",
      "train loss:1.0517955302246098\n",
      "train loss:0.6220844683540748\n",
      "train loss:0.609306400169603\n",
      "train loss:0.5270085063898075\n",
      "train loss:0.4649902441880876\n",
      "train loss:0.8927690874908967\n",
      "train loss:0.5569822555778321\n",
      "train loss:0.6874974531185384\n",
      "train loss:0.6227979951571536\n",
      "train loss:0.6725297292691916\n",
      "train loss:0.6669833094168864\n",
      "train loss:0.8296486310059745\n",
      "train loss:0.567371296158058\n",
      "train loss:0.643253433509955\n",
      "train loss:0.6742239146984448\n",
      "train loss:0.6435809087261443\n",
      "train loss:0.5745967771071354\n",
      "train loss:0.596707014168777\n",
      "train loss:0.6339669700131242\n",
      "train loss:0.6281928536825488\n",
      "train loss:0.5627128685873714\n",
      "train loss:0.5482328316371998\n",
      "train loss:0.5184875057802925\n",
      "train loss:0.7002430972986439\n",
      "train loss:0.6968772689983449\n",
      "train loss:0.6338891291935222\n",
      "train loss:0.6093271602737884\n",
      "train loss:0.5323082450599773\n",
      "train loss:0.49998038102785525\n",
      "train loss:0.37379132521063074\n",
      "train loss:0.6390432382088169\n",
      "train loss:0.6471713897945148\n",
      "train loss:0.359083694801515\n",
      "train loss:0.36274469442694823\n",
      "train loss:0.772844934092001\n",
      "train loss:0.6546868410055032\n",
      "train loss:0.5013943250814752\n",
      "train loss:0.7804381244452807\n",
      "train loss:0.8958201884856312\n",
      "train loss:0.5067150642557077\n",
      "train loss:0.6023420926513393\n",
      "train loss:0.38426722430977833\n",
      "train loss:0.7060040560756572\n",
      "train loss:0.5166267875155676\n",
      "train loss:0.512565977603118\n",
      "train loss:0.6872042119559616\n",
      "train loss:0.6153906241116237\n",
      "train loss:0.6938914458613812\n",
      "train loss:0.5381689336724819\n",
      "train loss:0.4588594793691348\n",
      "train loss:0.6289473215570232\n",
      "train loss:0.45402141021523335\n",
      "train loss:0.6027996354742988\n",
      "train loss:0.7168178744071836\n",
      "train loss:0.7090169179845638\n",
      "train loss:0.6139592613456333\n",
      "train loss:0.6037055790858938\n",
      "train loss:0.780642953006871\n",
      "train loss:0.5162786613436765\n",
      "train loss:0.6960836430446259\n",
      "train loss:0.542825936297034\n",
      "train loss:0.6921049087746749\n",
      "train loss:0.6082856996955783\n",
      "train loss:0.5314376484144863\n",
      "train loss:0.4489101799773172\n",
      "train loss:0.8993898654405881\n",
      "train loss:0.6754616159485722\n",
      "train loss:0.8089045393760188\n",
      "train loss:0.7495013024422625\n",
      "train loss:0.5539649206939246\n",
      "train loss:0.6230886326160464\n",
      "train loss:0.6837122106953377\n",
      "train loss:0.624005054567195\n",
      "train loss:0.6248211083080619\n",
      "train loss:0.4705406261824292\n",
      "train loss:0.573923632510426\n",
      "train loss:0.491220820543378\n",
      "train loss:0.5238654762481726\n",
      "train loss:0.6138746973943083\n",
      "train loss:0.7088194966558793\n",
      "train loss:0.7952471542169439\n",
      "train loss:0.40853285404258016\n",
      "train loss:0.5102599092471964\n",
      "train loss:0.6162629857760356\n",
      "train loss:0.620142543512408\n",
      "train loss:0.37255270559171233\n",
      "train loss:0.5934755550595268\n",
      "train loss:0.5043825652724901\n",
      "train loss:0.5071814270186776\n",
      "train loss:0.625113377347174\n",
      "train loss:0.20540275397710248\n",
      "train loss:0.8025917188712999\n",
      "train loss:0.7937628926360298\n",
      "train loss:0.6310459013818337\n",
      "train loss:0.6498119057571572\n",
      "train loss:0.9781064557236183\n",
      "train loss:0.5177348580683011\n",
      "train loss:0.6138065760422732\n",
      "train loss:0.8440045397537489\n",
      "train loss:0.6743542450564378\n",
      "train loss:0.6272219795341847\n",
      "train loss:0.7143878794160775\n",
      "train loss:0.6395489203818714\n",
      "train loss:0.6496859639706534\n",
      "train loss:0.6524012200308384\n",
      "train loss:0.5953169280739226\n",
      "train loss:0.6532237632581361\n",
      "train loss:0.707640646321425\n",
      "train loss:0.6799505432615834\n",
      "train loss:0.6509164877565008\n",
      "train loss:0.5766189434683967\n",
      "train loss:0.6446443144922863\n",
      "train loss:0.576910315398379\n",
      "train loss:0.7304268164817929\n",
      "train loss:0.6734624926012609\n",
      "train loss:0.49041382492666374\n",
      "train loss:0.5455191659661891\n",
      "train loss:0.358023354382521\n",
      "train loss:0.5980683767677555\n",
      "train loss:0.7216592840422111\n",
      "train loss:0.5977610762478979\n",
      "train loss:0.7564599558419566\n",
      "train loss:0.6055034965369355\n",
      "train loss:0.6345232130393722\n",
      "train loss:0.38042316268950016\n",
      "train loss:0.6240830066644649\n",
      "train loss:0.8597262734991254\n",
      "train loss:0.7389255999193696\n",
      "train loss:0.4867572520911671\n",
      "train loss:0.723222577265189\n",
      "train loss:0.5931680833437352\n",
      "train loss:0.3144901580680194\n",
      "train loss:0.6120347035509968\n",
      "train loss:0.5259016682733121\n",
      "train loss:0.42433689109807843\n",
      "train loss:0.7240570015068667\n",
      "train loss:0.7124206550127439\n",
      "train loss:0.6968819634834933\n",
      "train loss:0.5371616176444988\n",
      "train loss:0.5178999768695666\n",
      "train loss:0.7900919660438658\n",
      "train loss:0.5279005079972522\n",
      "train loss:0.5076766327958258\n",
      "train loss:0.5242767036022059\n",
      "train loss:0.43479879330044174\n",
      "train loss:0.5263156110261346\n",
      "train loss:0.5114636097050649\n",
      "train loss:0.7196111874976903\n",
      "train loss:0.39230418386668786\n",
      "train loss:0.7081605268105313\n",
      "train loss:0.5034349935023213\n",
      "train loss:0.6268288654042254\n",
      "train loss:0.49900577194479234\n",
      "train loss:0.48305913208812257\n",
      "train loss:0.5196340907770447\n",
      "train loss:0.5021601555990116\n",
      "train loss:0.6142834671603891\n",
      "train loss:0.6459198160376282\n",
      "train loss:0.6044557379621897\n",
      "train loss:0.37363222133523716\n",
      "train loss:0.5044988438780527\n",
      "train loss:0.49796943434459395\n",
      "train loss:0.49866904183190686\n",
      "train loss:0.754251125869634\n",
      "train loss:0.48394644874720055\n",
      "train loss:0.502908235318911\n",
      "train loss:0.637483876004689\n",
      "train loss:0.7412567018987894\n",
      "train loss:0.6157033267555702\n",
      "train loss:0.4116291921496712\n",
      "train loss:0.4232254242819993\n",
      "train loss:0.41497579468585216\n",
      "train loss:1.0297749231978366\n",
      "train loss:0.5179715099241302\n",
      "train loss:0.792333787569269\n",
      "train loss:0.7804570458124785\n",
      "train loss:0.7513858440218867\n",
      "train loss:0.4744842178077692\n",
      "train loss:0.6251679089203173\n",
      "train loss:0.6194361798965323\n",
      "train loss:0.7265488023396485\n",
      "train loss:0.5806819061222248\n",
      "train loss:0.532039137791891\n",
      "train loss:0.6693356992941042\n",
      "train loss:0.5216173381775031\n",
      "train loss:0.49354573985728944\n",
      "train loss:0.7587177467171726\n",
      "train loss:0.40035814326194163\n",
      "train loss:0.7655126561992935\n",
      "train loss:0.44608911772409676\n",
      "train loss:0.528463068935288\n",
      "train loss:0.3937724563501158\n",
      "train loss:0.8382771407715174\n",
      "train loss:0.36661378894959745\n",
      "train loss:0.7561944742467832\n",
      "train loss:0.6247997705963444\n",
      "train loss:0.8694162529849372\n",
      "train loss:0.72837193080338\n",
      "train loss:0.3999132735945211\n",
      "train loss:0.49803709107480937\n",
      "train loss:0.6123988375805891\n",
      "train loss:0.5165230145268767\n",
      "train loss:0.7150908557802053\n",
      "train loss:0.5066816098112461\n",
      "train loss:0.6079268174908307\n",
      "train loss:0.5101910167445101\n",
      "train loss:0.39440879061987666\n",
      "train loss:0.4013448541905804\n",
      "train loss:0.6226399748290419\n",
      "train loss:0.617117078896109\n",
      "train loss:0.4126224803006327\n",
      "train loss:0.9430344884627072\n",
      "train loss:0.2698886300237741\n",
      "train loss:0.5006726260792747\n",
      "train loss:0.7300144473639879\n",
      "train loss:0.5884431281067185\n",
      "train loss:0.3710485745890979\n",
      "train loss:0.6542081250702998\n",
      "train loss:0.49374261345363807\n",
      "train loss:0.6319211962802428\n",
      "train loss:0.739798990813011\n",
      "train loss:0.7347960183738623\n",
      "train loss:0.8277355378735292\n",
      "train loss:0.7240847299582451\n",
      "train loss:0.6136599678080171\n",
      "train loss:0.8202689975490101\n",
      "train loss:0.5520486189063576\n",
      "train loss:0.5142236538842753\n",
      "train loss:0.5696763791761417\n",
      "train loss:0.6756865955778246\n",
      "train loss:0.625531148541865\n",
      "train loss:0.7240313690696493\n",
      "train loss:0.5825581106662236\n",
      "train loss:0.6320063591652757\n",
      "train loss:0.6798062417544475\n",
      "train loss:0.5694227705704515\n",
      "train loss:0.6335945341829732\n",
      "train loss:0.7547777595789908\n",
      "train loss:0.5049937551499387\n",
      "train loss:0.7337630472989103\n",
      "train loss:0.5522998117846503\n",
      "train loss:0.5607948733281463\n",
      "train loss:0.39019465098601935\n",
      "train loss:0.4271987689471219\n",
      "train loss:0.511053098348078\n",
      "train loss:0.4911083163084797\n",
      "train loss:0.23628756291356354\n",
      "train loss:0.6384329447015296\n",
      "train loss:0.6536698570990254\n",
      "train loss:0.6532542866556674\n",
      "train loss:0.6592109914373304\n",
      "train loss:0.8575619904294818\n",
      "train loss:0.5081118791123039\n",
      "train loss:0.5044187964367579\n",
      "train loss:0.6634008019895139\n",
      "train loss:0.3583230360023525\n",
      "train loss:0.6101709373096073\n",
      "train loss:0.5071784839258242\n",
      "train loss:0.6309483509272902\n",
      "train loss:0.49808871143328026\n",
      "train loss:0.6175974061562028\n",
      "train loss:0.49304644207349424\n",
      "train loss:0.7258322607585147\n",
      "train loss:0.7164117597232122\n",
      "train loss:0.7178654797216495\n",
      "train loss:0.4468394997679959\n",
      "train loss:0.6108358911820834\n",
      "train loss:0.5389146006496407\n",
      "train loss:0.5336669904923994\n",
      "train loss:0.6917029972401718\n",
      "train loss:0.6174626748630481\n",
      "train loss:0.688173848620399\n",
      "train loss:0.6846571212176846\n",
      "train loss:0.5401404236303066\n",
      "train loss:0.6127332682505104\n",
      "train loss:0.6139777481174108\n",
      "train loss:0.6115618072332731\n",
      "train loss:0.6755260183729697\n",
      "train loss:0.5426031889526335\n",
      "train loss:0.6220574575735899\n",
      "train loss:0.6065624107741258\n",
      "train loss:0.4482055037359389\n",
      "train loss:0.6941973564914918\n",
      "train loss:0.6861165315774354\n",
      "train loss:0.5992471570690505\n",
      "train loss:0.8728568646152018\n",
      "train loss:0.7635118032124438\n",
      "train loss:0.6136242169915676\n",
      "train loss:0.6047880403647289\n",
      "train loss:0.5335982748867011\n",
      "train loss:0.5419079163278246\n",
      "train loss:0.5434434597726427\n",
      "train loss:0.46146509836952676\n",
      "train loss:0.6087806181025688\n",
      "train loss:0.5943782311348358\n",
      "train loss:0.42438042803489145\n",
      "train loss:0.4954310807230054\n",
      "train loss:0.6216331476120367\n",
      "train loss:0.8306715111633173\n",
      "train loss:0.4822908827924655\n",
      "train loss:0.5308916675821724\n",
      "train loss:0.7293362285959276\n",
      "train loss:0.6145770307584008\n",
      "train loss:0.6235502559511444\n",
      "train loss:0.3803856339979119\n",
      "train loss:0.4004020372125236\n",
      "train loss:0.6467579415552466\n",
      "train loss:0.9747738247603619\n",
      "train loss:0.5100513658630424\n",
      "train loss:0.40769221190700805\n",
      "train loss:0.601073209175637\n",
      "train loss:0.7166055007446309\n",
      "train loss:0.6008421382819898\n",
      "train loss:0.40431885731748685\n",
      "train loss:0.5165772114877684\n",
      "train loss:0.7158327740328414\n",
      "train loss:0.6542212037107462\n",
      "train loss:0.6089362822176596\n",
      "train loss:0.535023010296034\n",
      "train loss:0.5158612258154431\n",
      "train loss:0.4315904789600401\n",
      "train loss:0.30905372923093444\n",
      "train loss:0.8207227349294225\n",
      "train loss:0.935414831462927\n",
      "train loss:0.6271190623382943\n",
      "train loss:0.4227100588229658\n",
      "train loss:0.42200791258103704\n",
      "train loss:0.720295036519881\n",
      "train loss:0.8022627703494128\n",
      "train loss:0.5255964873659098\n",
      "train loss:0.5156038417371533\n",
      "train loss:0.6927399345452105\n",
      "train loss:0.6924859856478259\n",
      "train loss:0.6715746381388081\n",
      "train loss:0.36245414017276917\n",
      "train loss:0.6205571021358297\n",
      "train loss:0.9036834526879163\n",
      "train loss:0.6723259718781905\n",
      "train loss:0.6723385691078028\n",
      "train loss:0.6127217258007638\n",
      "train loss:0.5599786901470997\n",
      "train loss:0.7282430999828787\n",
      "train loss:0.624907765879797\n",
      "train loss:0.5619544712629156\n",
      "train loss:0.4997748329399506\n",
      "train loss:0.8113197485219669\n",
      "train loss:0.8354126202454355\n",
      "train loss:0.5659083680433691\n",
      "train loss:0.6806117193693155\n",
      "train loss:0.7692958571645598\n",
      "train loss:0.584832567506304\n",
      "train loss:0.7078165828773987\n",
      "train loss:0.5924944338282973\n",
      "train loss:0.6347652842307419\n",
      "train loss:0.6299071607818961\n",
      "train loss:0.6307581103005162\n",
      "train loss:0.7157322189549576\n",
      "train loss:0.7594649219711623\n",
      "train loss:0.6245820442207013\n",
      "train loss:0.5214769128374199\n",
      "train loss:0.6700442055839867\n",
      "train loss:0.619868471493008\n",
      "train loss:0.5462485941556299\n",
      "train loss:0.5359606955028178\n",
      "train loss:0.5545045467554571\n",
      "train loss:0.6009687885097141\n",
      "train loss:0.7768877406941957\n",
      "train loss:0.6941307982343513\n",
      "train loss:0.7340313547309342\n",
      "train loss:0.6000985826728592\n",
      "train loss:0.6056814706347021\n",
      "train loss:0.6325347566105284\n",
      "train loss:0.8569328540296862\n",
      "train loss:0.6181774572823441\n",
      "train loss:0.5274923691170628\n",
      "train loss:0.6658899824574224\n",
      "train loss:0.7383567185457647\n",
      "train loss:0.5944145173211871\n",
      "train loss:0.6197546491559887\n",
      "train loss:0.616727834544263\n",
      "train loss:0.8425144854492415\n",
      "train loss:0.5627332262988066\n",
      "train loss:0.6762001753708876\n",
      "train loss:0.7587623643559823\n",
      "train loss:0.5752690807974667\n",
      "train loss:0.682484777756794\n",
      "train loss:0.5431907674161891\n",
      "train loss:0.7538689444318742\n",
      "train loss:0.5351180144618138\n",
      "train loss:0.7133914051817869\n",
      "train loss:0.5733354333382501\n",
      "train loss:0.6819042816565811\n",
      "train loss:0.6216098584876562\n",
      "train loss:0.6207040308908829\n",
      "train loss:0.5530860396799859\n",
      "train loss:0.4639035821133507\n",
      "train loss:0.6193502429051871\n",
      "train loss:0.5270940068156129\n",
      "train loss:0.7161318342969258\n",
      "train loss:0.621176690308456\n",
      "train loss:0.5275530524600379\n",
      "train loss:0.7337156794061304\n",
      "train loss:1.0031492822058656\n",
      "train loss:0.5951548699973256\n",
      "train loss:0.682444786365257\n",
      "train loss:0.7584686860238813\n",
      "train loss:0.6020737834377495\n",
      "train loss:0.6146778325249387\n",
      "train loss:0.5355955444099457\n",
      "train loss:0.598111681369695\n",
      "train loss:0.621551128949975\n",
      "train loss:0.6146708503363804\n",
      "train loss:0.5401460986447779\n",
      "train loss:0.8027133927047011\n",
      "train loss:0.6842696331026467\n",
      "train loss:0.6806772559396409\n",
      "train loss:0.49973021197892376\n",
      "train loss:0.555884540250951\n",
      "train loss:0.6880925679133933\n",
      "train loss:0.5530067084309521\n",
      "train loss:0.928352224099298\n",
      "train loss:0.6280726485597722\n",
      "train loss:0.6819619798256229\n",
      "train loss:0.615106545074\n",
      "train loss:0.6193815483431366\n",
      "train loss:0.6756682090523828\n",
      "train loss:0.5591237966707635\n",
      "train loss:0.5074677269570753\n",
      "train loss:0.553148770623727\n",
      "train loss:0.6110267157794433\n",
      "train loss:0.3764464349712768\n",
      "train loss:0.3407543411552988\n",
      "train loss:0.8158691410130702\n",
      "train loss:0.7915341868692031\n",
      "train loss:0.49193042946547844\n",
      "train loss:0.7341532094095292\n",
      "train loss:0.7162623461284777\n",
      "train loss:0.6155094630651763\n",
      "train loss:0.6092604844164711\n",
      "train loss:0.5943246965431481\n",
      "train loss:0.5051306822788345\n",
      "train loss:0.49367190429936364\n",
      "train loss:0.7192583964976187\n",
      "train loss:0.8044845460659154\n",
      "train loss:0.31700419116728895\n",
      "train loss:0.7205099422850798\n",
      "train loss:0.7672612288193611\n",
      "train loss:0.3388701080745366\n",
      "train loss:0.7175026677275167\n",
      "train loss:0.43750674430422604\n",
      "train loss:0.420456106272182\n",
      "train loss:0.7983553271158058\n",
      "train loss:0.5233828352781764\n",
      "train loss:0.6837476048161496\n",
      "train loss:0.6007849386030937\n",
      "train loss:0.6101083348480348\n",
      "train loss:0.5834090538676766\n",
      "train loss:0.6037132357838381\n",
      "train loss:0.6152195180084337\n",
      "train loss:0.5867706279527607\n",
      "train loss:0.5271330179086176\n",
      "train loss:0.6929331896407321\n",
      "train loss:0.4109652921965812\n",
      "train loss:0.4115007341742916\n",
      "train loss:0.8844972874678202\n",
      "train loss:0.6115466521037819\n",
      "train loss:0.618436936380499\n",
      "train loss:0.5257504528585468\n",
      "train loss:0.3095701909802398\n",
      "train loss:0.6983255285996463\n",
      "train loss:0.40336749383964826\n",
      "train loss:0.7079960229170965\n",
      "train loss:0.6161116338629357\n",
      "train loss:1.1153777111242573\n",
      "train loss:0.6201272785887084\n",
      "train loss:0.7838330767097017\n",
      "train loss:0.546268379983771\n",
      "train loss:0.45319471325928085\n",
      "train loss:0.617363318796543\n",
      "train loss:0.8180995254789053\n",
      "train loss:0.9020154191930894\n",
      "train loss:0.7314333710308608\n",
      "train loss:0.595989889006722\n",
      "train loss:0.7041668069630168\n",
      "train loss:0.5837484363268282\n",
      "train loss:0.6634507976650853\n",
      "train loss:0.6518279389978263\n",
      "train loss:0.6323005003933214\n",
      "train loss:0.7291899162016804\n",
      "train loss:0.7010986377918837\n",
      "train loss:0.6219781754139168\n",
      "train loss:0.6237401603166612\n",
      "train loss:0.6426042200169894\n",
      "train loss:0.6068110757794429\n",
      "train loss:0.6454298398570695\n",
      "train loss:0.6691562064251334\n",
      "train loss:0.671610520680112\n",
      "train loss:0.7302430643553486\n",
      "train loss:0.6127689698015286\n",
      "train loss:0.681509508847129\n",
      "train loss:0.7004081775479319\n",
      "train loss:0.7397989680149223\n",
      "train loss:0.6858613699212281\n",
      "train loss:0.5507230362099487\n",
      "train loss:0.7891161899472134\n",
      "train loss:0.6809713527857331\n",
      "train loss:0.5481316840007889\n",
      "train loss:0.5576568622764175\n",
      "train loss:0.5667767732659388\n",
      "train loss:0.5569938285308043\n",
      "train loss:0.6175444226467214\n",
      "train loss:0.5356601031863993\n",
      "train loss:0.5309210540140382\n",
      "train loss:0.4982936514999038\n",
      "train loss:0.7175053710058362\n",
      "train loss:0.5132751698978346\n",
      "train loss:0.6284257953648262\n",
      "train loss:0.9573577748931482\n",
      "train loss:0.6041102047973472\n",
      "train loss:0.5063144914974391\n",
      "train loss:0.48630426625816375\n",
      "train loss:0.4956644737634591\n",
      "train loss:0.5250482883795191\n",
      "train loss:0.7656939823039502\n",
      "train loss:0.7118968315742145\n",
      "train loss:0.6198539981677396\n",
      "train loss:0.4092484011717904\n",
      "train loss:0.6092053566092102\n",
      "train loss:0.5236092755636609\n",
      "train loss:0.8101835874879211\n",
      "train loss:0.5982209719664668\n",
      "train loss:0.6832469733971598\n",
      "train loss:0.5202468888318286\n",
      "train loss:0.5219248000375001\n",
      "train loss:0.4480404778429904\n",
      "train loss:0.7638594481690354\n",
      "train loss:0.6079035571169371\n",
      "train loss:0.5393033382375302\n",
      "train loss:0.6960308604053236\n",
      "train loss:0.508469588025471\n",
      "train loss:0.6120665824498202\n",
      "train loss:0.4212618856930542\n",
      "train loss:0.5121527578523788\n",
      "train loss:0.7249427269325615\n",
      "train loss:0.7036169059838274\n",
      "train loss:0.5819172091047575\n",
      "train loss:0.6048798031548006\n",
      "train loss:0.875399816803049\n",
      "train loss:0.6154204487949734\n",
      "train loss:0.5095491088064474\n",
      "train loss:0.6177856358179633\n",
      "train loss:0.7614172930167147\n",
      "train loss:0.6774394663413901\n",
      "train loss:0.4703566584944781\n",
      "train loss:0.5314182095043712\n",
      "train loss:0.6164975085277513\n",
      "train loss:0.6181613007764328\n",
      "train loss:0.7569224474150007\n",
      "train loss:0.6895811207039664\n",
      "train loss:0.5990076889020176\n",
      "train loss:0.6044010180544053\n",
      "train loss:0.6224138761419387\n",
      "train loss:0.456697902909883\n",
      "train loss:0.5461580849525106\n",
      "train loss:0.7815261614452201\n",
      "train loss:0.6168777533530674\n",
      "train loss:0.8339765433725221\n",
      "train loss:0.5483946442929464\n",
      "train loss:0.6195321106761501\n",
      "train loss:0.6016996637586809\n",
      "train loss:0.44129723528704695\n",
      "train loss:0.6947069819692879\n",
      "train loss:0.7541866670718332\n",
      "train loss:0.6811668667130626\n",
      "train loss:0.6028195439776282\n",
      "train loss:0.4537531066723914\n",
      "train loss:0.6035468086344167\n",
      "train loss:0.7443758373978799\n",
      "train loss:0.5186876499962649\n",
      "train loss:0.7545479362579889\n",
      "train loss:0.7768742559601614\n",
      "train loss:0.37898246184744766\n",
      "train loss:0.6706623816790543\n",
      "train loss:0.6778841064483828\n",
      "train loss:0.5970590122846843\n",
      "train loss:0.5186557340505129\n",
      "train loss:0.8580884611967013\n",
      "train loss:0.4898566118894136\n",
      "train loss:0.5188998525491744\n",
      "train loss:0.7539719564215097\n",
      "train loss:0.5199296388689786\n",
      "train loss:0.5129064184461104\n",
      "train loss:0.5199481009133243\n",
      "train loss:0.5095061110482894\n",
      "train loss:0.5102261189575655\n",
      "train loss:0.7135756677643702\n",
      "train loss:0.7267480056229718\n",
      "train loss:0.41080027719359624\n",
      "train loss:0.7177816162384824\n",
      "train loss:0.613600645585019\n",
      "train loss:0.9022101399538002\n",
      "train loss:0.585453560279985\n",
      "train loss:0.49917460353366394\n",
      "train loss:0.769005348092213\n",
      "train loss:0.6012966452151247\n",
      "train loss:0.6924257893934641\n",
      "train loss:0.6056177976877539\n",
      "train loss:0.5495398842662367\n",
      "train loss:0.542115197187902\n",
      "train loss:0.677540134617747\n",
      "train loss:0.4622292031771301\n",
      "train loss:0.7517576650228501\n",
      "train loss:0.6758276743471775\n",
      "train loss:0.8011261686099184\n",
      "train loss:0.6142628245821813\n",
      "train loss:0.5614842413390922\n",
      "train loss:0.5415608467491373\n",
      "train loss:0.4834618259922404\n",
      "train loss:0.4699087546346439\n",
      "train loss:0.5195848635095646\n",
      "train loss:0.6198702552088301\n",
      "train loss:0.5054563178854646\n",
      "train loss:0.6986788615154575\n",
      "train loss:0.8218764792853636\n",
      "train loss:0.6988365086881764\n",
      "train loss:0.7450954816487418\n",
      "train loss:0.6354742768097088\n",
      "train loss:0.527417957265216\n",
      "train loss:0.5885141948116196\n",
      "train loss:0.5189085766915738\n",
      "train loss:0.5170283325481543\n",
      "train loss:0.5950932104505756\n",
      "train loss:0.530312671022679\n",
      "train loss:0.41317029834762264\n",
      "train loss:0.6075103102271174\n",
      "train loss:0.8919431089917926\n",
      "train loss:0.6027334582101578\n",
      "train loss:0.704172529191023\n",
      "train loss:0.6789373263646052\n",
      "train loss:0.5101252132314398\n",
      "train loss:0.7454593340583949\n",
      "train loss:0.6315596031808266\n",
      "train loss:0.6705739718478995\n",
      "train loss:0.5385802981522335\n",
      "train loss:0.7967829438860535\n",
      "train loss:0.7492739598166176\n",
      "train loss:0.6787417166917458\n",
      "train loss:0.6217041344695009\n",
      "train loss:0.65612195258905\n",
      "train loss:0.6736450090721438\n",
      "train loss:0.5871839436110169\n",
      "train loss:0.7213114389540212\n",
      "train loss:0.6475652213208877\n",
      "train loss:0.6386527941450498\n",
      "train loss:0.674243722121193\n",
      "train loss:0.6873538442967723\n",
      "train loss:0.5202804632892987\n",
      "train loss:0.6741827713961818\n",
      "train loss:0.6147744741584652\n",
      "train loss:0.5632006851407686\n",
      "train loss:0.5432368709459768\n",
      "train loss:0.5505878400932065\n",
      "train loss:0.6150727984400987\n",
      "train loss:0.5978339324511999\n",
      "train loss:0.5003542332411428\n",
      "train loss:0.5070557982060554\n",
      "train loss:0.7420519768321908\n",
      "train loss:0.619283780484283\n",
      "train loss:0.8558154304987292\n",
      "train loss:0.7279285693697226\n",
      "train loss:0.7383111964639255\n",
      "train loss:0.8014330954361979\n",
      "train loss:0.5322886174182512\n",
      "train loss:0.4319713652513011\n",
      "train loss:0.43547600666907227\n",
      "train loss:0.5034740977376245\n",
      "train loss:0.5959637253746066\n",
      "train loss:0.5369683454382608\n",
      "train loss:0.5849951783133923\n",
      "train loss:0.772184106417634\n",
      "train loss:0.515000552029176\n",
      "train loss:0.790876733156058\n",
      "train loss:0.5204033273626183\n",
      "train loss:0.5200095978011738\n",
      "train loss:0.7146116195274402\n",
      "train loss:0.8484349420543911\n",
      "train loss:0.6191850114070616\n",
      "train loss:0.6795530813788659\n",
      "train loss:0.5197942614795985\n",
      "train loss:0.6686224603203363\n",
      "train loss:0.6397445653833644\n",
      "train loss:0.6247065528173996\n",
      "train loss:0.6354166308251882\n",
      "train loss:0.6241957177311188\n",
      "train loss:0.5025143177483122\n",
      "train loss:0.54800748288539\n",
      "train loss:0.5368682517450368\n",
      "train loss:0.6374579356681266\n",
      "train loss:0.5130848817661032\n",
      "train loss:0.7634656467209304\n",
      "train loss:0.49086862805447484\n",
      "train loss:0.6166308866013307\n",
      "train loss:0.5010714225285074\n",
      "train loss:0.3881922554221617\n",
      "train loss:0.5162155962170843\n",
      "train loss:0.6120007676546972\n",
      "train loss:0.7281624880141669\n",
      "train loss:0.629271750951481\n",
      "train loss:0.5990232267039646\n",
      "train loss:0.4860675501903994\n",
      "train loss:0.5326714874689722\n",
      "train loss:0.6373581353511595\n",
      "train loss:0.6354461320672729\n",
      "train loss:0.7946454376032274\n",
      "train loss:0.710817935729277\n",
      "train loss:0.3096288805024449\n",
      "train loss:0.7734251869528557\n",
      "train loss:0.4132775876437661\n",
      "train loss:0.5196446175207854\n",
      "train loss:0.7193356015820126\n",
      "train loss:0.33398117536797345\n",
      "train loss:0.41825761168101233\n",
      "train loss:0.5120435802433243\n",
      "train loss:0.6994555483099394\n",
      "train loss:0.40216642066854524\n",
      "train loss:0.3690614706037983\n",
      "train loss:0.5235965482910281\n",
      "train loss:0.6088244139147097\n",
      "train loss:0.507129066125243\n",
      "train loss:0.8076826285281816\n",
      "train loss:0.7556919375101199\n",
      "train loss:0.8967739825085582\n",
      "train loss:0.5978619880101858\n",
      "train loss:0.5840150261869272\n",
      "train loss:0.47665966887496475\n",
      "train loss:0.514570721483574\n",
      "train loss:0.7573696036170451\n",
      "train loss:0.6700728531220302\n",
      "train loss:0.6078540110330944\n",
      "train loss:0.6645294056996623\n",
      "train loss:0.7393321007337487\n",
      "train loss:0.7346630769004631\n",
      "train loss:0.6224359593452329\n",
      "train loss:0.6414164675732751\n",
      "train loss:0.6526262149347903\n",
      "train loss:0.6160921820676684\n",
      "train loss:0.6206671833420038\n",
      "train loss:0.5505781962769802\n",
      "train loss:0.45947940104992213\n",
      "train loss:0.6125228429064782\n",
      "train loss:0.5416444916731754\n",
      "train loss:0.521162047813813\n",
      "train loss:0.628729414865015\n",
      "train loss:0.5953020963492374\n",
      "train loss:0.607896909477036\n",
      "train loss:0.5836331789431684\n",
      "train loss:0.7423175014741984\n",
      "train loss:0.48687820485757494\n",
      "train loss:0.5206989829679918\n",
      "train loss:0.4961718546677272\n",
      "train loss:0.48061102623741087\n",
      "train loss:0.8653558111037359\n",
      "train loss:0.5977512015643822\n",
      "train loss:0.5833284481867231\n",
      "train loss:0.6839634773471162\n",
      "train loss:0.39167780005109043\n",
      "train loss:0.5217488055583229\n",
      "train loss:0.6375247483514352\n",
      "train loss:0.38458924770984415\n",
      "train loss:0.6631091409868675\n",
      "train loss:0.49969231625908234\n",
      "train loss:0.6926118342524555\n",
      "train loss:0.5980393968367592\n",
      "train loss:0.7006780385583893\n",
      "train loss:0.3541918970353162\n",
      "train loss:0.8665672298661107\n",
      "train loss:0.9244887526640666\n",
      "train loss:0.5259758478675358\n",
      "train loss:0.5410377422132047\n",
      "train loss:0.541073186564807\n",
      "train loss:0.619153767155759\n",
      "train loss:0.4650386510820386\n",
      "train loss:0.7060803401316\n",
      "train loss:0.7501477488215318\n",
      "train loss:0.5494626313705031\n",
      "train loss:0.6204347451882011\n",
      "train loss:0.500304889804473\n",
      "train loss:0.5455363941965297\n",
      "train loss:0.6323111380805235\n",
      "train loss:0.579779673846973\n",
      "train loss:0.623277737998075\n",
      "train loss:0.3246480491952307\n",
      "train loss:0.7120475583114846\n",
      "train loss:0.6218351378696926\n",
      "train loss:0.4977674254084188\n",
      "train loss:0.6177091562451855\n",
      "train loss:0.37795023384935433\n",
      "train loss:0.725628291900813\n",
      "train loss:0.8129494189407318\n",
      "train loss:0.7459951494235997\n",
      "train loss:0.44098865202864895\n",
      "train loss:0.5739347277385007\n",
      "train loss:0.5080502874954068\n",
      "train loss:0.6663208882201104\n",
      "train loss:0.4970946789181423\n",
      "train loss:0.9675071823177179\n",
      "train loss:0.5312426818459756\n",
      "train loss:0.6000230534232447\n",
      "train loss:0.5923212840564157\n",
      "train loss:0.6216646891919028\n",
      "train loss:0.4629592387627133\n",
      "train loss:0.5440364541529821\n",
      "train loss:0.4528270849871451\n",
      "train loss:0.6424586292536557\n",
      "train loss:0.6008679444400531\n",
      "train loss:0.5156758325585048\n",
      "train loss:0.48631046440058323\n",
      "train loss:0.6979293193947129\n",
      "train loss:0.6487303997014626\n",
      "train loss:0.4603922052699616\n",
      "train loss:0.8124094103447123\n",
      "train loss:0.8465433428149692\n",
      "train loss:0.6547729258265849\n",
      "train loss:0.6126723641143959\n",
      "train loss:0.5977492663559\n",
      "train loss:0.6275705066588704\n",
      "train loss:0.38085484321025154\n",
      "train loss:0.5194812375160237\n",
      "train loss:0.8249176698318891\n",
      "train loss:0.6723680925293778\n",
      "train loss:0.6533479139605889\n",
      "train loss:0.6774476577050196\n",
      "train loss:0.5588641749998186\n",
      "train loss:0.6760815536804017\n",
      "train loss:0.5554847324947672\n",
      "train loss:0.4803066563312536\n",
      "train loss:0.5876425548524122\n",
      "train loss:0.5294979220939672\n",
      "train loss:0.5777442666189234\n",
      "train loss:0.6912345301051542\n",
      "train loss:0.6885075845436613\n",
      "train loss:0.3585009761641498\n",
      "train loss:0.520465168572368\n",
      "train loss:0.5840116964285851\n",
      "train loss:0.38317764781698516\n",
      "train loss:0.47756115698663476\n",
      "train loss:0.7709966227457141\n",
      "train loss:0.3707384413563726\n",
      "train loss:0.662462885239275\n",
      "train loss:0.4863904949824339\n",
      "train loss:0.9627471474538958\n",
      "train loss:0.9779125137644126\n",
      "train loss:0.3841140474073731\n",
      "train loss:0.6281780856512984\n",
      "train loss:0.6901951384581047\n",
      "train loss:0.5352183713863866\n",
      "train loss:0.6285570651718458\n",
      "train loss:0.5800929320020761\n",
      "train loss:0.6714245069341958\n",
      "train loss:0.6962232135376165\n",
      "train loss:0.6248931665540132\n",
      "train loss:0.4838950459879099\n",
      "train loss:0.47757660135803215\n",
      "train loss:0.6067399085292601\n",
      "train loss:0.6695741585204745\n",
      "train loss:0.6511245218240947\n",
      "train loss:0.5374333570904579\n",
      "train loss:0.5270131056258528\n",
      "train loss:0.48492629684061067\n",
      "train loss:0.7086706517877929\n",
      "train loss:0.5817504102436426\n",
      "train loss:0.517254726057226\n",
      "train loss:0.4262158590782869\n",
      "train loss:0.6008724506251532\n",
      "train loss:0.3873005948588495\n",
      "train loss:0.7818044840288236\n",
      "train loss:0.6047175506606626\n",
      "train loss:0.9205181731576715\n",
      "train loss:0.7143810868538403\n",
      "train loss:0.6060161110890574\n",
      "train loss:0.5480718919903473\n",
      "train loss:0.5985354478692152\n",
      "train loss:0.6656189243777739\n",
      "train loss:0.6036929967460527\n",
      "train loss:0.7754928736405693\n",
      "train loss:0.5219493972754526\n",
      "train loss:0.5877619618793324\n",
      "train loss:0.78252926487418\n",
      "train loss:0.49318153244111684\n",
      "train loss:0.6869891498952094\n",
      "train loss:0.5801323156943805\n",
      "train loss:0.5572719268665752\n",
      "train loss:0.5814559196316891\n",
      "train loss:0.5346923532587222\n",
      "train loss:0.5887961946965733\n",
      "train loss:0.7347458537854609\n",
      "train loss:0.613625049229708\n",
      "train loss:0.5979007401236551\n",
      "train loss:0.8208650551483009\n",
      "train loss:0.5532116646564329\n",
      "train loss:0.5394174232059104\n",
      "train loss:0.625441741706404\n",
      "train loss:0.7068123525491556\n",
      "train loss:0.6600619781864969\n",
      "train loss:0.6621623713889141\n",
      "train loss:0.5129525384929563\n",
      "train loss:0.6458903772482947\n",
      "train loss:0.8583665601348635\n",
      "train loss:0.6049584810469603\n",
      "train loss:0.535291656917883\n",
      "train loss:0.6149964721031793\n",
      "train loss:0.45341539762427774\n",
      "train loss:0.5526489895171605\n",
      "train loss:0.44708930778925204\n",
      "train loss:0.6096659262705308\n",
      "train loss:0.5202902519238253\n",
      "train loss:0.5866466051198829\n",
      "train loss:0.5209335140881597\n",
      "train loss:0.7216431111880244\n",
      "train loss:0.5619254258440478\n",
      "train loss:0.43761398653280337\n",
      "train loss:0.6293182662491593\n",
      "train loss:0.6831112589778476\n",
      "train loss:0.661879487016794\n",
      "train loss:0.6983996717237881\n",
      "train loss:0.9184582839959946\n",
      "train loss:0.5382742429030432\n",
      "train loss:0.6132476116316425\n",
      "train loss:0.5608716725206542\n",
      "train loss:0.6280831793284616\n",
      "train loss:0.5438514919200388\n",
      "train loss:0.4571616772606846\n",
      "train loss:0.5376436679633599\n",
      "train loss:0.689477666007478\n",
      "train loss:0.8397335423344326\n",
      "train loss:0.6315040590601557\n",
      "train loss:0.6452977562932806\n",
      "train loss:0.7175985793742287\n",
      "train loss:0.5721001685422469\n",
      "train loss:0.39997753630350064\n",
      "train loss:0.6829146216902988\n",
      "train loss:0.6855704600263114\n",
      "train loss:0.5661202396491819\n",
      "train loss:0.6670422053365396\n",
      "train loss:0.528458808917406\n",
      "train loss:0.4781276366912836\n",
      "train loss:0.8064893252042387\n",
      "train loss:0.7895087688839455\n",
      "train loss:0.6698045165521342\n",
      "train loss:0.7696147473091177\n",
      "train loss:0.6551052149712181\n",
      "train loss:0.7034429227058288\n",
      "train loss:0.6849960629492635\n",
      "train loss:0.561020570896366\n",
      "train loss:0.8017413442396414\n",
      "train loss:0.6526869354764255\n",
      "train loss:0.6192363942796025\n",
      "train loss:0.5299676767950056\n",
      "train loss:0.5603781336145991\n",
      "train loss:0.6375633065915631\n",
      "train loss:0.5988587159388192\n",
      "train loss:0.6085126022059384\n",
      "train loss:0.47090925482914664\n",
      "train loss:0.6457689152381189\n",
      "train loss:0.5228576827496565\n",
      "train loss:0.39556808736286525\n",
      "train loss:0.6047036748799641\n",
      "train loss:0.49683784758808525\n",
      "train loss:0.5815003150270517\n",
      "train loss:0.5012712125677016\n",
      "train loss:0.4861799834896992\n",
      "train loss:0.4959620307769149\n",
      "train loss:0.9523906881754103\n",
      "train loss:0.7735169135832362\n",
      "train loss:0.7448680599150126\n",
      "train loss:0.39274043565886874\n",
      "train loss:0.879753204202613\n",
      "train loss:0.39140995402822976\n",
      "train loss:0.6181744156263099\n",
      "train loss:0.533451468987937\n",
      "train loss:0.569707784810765\n",
      "train loss:0.6433621005741907\n",
      "train loss:0.6020186586240655\n",
      "train loss:0.6165460496758003\n",
      "train loss:0.5757832452539289\n",
      "train loss:0.62618784036124\n",
      "train loss:0.46004378826551584\n",
      "train loss:0.44161077587944525\n",
      "train loss:0.6289660396853105\n",
      "train loss:0.7803133666796185\n",
      "train loss:0.6162921127831925\n",
      "train loss:0.6642841363161176\n",
      "train loss:0.4280764638578923\n",
      "train loss:0.9303373439360912\n",
      "train loss:0.5225575039228713\n",
      "train loss:0.742174877317306\n",
      "train loss:0.8590142784005457\n",
      "train loss:0.5695979178726397\n",
      "train loss:0.6408838387514394\n",
      "train loss:0.6701244924705367\n",
      "train loss:0.48548215392645205\n",
      "train loss:0.7176834047860812\n",
      "train loss:0.49357924777203444\n",
      "train loss:0.7148886356387847\n",
      "train loss:0.4956577786891626\n",
      "train loss:0.6265348938827264\n",
      "train loss:0.5333940778895344\n",
      "train loss:0.4494629638482575\n",
      "train loss:0.7851196750397401\n",
      "train loss:0.6772316141563254\n",
      "train loss:0.618211315339976\n",
      "train loss:0.7695736575422257\n",
      "train loss:0.3257070746504044\n",
      "train loss:0.5419453697057361\n",
      "train loss:0.8867472151347497\n",
      "train loss:0.7749577168043216\n",
      "train loss:0.6164671278528805\n",
      "train loss:0.5252615734151538\n",
      "train loss:0.45992243983685743\n",
      "train loss:0.4338210638103674\n",
      "train loss:0.822793148296587\n",
      "train loss:0.6089601808672122\n",
      "train loss:0.7251071893585577\n",
      "train loss:0.597903442559014\n",
      "train loss:0.5160659581495275\n",
      "train loss:0.6462830052559468\n",
      "train loss:0.603001711908761\n",
      "train loss:0.6350687189877309\n",
      "train loss:0.6053625963240477\n",
      "train loss:0.6920429513154394\n",
      "train loss:0.521452903021563\n",
      "train loss:0.6128873081715362\n",
      "train loss:0.6853960495263267\n",
      "train loss:0.5297029228437049\n",
      "train loss:0.669052233673959\n",
      "train loss:0.4409310302625413\n",
      "train loss:0.5309508006474145\n",
      "train loss:0.5734024949442831\n",
      "train loss:0.6935765483982127\n",
      "train loss:0.6147898513379599\n",
      "train loss:0.6050821659455397\n",
      "train loss:0.5133107950756433\n",
      "train loss:0.5068975954816292\n",
      "train loss:0.629209168794093\n",
      "train loss:0.6982884584630547\n",
      "train loss:0.6974465709841093\n",
      "train loss:0.6157788716333592\n",
      "train loss:0.6351164108271946\n",
      "train loss:0.41421096178271866\n",
      "train loss:1.0219660086623295\n",
      "train loss:0.540911732825486\n",
      "train loss:0.5404287927858766\n",
      "train loss:0.5863068438248191\n",
      "train loss:0.47319732755747596\n",
      "train loss:0.6426791004494499\n",
      "train loss:0.461050358133445\n",
      "train loss:0.8338057517303724\n",
      "train loss:0.6802209534332753\n",
      "train loss:0.6144988166175057\n",
      "train loss:0.37753680313710775\n",
      "train loss:0.49107641864424434\n",
      "train loss:0.5190753112865424\n",
      "train loss:0.7524711054709927\n",
      "train loss:0.6260597181723073\n",
      "train loss:0.5373369681128108\n",
      "train loss:0.6972008233304864\n",
      "train loss:0.43423884474728214\n",
      "train loss:0.6586743792017714\n",
      "train loss:0.5895251761749895\n",
      "train loss:0.40212065550176657\n",
      "train loss:0.7174372682676734\n",
      "train loss:0.3063667939026874\n",
      "train loss:0.24573520137417235\n",
      "train loss:0.6132240989624019\n",
      "train loss:0.600884589771925\n",
      "train loss:0.49888577338576373\n",
      "train loss:0.6048787556060772\n",
      "train loss:0.8963336155338727\n",
      "train loss:0.44042984665744067\n",
      "train loss:0.7597530486597961\n",
      "train loss:0.515363778435531\n",
      "train loss:0.47781653995215967\n",
      "train loss:0.6060122019364493\n",
      "train loss:0.740112907589079\n",
      "train loss:0.6745953787791772\n",
      "train loss:0.5245893386264785\n",
      "train loss:0.6948076272866952\n",
      "train loss:0.5107957816317216\n",
      "train loss:0.6116450173939275\n",
      "train loss:0.6999699441310081\n",
      "train loss:0.41548663475597475\n",
      "train loss:0.6471511891212633\n",
      "train loss:0.5604622176138099\n",
      "train loss:0.6193769276856139\n",
      "train loss:0.6178977925969608\n",
      "train loss:0.6249716536819994\n",
      "train loss:0.566105763374803\n",
      "train loss:0.672292501132736\n",
      "train loss:0.5538414859598488\n",
      "train loss:0.6607218278054324\n",
      "train loss:0.6964907154956753\n",
      "train loss:0.6162425827093274\n",
      "train loss:0.46474073684750383\n",
      "train loss:0.6319043858393274\n",
      "train loss:0.5798927999946087\n",
      "train loss:0.6940358294586657\n",
      "train loss:0.6651984908279909\n",
      "train loss:0.6250500402054345\n",
      "train loss:0.4285083642622697\n",
      "train loss:0.5119454280328408\n",
      "train loss:0.5393638261713746\n",
      "train loss:0.48017617317135625\n",
      "train loss:0.7758128772011996\n",
      "train loss:0.7085199999797359\n",
      "train loss:0.5093904826135464\n",
      "train loss:0.673498718011085\n",
      "train loss:0.3950726817721253\n",
      "train loss:0.391084131490133\n",
      "train loss:0.6447753691094107\n",
      "train loss:0.7008334624255573\n",
      "train loss:0.5101875089387955\n",
      "train loss:0.44722808586432883\n",
      "train loss:0.4890521897364085\n",
      "train loss:0.40212608519158205\n",
      "train loss:0.4249963993044704\n",
      "train loss:0.7523630160178543\n",
      "train loss:0.6539851189067973\n",
      "train loss:0.7668844611084812\n",
      "train loss:0.46106653324035607\n",
      "train loss:0.4307554276203228\n",
      "train loss:0.7191448238315863\n",
      "train loss:0.6164570743004922\n",
      "train loss:0.6258689145487265\n",
      "train loss:0.538579905259015\n",
      "train loss:0.8172398008210223\n",
      "train loss:0.5419732101534882\n",
      "train loss:0.5815960546021163\n",
      "train loss:0.5528467026874218\n",
      "train loss:0.4605097836317948\n",
      "train loss:0.6087759781928137\n",
      "train loss:0.8105474103753485\n",
      "train loss:0.610072856702917\n",
      "train loss:0.6879332016396307\n",
      "train loss:0.7986256234088541\n",
      "train loss:0.3770676365124243\n",
      "train loss:0.33963216325846285\n",
      "train loss:0.7033577358602195\n",
      "train loss:0.8105911195487339\n",
      "train loss:0.5159624958584155\n",
      "train loss:0.5274207431253076\n",
      "train loss:0.5498276034534854\n",
      "train loss:0.7305838827242417\n",
      "train loss:0.6922858870242311\n",
      "train loss:0.4424963649629694\n",
      "train loss:0.7100714955995778\n",
      "train loss:0.7218333539385104\n",
      "train loss:0.5905749921582443\n",
      "train loss:0.5258769131300477\n",
      "train loss:0.388431689046542\n",
      "train loss:0.645287655735185\n",
      "train loss:0.660969134066418\n",
      "train loss:0.4654796237601393\n",
      "train loss:0.45284509493397307\n",
      "train loss:0.3938153917471316\n",
      "train loss:0.380744243740987\n",
      "train loss:0.5671766786687703\n",
      "train loss:0.532415287088901\n",
      "train loss:0.20530984534251\n",
      "train loss:0.5121625403120175\n",
      "train loss:0.569830825529943\n",
      "train loss:0.993148795747034\n",
      "train loss:0.7681499762445589\n",
      "train loss:0.7947656967756117\n",
      "train loss:0.29226351656911465\n",
      "train loss:0.6016074622155987\n",
      "train loss:0.5552175971102449\n",
      "train loss:0.5821712445985716\n",
      "train loss:0.43969046683469654\n",
      "train loss:0.605162449971021\n",
      "train loss:0.6983917903622523\n",
      "train loss:0.5475568424730626\n",
      "train loss:0.6883339075476149\n",
      "train loss:0.46430247792110996\n",
      "train loss:0.6718352883621829\n",
      "train loss:0.7093599660156688\n",
      "train loss:0.921567915272691\n",
      "train loss:0.7560534104241681\n",
      "train loss:0.6183042253162905\n",
      "train loss:0.6012166401550344\n",
      "train loss:0.4850779604914055\n",
      "train loss:0.5863232014450084\n",
      "train loss:0.598206983913469\n",
      "train loss:0.697158113910987\n",
      "train loss:0.5054957744451098\n",
      "train loss:0.6801234624347726\n",
      "train loss:0.565190020603859\n",
      "train loss:0.39596485818169946\n",
      "train loss:0.6654504576211895\n",
      "train loss:0.579563605401309\n",
      "train loss:0.7046697009782472\n",
      "train loss:0.6200289085015764\n",
      "train loss:0.4339083108978654\n",
      "train loss:0.5789448798239725\n",
      "train loss:0.6563875119161499\n",
      "train loss:0.6163427237680301\n",
      "train loss:0.3738916222649598\n",
      "train loss:0.8587540337035634\n",
      "train loss:0.5647016430871095\n",
      "train loss:0.6220466395981376\n",
      "train loss:0.39225351236934336\n",
      "train loss:0.6825907014554421\n",
      "train loss:0.41350654761369865\n",
      "train loss:0.6556431553279648\n",
      "train loss:0.6203829512168552\n",
      "train loss:0.5593955904773975\n",
      "train loss:0.6058323996565722\n",
      "train loss:0.6601338873849023\n",
      "train loss:0.40283715295474554\n",
      "train loss:0.6565491376044938\n",
      "train loss:0.40206756580920694\n",
      "train loss:0.9304032025743307\n",
      "train loss:0.6092886457742537\n",
      "train loss:0.4882175315202314\n",
      "train loss:0.5477358013209088\n",
      "train loss:0.621387389647029\n",
      "train loss:0.6506179234724255\n",
      "train loss:0.6088913869284841\n",
      "train loss:0.7876249272933759\n",
      "train loss:0.583884021653793\n",
      "train loss:0.5542357909190001\n",
      "train loss:0.5333259583318871\n",
      "train loss:0.713201966480311\n",
      "train loss:0.7936346113004799\n",
      "train loss:0.6030402696715986\n",
      "train loss:0.6170956802703884\n",
      "train loss:0.7475403551154007\n",
      "train loss:0.47152619241055227\n",
      "train loss:0.5785228050467637\n",
      "train loss:0.49560594052030027\n",
      "train loss:0.484471796292077\n",
      "train loss:0.6269109656797791\n",
      "train loss:0.5415509738597029\n",
      "train loss:0.5150613288377526\n",
      "train loss:0.6902058757866787\n",
      "train loss:0.6254151574072778\n",
      "train loss:0.6179114735625191\n",
      "train loss:0.6542401322959018\n",
      "train loss:0.4910807271643611\n",
      "train loss:0.6526632963777009\n",
      "train loss:0.8327931928803126\n",
      "train loss:0.7096559405664861\n",
      "train loss:0.5966738126272074\n",
      "train loss:0.6509068225700504\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.45098039215686275\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.15, random_state=40)\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b425961b-b12b-4425-ac78-1dbd500074f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299392328949975\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:2.2950046341805637\n",
      "train loss:2.28712718526385\n",
      "train loss:2.2698698388879457\n",
      "train loss:2.24492553511601\n",
      "train loss:2.205674646348848\n",
      "train loss:2.1417169219281527\n",
      "train loss:2.0749750754667917\n",
      "train loss:1.968864852766784\n",
      "train loss:1.8501291586411992\n",
      "train loss:1.7240310869726816\n",
      "train loss:1.5394126717962675\n",
      "train loss:1.3955442477810587\n",
      "train loss:1.1599159517179995\n",
      "train loss:1.0925059455819703\n",
      "train loss:0.8033004084160501\n",
      "train loss:0.6285353247386251\n",
      "train loss:0.8261636573645397\n",
      "train loss:0.5075611525487587\n",
      "train loss:0.4935680522292084\n",
      "train loss:0.8634072172712489\n",
      "train loss:0.6303847973753809\n",
      "train loss:0.33558613175915714\n",
      "train loss:0.33014419987371657\n",
      "train loss:0.3396195489957733\n",
      "train loss:1.045134434776887\n",
      "train loss:0.4899260276109529\n",
      "train loss:0.77401524409268\n",
      "train loss:0.6153553066533987\n",
      "train loss:0.6347854993510623\n",
      "train loss:0.590666035566908\n",
      "train loss:0.6789371925951329\n",
      "train loss:0.620504618115247\n",
      "train loss:0.5467500400439805\n",
      "train loss:0.539772273580649\n",
      "train loss:0.9102134232731425\n",
      "train loss:0.49938596294962806\n",
      "train loss:0.6295624073885928\n",
      "train loss:0.3856200338319151\n",
      "train loss:0.5126968836046486\n",
      "train loss:0.896254300565565\n",
      "train loss:0.6226855836678704\n",
      "train loss:0.490479495082038\n",
      "train loss:0.3693477006568763\n",
      "train loss:0.47474450393937645\n",
      "train loss:0.9965852838197279\n",
      "train loss:0.6348968537204773\n",
      "train loss:0.6935553910462111\n",
      "train loss:0.7747152947273084\n",
      "train loss:0.6601997886294185\n",
      "train loss:0.674641148777898\n",
      "train loss:0.6773097024115757\n",
      "train loss:0.6188941135165481\n",
      "train loss:0.7103434010376961\n",
      "train loss:0.5684529170532604\n",
      "train loss:0.6143602290891291\n",
      "train loss:0.608694193029063\n",
      "train loss:0.604779114614912\n",
      "train loss:0.7239720354280019\n",
      "train loss:0.8142703197992492\n",
      "train loss:0.5744985153965259\n",
      "train loss:0.5031135397612838\n",
      "train loss:0.2595953248393307\n",
      "train loss:0.6253165161073719\n",
      "train loss:0.4980134033249358\n",
      "train loss:0.4965436888803594\n",
      "train loss:0.6368678643987384\n",
      "train loss:0.522068426781877\n",
      "train loss:0.6353261228295388\n",
      "train loss:0.6662936435719229\n",
      "train loss:0.7549583537251056\n",
      "train loss:0.640162739279595\n",
      "train loss:0.33282036301042667\n",
      "train loss:0.6355673458100031\n",
      "train loss:0.8196307842135175\n",
      "train loss:0.5583704841250678\n",
      "train loss:0.6440802002066078\n",
      "train loss:0.5877586068441091\n",
      "train loss:0.5782606732663982\n",
      "train loss:0.5004090016479223\n",
      "train loss:0.7310634516781682\n",
      "train loss:0.5215451449795746\n",
      "train loss:0.419542344824342\n",
      "train loss:0.6144055013770606\n",
      "train loss:0.4836235388189887\n",
      "train loss:0.3783761969100003\n",
      "train loss:0.6634841767418002\n",
      "train loss:0.8223498658546434\n",
      "train loss:0.5173972619332116\n",
      "train loss:0.6449663468786773\n",
      "train loss:0.7767930894425843\n",
      "train loss:0.22229790539189603\n",
      "train loss:0.5814050260509978\n",
      "train loss:0.37933330202440835\n",
      "train loss:0.6224693306597938\n",
      "train loss:0.626596516633483\n",
      "train loss:0.6500922696202698\n",
      "train loss:0.6955810135348021\n",
      "train loss:0.5424821224726027\n",
      "train loss:0.7647444117562728\n",
      "train loss:0.5517090283621547\n",
      "train loss:0.6815059161333799\n",
      "train loss:0.5258655273759154\n",
      "train loss:0.6652815562439017\n",
      "train loss:0.5614651342578335\n",
      "train loss:0.6344687473004795\n",
      "train loss:0.554649987878424\n",
      "train loss:0.4631583223366535\n",
      "train loss:0.842359346118216\n",
      "train loss:0.530812701130162\n",
      "train loss:0.6945435562934144\n",
      "train loss:0.5879801568207391\n",
      "train loss:0.6895222023915537\n",
      "train loss:0.6490371803363228\n",
      "train loss:0.6111872505199123\n",
      "train loss:0.5325640030353422\n",
      "train loss:0.716211489155168\n",
      "train loss:0.5366898783183054\n",
      "train loss:0.6991694041585563\n",
      "train loss:0.684618666947036\n",
      "train loss:0.456603986628478\n",
      "train loss:0.6701683281649151\n",
      "train loss:0.4541670608600321\n",
      "train loss:0.41419362580302543\n",
      "train loss:0.579245472690175\n",
      "train loss:0.615502392656737\n",
      "train loss:0.5591725889378729\n",
      "train loss:0.39791788312832493\n",
      "train loss:0.49585534915883506\n",
      "train loss:0.6366300045260982\n",
      "train loss:1.0301749283611985\n",
      "train loss:0.8494578595303827\n",
      "train loss:0.7409804189313434\n",
      "train loss:0.5980471765521562\n",
      "train loss:0.6059108741386464\n",
      "train loss:0.6597514022903839\n",
      "train loss:0.7116743514762178\n",
      "train loss:0.6065712239577556\n",
      "train loss:0.6284809243262026\n",
      "train loss:0.7011557264525471\n",
      "train loss:0.696780460003567\n",
      "train loss:0.6000447289496024\n",
      "train loss:0.6227222283715573\n",
      "train loss:0.6289848346900551\n",
      "train loss:0.5760646071880312\n",
      "train loss:0.7224388944499205\n",
      "train loss:0.6442287725174369\n",
      "train loss:0.5445378165420054\n",
      "train loss:0.7578543882252073\n",
      "train loss:0.7010815551750313\n",
      "train loss:0.7811604618777993\n",
      "train loss:0.3457851536260638\n",
      "train loss:0.5428238935076315\n",
      "train loss:0.522473194806487\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.3907455415099393\n",
      "train loss:0.4965252575609222\n",
      "train loss:0.6136757987042886\n",
      "train loss:0.4858505806139708\n",
      "train loss:0.5414272122490582\n",
      "train loss:0.5133316358673882\n",
      "train loss:0.3499839615760705\n",
      "train loss:0.3304756659740706\n",
      "train loss:0.8401604532567888\n",
      "train loss:1.0563665513191483\n",
      "train loss:0.6378192408467259\n",
      "train loss:0.3345624403009147\n",
      "train loss:1.1135301858534743\n",
      "train loss:0.28841886666707583\n",
      "train loss:0.627423003727921\n",
      "train loss:0.6247002734704526\n",
      "train loss:0.6174597896343453\n",
      "train loss:0.7468296578817741\n",
      "train loss:0.4831897095838862\n",
      "train loss:0.5457349370789316\n",
      "train loss:0.48315848557276403\n",
      "train loss:0.6200297402080392\n",
      "train loss:0.6111038711099267\n",
      "train loss:0.8176078421845023\n",
      "train loss:0.5474150045888146\n",
      "train loss:0.5417353768590181\n",
      "train loss:0.8742371255181274\n",
      "train loss:0.5737383499895374\n",
      "train loss:0.5576599108308093\n",
      "train loss:0.45372004556287154\n",
      "train loss:0.837735500848329\n",
      "train loss:0.747635387651497\n",
      "train loss:0.5633017020880463\n",
      "train loss:0.6130752692079472\n",
      "train loss:0.6857174416284171\n",
      "train loss:0.5505570767036303\n",
      "train loss:0.5275041288315936\n",
      "train loss:0.5252155003386163\n",
      "train loss:0.5167077074963289\n",
      "train loss:0.3941383782913401\n",
      "train loss:0.8229221741039335\n",
      "train loss:0.6465333565612168\n",
      "train loss:0.6406570625980038\n",
      "train loss:0.5118804339769913\n",
      "train loss:0.493473933050146\n",
      "train loss:0.7472883963615422\n",
      "train loss:0.8293888879850903\n",
      "train loss:0.5991726948041934\n",
      "train loss:0.7204425092770917\n",
      "train loss:0.7003588940838755\n",
      "train loss:0.6107639501681904\n",
      "train loss:0.6969051086217577\n",
      "train loss:0.7457570126027934\n",
      "train loss:0.5848343728476508\n",
      "train loss:0.581616508549693\n",
      "train loss:0.5437401762017096\n",
      "train loss:0.5830190751778603\n",
      "train loss:0.5684511051504958\n",
      "train loss:0.5628851275850856\n",
      "train loss:0.6141328720551498\n",
      "train loss:0.6802107078690465\n",
      "train loss:0.6840317156135327\n",
      "train loss:0.5181862361769703\n",
      "train loss:0.699807360849918\n",
      "train loss:0.6067689835418222\n",
      "train loss:0.6177745981999337\n",
      "train loss:0.4965899367328692\n",
      "train loss:0.6162987080142659\n",
      "train loss:0.4048731949691146\n",
      "train loss:0.6784901544142964\n",
      "train loss:0.510503138187149\n",
      "train loss:0.6487585552277103\n",
      "train loss:0.520338821321596\n",
      "train loss:0.7194036934675313\n",
      "train loss:0.8380183191934749\n",
      "train loss:0.9085715061888499\n",
      "train loss:0.44508682109376263\n",
      "train loss:0.5471361594194514\n",
      "train loss:0.6336858962100986\n",
      "train loss:0.5384848117723747\n",
      "train loss:0.6804263945165905\n",
      "train loss:0.8005059886861184\n",
      "train loss:0.6785127055713158\n",
      "train loss:0.6305449361054516\n",
      "train loss:0.6647971183434364\n",
      "train loss:0.6801969387875444\n",
      "train loss:0.6021589164285451\n",
      "train loss:0.63965512201266\n",
      "train loss:0.5404892105377127\n",
      "train loss:0.5791490171239907\n",
      "train loss:0.6763231312008238\n",
      "train loss:0.5889771962387285\n",
      "train loss:0.7458091598251478\n",
      "train loss:0.527055815743702\n",
      "train loss:0.5532082538224281\n",
      "train loss:0.6848374547988427\n",
      "train loss:0.3314900788853804\n",
      "train loss:0.703463342774074\n",
      "train loss:0.727250031536932\n",
      "train loss:0.6461996634607042\n",
      "train loss:0.6046677251146612\n",
      "train loss:0.38147725755882744\n",
      "train loss:0.48561505483434153\n",
      "train loss:0.39888099463894194\n",
      "train loss:0.3881661460511174\n",
      "train loss:0.48735047734242676\n",
      "train loss:0.6846970304547757\n",
      "train loss:0.1831211747791473\n",
      "train loss:0.661920751018126\n",
      "train loss:0.501871068793347\n",
      "train loss:0.5593740659970823\n",
      "train loss:0.3348598427538204\n",
      "train loss:0.5221922713670896\n",
      "train loss:0.5012212924176328\n",
      "train loss:0.16380673615194302\n",
      "train loss:0.5046894519694289\n",
      "train loss:0.4857849921559133\n",
      "train loss:0.5396933884180111\n",
      "train loss:0.6573582188889793\n",
      "train loss:0.8095224762993107\n",
      "train loss:0.7524845994419603\n",
      "train loss:0.7784142813006591\n",
      "train loss:0.7039477959850107\n",
      "train loss:0.7626497690512469\n",
      "train loss:0.6128768571763563\n",
      "train loss:0.626619608836835\n",
      "train loss:0.6317502638916651\n",
      "train loss:0.7408928874913426\n",
      "train loss:0.6316962385680542\n",
      "train loss:0.6691334012588447\n",
      "train loss:0.6511984190119471\n",
      "train loss:0.6696800568539736\n",
      "train loss:0.6474590794258717\n",
      "train loss:0.6139321879278685\n",
      "train loss:0.6524590457554708\n",
      "train loss:0.6114949369813604\n",
      "train loss:0.7174713722011098\n",
      "train loss:0.5285351848111136\n",
      "train loss:0.5524858713551946\n",
      "train loss:0.4731056271790444\n",
      "train loss:0.5428159584672676\n",
      "train loss:0.6125294413197806\n",
      "train loss:0.5242053007672427\n",
      "train loss:0.3635985023111894\n",
      "train loss:0.505558040519275\n",
      "train loss:0.3204242941663981\n",
      "train loss:1.2886784026060067\n",
      "train loss:1.0229142440306807\n",
      "train loss:0.624954389272324\n",
      "train loss:0.7420679509272083\n",
      "train loss:0.6100075493889537\n",
      "train loss:0.3656615822529705\n",
      "train loss:0.6199378647859083\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7136934726269036\n",
      "train loss:0.5232345794585354\n",
      "train loss:0.6071129057600936\n",
      "train loss:0.6780177757294765\n",
      "train loss:0.5494090537184385\n",
      "train loss:0.6166987525709887\n",
      "train loss:0.39572842637648586\n",
      "train loss:0.5472306736474086\n",
      "train loss:0.6255393637703317\n",
      "train loss:0.5319695517965608\n",
      "train loss:0.6907553063860836\n",
      "train loss:0.6268951749734486\n",
      "train loss:0.6045361735415877\n",
      "train loss:0.5154119565439341\n",
      "train loss:0.6026676822429436\n",
      "train loss:0.5877449898009346\n",
      "train loss:0.620060932499122\n",
      "train loss:0.5042457346789831\n",
      "train loss:0.39078234070488366\n",
      "train loss:0.26717639130091725\n",
      "train loss:0.7576174334610184\n",
      "train loss:0.7782110938305522\n",
      "train loss:0.6177734133167156\n",
      "train loss:0.6187655648302727\n",
      "train loss:0.47286564037720885\n",
      "train loss:0.2512022364980238\n",
      "train loss:0.5699597004273731\n",
      "train loss:0.6470094416460261\n",
      "train loss:0.6337132239119516\n",
      "train loss:0.8582821963037162\n",
      "train loss:0.826081279730093\n",
      "train loss:0.5132446711872478\n",
      "train loss:0.611576462651066\n",
      "train loss:0.6193794414467166\n",
      "train loss:0.687860753054304\n",
      "train loss:0.46368355122748095\n",
      "train loss:0.677024861586945\n",
      "train loss:0.5543377588465574\n",
      "train loss:0.41151798829766495\n",
      "train loss:0.5366408047625758\n",
      "train loss:0.3704053804802646\n",
      "train loss:0.606600768886177\n",
      "train loss:0.4035988211523077\n",
      "train loss:0.5055188813058983\n",
      "train loss:0.7615836390301027\n",
      "train loss:0.6393367736675859\n",
      "train loss:0.636215828538712\n",
      "train loss:0.5247067781020781\n",
      "train loss:0.7522177175297142\n",
      "train loss:0.6191588036719601\n",
      "train loss:0.7360899456115733\n",
      "train loss:0.2593804284177046\n",
      "train loss:0.6727767609552249\n",
      "train loss:0.4956653718027516\n",
      "train loss:0.7134153577664152\n",
      "train loss:0.41848761846527277\n",
      "train loss:0.8191473105089008\n",
      "train loss:0.5154381599612236\n",
      "train loss:0.5333292095023016\n",
      "train loss:0.527454458383817\n",
      "train loss:0.6097305976347195\n",
      "train loss:0.8866038091650635\n",
      "train loss:0.6163094831379142\n",
      "train loss:0.6928285923927714\n",
      "train loss:0.6892672668208357\n",
      "train loss:0.6067645230579919\n",
      "train loss:0.6115447821593792\n",
      "train loss:0.5491162028243262\n",
      "train loss:0.6834262980629793\n",
      "train loss:0.5615699560362323\n",
      "train loss:0.5594545436430028\n",
      "train loss:0.49196447934688853\n",
      "train loss:0.8171093498904712\n",
      "train loss:0.39951528897420047\n",
      "train loss:0.6883282288861721\n",
      "train loss:0.6234650874308856\n",
      "train loss:0.5285320755225651\n",
      "train loss:0.8728046405504438\n",
      "train loss:0.5985643876666689\n",
      "train loss:0.9359700623336256\n",
      "train loss:0.5354168763983618\n",
      "train loss:0.6959155207760443\n",
      "train loss:0.6822424567747223\n",
      "train loss:0.6251418581018473\n",
      "train loss:0.6728550649293359\n",
      "train loss:0.5058346857836516\n",
      "train loss:0.7354432426241242\n",
      "train loss:0.6708158450012143\n",
      "train loss:0.6262151180003053\n",
      "train loss:0.6717668510889971\n",
      "train loss:0.569555853816303\n",
      "train loss:0.6806205849921501\n",
      "train loss:0.6739234873710276\n",
      "train loss:0.7715389549233967\n",
      "train loss:0.7194924523965376\n",
      "train loss:0.5890007157339143\n",
      "train loss:0.6307251078585255\n",
      "train loss:0.5286257337622529\n",
      "train loss:0.7702560954717461\n",
      "train loss:0.6747656282831211\n",
      "train loss:0.623372725394151\n",
      "train loss:0.5043320524477769\n",
      "train loss:0.6222552286741242\n",
      "train loss:0.46993909418020713\n",
      "train loss:0.7531146644050256\n",
      "train loss:0.6182114269729415\n",
      "train loss:0.4347988295090481\n",
      "train loss:0.61039109250945\n",
      "train loss:0.518774430088439\n",
      "train loss:0.5190336324106551\n",
      "train loss:0.35612683168543113\n",
      "train loss:0.632404897386283\n",
      "train loss:0.9123044411707969\n",
      "train loss:0.5099172490210969\n",
      "train loss:0.642946496545541\n",
      "train loss:0.6148473250345627\n",
      "train loss:0.3783631198155134\n",
      "train loss:0.7468883445645631\n",
      "train loss:0.8499345245181577\n",
      "train loss:0.7208415452128317\n",
      "train loss:0.41385104925446903\n",
      "train loss:0.6254557467713322\n",
      "train loss:0.6952720465765557\n",
      "train loss:0.45430536515472636\n",
      "train loss:0.6025377960017233\n",
      "train loss:0.6852332040458644\n",
      "train loss:0.7541359075315688\n",
      "train loss:0.5509657125011131\n",
      "train loss:0.6133530103048374\n",
      "train loss:0.6063366700581236\n",
      "train loss:0.7517140986236838\n",
      "train loss:0.6223470672226333\n",
      "train loss:0.6809182219796813\n",
      "train loss:0.6849684020305685\n",
      "train loss:0.674912408072235\n",
      "train loss:0.6759620356745774\n",
      "train loss:0.578884676393938\n",
      "train loss:0.7625689603065505\n",
      "train loss:0.5435274526730336\n",
      "train loss:0.6291127865056534\n",
      "train loss:0.5206231006894535\n",
      "train loss:0.7327899445090533\n",
      "train loss:0.5052648866542429\n",
      "train loss:0.6853743355104259\n",
      "train loss:0.5425525155606487\n",
      "train loss:0.5321537531695949\n",
      "train loss:0.6117149860060559\n",
      "train loss:0.8932587154013435\n",
      "train loss:0.7711424705462777\n",
      "train loss:0.5302676342101474\n",
      "train loss:0.6150794168442439\n",
      "train loss:0.7062179885036934\n",
      "train loss:0.6277778915188126\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.51805163793275\n",
      "train loss:0.5365466836206906\n",
      "train loss:0.5088762311651791\n",
      "train loss:0.6172251932953741\n",
      "train loss:0.6056483559930486\n",
      "train loss:0.42193456379609895\n",
      "train loss:0.7312832330529192\n",
      "train loss:0.5972940750545731\n",
      "train loss:0.6012602969354854\n",
      "train loss:0.6156922188884646\n",
      "train loss:0.528206366857024\n",
      "train loss:0.7182911236580204\n",
      "train loss:0.7297321570771484\n",
      "train loss:0.5154110424616821\n",
      "train loss:0.6175550588138218\n",
      "train loss:0.5099542572210087\n",
      "train loss:0.6203454527333377\n",
      "train loss:0.7723757006806375\n",
      "train loss:0.6133923612915353\n",
      "train loss:0.6228581975160773\n",
      "train loss:0.6090991393597218\n",
      "train loss:0.745465295007602\n",
      "train loss:0.5484069775648559\n",
      "train loss:0.6918112027689027\n",
      "train loss:0.6239927271339457\n",
      "train loss:0.6179380762313401\n",
      "train loss:0.7332925023329327\n",
      "train loss:0.621977568732607\n",
      "train loss:0.6262278290125349\n",
      "train loss:0.6705124682734764\n",
      "train loss:0.5094704467573281\n",
      "train loss:0.5059637777963336\n",
      "train loss:0.5522543302897474\n",
      "train loss:0.5383326593134135\n",
      "train loss:0.7763610972135434\n",
      "train loss:0.5945000520571438\n",
      "train loss:0.42555097956869464\n",
      "train loss:0.40096358466853677\n",
      "train loss:0.38054658391918705\n",
      "train loss:0.7720177050602616\n",
      "train loss:0.362649529250061\n",
      "train loss:0.5187957546858374\n",
      "train loss:0.511575021478542\n",
      "train loss:1.2206885918340191\n",
      "train loss:0.780693948485814\n",
      "train loss:0.5083967444638459\n",
      "train loss:0.7380211550779381\n",
      "train loss:0.7164769825322892\n",
      "train loss:0.5210436967281857\n",
      "train loss:0.6935632947949409\n",
      "train loss:0.46491077093510186\n",
      "train loss:0.677954424135528\n",
      "train loss:0.47636087751678835\n",
      "train loss:0.7449648516304171\n",
      "train loss:0.557328392237488\n",
      "train loss:0.49619769679953424\n",
      "train loss:0.6202792629067567\n",
      "train loss:0.622232144952704\n",
      "train loss:0.8167951013118058\n",
      "train loss:0.6190311970665617\n",
      "train loss:0.7471354190034375\n",
      "train loss:0.56538092624731\n",
      "train loss:0.6084802656484796\n",
      "train loss:0.5004497841190125\n",
      "train loss:0.551723171828431\n",
      "train loss:0.5373415891893832\n",
      "train loss:0.5320010397954767\n",
      "train loss:0.5233491033068873\n",
      "train loss:0.8007535614151304\n",
      "train loss:0.6051958775635428\n",
      "train loss:0.4983761523020746\n",
      "train loss:0.6101724535284478\n",
      "train loss:0.6418230484028259\n",
      "train loss:0.5133196883926189\n",
      "train loss:0.7231385680567524\n",
      "train loss:0.5103742290986724\n",
      "train loss:0.724187546132139\n",
      "train loss:0.7302362373872497\n",
      "train loss:0.5143265675952329\n",
      "train loss:0.5049562666710987\n",
      "train loss:0.9837789811003278\n",
      "train loss:0.6102830119267272\n",
      "train loss:0.5501912900227327\n",
      "train loss:0.6710986059378938\n",
      "train loss:0.7519271240485983\n",
      "train loss:0.5625432040571534\n",
      "train loss:0.4373002902673103\n",
      "train loss:0.4867762081034255\n",
      "train loss:0.40652173514908246\n",
      "train loss:0.6075736345984617\n",
      "train loss:0.6080581013161938\n",
      "train loss:0.5259868627167711\n",
      "train loss:0.41815470001676747\n",
      "train loss:0.7090640541316822\n",
      "train loss:0.5099622349654196\n",
      "train loss:0.7326627990200298\n",
      "train loss:0.484019184034591\n",
      "train loss:0.6415700906838687\n",
      "train loss:0.7551417220213509\n",
      "train loss:0.6302431413212809\n",
      "train loss:0.7440358687341603\n",
      "train loss:0.2763258985973855\n",
      "train loss:0.9329827943671262\n",
      "train loss:0.636431573528329\n",
      "train loss:0.7051285517275516\n",
      "train loss:0.5273450559514241\n",
      "train loss:0.6201954385429035\n",
      "train loss:0.6917250925341665\n",
      "train loss:0.6789081929472296\n",
      "train loss:0.6207581336870456\n",
      "train loss:0.5055135908445402\n",
      "train loss:0.6878459936153395\n",
      "train loss:0.7341990156706854\n",
      "train loss:0.6309479921821266\n",
      "train loss:0.6652621696688115\n",
      "train loss:0.6277943857580166\n",
      "train loss:0.5173714771085619\n",
      "train loss:0.5116642614676086\n",
      "train loss:0.6159696389499825\n",
      "train loss:0.48098890049613174\n",
      "train loss:0.36414941395937406\n",
      "train loss:0.6937878222454141\n",
      "train loss:0.6148042880023991\n",
      "train loss:0.5160565694482179\n",
      "train loss:0.7267717214321059\n",
      "train loss:0.6324866131705881\n",
      "train loss:0.37230716118961465\n",
      "train loss:0.5010188306872818\n",
      "train loss:0.7667018747615908\n",
      "train loss:0.494486835727716\n",
      "train loss:0.6465765458924528\n",
      "train loss:0.787762606249899\n",
      "train loss:0.3539604422899709\n",
      "train loss:0.7341343931656988\n",
      "train loss:0.6248976049592727\n",
      "train loss:0.620881438627027\n",
      "train loss:0.8228109950605473\n",
      "train loss:0.6979020551431322\n",
      "train loss:0.4552081036045038\n",
      "train loss:0.5368554542549909\n",
      "train loss:0.5257906000420423\n",
      "train loss:0.6919541689869659\n",
      "train loss:0.682733411295324\n",
      "train loss:0.6226399308198839\n",
      "train loss:0.7400806355243883\n",
      "train loss:0.6131003362316431\n",
      "train loss:0.5596355110302011\n",
      "train loss:0.625211702999698\n",
      "train loss:0.6224078042198217\n",
      "train loss:0.6192718699272055\n",
      "train loss:0.6298440543725764\n",
      "train loss:0.6218651172642439\n",
      "train loss:0.48056574930344353\n",
      "=== epoch:5, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6869050766305959\n",
      "train loss:0.6849008287344638\n",
      "train loss:0.4497023495745983\n",
      "train loss:0.6145558431412134\n",
      "train loss:0.6097976360422201\n",
      "train loss:0.6055346567724953\n",
      "train loss:0.5064236408837604\n",
      "train loss:0.6060021570274191\n",
      "train loss:0.7309186402923562\n",
      "train loss:0.6125106939825775\n",
      "train loss:0.9003878887894533\n",
      "train loss:0.39730295214059613\n",
      "train loss:0.6991729508430234\n",
      "train loss:0.519163846934893\n",
      "train loss:0.4990129885919827\n",
      "train loss:0.7164897947533598\n",
      "train loss:0.5164037743333877\n",
      "train loss:0.521385831961722\n",
      "train loss:0.7100712697106208\n",
      "train loss:0.6940895781102123\n",
      "train loss:0.6978049949926034\n",
      "train loss:0.6142558700418576\n",
      "train loss:0.6188935629566634\n",
      "train loss:0.6101798446055431\n",
      "train loss:0.6802117129479773\n",
      "train loss:0.6216540484107624\n",
      "train loss:0.5510013864807182\n",
      "train loss:0.40190552231507387\n",
      "train loss:0.6108356875913914\n",
      "train loss:0.6075861761835243\n",
      "train loss:0.44172945097191957\n",
      "train loss:0.5073500061197933\n",
      "train loss:0.7053586619439989\n",
      "train loss:0.7249547770862493\n",
      "train loss:0.3944927130572142\n",
      "train loss:0.38016786036961847\n",
      "train loss:0.9546909846693834\n",
      "train loss:0.488676272664621\n",
      "train loss:0.7165198434315349\n",
      "train loss:0.5082929891551099\n",
      "train loss:0.25871615775335405\n",
      "train loss:0.7490651031507041\n",
      "train loss:0.37233014617113613\n",
      "train loss:0.6200015610082823\n",
      "train loss:0.47848301585367425\n",
      "train loss:0.6419867557656068\n",
      "train loss:0.7508558503326154\n",
      "train loss:0.5173492596966421\n",
      "train loss:0.38807613156947257\n",
      "train loss:0.507739368646225\n",
      "train loss:0.40026039204646474\n",
      "train loss:0.3518298116964961\n",
      "train loss:0.7433470404093927\n",
      "train loss:0.7540091899932377\n",
      "train loss:0.25252804222565584\n",
      "train loss:0.6204400279611593\n",
      "train loss:1.0962894188754064\n",
      "train loss:0.39824880550320574\n",
      "train loss:0.8043452225976676\n",
      "train loss:0.31535865593724977\n",
      "train loss:0.786645998704855\n",
      "train loss:0.6767645055547769\n",
      "train loss:0.5386723336338018\n",
      "train loss:0.46599768738967373\n",
      "train loss:0.8884833465220586\n",
      "train loss:0.6298428001486994\n",
      "train loss:0.5608697965166463\n",
      "train loss:0.6169931772745987\n",
      "train loss:0.6715476427297886\n",
      "train loss:0.5050644032412435\n",
      "train loss:0.549222047368755\n",
      "train loss:0.684618793510557\n",
      "train loss:0.7428925088937259\n",
      "train loss:0.7994498454182801\n",
      "train loss:0.674801322265866\n",
      "train loss:0.8247501432982896\n",
      "train loss:0.6227336461615834\n",
      "train loss:0.6740689982073305\n",
      "train loss:0.7138092640018281\n",
      "train loss:0.6446179896680693\n",
      "train loss:0.6110735409883788\n",
      "train loss:0.7072137983870693\n",
      "train loss:0.6394343120907794\n",
      "train loss:0.5670684476612721\n",
      "train loss:0.604113588438641\n",
      "train loss:0.5779704911352439\n",
      "train loss:0.5048424762343539\n",
      "train loss:0.5505818745808162\n",
      "train loss:0.6095617551585992\n",
      "train loss:0.6137507871593184\n",
      "train loss:0.6117791305115882\n",
      "train loss:0.3959294527281552\n",
      "train loss:0.3836441573773232\n",
      "train loss:0.3636696193776839\n",
      "train loss:0.1776154995517263\n",
      "train loss:0.8336296717806801\n",
      "train loss:0.5027895834531346\n",
      "train loss:0.3273816674723372\n",
      "train loss:0.3277093817618705\n",
      "train loss:0.9879697130987383\n",
      "train loss:1.0712395023823367\n",
      "train loss:0.5261415072576641\n",
      "train loss:0.5003456299529244\n",
      "train loss:0.6083709615736841\n",
      "train loss:0.7447615152932563\n",
      "train loss:0.39831161953985955\n",
      "train loss:0.527801246333723\n",
      "train loss:0.9552273554977289\n",
      "train loss:0.6124416951657672\n",
      "train loss:0.5515215448535848\n",
      "train loss:0.49015750203822106\n",
      "train loss:0.4988620468974395\n",
      "train loss:0.5692335255425036\n",
      "train loss:0.7633898534404431\n",
      "train loss:0.471585097462326\n",
      "train loss:0.7663558132949322\n",
      "train loss:0.8253636427136758\n",
      "train loss:0.6332190501957694\n",
      "train loss:0.548610876854866\n",
      "train loss:0.6899103265490487\n",
      "train loss:0.6793764664630334\n",
      "train loss:0.6214884737466564\n",
      "train loss:0.7354158304933286\n",
      "train loss:0.5526551003685225\n",
      "train loss:0.7384230364068409\n",
      "train loss:0.4961203855543398\n",
      "train loss:0.6151406748092372\n",
      "train loss:0.613834630087523\n",
      "train loss:0.7521388605088017\n",
      "train loss:0.749112027020963\n",
      "train loss:0.6121702104627675\n",
      "train loss:0.6096976950311497\n",
      "train loss:0.6184336785854592\n",
      "train loss:0.6755627605630331\n",
      "train loss:0.5401729361427554\n",
      "train loss:0.8091999411975832\n",
      "train loss:0.5427606925315891\n",
      "train loss:0.7479966195896786\n",
      "train loss:0.6732381254419899\n",
      "train loss:0.4842044417102752\n",
      "train loss:0.6794188615534724\n",
      "train loss:0.4784078531129814\n",
      "train loss:0.7405070431612266\n",
      "train loss:0.6219669105811596\n",
      "train loss:0.6912863536154334\n",
      "train loss:0.7422178373346859\n",
      "train loss:0.6093017194923946\n",
      "train loss:0.47793393508264553\n",
      "train loss:0.4625649394415558\n",
      "train loss:0.6749271511171009\n",
      "train loss:0.5340764694126721\n",
      "train loss:0.6890665047733837\n",
      "train loss:0.856056294474578\n",
      "=== epoch:6, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6937608961039239\n",
      "train loss:0.7723167759689498\n",
      "train loss:0.7439104252983886\n",
      "train loss:0.6137140716269659\n",
      "train loss:0.6104578065111046\n",
      "train loss:0.6744538471680166\n",
      "train loss:0.6806932553138332\n",
      "train loss:0.5712810343250292\n",
      "train loss:0.5728069290070453\n",
      "train loss:0.6246970622776732\n",
      "train loss:0.6290987102228041\n",
      "train loss:0.7293016836165748\n",
      "train loss:0.6165934026976944\n",
      "train loss:0.6794270613130982\n",
      "train loss:0.4367466785802935\n",
      "train loss:0.4012106738611405\n",
      "train loss:0.4486921029647989\n",
      "train loss:0.7836691940005502\n",
      "train loss:0.7153893460988313\n",
      "train loss:0.8086565172421085\n",
      "train loss:0.6411066001810131\n",
      "train loss:0.6959403215436123\n",
      "train loss:0.7227831136728363\n",
      "train loss:0.5183393126536217\n",
      "train loss:0.6165546058496835\n",
      "train loss:0.5100724702123622\n",
      "train loss:0.7816067808313998\n",
      "train loss:0.5240634574527248\n",
      "train loss:0.6316601329393062\n",
      "train loss:0.6825855816594841\n",
      "train loss:0.610801635644125\n",
      "train loss:0.7490995165055996\n",
      "train loss:0.5454687574677058\n",
      "train loss:0.806489038581461\n",
      "train loss:0.6134868807080787\n",
      "train loss:0.5519759557811698\n",
      "train loss:0.7396535134897382\n",
      "train loss:0.43507938407384267\n",
      "train loss:0.7982075838161732\n",
      "train loss:0.6171226176284692\n",
      "train loss:0.7318891900877407\n",
      "train loss:0.5652947329622096\n",
      "train loss:0.5675798467675327\n",
      "train loss:0.6133574021919246\n",
      "train loss:0.4926247269769702\n",
      "train loss:0.5501571779270031\n",
      "train loss:0.6802691528786433\n",
      "train loss:0.7620389694474008\n",
      "train loss:0.4457173304532175\n",
      "train loss:0.6205090235277131\n",
      "train loss:0.8041844361571441\n",
      "train loss:0.4314038940363457\n",
      "train loss:0.5186554289837608\n",
      "train loss:0.7221984723842353\n",
      "train loss:0.5051753527130718\n",
      "train loss:0.7407264720192966\n",
      "train loss:0.6174983845526593\n",
      "train loss:0.6104925494985991\n",
      "train loss:0.8092329947859748\n",
      "train loss:0.6065213035894284\n",
      "train loss:0.5106563528765979\n",
      "train loss:0.6887318171280848\n",
      "train loss:0.7662627513231532\n",
      "train loss:0.6151550818441447\n",
      "train loss:0.6074331725395399\n",
      "train loss:0.7445574900677218\n",
      "train loss:0.6184860889321431\n",
      "train loss:0.6182049150736312\n",
      "train loss:0.7335985001675254\n",
      "train loss:0.6225278257918311\n",
      "train loss:0.6692524291394824\n",
      "train loss:0.5780196741696204\n",
      "train loss:0.630425792037952\n",
      "train loss:0.5822778478921876\n",
      "train loss:0.5689106468683581\n",
      "train loss:0.5630627893059146\n",
      "train loss:0.7417950490404559\n",
      "train loss:0.6768403223946713\n",
      "train loss:0.47608533513953655\n",
      "train loss:0.4537285231854808\n",
      "train loss:0.6130979328490355\n",
      "train loss:0.6926540360822997\n",
      "train loss:0.6098549384496263\n",
      "train loss:0.5164832304212142\n",
      "train loss:0.8896969275864948\n",
      "train loss:0.4069340518042199\n",
      "train loss:0.2814742956157891\n",
      "train loss:0.5075350400845187\n",
      "train loss:0.6367470517103012\n",
      "train loss:0.502942255408639\n",
      "train loss:0.6553710667606548\n",
      "train loss:0.7979702228949915\n",
      "train loss:0.6556506829745316\n",
      "train loss:0.6185271202793701\n",
      "train loss:0.4149142481670153\n",
      "train loss:0.6226152520174281\n",
      "train loss:0.7262877155102568\n",
      "train loss:0.6191982555066211\n",
      "train loss:0.863958472716169\n",
      "train loss:0.5302335435528025\n",
      "train loss:0.5477957814493148\n",
      "train loss:0.5424587953670893\n",
      "train loss:0.4732957163738215\n",
      "train loss:0.5493958280307832\n",
      "train loss:0.5406630101885448\n",
      "train loss:0.5365436929180983\n",
      "train loss:0.4421045872624612\n",
      "train loss:0.6054414742547436\n",
      "train loss:0.7921786320422533\n",
      "train loss:0.7004793120835823\n",
      "train loss:0.41740202601321624\n",
      "train loss:0.7256688375341245\n",
      "train loss:0.5120234108424305\n",
      "train loss:0.6226019918066801\n",
      "train loss:0.7212514331940361\n",
      "train loss:0.6286459720456832\n",
      "train loss:0.716894147301269\n",
      "train loss:0.6014427774240423\n",
      "train loss:0.6195917151736782\n",
      "train loss:0.6032757141067743\n",
      "train loss:0.524468536964058\n",
      "train loss:0.44466004171501794\n",
      "train loss:0.533549570589291\n",
      "train loss:0.5124588880192693\n",
      "train loss:0.6114295199137321\n",
      "train loss:0.8100226834758029\n",
      "train loss:0.794475502897003\n",
      "train loss:0.7742023467351126\n",
      "train loss:0.5276504312705325\n",
      "train loss:0.6860389930409602\n",
      "train loss:0.6188780177872502\n",
      "train loss:0.7515690087653867\n",
      "train loss:0.4805581920708719\n",
      "train loss:0.8026509064477942\n",
      "train loss:0.5057731278445268\n",
      "train loss:0.5662306105701596\n",
      "train loss:0.6690461798948779\n",
      "train loss:0.7421705855443173\n",
      "train loss:0.6779655665458735\n",
      "train loss:0.5708039925004302\n",
      "train loss:0.6177282631821839\n",
      "train loss:0.678953395672254\n",
      "train loss:0.6809496737264991\n",
      "train loss:0.559448447782294\n",
      "train loss:0.555324966800568\n",
      "train loss:0.40679258270966645\n",
      "train loss:0.45427200316627453\n",
      "train loss:0.6157054249677014\n",
      "train loss:0.7087810639567449\n",
      "train loss:0.5094353459346346\n",
      "train loss:0.6069451895677827\n",
      "train loss:0.7256872116499423\n",
      "train loss:0.26770489726823293\n",
      "=== epoch:7, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5320854443086332\n",
      "train loss:0.6379281432247897\n",
      "train loss:0.5046404201015555\n",
      "train loss:0.6246285500915103\n",
      "train loss:0.6445666939056945\n",
      "train loss:0.7750889870062336\n",
      "train loss:0.6367723117304707\n",
      "train loss:0.502015964183893\n",
      "train loss:0.6120450077833302\n",
      "train loss:0.7169439762476766\n",
      "train loss:0.7956970908914954\n",
      "train loss:0.6837065445207617\n",
      "train loss:0.6098676544224739\n",
      "train loss:0.547352172695011\n",
      "train loss:0.691653577122165\n",
      "train loss:0.6782314775605321\n",
      "train loss:0.5665933254432358\n",
      "train loss:0.5733972967038639\n",
      "train loss:0.5216174428765121\n",
      "train loss:0.6284553014613322\n",
      "train loss:0.5656594134628455\n",
      "train loss:0.6110633431075556\n",
      "train loss:0.6226432551779191\n",
      "train loss:0.5556425146648502\n",
      "train loss:0.7501729633664411\n",
      "train loss:0.6881209132152872\n",
      "train loss:0.7516806381813035\n",
      "train loss:0.6223163060804295\n",
      "train loss:0.6851667008422029\n",
      "train loss:0.471182963364433\n",
      "train loss:0.5377359953328884\n",
      "train loss:0.45351159222770826\n",
      "train loss:0.6083618910351427\n",
      "train loss:0.52664882810578\n",
      "train loss:0.6126350291459237\n",
      "train loss:0.8086579848527498\n",
      "train loss:0.6167429452014226\n",
      "train loss:0.6198087644476871\n",
      "train loss:0.6181946633449761\n",
      "train loss:0.513170714896501\n",
      "train loss:0.7156978355888951\n",
      "train loss:0.5118128450247361\n",
      "train loss:0.8040077523978892\n",
      "train loss:0.5229681715966811\n",
      "train loss:0.9482997395584768\n",
      "train loss:0.5369321198210124\n",
      "train loss:0.5320938499668365\n",
      "train loss:0.5413502451354701\n",
      "train loss:0.6860341701911128\n",
      "train loss:0.47002013919009633\n",
      "train loss:0.549141660648325\n",
      "train loss:0.753572673896559\n",
      "train loss:0.6167838805830371\n",
      "train loss:0.6893826113158119\n",
      "train loss:0.537401782766663\n",
      "train loss:0.6158530824853914\n",
      "train loss:0.7543819536975979\n",
      "train loss:0.6820352920663125\n",
      "train loss:0.5431647896131021\n",
      "train loss:0.5403363722236666\n",
      "train loss:0.5353968291144138\n",
      "train loss:0.8323036991400057\n",
      "train loss:0.6113642149548583\n",
      "train loss:0.7547830902851645\n",
      "train loss:0.46471825994385807\n",
      "train loss:0.6837854385242005\n",
      "train loss:0.7565760633720846\n",
      "train loss:0.4741891032718625\n",
      "train loss:0.5410667635701133\n",
      "train loss:0.540416759351902\n",
      "train loss:0.4511022036801425\n",
      "train loss:0.7812913351140198\n",
      "train loss:0.7804023758348948\n",
      "train loss:0.4356965359855584\n",
      "train loss:0.3257815750308707\n",
      "train loss:0.7134901994438868\n",
      "train loss:0.8002351069661211\n",
      "train loss:0.798315341814531\n",
      "train loss:0.7013186209502289\n",
      "train loss:0.7697251370652319\n",
      "train loss:0.6139293785642576\n",
      "train loss:0.6103627368123399\n",
      "train loss:0.6118343389407968\n",
      "train loss:0.5471902132642332\n",
      "train loss:0.6134039634736413\n",
      "train loss:0.6773161156414325\n",
      "train loss:0.4820436773810754\n",
      "train loss:0.5418416121450141\n",
      "train loss:0.6092942471630493\n",
      "train loss:0.6160071055852969\n",
      "train loss:0.3585680549405771\n",
      "train loss:0.5200089313709457\n",
      "train loss:0.4099584054222009\n",
      "train loss:0.5028838534797083\n",
      "train loss:0.6304904378959758\n",
      "train loss:0.6241951284026718\n",
      "train loss:0.4999992754993177\n",
      "train loss:0.6413261138189348\n",
      "train loss:0.793513882386642\n",
      "train loss:0.6440126050080545\n",
      "train loss:0.48735845370016956\n",
      "train loss:0.5023096097654579\n",
      "train loss:0.4978454239971102\n",
      "train loss:0.7400727193312309\n",
      "train loss:0.7222726049580375\n",
      "train loss:0.7226734372362376\n",
      "train loss:0.7016488048549061\n",
      "train loss:0.5222891522793287\n",
      "train loss:0.6112186217545492\n",
      "train loss:0.61378109608523\n",
      "train loss:0.47467944436893655\n",
      "train loss:0.6111965356959784\n",
      "train loss:0.678615361508188\n",
      "train loss:0.6144071537866437\n",
      "train loss:0.7384737616659117\n",
      "train loss:0.555579431994201\n",
      "train loss:0.7401066485848056\n",
      "train loss:0.6773194967057784\n",
      "train loss:0.5662971429450728\n",
      "train loss:0.5002283110651873\n",
      "train loss:0.8045428333782327\n",
      "train loss:0.5545335736973198\n",
      "train loss:0.6766964879219681\n",
      "train loss:0.6124853754649621\n",
      "train loss:0.610230067895974\n",
      "train loss:0.5506697170136234\n",
      "train loss:0.7560461824855167\n",
      "train loss:0.61122418272653\n",
      "train loss:0.7463453332557564\n",
      "train loss:0.6831046976324879\n",
      "train loss:0.5448081591542686\n",
      "train loss:0.4713071199463622\n",
      "train loss:0.45219246860687334\n",
      "train loss:0.34796181053336744\n",
      "train loss:0.4158049139752114\n",
      "train loss:0.26769341870539565\n",
      "train loss:0.6304346026012673\n",
      "train loss:0.8147525662422446\n",
      "train loss:0.8130159168318482\n",
      "train loss:0.772896082132592\n",
      "train loss:0.35883832616111677\n",
      "train loss:0.9057059584335165\n",
      "train loss:0.6144553162125297\n",
      "train loss:0.3876133052559397\n",
      "train loss:0.7185809464754495\n",
      "train loss:0.6081506968285454\n",
      "train loss:0.4152425862627407\n",
      "train loss:0.7034241549957636\n",
      "train loss:0.6978297383903989\n",
      "train loss:0.8472489505067922\n",
      "train loss:0.5344470521720339\n",
      "train loss:0.46454664077626556\n",
      "train loss:0.6167722260281202\n",
      "=== epoch:8, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5400501576444078\n",
      "train loss:0.6863410257444664\n",
      "train loss:0.5455886229340364\n",
      "train loss:0.6948731878599641\n",
      "train loss:0.39847211187318077\n",
      "train loss:0.611727652134795\n",
      "train loss:0.6944013240161153\n",
      "train loss:0.7635389731367219\n",
      "train loss:0.454103670973359\n",
      "train loss:0.913959388880602\n",
      "train loss:0.6810400735802409\n",
      "train loss:0.6898439638522365\n",
      "train loss:0.5476486409393542\n",
      "train loss:0.48245348452374853\n",
      "train loss:0.6946519479501163\n",
      "train loss:0.4724817023077484\n",
      "train loss:0.6116995965328188\n",
      "train loss:0.6876279875952502\n",
      "train loss:0.5333663674220316\n",
      "train loss:0.6135308952479768\n",
      "train loss:0.8474099456646413\n",
      "train loss:0.7597767864437717\n",
      "train loss:0.5333380409219785\n",
      "train loss:0.5364622877431602\n",
      "train loss:0.7588251639766119\n",
      "train loss:0.6777505896451314\n",
      "train loss:0.5430948816788357\n",
      "train loss:0.5366006074494482\n",
      "train loss:0.610722903686823\n",
      "train loss:0.5356172068927114\n",
      "train loss:0.691892808013187\n",
      "train loss:0.4496668278088601\n",
      "train loss:0.5282260373589676\n",
      "train loss:0.6910659081708783\n",
      "train loss:0.7989516722801093\n",
      "train loss:0.8654360836310883\n",
      "train loss:0.5985552989146611\n",
      "train loss:0.526615608939828\n",
      "train loss:0.5234521302485653\n",
      "train loss:0.6179322075358906\n",
      "train loss:0.912293936440362\n",
      "train loss:0.46144314576948114\n",
      "train loss:0.4663613755137043\n",
      "train loss:0.6781139936113852\n",
      "train loss:0.6858590832069338\n",
      "train loss:0.531072714578371\n",
      "train loss:0.6894058949468337\n",
      "train loss:0.6107374321927126\n",
      "train loss:0.6091236172654311\n",
      "train loss:0.7481081275037293\n",
      "train loss:0.4610358411211867\n",
      "train loss:0.6048976900672037\n",
      "train loss:0.5369990317571788\n",
      "train loss:0.6840007774541931\n",
      "train loss:0.611148481916253\n",
      "train loss:0.4382766075754856\n",
      "train loss:0.5253387874085317\n",
      "train loss:0.6096511873594104\n",
      "train loss:0.3999412652652784\n",
      "train loss:0.6133126754193878\n",
      "train loss:0.7513643086600784\n",
      "train loss:0.6174967673592305\n",
      "train loss:0.5064822976137932\n",
      "train loss:0.7137461674755277\n",
      "train loss:0.6242777264816939\n",
      "train loss:0.5056387585264083\n",
      "train loss:0.7449720288121107\n",
      "train loss:0.5039079624446295\n",
      "train loss:0.8783115633821197\n",
      "train loss:0.8577991114014539\n",
      "train loss:0.6087153942502554\n",
      "train loss:0.6821988079199517\n",
      "train loss:0.6183083231606469\n",
      "train loss:0.5619124143943887\n",
      "train loss:0.679132062485367\n",
      "train loss:0.6767286800913633\n",
      "train loss:0.6781734185104492\n",
      "train loss:0.5521869316075756\n",
      "train loss:0.5921282154037215\n",
      "train loss:0.5789050722495739\n",
      "train loss:0.528030850888163\n",
      "train loss:0.6738176596234211\n",
      "train loss:0.6168792732969082\n",
      "train loss:0.49574564834468904\n",
      "train loss:0.6204002943896126\n",
      "train loss:0.6150177188930492\n",
      "train loss:0.6152609308237811\n",
      "train loss:0.6108472504913111\n",
      "train loss:0.4114875224756873\n",
      "train loss:0.5183021764777831\n",
      "train loss:0.7336445675805197\n",
      "train loss:0.6387243737949455\n",
      "train loss:0.3784356235724222\n",
      "train loss:0.49377963976241174\n",
      "train loss:0.6341629856678224\n",
      "train loss:0.6265061477971796\n",
      "train loss:0.7921255964435129\n",
      "train loss:0.7456465682160932\n",
      "train loss:0.3776691793483404\n",
      "train loss:0.5040009484412231\n",
      "train loss:0.380213730116052\n",
      "train loss:0.25170011965357\n",
      "train loss:0.6358836463309048\n",
      "train loss:0.7213925199590037\n",
      "train loss:0.7473440898715081\n",
      "train loss:0.7298786892347046\n",
      "train loss:0.5131169759914039\n",
      "train loss:0.7084216390023907\n",
      "train loss:0.6942400952820671\n",
      "train loss:0.43156018837248694\n",
      "train loss:0.5309505041849223\n",
      "train loss:0.5273816169643064\n",
      "train loss:0.44058151575227117\n",
      "train loss:0.5155971589904563\n",
      "train loss:0.5193293194452776\n",
      "train loss:0.7960972251707439\n",
      "train loss:0.5206264199977225\n",
      "train loss:0.7992044112906121\n",
      "train loss:0.5369374876705815\n",
      "train loss:0.7888349206829945\n",
      "train loss:0.6891718123173483\n",
      "train loss:0.5312030355963694\n",
      "train loss:0.6109565088843972\n",
      "train loss:0.5375164498366474\n",
      "train loss:0.45332968677511704\n",
      "train loss:0.6043126535798425\n",
      "train loss:0.6054421759346884\n",
      "train loss:0.7710673721281533\n",
      "train loss:0.8463802031117815\n",
      "train loss:0.7709811397224887\n",
      "train loss:0.7371091861909761\n",
      "train loss:0.6062232907788926\n",
      "train loss:0.5568958391912024\n",
      "train loss:0.6200145630628773\n",
      "train loss:0.553156879953425\n",
      "train loss:0.5646177782602015\n",
      "train loss:0.42315711886231766\n",
      "train loss:0.6826826492886736\n",
      "train loss:0.539365988659147\n",
      "train loss:0.7682110631074218\n",
      "train loss:0.5274068598870664\n",
      "train loss:0.5294755891280019\n",
      "train loss:0.6186053010189299\n",
      "train loss:0.7912240397535322\n",
      "train loss:0.5251878346163201\n",
      "train loss:0.8052422924690015\n",
      "train loss:0.5240364888287153\n",
      "train loss:0.8000315625615142\n",
      "train loss:0.3383662561451149\n",
      "train loss:0.41809152073649747\n",
      "train loss:0.7065253156853186\n",
      "train loss:0.4045680711217452\n",
      "train loss:1.0103789530733813\n",
      "=== epoch:9, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5248094074567204\n",
      "train loss:0.42186235617153595\n",
      "train loss:0.4072704067742995\n",
      "train loss:0.8814153197782615\n",
      "train loss:0.6874256815775113\n",
      "train loss:0.7930589457524991\n",
      "train loss:0.6136023190811704\n",
      "train loss:0.3561923118642606\n",
      "train loss:0.7003575886625781\n",
      "train loss:0.6170006749333832\n",
      "train loss:0.592835697822887\n",
      "train loss:0.7651359241903628\n",
      "train loss:0.6767816172843781\n",
      "train loss:0.5434314022138009\n",
      "train loss:0.5912272675102728\n",
      "train loss:0.5318591431848295\n",
      "train loss:0.5462817404971378\n",
      "train loss:0.5419229297591772\n",
      "train loss:0.6840331602223826\n",
      "train loss:0.6822402987801389\n",
      "train loss:0.6100812752781313\n",
      "train loss:0.44636922960416403\n",
      "train loss:0.7808519831531332\n",
      "train loss:0.5797086635494553\n",
      "train loss:0.6253107144111573\n",
      "train loss:0.6699884845784371\n",
      "train loss:0.5195751888914566\n",
      "train loss:0.6113186700840112\n",
      "train loss:0.776623374923665\n",
      "train loss:0.6121303001763981\n",
      "train loss:0.6107214464794155\n",
      "train loss:0.6067380525147702\n",
      "train loss:0.4456262658289655\n",
      "train loss:0.43415278657987944\n",
      "train loss:0.602042966944139\n",
      "train loss:0.30863959906103555\n",
      "train loss:0.51362241841944\n",
      "train loss:0.7380686554668472\n",
      "train loss:0.3845247951435672\n",
      "train loss:0.22751643229575672\n",
      "train loss:1.0630290076369984\n",
      "train loss:0.5107122442636144\n",
      "train loss:0.7663957321820406\n",
      "train loss:0.6302676007193813\n",
      "train loss:0.5029260566764047\n",
      "train loss:0.24926748472822755\n",
      "train loss:0.8484268564382174\n",
      "train loss:0.5004769320216708\n",
      "train loss:0.38210030235541814\n",
      "train loss:0.38618605043364684\n",
      "train loss:0.8498783511229565\n",
      "train loss:0.5154392202573146\n",
      "train loss:0.7178498362279809\n",
      "train loss:0.4993921682744949\n",
      "train loss:0.3029027615770173\n",
      "train loss:1.0230481440423014\n",
      "train loss:0.5079356363547111\n",
      "train loss:0.5161918700810674\n",
      "train loss:0.8462444091911145\n",
      "train loss:0.5321220415134992\n",
      "train loss:0.7684561305381663\n",
      "train loss:0.7428640498884398\n",
      "train loss:0.5519769110460947\n",
      "train loss:0.6891200011171998\n",
      "train loss:0.4994240580597589\n",
      "train loss:0.621590050720034\n",
      "train loss:0.4901199465142434\n",
      "train loss:0.6754504207487347\n",
      "train loss:0.496254040281912\n",
      "train loss:0.5430058973280764\n",
      "train loss:0.6063819132349859\n",
      "train loss:0.4488828514879584\n",
      "train loss:0.42975154595898585\n",
      "train loss:0.5060965050985151\n",
      "train loss:0.3849206972135437\n",
      "train loss:0.7410508667706677\n",
      "train loss:0.7485390903631596\n",
      "train loss:0.6303238045590744\n",
      "train loss:0.7555623102411391\n",
      "train loss:0.6336207384014958\n",
      "train loss:0.7373268913372524\n",
      "train loss:0.509317376224075\n",
      "train loss:0.3970756132012752\n",
      "train loss:0.7879882866988387\n",
      "train loss:0.7993009430823117\n",
      "train loss:0.7009592325407101\n",
      "train loss:0.7675273104424073\n",
      "train loss:0.6152255265159382\n",
      "train loss:0.6132873848774996\n",
      "train loss:0.6128874701497089\n",
      "train loss:0.6235054085624164\n",
      "train loss:0.6205818045239067\n",
      "train loss:0.7373343740791242\n",
      "train loss:0.677638225632829\n",
      "train loss:0.6260650516183995\n",
      "train loss:0.6291706849252092\n",
      "train loss:0.5817775099877182\n",
      "train loss:0.5732925476775264\n",
      "train loss:0.7739595937954487\n",
      "train loss:0.5683824470502545\n",
      "train loss:0.6633760469142098\n",
      "train loss:0.6699513969183275\n",
      "train loss:0.7371775402837505\n",
      "train loss:0.558314170040433\n",
      "train loss:0.6015098683497804\n",
      "train loss:0.535921224422368\n",
      "train loss:0.6142926650788261\n",
      "train loss:0.37882835663498315\n",
      "train loss:0.5984578469607793\n",
      "train loss:0.6870841690431057\n",
      "train loss:0.8698833271006923\n",
      "train loss:0.7987126785276375\n",
      "train loss:0.6763157637958159\n",
      "train loss:0.5981243073174152\n",
      "train loss:0.6920339484515223\n",
      "train loss:0.6724566544646143\n",
      "train loss:0.8153253705875032\n",
      "train loss:0.4789942455378326\n",
      "train loss:0.7383047327582383\n",
      "train loss:0.60951461084286\n",
      "train loss:0.7279960193742189\n",
      "train loss:0.6282733464066407\n",
      "train loss:0.6438978517456364\n",
      "train loss:0.5347510668817336\n",
      "train loss:0.6754547828107946\n",
      "train loss:0.6725434081810239\n",
      "train loss:0.5736246316934849\n",
      "train loss:0.772157922234957\n",
      "train loss:0.5202061547894999\n",
      "train loss:0.6769900519075553\n",
      "train loss:0.6170373052126407\n",
      "train loss:0.5006709952071541\n",
      "train loss:0.5474201314035662\n",
      "train loss:0.6121023413429605\n",
      "train loss:0.6952667712668541\n",
      "train loss:0.7745466604472687\n",
      "train loss:0.43520176502507\n",
      "train loss:0.5123697219034316\n",
      "train loss:0.7057107199194113\n",
      "train loss:0.6127169976802451\n",
      "train loss:0.5072126880438941\n",
      "train loss:0.5059744163844285\n",
      "train loss:0.8320325118078584\n",
      "train loss:0.6093357312936292\n",
      "train loss:0.9180121739277005\n",
      "train loss:0.8005240981524201\n",
      "train loss:0.5327387869375015\n",
      "train loss:0.576550520224037\n",
      "train loss:0.683139239174607\n",
      "train loss:0.47521393968534886\n",
      "train loss:0.8103158870966347\n",
      "train loss:0.7406077877682113\n",
      "train loss:0.4955268894736705\n",
      "=== epoch:10, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6768491225476307\n",
      "train loss:0.5685552348717443\n",
      "train loss:0.5591965499570513\n",
      "train loss:0.6192428495347113\n",
      "train loss:0.6132105012598257\n",
      "train loss:0.7334815901084658\n",
      "train loss:0.7225153582681944\n",
      "train loss:0.5649386322186096\n",
      "train loss:0.5551362006201704\n",
      "train loss:0.6173058484076162\n",
      "train loss:0.5305148641894843\n",
      "train loss:0.5791642180464472\n",
      "train loss:0.681655604894584\n",
      "train loss:0.6925465008246399\n",
      "train loss:0.7745807966891168\n",
      "train loss:0.6128632833799388\n",
      "train loss:0.5808800578339597\n",
      "train loss:0.5191961244643745\n",
      "train loss:0.7830473371941201\n",
      "train loss:0.6726994383400585\n",
      "train loss:0.6023956752715528\n",
      "train loss:0.59870524831115\n",
      "train loss:0.6262310591465031\n",
      "train loss:0.37515656304041445\n",
      "train loss:0.5954144655549909\n",
      "train loss:0.526451218707986\n",
      "train loss:0.6189815391213555\n",
      "train loss:0.6237387783666735\n",
      "train loss:0.6866222713980433\n",
      "train loss:0.7073337737181993\n",
      "train loss:0.4141676660581185\n",
      "train loss:0.513875374227062\n",
      "train loss:0.6211165372439738\n",
      "train loss:0.5097303674366153\n",
      "train loss:0.6226641732643408\n",
      "train loss:0.5055174028718739\n",
      "train loss:0.5011088868193443\n",
      "train loss:0.5903079605861523\n",
      "train loss:0.9205442177229883\n",
      "train loss:0.6173512557421077\n",
      "train loss:0.5137998595123439\n",
      "train loss:0.4022196519372259\n",
      "train loss:0.5062295766093325\n",
      "train loss:0.6124984770421645\n",
      "train loss:0.49621577214721924\n",
      "train loss:0.7167266298613132\n",
      "train loss:0.8190806331722718\n",
      "train loss:0.5210774329307242\n",
      "train loss:0.5947080438018326\n",
      "train loss:0.6078838056317883\n",
      "train loss:0.5583834396343217\n",
      "train loss:0.4269607703472905\n",
      "train loss:0.7927509654460934\n",
      "train loss:0.6273629153237253\n",
      "train loss:0.6063611051334522\n",
      "train loss:0.773986460251676\n",
      "train loss:0.7456301657997297\n",
      "train loss:0.4023714156375863\n",
      "train loss:0.6226890977200129\n",
      "train loss:0.6908132778681482\n",
      "train loss:0.614239550584608\n",
      "train loss:0.4697037839405883\n",
      "train loss:0.5325615030877182\n",
      "train loss:0.528713539101254\n",
      "train loss:0.69807921956031\n",
      "train loss:0.5250440387549711\n",
      "train loss:0.7630913970315064\n",
      "train loss:0.6114250607384993\n",
      "train loss:0.524383558147937\n",
      "train loss:0.7774764058586864\n",
      "train loss:0.49777724559290143\n",
      "train loss:0.613213088518361\n",
      "train loss:0.6062364902023567\n",
      "train loss:0.5034092974354729\n",
      "train loss:0.602857289851088\n",
      "train loss:0.5918385854578825\n",
      "train loss:0.3017168014192638\n",
      "train loss:0.393667083246723\n",
      "train loss:0.3754325603645464\n",
      "train loss:0.6465061643699446\n",
      "train loss:0.5978103615983156\n",
      "train loss:0.505100824756939\n",
      "train loss:0.4828641183800107\n",
      "train loss:1.052471120121462\n",
      "train loss:0.5946077594163525\n",
      "train loss:0.6054981311357965\n",
      "train loss:0.3900699136141368\n",
      "train loss:0.3871440627692165\n",
      "train loss:0.5816084446779233\n",
      "train loss:0.5102421478461628\n",
      "train loss:0.5869416880846835\n",
      "train loss:0.7174054452992145\n",
      "train loss:0.8111695167104342\n",
      "train loss:0.6153100507389613\n",
      "train loss:0.6891770657932602\n",
      "train loss:0.5281990706078161\n",
      "train loss:0.5470163602542876\n",
      "train loss:0.6135943877039235\n",
      "train loss:0.7653581212780904\n",
      "train loss:0.6323170480489709\n",
      "train loss:0.6080518797910874\n",
      "train loss:0.619779894680301\n",
      "train loss:0.6357115806635789\n",
      "train loss:0.5318384536309698\n",
      "train loss:0.5377125456384374\n",
      "train loss:0.6823735773527548\n",
      "train loss:0.8238490516060926\n",
      "train loss:0.6819999235685407\n",
      "train loss:0.5945082217216722\n",
      "train loss:0.6124633520254322\n",
      "train loss:0.5407789715423954\n",
      "train loss:0.4689393152452898\n",
      "train loss:0.7530224952957796\n",
      "train loss:0.5306518192397344\n",
      "train loss:0.6019849582014347\n",
      "train loss:0.6131985571852491\n",
      "train loss:0.5202904196980483\n",
      "train loss:0.5082566372345149\n",
      "train loss:0.6112432496339294\n",
      "train loss:0.897857485781118\n",
      "train loss:0.41114523454089474\n",
      "train loss:0.5204321414341851\n",
      "train loss:0.5274374697252167\n",
      "train loss:0.4985476761582432\n",
      "train loss:0.5067362721006659\n",
      "train loss:0.6498321686690783\n",
      "train loss:0.7297701686668375\n",
      "train loss:0.8410248066451608\n",
      "train loss:0.4868616297160749\n",
      "train loss:0.6165872141841825\n",
      "train loss:0.4913661109909426\n",
      "train loss:0.7857148783739891\n",
      "train loss:0.5118036600669337\n",
      "train loss:0.5905468188638093\n",
      "train loss:0.5228334858349571\n",
      "train loss:0.520753954270429\n",
      "train loss:0.5096613869738145\n",
      "train loss:0.6128692401948348\n",
      "train loss:0.41551215844878386\n",
      "train loss:0.5118037472599724\n",
      "train loss:0.8771844357859372\n",
      "train loss:0.7763958810299169\n",
      "train loss:0.6903340396679006\n",
      "train loss:0.5321312521929757\n",
      "train loss:0.44018623409257784\n",
      "train loss:0.5315211702692306\n",
      "train loss:0.495401538560963\n",
      "train loss:0.6219088934037234\n",
      "train loss:0.7779759123208605\n",
      "train loss:0.578294094860673\n",
      "train loss:0.6064342441861414\n",
      "train loss:0.42496180910036074\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5490196078431373\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.25, random_state=40)\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45e5b83d-b87d-4aba-a68d-611105d9190f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2944291463241457\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:2.286114191122709\n",
      "train loss:2.2602273122761987\n",
      "train loss:2.2286774683394883\n",
      "train loss:2.1725467791755277\n",
      "train loss:2.1280821619469386\n",
      "train loss:2.019959157296381\n",
      "train loss:2.0094505746799536\n",
      "train loss:1.6790661036956211\n",
      "train loss:1.5389636176583399\n",
      "train loss:1.3921820523209143\n",
      "train loss:1.105583998400359\n",
      "train loss:0.8559330355825635\n",
      "train loss:0.9455569539029837\n",
      "train loss:0.855584597249518\n",
      "train loss:1.1158071108773684\n",
      "train loss:0.5224483027316812\n",
      "train loss:0.6358725289617732\n",
      "train loss:0.6372916280601062\n",
      "train loss:0.5481721124321514\n",
      "train loss:0.22073918376027546\n",
      "train loss:0.5079224870147353\n",
      "train loss:1.0867919570132982\n",
      "train loss:0.6545479250156514\n",
      "train loss:0.48146539988622694\n",
      "train loss:0.2985185410910263\n",
      "train loss:0.3751014434070157\n",
      "train loss:0.6594143557406444\n",
      "train loss:0.23204699259267286\n",
      "train loss:0.970979294409586\n",
      "train loss:0.6337548026912694\n",
      "train loss:0.7126471772167089\n",
      "train loss:0.7486998265136386\n",
      "train loss:0.5712896019145521\n",
      "train loss:0.7016581911545516\n",
      "train loss:0.6442912093932633\n",
      "train loss:0.5727184791518788\n",
      "train loss:0.577538872936168\n",
      "train loss:0.7621689436443644\n",
      "train loss:0.5378827385099031\n",
      "train loss:0.6121173697413084\n",
      "train loss:0.6252498496574547\n",
      "train loss:0.4825083032581167\n",
      "train loss:0.34712003106924877\n",
      "train loss:0.6799025221330126\n",
      "train loss:0.984206913644036\n",
      "train loss:0.6107840526244543\n",
      "train loss:0.6475865388873441\n",
      "train loss:0.9715191583872176\n",
      "train loss:0.6002052221117041\n",
      "train loss:0.6412760408339209\n",
      "train loss:0.6751191888015181\n",
      "train loss:0.6846371887097843\n",
      "train loss:0.6416494845327954\n",
      "train loss:0.5647493836442292\n",
      "train loss:0.6312331445641182\n",
      "train loss:0.6141355045699094\n",
      "train loss:0.8199762678954461\n",
      "train loss:0.6159364174051124\n",
      "train loss:0.4900884842227928\n",
      "train loss:0.4568330158239206\n",
      "train loss:0.8313855375984376\n",
      "train loss:0.8763055804006497\n",
      "train loss:0.605597907640019\n",
      "train loss:0.45917117815561326\n",
      "train loss:0.2654888866372652\n",
      "train loss:0.8982579959921437\n",
      "train loss:0.6282027497742295\n",
      "train loss:0.5163524786050598\n",
      "train loss:0.7653890166816203\n",
      "train loss:0.5730555179168044\n",
      "train loss:0.5302099943704519\n",
      "train loss:0.6229332262954564\n",
      "train loss:0.6035786414436402\n",
      "train loss:0.6156262619881225\n",
      "train loss:0.5296340152839151\n",
      "train loss:0.5920169640288788\n",
      "train loss:0.6277994005759566\n",
      "train loss:0.7668560780934277\n",
      "train loss:0.6040023746285436\n",
      "train loss:0.4296402591909609\n",
      "train loss:0.5428573960480731\n",
      "train loss:0.48859508824856446\n",
      "train loss:0.49654025519199896\n",
      "train loss:0.7358718983708182\n",
      "train loss:0.876321482051099\n",
      "train loss:0.4018509235460769\n",
      "train loss:0.4110177153775367\n",
      "train loss:0.5269500866338684\n",
      "train loss:0.6211792695981749\n",
      "train loss:0.5033141464728415\n",
      "train loss:0.3622235588618136\n",
      "train loss:0.5063271540393239\n",
      "train loss:0.7613376120282226\n",
      "train loss:0.3500293088540253\n",
      "train loss:0.8995150088982063\n",
      "train loss:0.8367223346950109\n",
      "train loss:0.4156563275035909\n",
      "train loss:0.8120067731144267\n",
      "train loss:0.7011051064123552\n",
      "train loss:0.5045920630259344\n",
      "train loss:0.6579485123491376\n",
      "train loss:0.5295886576578787\n",
      "train loss:0.6284784610290377\n",
      "train loss:0.7031188008336249\n",
      "train loss:0.6270757057116939\n",
      "train loss:0.6816159990926547\n",
      "train loss:0.6992226764832807\n",
      "train loss:0.6691305599406838\n",
      "train loss:0.5877363106579228\n",
      "train loss:0.6274005392236103\n",
      "train loss:0.6150923063643752\n",
      "train loss:0.5029281552806615\n",
      "train loss:0.5199345350346181\n",
      "train loss:0.7207646680693724\n",
      "train loss:0.4454135659764454\n",
      "train loss:0.47678845804254893\n",
      "train loss:0.5102999751367855\n",
      "train loss:0.21323232580423604\n",
      "train loss:0.5267636051048447\n",
      "train loss:0.5437102861446936\n",
      "train loss:0.7684344387096991\n",
      "train loss:0.6828075132110172\n",
      "train loss:1.0311304307192495\n",
      "train loss:0.6210931613353803\n",
      "train loss:0.7447762116615281\n",
      "train loss:0.2669521441797375\n",
      "train loss:0.5168720575069882\n",
      "train loss:0.6089129010411074\n",
      "train loss:0.6951324792977169\n",
      "train loss:0.821910831616677\n",
      "train loss:0.4179644378679421\n",
      "train loss:0.4852801471722682\n",
      "train loss:0.4853980024531497\n",
      "train loss:0.6248419278874706\n",
      "train loss:0.6039670263992772\n",
      "train loss:0.6238845026642235\n",
      "train loss:0.8367539317914835\n",
      "train loss:0.46676399094439025\n",
      "train loss:0.5417982090747\n",
      "train loss:0.535447441477993\n",
      "train loss:0.48772735429409175\n",
      "train loss:0.6147683123415089\n",
      "train loss:0.5103495801734236\n",
      "train loss:0.5938118943142838\n",
      "train loss:0.6127430097218224\n",
      "train loss:0.7675263149263175\n",
      "train loss:0.7317931006291472\n",
      "train loss:0.6100514468027673\n",
      "train loss:0.7148703382601901\n",
      "train loss:0.6346112369755648\n",
      "train loss:0.6163329707412236\n",
      "train loss:0.890039422698724\n",
      "train loss:0.7516409943704152\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6271149126771156\n",
      "train loss:0.6336860873779692\n",
      "train loss:0.6771901871544854\n",
      "train loss:0.6242106380687001\n",
      "train loss:0.6444574136612323\n",
      "train loss:0.6184836920255135\n",
      "train loss:0.644828399491759\n",
      "train loss:0.5926845014780721\n",
      "train loss:0.7528021981844786\n",
      "train loss:0.6124323188484795\n",
      "train loss:0.6253184258280899\n",
      "train loss:0.6686853447915355\n",
      "train loss:0.6844026378303176\n",
      "train loss:0.5524901477144352\n",
      "train loss:0.4685490331449519\n",
      "train loss:0.5837715618784673\n",
      "train loss:0.7073451929354857\n",
      "train loss:0.539842943629065\n",
      "train loss:0.6377642145380296\n",
      "train loss:0.958196207584377\n",
      "train loss:0.541520036162358\n",
      "train loss:0.7300865286346913\n",
      "train loss:0.6142443520466537\n",
      "train loss:0.7649864644967983\n",
      "train loss:0.44010801751274775\n",
      "train loss:0.7678774582497792\n",
      "train loss:0.6258751654314533\n",
      "train loss:0.6765100855769838\n",
      "train loss:0.6073020607342499\n",
      "train loss:0.6241680744215189\n",
      "train loss:0.4281573662478957\n",
      "train loss:0.6285854763215564\n",
      "train loss:0.47617521452399253\n",
      "train loss:0.457279227209039\n",
      "train loss:0.7521793866650753\n",
      "train loss:0.5310262653343967\n",
      "train loss:0.4939412064763717\n",
      "train loss:0.3974061617478275\n",
      "train loss:0.6670303945685345\n",
      "train loss:0.499373410522182\n",
      "train loss:0.6281419433021681\n",
      "train loss:0.3694863749219677\n",
      "train loss:0.6671294267583958\n",
      "train loss:0.5118059111874779\n",
      "train loss:0.6339826054816219\n",
      "train loss:0.6030599045316303\n",
      "train loss:0.5096045507314002\n",
      "train loss:0.6203250323738995\n",
      "train loss:0.766104201315161\n",
      "train loss:0.8974664690205213\n",
      "train loss:0.4042692583520453\n",
      "train loss:0.6997711274399503\n",
      "train loss:0.6285523556544993\n",
      "train loss:0.6152546255521208\n",
      "train loss:0.620519092107058\n",
      "train loss:0.6140412938899045\n",
      "train loss:0.569766516847013\n",
      "train loss:0.6802397863065996\n",
      "train loss:0.5812414091886583\n",
      "train loss:0.6173240159665947\n",
      "train loss:0.629951153202079\n",
      "train loss:0.4437328668501472\n",
      "train loss:0.4893290045895702\n",
      "train loss:0.5353660601581602\n",
      "train loss:0.4301167262326322\n",
      "train loss:0.5844996335271827\n",
      "train loss:0.3755331386916486\n",
      "train loss:0.7416452067711912\n",
      "train loss:0.35001702767995707\n",
      "train loss:0.5061822268182815\n",
      "train loss:0.8611546171050882\n",
      "train loss:0.16122211691082208\n",
      "train loss:0.8925872659171628\n",
      "train loss:0.8344692702667478\n",
      "train loss:0.488595808805282\n",
      "train loss:0.8225507024974166\n",
      "train loss:0.6371414277900918\n",
      "train loss:0.9442330891919204\n",
      "train loss:0.5274469783910377\n",
      "train loss:0.6871010836207796\n",
      "train loss:0.6024437087712419\n",
      "train loss:0.630727666734889\n",
      "train loss:0.6223008449367609\n",
      "train loss:0.6352881499423257\n",
      "train loss:0.647794094137432\n",
      "train loss:0.5519730983149532\n",
      "train loss:0.6830814819319756\n",
      "train loss:0.548441776300326\n",
      "train loss:0.5362310914302133\n",
      "train loss:0.6954031653120123\n",
      "train loss:0.5456669264766105\n",
      "train loss:0.5030586585979219\n",
      "train loss:0.5450591131381939\n",
      "train loss:0.798177243154087\n",
      "train loss:0.5351361543278113\n",
      "train loss:0.5308194405630837\n",
      "train loss:0.4999920214583571\n",
      "train loss:0.5155901008550032\n",
      "train loss:0.8740486548775029\n",
      "train loss:0.5140914282133867\n",
      "train loss:0.5135730273455742\n",
      "train loss:0.3438351868870098\n",
      "train loss:0.772373143456768\n",
      "train loss:0.7499971219419704\n",
      "train loss:0.6532034543895243\n",
      "train loss:0.624531678442372\n",
      "train loss:0.39859600363801295\n",
      "train loss:0.5072405670631234\n",
      "train loss:0.7226535226556887\n",
      "train loss:0.5196209553108407\n",
      "train loss:0.40852058873125285\n",
      "train loss:0.4068635384487454\n",
      "train loss:0.40702119569589373\n",
      "train loss:0.6164430808115435\n",
      "train loss:0.6455278752080482\n",
      "train loss:0.6233208243526998\n",
      "train loss:0.776726432112735\n",
      "train loss:0.3703913157401432\n",
      "train loss:0.6214426358032012\n",
      "train loss:0.8392977764959\n",
      "train loss:0.7135403383836872\n",
      "train loss:0.5251489610701737\n",
      "train loss:0.6095945069587482\n",
      "train loss:0.6083252357889702\n",
      "train loss:0.6121096469474543\n",
      "train loss:0.6150665692384981\n",
      "train loss:0.5377402803267145\n",
      "train loss:0.8187969004424464\n",
      "train loss:0.5565002404949931\n",
      "train loss:0.6789647674298537\n",
      "train loss:0.6117926550500723\n",
      "train loss:0.7372897184583118\n",
      "train loss:0.5040014175167294\n",
      "train loss:0.559597246051857\n",
      "train loss:0.6151731086271744\n",
      "train loss:0.6222498584624239\n",
      "train loss:0.8075904701181607\n",
      "train loss:0.6246382794176953\n",
      "train loss:0.682677028184376\n",
      "train loss:0.6255083482762273\n",
      "train loss:0.6903508842753667\n",
      "train loss:0.5631658724752445\n",
      "train loss:0.7482525054164236\n",
      "train loss:0.6189771502370631\n",
      "train loss:0.6228861822270959\n",
      "train loss:0.6263363731908543\n",
      "train loss:0.6749697270292911\n",
      "train loss:0.5643621616957069\n",
      "train loss:0.6836249627110119\n",
      "train loss:0.739063427766957\n",
      "train loss:0.49571654794898556\n",
      "train loss:0.5417282471166901\n",
      "train loss:0.5275984076199981\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6812016145597196\n",
      "train loss:0.5237561839912215\n",
      "train loss:0.5986132491690249\n",
      "train loss:0.5042189399233787\n",
      "train loss:0.27896859776089733\n",
      "train loss:0.26613602894870714\n",
      "train loss:0.9278312136580474\n",
      "train loss:0.7734173380860075\n",
      "train loss:0.5111947209377197\n",
      "train loss:0.624670944567891\n",
      "train loss:0.48107548509886133\n",
      "train loss:0.6222315426411511\n",
      "train loss:0.64322487700505\n",
      "train loss:0.6253490338924212\n",
      "train loss:0.635001717761341\n",
      "train loss:0.7120576064100581\n",
      "train loss:0.637517746979123\n",
      "train loss:0.6841461985261954\n",
      "train loss:0.6122179048542417\n",
      "train loss:0.5407502416427509\n",
      "train loss:0.7141822417666083\n",
      "train loss:0.6091835191266571\n",
      "train loss:0.5664750860104593\n",
      "train loss:0.6332555576820909\n",
      "train loss:0.5110399658087312\n",
      "train loss:0.43063639687485217\n",
      "train loss:0.6257048620683663\n",
      "train loss:0.6073927235810155\n",
      "train loss:0.6115340937273798\n",
      "train loss:0.528739468110592\n",
      "train loss:0.7066815313768106\n",
      "train loss:0.3223906409008619\n",
      "train loss:0.8377115834808239\n",
      "train loss:0.719647922235609\n",
      "train loss:0.6970628751506394\n",
      "train loss:0.6284147764851762\n",
      "train loss:0.5889700916139219\n",
      "train loss:0.41022955227541\n",
      "train loss:0.3903229413413817\n",
      "train loss:0.27955536981314133\n",
      "train loss:0.7473077877148782\n",
      "train loss:0.739511095331336\n",
      "train loss:0.5132493443405884\n",
      "train loss:1.0193114954543347\n",
      "train loss:0.8121086415507145\n",
      "train loss:0.5344125933604118\n",
      "train loss:0.6134578235916262\n",
      "train loss:0.766134727779388\n",
      "train loss:0.6115315584004684\n",
      "train loss:0.6837529514789972\n",
      "train loss:0.6871669982349993\n",
      "train loss:0.5791362093760972\n",
      "train loss:0.6373502332233437\n",
      "train loss:0.4979722496030609\n",
      "train loss:0.7646369287532137\n",
      "train loss:0.543796826542823\n",
      "train loss:0.6727938541619218\n",
      "train loss:0.5757317773928352\n",
      "train loss:0.5625652435718795\n",
      "train loss:0.6197966318855571\n",
      "train loss:0.6326876463241236\n",
      "train loss:0.6672134793949362\n",
      "train loss:0.45609942304838935\n",
      "train loss:0.5394456094894822\n",
      "train loss:0.3233733105760992\n",
      "train loss:0.3938627956970861\n",
      "train loss:1.0971154088009936\n",
      "train loss:0.6479590226838285\n",
      "train loss:0.6358410560231947\n",
      "train loss:0.6229813699487899\n",
      "train loss:0.8622502335133342\n",
      "train loss:0.6147195626549005\n",
      "train loss:0.6061864700490542\n",
      "train loss:0.5200433106559088\n",
      "train loss:0.5092613333045354\n",
      "train loss:0.43231650118189313\n",
      "train loss:0.5125344332672768\n",
      "train loss:0.7013057914469878\n",
      "train loss:0.5916341273913852\n",
      "train loss:0.5129721801135456\n",
      "train loss:0.7107332797351891\n",
      "train loss:0.6062962588017691\n",
      "train loss:0.6023600376019964\n",
      "train loss:0.6042581796014383\n",
      "train loss:0.6966162758537008\n",
      "train loss:0.7898055344810936\n",
      "train loss:0.441789320595339\n",
      "train loss:0.709839188326203\n",
      "train loss:0.5292967374593541\n",
      "train loss:0.536349389908707\n",
      "train loss:0.6857029856418162\n",
      "train loss:0.6153208493450412\n",
      "train loss:0.539728787972317\n",
      "train loss:0.7818056613456222\n",
      "train loss:0.8220894623654436\n",
      "train loss:0.623572530193757\n",
      "train loss:0.48529517844145004\n",
      "train loss:0.9121626104050756\n",
      "train loss:0.5625477199965281\n",
      "train loss:0.5636375647502564\n",
      "train loss:0.6197996553966474\n",
      "train loss:0.5699905656645828\n",
      "train loss:0.5692633647065322\n",
      "train loss:0.5488640321003665\n",
      "train loss:0.6195029362501916\n",
      "train loss:0.7618331375534628\n",
      "train loss:0.6822272380819202\n",
      "train loss:0.550837096097196\n",
      "train loss:0.7552815262320791\n",
      "train loss:0.5387975699849129\n",
      "train loss:0.6246997245930302\n",
      "train loss:0.8411561558090268\n",
      "train loss:0.4676434274472941\n",
      "train loss:0.6230057959557881\n",
      "train loss:0.5383181166320188\n",
      "train loss:0.6086753698106149\n",
      "train loss:0.6827281184858759\n",
      "train loss:0.6214378164521127\n",
      "train loss:0.8412059825369751\n",
      "train loss:0.6454608915443447\n",
      "train loss:0.460308941561446\n",
      "train loss:0.6839020225246506\n",
      "train loss:0.46615828387559227\n",
      "train loss:0.6033706846627449\n",
      "train loss:0.4531546605028155\n",
      "train loss:0.42668847496531914\n",
      "train loss:0.8213333404884606\n",
      "train loss:0.516194755064862\n",
      "train loss:0.5052510474325291\n",
      "train loss:0.7269644702737604\n",
      "train loss:0.6353364338865445\n",
      "train loss:0.7117028869690147\n",
      "train loss:0.7184570150662157\n",
      "train loss:0.4079022682592156\n",
      "train loss:0.41149946482005345\n",
      "train loss:0.6165193473298437\n",
      "train loss:0.3970886752603213\n",
      "train loss:0.38079572356936864\n",
      "train loss:0.8502873784615079\n",
      "train loss:0.6209293181803672\n",
      "train loss:0.8358867442977566\n",
      "train loss:0.7367656500339568\n",
      "train loss:0.5194135799770002\n",
      "train loss:0.6963062357226791\n",
      "train loss:0.43413097359377983\n",
      "train loss:0.7846039544212374\n",
      "train loss:0.6176168852921833\n",
      "train loss:0.6083776670250253\n",
      "train loss:0.6831821258021302\n",
      "train loss:0.5533401048936406\n",
      "train loss:0.5471274239635229\n",
      "train loss:0.6811387711972213\n",
      "train loss:0.5587735188572167\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6104405006652307\n",
      "train loss:0.8069452464869202\n",
      "train loss:0.6285449180648585\n",
      "train loss:0.6708982226100926\n",
      "train loss:0.6170991409690038\n",
      "train loss:0.5593878186605481\n",
      "train loss:0.7967838323173315\n",
      "train loss:0.5674253910978269\n",
      "train loss:0.7283556442681443\n",
      "train loss:0.504293826995199\n",
      "train loss:0.5544504788307816\n",
      "train loss:0.6098084127223833\n",
      "train loss:0.6786322638341502\n",
      "train loss:0.6122025381635171\n",
      "train loss:0.687872919607228\n",
      "train loss:0.6056815148907279\n",
      "train loss:0.6851503182685824\n",
      "train loss:0.5291513474020761\n",
      "train loss:0.626329826012955\n",
      "train loss:0.6076932233096828\n",
      "train loss:0.6805614488628495\n",
      "train loss:0.5293401627726312\n",
      "train loss:0.7766261364374174\n",
      "train loss:0.5095147393048672\n",
      "train loss:0.5246503627964423\n",
      "train loss:0.8733210525352899\n",
      "train loss:0.5255613062569598\n",
      "train loss:0.5344419043219213\n",
      "train loss:0.6120032328350163\n",
      "train loss:0.43152279633695595\n",
      "train loss:0.7817530464746353\n",
      "train loss:0.6952213394822195\n",
      "train loss:0.43181823518295614\n",
      "train loss:0.5136342148188088\n",
      "train loss:0.6081262258766448\n",
      "train loss:0.6086968680545455\n",
      "train loss:0.699708787220358\n",
      "train loss:0.7122531431155449\n",
      "train loss:0.41710039336511767\n",
      "train loss:0.41632298879763496\n",
      "train loss:0.90440463867972\n",
      "train loss:0.6195940558554959\n",
      "train loss:0.7070560148069791\n",
      "train loss:0.3355148910544406\n",
      "train loss:0.5175448435621183\n",
      "train loss:0.4127802980814462\n",
      "train loss:0.611104926381282\n",
      "train loss:0.528471615371022\n",
      "train loss:0.49840553659945497\n",
      "train loss:0.5010186342845107\n",
      "train loss:0.6108711680257626\n",
      "train loss:0.6364209632978921\n",
      "train loss:0.7551010745564559\n",
      "train loss:0.5001614333214894\n",
      "train loss:0.8469689430020949\n",
      "train loss:0.7021111816078803\n",
      "train loss:0.4148687942554684\n",
      "train loss:0.691824018508305\n",
      "train loss:0.5234662027190148\n",
      "train loss:0.6055869084316109\n",
      "train loss:0.6120009704246886\n",
      "train loss:0.5317453674536331\n",
      "train loss:0.4467532709926486\n",
      "train loss:0.8463489404671215\n",
      "train loss:0.6129079593014607\n",
      "train loss:0.6858169500817433\n",
      "train loss:0.7513544118050361\n",
      "train loss:0.6120299368758009\n",
      "train loss:0.4774752166267825\n",
      "train loss:0.5515758921407354\n",
      "train loss:0.6170285361618232\n",
      "train loss:0.6957423788531306\n",
      "train loss:0.473672354536816\n",
      "train loss:0.6142537296874359\n",
      "train loss:0.6056113081397666\n",
      "train loss:0.613720331837787\n",
      "train loss:0.6090830637308504\n",
      "train loss:0.6871615516135477\n",
      "train loss:0.7003789104560495\n",
      "train loss:0.4233910224483445\n",
      "train loss:0.5125983876783812\n",
      "train loss:0.7031267321070989\n",
      "train loss:0.30138602031485057\n",
      "train loss:0.5074794387640127\n",
      "train loss:0.6211605231236261\n",
      "train loss:0.5040976770213643\n",
      "train loss:0.3737445569098937\n",
      "train loss:0.6182380386775667\n",
      "train loss:0.7967455022429613\n",
      "train loss:0.3589899571264001\n",
      "train loss:0.36765054845090506\n",
      "train loss:0.5219081719136003\n",
      "train loss:0.8029237806220918\n",
      "train loss:0.4921927057340457\n",
      "train loss:0.35292343961453093\n",
      "train loss:1.1579863759035245\n",
      "train loss:0.3688243700401538\n",
      "train loss:0.6171988015453784\n",
      "train loss:0.3942112761089747\n",
      "train loss:0.6091866066281807\n",
      "train loss:0.3078931901994337\n",
      "train loss:0.401073154236595\n",
      "train loss:0.40090030457363673\n",
      "train loss:0.6173196825950018\n",
      "train loss:0.5008534074882737\n",
      "train loss:0.5200396593663131\n",
      "train loss:0.6240597017807545\n",
      "train loss:0.5134448385093139\n",
      "train loss:0.6108053719483425\n",
      "train loss:0.5054868722743548\n",
      "train loss:0.6175926724207081\n",
      "train loss:0.8714376222546646\n",
      "train loss:0.6410801554510174\n",
      "train loss:0.6178042164625525\n",
      "train loss:0.5249644871387663\n",
      "train loss:0.6008885634430149\n",
      "train loss:0.7019340584864528\n",
      "train loss:0.6874437249396099\n",
      "train loss:0.4602152882019629\n",
      "train loss:0.6168771247329741\n",
      "train loss:0.5480130559704992\n",
      "train loss:0.47174725684217556\n",
      "train loss:0.3790492545888885\n",
      "train loss:0.516541829852472\n",
      "train loss:0.6190268894551822\n",
      "train loss:0.6178300948737094\n",
      "train loss:0.4062425819541538\n",
      "train loss:0.6269234456039523\n",
      "train loss:0.615998037661965\n",
      "train loss:0.5047308067568445\n",
      "train loss:0.7456703345902969\n",
      "train loss:0.3610805165796368\n",
      "train loss:0.6195713531468158\n",
      "train loss:0.6287236005403534\n",
      "train loss:0.4828530899452124\n",
      "train loss:0.6450754488470773\n",
      "train loss:0.35808163392421705\n",
      "train loss:0.8924641528321807\n",
      "train loss:0.5080263548255888\n",
      "train loss:0.6153084721200093\n",
      "train loss:0.40626775580982877\n",
      "train loss:0.5107094383345994\n",
      "train loss:0.5223075995449504\n",
      "train loss:0.5105619512842481\n",
      "train loss:0.7194015067679438\n",
      "train loss:0.2776759680639035\n",
      "train loss:0.627828914215768\n",
      "train loss:0.6381181441687831\n",
      "train loss:0.72154903201271\n",
      "train loss:0.6135650222069717\n",
      "train loss:0.5019516806011696\n",
      "train loss:0.7262494915105224\n",
      "train loss:0.6007905135676608\n",
      "=== epoch:5, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7082003283807012\n",
      "train loss:0.6203107728194084\n",
      "train loss:0.6114218009666563\n",
      "train loss:0.685413807205139\n",
      "train loss:0.6193103229262943\n",
      "train loss:0.7429632129551617\n",
      "train loss:0.7373268746957514\n",
      "train loss:0.6282723883354047\n",
      "train loss:0.672626343396816\n",
      "train loss:0.5336683948242604\n",
      "train loss:0.580369761716034\n",
      "train loss:0.5237111732942652\n",
      "train loss:0.6757280624784416\n",
      "train loss:0.6196086633565802\n",
      "train loss:0.5608149600103622\n",
      "train loss:0.6733827009852149\n",
      "train loss:0.4867436708837542\n",
      "train loss:0.673918649208567\n",
      "train loss:0.6026711161111674\n",
      "train loss:0.6115066901083286\n",
      "train loss:0.5182661403041596\n",
      "train loss:0.6112258525749396\n",
      "train loss:0.6116313446640179\n",
      "train loss:0.8119802384349273\n",
      "train loss:0.6060023328265646\n",
      "train loss:0.5010375964541001\n",
      "train loss:0.5143265030296008\n",
      "train loss:0.39573275699843463\n",
      "train loss:0.8412129970439253\n",
      "train loss:0.9090934157492827\n",
      "train loss:0.7957211490988865\n",
      "train loss:0.6160032724667003\n",
      "train loss:0.610982665150871\n",
      "train loss:0.539539989306064\n",
      "train loss:0.8309259731018702\n",
      "train loss:0.5644238002542099\n",
      "train loss:0.6161241180295878\n",
      "train loss:0.6289000011156236\n",
      "train loss:0.6731270987515325\n",
      "train loss:0.6233254886648869\n",
      "train loss:0.5706300686401519\n",
      "train loss:0.5788620052525405\n",
      "train loss:0.6265687458972031\n",
      "train loss:0.7267413603915119\n",
      "train loss:0.5714573261761668\n",
      "train loss:0.4997874021153635\n",
      "train loss:0.7992840264243579\n",
      "train loss:0.5525151884246978\n",
      "train loss:0.5388512704956367\n",
      "train loss:0.5417313112761033\n",
      "train loss:0.6812604225294333\n",
      "train loss:0.5211573713271779\n",
      "train loss:0.7811931649977908\n",
      "train loss:0.6943491589070744\n",
      "train loss:0.6890385103401185\n",
      "train loss:0.7060387077312573\n",
      "train loss:0.35311017775605913\n",
      "train loss:0.6036698960278047\n",
      "train loss:0.5139613890324686\n",
      "train loss:0.780100404811596\n",
      "train loss:0.5169454027020379\n",
      "train loss:0.6067855273479486\n",
      "train loss:0.526034168716471\n",
      "train loss:0.4962819265958098\n",
      "train loss:0.7178864083966654\n",
      "train loss:0.8048712602952495\n",
      "train loss:0.5142667386076318\n",
      "train loss:0.6122748592558253\n",
      "train loss:0.6121069395443655\n",
      "train loss:0.6171046610411838\n",
      "train loss:0.7024431663117383\n",
      "train loss:0.6082495082101733\n",
      "train loss:0.4490204270003118\n",
      "train loss:0.527792589693798\n",
      "train loss:0.698630417158687\n",
      "train loss:0.8525245978123557\n",
      "train loss:0.7732556062545413\n",
      "train loss:0.6083118143936048\n",
      "train loss:0.5558034792267096\n",
      "train loss:0.6109539772555572\n",
      "train loss:0.7281215655152922\n",
      "train loss:0.500105929102462\n",
      "train loss:0.6238003146666331\n",
      "train loss:0.5610017638469966\n",
      "train loss:0.7398466948618205\n",
      "train loss:0.6701790232810014\n",
      "train loss:0.7374623885525275\n",
      "train loss:0.732626718341141\n",
      "train loss:0.5670153107856843\n",
      "train loss:0.6734357191498235\n",
      "train loss:0.5721154899712964\n",
      "train loss:0.6154233342812355\n",
      "train loss:0.6156652021968331\n",
      "train loss:0.4981962131437032\n",
      "train loss:0.6105662693160935\n",
      "train loss:0.5459054332999476\n",
      "train loss:0.7690485283923332\n",
      "train loss:0.5290782995697001\n",
      "train loss:0.5232969464980942\n",
      "train loss:0.6049194044864605\n",
      "train loss:0.6004838802352441\n",
      "train loss:0.6184432647212936\n",
      "train loss:0.6066827405397952\n",
      "train loss:0.6187026616573588\n",
      "train loss:0.5062099638253288\n",
      "train loss:0.5078903912691904\n",
      "train loss:0.3869557255783393\n",
      "train loss:0.8625576653345334\n",
      "train loss:0.9755091439931439\n",
      "train loss:0.7144908974064506\n",
      "train loss:0.5013988958088145\n",
      "train loss:0.6982138207040629\n",
      "train loss:0.5280479237139959\n",
      "train loss:0.45208389123766235\n",
      "train loss:0.5326498882073815\n",
      "train loss:0.44597909955598236\n",
      "train loss:0.5285075734300017\n",
      "train loss:0.5167496857765462\n",
      "train loss:0.8043230827638809\n",
      "train loss:0.32758576055572736\n",
      "train loss:0.39963587168156817\n",
      "train loss:0.725375040771005\n",
      "train loss:0.6202229499988731\n",
      "train loss:0.7343707132774309\n",
      "train loss:0.5130737610549698\n",
      "train loss:0.8173226027264933\n",
      "train loss:0.7190133241584817\n",
      "train loss:0.6170649952828338\n",
      "train loss:0.697794493053275\n",
      "train loss:0.8471691303562447\n",
      "train loss:0.6116906846839341\n",
      "train loss:0.7400897931924868\n",
      "train loss:0.5613913666176622\n",
      "train loss:0.5140583570536927\n",
      "train loss:0.5804313509431314\n",
      "train loss:0.5152846760356058\n",
      "train loss:0.5666927889625357\n",
      "train loss:0.5650029684921606\n",
      "train loss:0.49253538790730483\n",
      "train loss:0.4755544859351154\n",
      "train loss:0.5359277356820153\n",
      "train loss:0.7024085761190321\n",
      "train loss:0.7815087272835809\n",
      "train loss:0.5140897519487596\n",
      "train loss:0.7092774972309794\n",
      "train loss:0.6116065826923947\n",
      "train loss:0.2992768885298759\n",
      "train loss:0.51227925696645\n",
      "train loss:0.6279762986187981\n",
      "train loss:0.8405936979207898\n",
      "train loss:0.5018004087993175\n",
      "train loss:0.6328421434721179\n",
      "train loss:0.5130532160296879\n",
      "=== epoch:6, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.3839324720276758\n",
      "train loss:0.7398370652618265\n",
      "train loss:0.6174339780137243\n",
      "train loss:0.7211779136530863\n",
      "train loss:0.8346122684631393\n",
      "train loss:0.701069437025783\n",
      "train loss:0.4338532676531431\n",
      "train loss:0.6937851550388099\n",
      "train loss:0.5319955432525486\n",
      "train loss:0.6022346231388053\n",
      "train loss:0.4610328597787306\n",
      "train loss:0.5384292461342917\n",
      "train loss:0.5338827780161869\n",
      "train loss:0.4549256111969197\n",
      "train loss:0.6191430099986535\n",
      "train loss:0.4332324235716027\n",
      "train loss:0.5955134897449081\n",
      "train loss:0.610311575398697\n",
      "train loss:0.5014004242410051\n",
      "train loss:0.6098347316320407\n",
      "train loss:0.7411921969612225\n",
      "train loss:0.3857748851646986\n",
      "train loss:0.856719330723093\n",
      "train loss:0.7326037889192548\n",
      "train loss:0.3896468491673527\n",
      "train loss:0.3928542036544499\n",
      "train loss:0.6203973560569599\n",
      "train loss:0.6197053469447092\n",
      "train loss:0.6259503702116959\n",
      "train loss:0.28203688383316605\n",
      "train loss:0.6112350150339859\n",
      "train loss:0.3894826049536932\n",
      "train loss:0.6161331619240678\n",
      "train loss:0.9660174239159852\n",
      "train loss:0.6163197592484039\n",
      "train loss:0.388586176412392\n",
      "train loss:0.3993501925661628\n",
      "train loss:0.5037911143334821\n",
      "train loss:0.6132968600088631\n",
      "train loss:0.7363890780832474\n",
      "train loss:0.6148485546015946\n",
      "train loss:0.8211915844890649\n",
      "train loss:0.5327526498936122\n",
      "train loss:0.426408807626067\n",
      "train loss:0.607908369815046\n",
      "train loss:0.5178115671980417\n",
      "train loss:0.7821509964114115\n",
      "train loss:0.6913173493798103\n",
      "train loss:0.4422918975327127\n",
      "train loss:0.7595169018756803\n",
      "train loss:0.611693622516654\n",
      "train loss:0.6839522445857042\n",
      "train loss:0.7463932738085163\n",
      "train loss:0.6779931199561963\n",
      "train loss:0.677655014255261\n",
      "train loss:0.6726341925400714\n",
      "train loss:0.5675498551149755\n",
      "train loss:0.6235633026393372\n",
      "train loss:0.5654528571865937\n",
      "train loss:0.7301196173300147\n",
      "train loss:0.5719451146805078\n",
      "train loss:0.6743022201776007\n",
      "train loss:0.5648036493793375\n",
      "train loss:0.5648958785800807\n",
      "train loss:0.6162425002892015\n",
      "train loss:0.47969774050275243\n",
      "train loss:0.6121534588460045\n",
      "train loss:0.7633011253091614\n",
      "train loss:0.6929207861015738\n",
      "train loss:0.6132181116139086\n",
      "train loss:0.6161367033199755\n",
      "train loss:0.7748507067714374\n",
      "train loss:0.6105055401716731\n",
      "train loss:0.6887083959485354\n",
      "train loss:0.37550221245547166\n",
      "train loss:0.694451380921761\n",
      "train loss:0.6928920769206429\n",
      "train loss:0.5387525776782053\n",
      "train loss:0.6168381105486777\n",
      "train loss:0.6911101442591356\n",
      "train loss:0.5280615659817288\n",
      "train loss:0.520793740034155\n",
      "train loss:0.42315004163876785\n",
      "train loss:0.6096359905278732\n",
      "train loss:0.5128563212184899\n",
      "train loss:0.9192402746411128\n",
      "train loss:0.7878499448183348\n",
      "train loss:0.5164286971422066\n",
      "train loss:0.7803032252652121\n",
      "train loss:0.5289430925135449\n",
      "train loss:0.6878252541049947\n",
      "train loss:0.7025287688094932\n",
      "train loss:0.5427108200643019\n",
      "train loss:0.681757169570401\n",
      "train loss:0.47442106091794634\n",
      "train loss:0.5445259319488056\n",
      "train loss:0.6108800494996103\n",
      "train loss:0.391869370522839\n",
      "train loss:0.7635748962391891\n",
      "train loss:0.6101957626861515\n",
      "train loss:0.6110458544369913\n",
      "train loss:0.6096235384683519\n",
      "train loss:0.6899065115528538\n",
      "train loss:0.6885614246193688\n",
      "train loss:0.7673704392994536\n",
      "train loss:0.531240070540955\n",
      "train loss:0.9755507492673294\n",
      "train loss:0.6188061577700944\n",
      "train loss:0.481302647819141\n",
      "train loss:0.6172480383578078\n",
      "train loss:0.5527268819632378\n",
      "train loss:0.61491804898266\n",
      "train loss:0.6787160935851777\n",
      "train loss:0.5443752700475678\n",
      "train loss:0.6800258367426939\n",
      "train loss:0.4790809953108878\n",
      "train loss:0.612650889684308\n",
      "train loss:0.5352569673245587\n",
      "train loss:0.69922795499348\n",
      "train loss:0.6097966090069395\n",
      "train loss:0.5228829861276482\n",
      "train loss:0.6068469114426525\n",
      "train loss:0.7051516707331547\n",
      "train loss:0.521970344756261\n",
      "train loss:0.5156802075873854\n",
      "train loss:0.6134749765045152\n",
      "train loss:0.6125097923030423\n",
      "train loss:0.7147417692619472\n",
      "train loss:0.6079451381142971\n",
      "train loss:0.7030083855665692\n",
      "train loss:0.7710373044880761\n",
      "train loss:0.4415405135513571\n",
      "train loss:0.5248517907212045\n",
      "train loss:0.6147463941575845\n",
      "train loss:0.6161233287875472\n",
      "train loss:0.5983934379997577\n",
      "train loss:0.8414424792522514\n",
      "train loss:0.5354903653841062\n",
      "train loss:0.6812970676554692\n",
      "train loss:0.610131581205348\n",
      "train loss:0.5406893125250718\n",
      "train loss:0.5419018157082298\n",
      "train loss:0.6852147035350199\n",
      "train loss:0.6063877958380772\n",
      "train loss:0.7415964583049971\n",
      "train loss:0.7453617973905213\n",
      "train loss:0.4812388548741411\n",
      "train loss:0.4705107103770191\n",
      "train loss:0.5376009839661338\n",
      "train loss:0.834670142065205\n",
      "train loss:0.6904172554455834\n",
      "train loss:0.4659238875784997\n",
      "train loss:0.7098182050357469\n",
      "=== epoch:7, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.606710463548999\n",
      "train loss:0.5252517327826102\n",
      "train loss:0.5986282143393001\n",
      "train loss:0.6986787880546375\n",
      "train loss:0.5293310842665698\n",
      "train loss:0.42819338970147863\n",
      "train loss:0.6020845871639444\n",
      "train loss:0.5038839297610045\n",
      "train loss:0.28263312703978277\n",
      "train loss:0.9567743776063578\n",
      "train loss:0.6162995547966025\n",
      "train loss:0.7344760746206951\n",
      "train loss:0.3868566442790391\n",
      "train loss:0.6396604104470385\n",
      "train loss:0.2723179238018871\n",
      "train loss:0.6273910475604432\n",
      "train loss:0.5082648489175297\n",
      "train loss:0.6194371205255672\n",
      "train loss:0.4967830917945412\n",
      "train loss:0.38843507541014344\n",
      "train loss:0.9854587744670609\n",
      "train loss:0.5012429620444829\n",
      "train loss:0.7315317353122721\n",
      "train loss:0.7162641898301296\n",
      "train loss:0.6043119540267485\n",
      "train loss:0.6038661982769681\n",
      "train loss:0.6754460823293584\n",
      "train loss:0.5351108551328362\n",
      "train loss:0.6744865957480105\n",
      "train loss:0.6151055960540582\n",
      "train loss:0.6829043469942935\n",
      "train loss:0.6778327906690644\n",
      "train loss:0.5608433189509328\n",
      "train loss:0.7806235738030614\n",
      "train loss:0.5173133721997419\n",
      "train loss:0.7599004156159247\n",
      "train loss:0.5379062334125531\n",
      "train loss:0.5766737285928885\n",
      "train loss:0.6317692654369823\n",
      "train loss:0.6774795689266273\n",
      "train loss:0.56205165680485\n",
      "train loss:0.6230065251646788\n",
      "train loss:0.6197690591429568\n",
      "train loss:0.6635316985742447\n",
      "train loss:0.4818218799346613\n",
      "train loss:0.614551294284739\n",
      "train loss:0.6117625461667094\n",
      "train loss:0.7517332653193075\n",
      "train loss:0.440137765504681\n",
      "train loss:0.5218084933271712\n",
      "train loss:0.8767357114639027\n",
      "train loss:0.7658684708092931\n",
      "train loss:0.7052462832145883\n",
      "train loss:0.5064048822527821\n",
      "train loss:0.526793463205879\n",
      "train loss:0.6959265615521464\n",
      "train loss:0.5323512222039349\n",
      "train loss:0.5333610677484092\n",
      "train loss:0.5205140071834944\n",
      "train loss:0.7102974587033895\n",
      "train loss:0.6143879435528364\n",
      "train loss:0.7781783526320931\n",
      "train loss:0.6007700231548913\n",
      "train loss:0.6040140839077386\n",
      "train loss:0.5325332226329638\n",
      "train loss:0.44572224657070764\n",
      "train loss:0.688841885305449\n",
      "train loss:0.5247091862134151\n",
      "train loss:0.6088596547800306\n",
      "train loss:0.5157868103461765\n",
      "train loss:0.4202011931703417\n",
      "train loss:0.3033817682289965\n",
      "train loss:0.3890422709184499\n",
      "train loss:0.5042770845669058\n",
      "train loss:0.4871212401296156\n",
      "train loss:0.7942200773278996\n",
      "train loss:0.6200132772645266\n",
      "train loss:0.5222467936284272\n",
      "train loss:0.8036658501943134\n",
      "train loss:0.8992172186266298\n",
      "train loss:0.8518169783594616\n",
      "train loss:0.8227065725618207\n",
      "train loss:0.5199733592385325\n",
      "train loss:0.7466652619019442\n",
      "train loss:0.5468956628286454\n",
      "train loss:0.5638230271199381\n",
      "train loss:0.6198471582591834\n",
      "train loss:0.5210628296046769\n",
      "train loss:0.5676359515180535\n",
      "train loss:0.7163512709538999\n",
      "train loss:0.6313626088339597\n",
      "train loss:0.581715447182699\n",
      "train loss:0.6182580553985638\n",
      "train loss:0.6909763438723889\n",
      "train loss:0.5222015647145815\n",
      "train loss:0.6812274417008564\n",
      "train loss:0.6713429160634905\n",
      "train loss:0.6699045278624898\n",
      "train loss:0.6782100526447347\n",
      "train loss:0.49230193172681214\n",
      "train loss:0.4847120232241062\n",
      "train loss:0.4612981152817678\n",
      "train loss:0.443831466018145\n",
      "train loss:0.8888262108247511\n",
      "train loss:0.7019200082716844\n",
      "train loss:0.7021270111958734\n",
      "train loss:0.5073134419464498\n",
      "train loss:0.4020863490514929\n",
      "train loss:0.6173317402190684\n",
      "train loss:0.7326298049145226\n",
      "train loss:0.6183494581948044\n",
      "train loss:0.4078119540797231\n",
      "train loss:0.8340749184233275\n",
      "train loss:0.6228591111039063\n",
      "train loss:0.6078373282585722\n",
      "train loss:0.7091470546120695\n",
      "train loss:0.611645367686405\n",
      "train loss:0.7005689704610865\n",
      "train loss:0.680987470170554\n",
      "train loss:0.6884647245016936\n",
      "train loss:0.6715743082514477\n",
      "train loss:0.6223236415498066\n",
      "train loss:0.6715732898373536\n",
      "train loss:0.6095368661307377\n",
      "train loss:0.6658957088265369\n",
      "train loss:0.6255791477766739\n",
      "train loss:0.6215897507276418\n",
      "train loss:0.5677242788930202\n",
      "train loss:0.6224769817986967\n",
      "train loss:0.6739366976927441\n",
      "train loss:0.677334405114321\n",
      "train loss:0.520781926718246\n",
      "train loss:0.731946148056551\n",
      "train loss:0.5686584086331911\n",
      "train loss:0.5037464951722792\n",
      "train loss:0.5518176368409274\n",
      "train loss:0.7360674797728555\n",
      "train loss:0.5374760533955717\n",
      "train loss:0.7608783977872567\n",
      "train loss:0.6906648332290464\n",
      "train loss:0.5337837449119777\n",
      "train loss:0.763542271578471\n",
      "train loss:0.5340243921478895\n",
      "train loss:0.6910714480575291\n",
      "train loss:0.6699184225901449\n",
      "train loss:0.5426184384226329\n",
      "train loss:0.4550787723308748\n",
      "train loss:0.6092821274284659\n",
      "train loss:0.6035358542668406\n",
      "train loss:0.6945034012615116\n",
      "train loss:0.7668644414688633\n",
      "train loss:0.5179542853805604\n",
      "train loss:0.43160259696324604\n",
      "=== epoch:8, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.4470111487070568\n",
      "train loss:0.7162525780001682\n",
      "train loss:0.5295173041883638\n",
      "train loss:0.9072276393424396\n",
      "train loss:0.6943326238752461\n",
      "train loss:0.6930156903504492\n",
      "train loss:0.6112107401791986\n",
      "train loss:0.4366642981841182\n",
      "train loss:0.7686389707701353\n",
      "train loss:0.449718229401492\n",
      "train loss:0.6833009240012256\n",
      "train loss:0.8222495564275251\n",
      "train loss:0.601428115497839\n",
      "train loss:0.46668366440627\n",
      "train loss:0.6131184120259775\n",
      "train loss:0.5396415823272156\n",
      "train loss:0.7463755808408022\n",
      "train loss:0.46819173783653634\n",
      "train loss:0.6123730739989617\n",
      "train loss:0.5256275304659658\n",
      "train loss:0.6053978780913325\n",
      "train loss:0.6050979114536447\n",
      "train loss:0.5142645782705719\n",
      "train loss:0.5211027864009187\n",
      "train loss:0.8800755120536753\n",
      "train loss:0.6076132504797392\n",
      "train loss:0.7772296310057298\n",
      "train loss:0.500392592930144\n",
      "train loss:0.7665331198194925\n",
      "train loss:0.5405921081335576\n",
      "train loss:0.6740101197346648\n",
      "train loss:0.4411105946497134\n",
      "train loss:0.5951883795072435\n",
      "train loss:0.4413954486930036\n",
      "train loss:0.5350183536750353\n",
      "train loss:0.6079283463622185\n",
      "train loss:0.4305605610212062\n",
      "train loss:0.502086497250015\n",
      "train loss:0.9313404904193785\n",
      "train loss:0.5904808746850776\n",
      "train loss:0.6088647060440346\n",
      "train loss:0.6718105417862015\n",
      "train loss:0.8101340346515705\n",
      "train loss:0.5854793607042532\n",
      "train loss:0.60969373942988\n",
      "train loss:0.6945285636334855\n",
      "train loss:0.42803995495831726\n",
      "train loss:0.6965313903288931\n",
      "train loss:0.6144340290312665\n",
      "train loss:0.6750637653519558\n",
      "train loss:0.45143074492434865\n",
      "train loss:0.6063068568120747\n",
      "train loss:0.5341031906543561\n",
      "train loss:0.5686143219910145\n",
      "train loss:0.9832129022325361\n",
      "train loss:0.6847303495048287\n",
      "train loss:0.6167618647752852\n",
      "train loss:0.8605481356566127\n",
      "train loss:0.5639573481609419\n",
      "train loss:0.514153467022596\n",
      "train loss:0.6988792459941015\n",
      "train loss:0.48478743371802\n",
      "train loss:0.5631697804791134\n",
      "train loss:0.7803388027429479\n",
      "train loss:0.4268446576995202\n",
      "train loss:0.6825034623074328\n",
      "train loss:0.4246732243744974\n",
      "train loss:0.7752921764495933\n",
      "train loss:0.4634538581077436\n",
      "train loss:0.6734694373245599\n",
      "train loss:0.44027756286027353\n",
      "train loss:0.43134470053601975\n",
      "train loss:0.8824861582215903\n",
      "train loss:0.6662398593176272\n",
      "train loss:0.4998587091004455\n",
      "train loss:0.84021376923426\n",
      "train loss:0.706907948999478\n",
      "train loss:0.5197060250537433\n",
      "train loss:0.7099425415697742\n",
      "train loss:0.6119112191347386\n",
      "train loss:0.44784871996435804\n",
      "train loss:0.6839344137865261\n",
      "train loss:0.6954846124484017\n",
      "train loss:0.730445672830846\n",
      "train loss:0.5282293770048389\n",
      "train loss:0.7708045336277106\n",
      "train loss:0.5366551833678879\n",
      "train loss:0.7458684473711519\n",
      "train loss:0.6124994477001124\n",
      "train loss:0.4729281895191484\n",
      "train loss:0.7528498807442107\n",
      "train loss:0.6160121173128025\n",
      "train loss:0.5489917594364582\n",
      "train loss:0.7030467594629178\n",
      "train loss:0.5709045138924104\n",
      "train loss:0.42122700576124716\n",
      "train loss:0.6399540639899415\n",
      "train loss:0.612605810453481\n",
      "train loss:0.5237328362484381\n",
      "train loss:0.4363064802848702\n",
      "train loss:0.49130643382362454\n",
      "train loss:0.7306606157973381\n",
      "train loss:0.5130072438017418\n",
      "train loss:0.748267773084007\n",
      "train loss:0.6362651429133253\n",
      "train loss:0.6064325259118855\n",
      "train loss:0.4756495025692056\n",
      "train loss:0.5096160034558299\n",
      "train loss:0.6099354913577686\n",
      "train loss:0.5013753175250562\n",
      "train loss:0.5220479861460317\n",
      "train loss:0.6131826294074787\n",
      "train loss:0.8340717459895097\n",
      "train loss:0.731933296686458\n",
      "train loss:0.8078949143250564\n",
      "train loss:0.6895170775907583\n",
      "train loss:0.45645688486950375\n",
      "train loss:0.46500748728640773\n",
      "train loss:0.5169102313825482\n",
      "train loss:0.4722340497636325\n",
      "train loss:0.7460591028227863\n",
      "train loss:0.4573776037314182\n",
      "train loss:0.7242018491906136\n",
      "train loss:0.5148411800161483\n",
      "train loss:0.6621320325715969\n",
      "train loss:0.5109941262774834\n",
      "train loss:0.5178972016593215\n",
      "train loss:0.4275679264888046\n",
      "train loss:0.6102644136422419\n",
      "train loss:0.29623023833585094\n",
      "train loss:0.6516058217686838\n",
      "train loss:0.38491340794886286\n",
      "train loss:0.36582571833440974\n",
      "train loss:0.5914671850033821\n",
      "train loss:0.46698517856397537\n",
      "train loss:0.7849513354797127\n",
      "train loss:0.503293211196491\n",
      "train loss:0.5047058420506664\n",
      "train loss:0.5098607551678012\n",
      "train loss:0.17935179752101169\n",
      "train loss:0.6815029372976659\n",
      "train loss:0.6220984657686113\n",
      "train loss:0.9351929174469784\n",
      "train loss:0.5115122150344208\n",
      "train loss:0.23509440777853569\n",
      "train loss:0.46435508907133893\n",
      "train loss:0.4959549201321109\n",
      "train loss:0.6380854535463979\n",
      "train loss:0.6135583573621755\n",
      "train loss:0.6099538919548977\n",
      "train loss:0.37103163090723845\n",
      "train loss:0.3965676938003734\n",
      "train loss:0.7145303930342664\n",
      "=== epoch:9, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.4044396039648975\n",
      "train loss:0.35999385745216284\n",
      "train loss:0.5070131182598387\n",
      "train loss:0.6279209496630556\n",
      "train loss:0.5672708238704335\n",
      "train loss:0.7359726243694633\n",
      "train loss:0.7147192862421748\n",
      "train loss:0.6079524137658849\n",
      "train loss:0.6239404689497453\n",
      "train loss:0.46373090315499566\n",
      "train loss:0.5641700352573153\n",
      "train loss:0.7591753942043155\n",
      "train loss:0.5284607121725295\n",
      "train loss:0.42223819311241473\n",
      "train loss:0.7582060675045277\n",
      "train loss:0.3885746007875904\n",
      "train loss:0.5146972713501285\n",
      "train loss:0.7089113275999466\n",
      "train loss:0.47925818043912705\n",
      "train loss:0.3913637909124157\n",
      "train loss:0.5063367631192013\n",
      "train loss:0.6188897669486576\n",
      "train loss:0.6171454615961438\n",
      "train loss:0.6296064365006608\n",
      "train loss:0.527712567792999\n",
      "train loss:0.5827225483749874\n",
      "train loss:0.4017022894814872\n",
      "train loss:0.5335824435822757\n",
      "train loss:0.7212622905573725\n",
      "train loss:0.7387566180034024\n",
      "train loss:0.5651443009888614\n",
      "train loss:0.5098939640715919\n",
      "train loss:0.34480695086257096\n",
      "train loss:0.5069805303041676\n",
      "train loss:0.5384991343373718\n",
      "train loss:0.5074540815986044\n",
      "train loss:0.7520141769589463\n",
      "train loss:0.7163925018621586\n",
      "train loss:0.49844824322962217\n",
      "train loss:0.5506280949651458\n",
      "train loss:0.4169844714755405\n",
      "train loss:0.6160093667939128\n",
      "train loss:0.6092470908718337\n",
      "train loss:0.5235804252016334\n",
      "train loss:0.5974128420558842\n",
      "train loss:0.6211775080218266\n",
      "train loss:0.691038248091203\n",
      "train loss:0.41312873506551373\n",
      "train loss:0.49972570035224895\n",
      "train loss:0.5693822645588866\n",
      "train loss:0.5950212647649427\n",
      "train loss:0.7364753345299049\n",
      "train loss:0.5179584151463892\n",
      "train loss:0.5132719191454822\n",
      "train loss:0.55673767252498\n",
      "train loss:0.5098773659283742\n",
      "train loss:0.7396230821688363\n",
      "train loss:0.5700770979685664\n",
      "train loss:0.5909463875961813\n",
      "train loss:0.5263929038252347\n",
      "train loss:0.3970438306671911\n",
      "train loss:0.7812228280115419\n",
      "train loss:0.46169587624956643\n",
      "train loss:0.5049784636089769\n",
      "train loss:0.43668306946257973\n",
      "train loss:0.7727902818974903\n",
      "train loss:0.9320989303622426\n",
      "train loss:0.649656901870949\n",
      "train loss:0.4732702006057396\n",
      "train loss:0.5697259811943678\n",
      "train loss:0.6808088041981158\n",
      "train loss:0.6450507403327801\n",
      "train loss:0.6180375750434581\n",
      "train loss:0.5415881118858479\n",
      "train loss:0.442107448379494\n",
      "train loss:0.57232976187985\n",
      "train loss:0.4342788050600289\n",
      "train loss:0.4345321174534581\n",
      "train loss:0.5073464705349116\n",
      "train loss:0.5100811310608596\n",
      "train loss:0.5984275764943121\n",
      "train loss:0.6381609500515364\n",
      "train loss:0.7223391405690195\n",
      "train loss:0.3608053723339104\n",
      "train loss:0.6357345569704311\n",
      "train loss:0.5079396363714237\n",
      "train loss:0.6255803781713066\n",
      "train loss:0.5088578626397577\n",
      "train loss:0.6378839753248013\n",
      "train loss:0.5076878151002254\n",
      "train loss:0.7391290901390266\n",
      "train loss:0.5050946115972501\n",
      "train loss:0.572844191450907\n",
      "train loss:0.3876650231369406\n",
      "train loss:0.8027280755325787\n",
      "train loss:0.5017230086755531\n",
      "train loss:0.516803251809993\n",
      "train loss:0.9886833013930902\n",
      "train loss:0.49793899526580654\n",
      "train loss:0.6144330436901925\n",
      "train loss:0.5496491647408974\n",
      "train loss:0.6199167466523164\n",
      "train loss:0.5581048110563568\n",
      "train loss:0.652262508287333\n",
      "train loss:0.4375778081699283\n",
      "train loss:0.7871367901227635\n",
      "train loss:0.690286453775133\n",
      "train loss:0.45675471865544504\n",
      "train loss:0.6889862103069396\n",
      "train loss:0.5634901054990965\n",
      "train loss:0.5784380062035719\n",
      "train loss:0.5994103416713638\n",
      "train loss:0.4259206703475151\n",
      "train loss:0.6441381353818243\n",
      "train loss:0.7567458913855145\n",
      "train loss:0.7572624183545255\n",
      "train loss:0.5063066580396878\n",
      "train loss:0.7013050433259809\n",
      "train loss:0.593599991043358\n",
      "train loss:0.5535268066509872\n",
      "train loss:0.630131364775227\n",
      "train loss:0.5167185732376467\n",
      "train loss:0.5142760988144242\n",
      "train loss:0.6302337439503516\n",
      "train loss:0.5325082918516459\n",
      "train loss:0.5352237326541907\n",
      "train loss:0.6019713340888246\n",
      "train loss:0.8447622388853985\n",
      "train loss:0.5793769785796163\n",
      "train loss:0.8257908698195273\n",
      "train loss:0.6047446899240985\n",
      "train loss:0.7361290093386585\n",
      "train loss:0.5170684744190427\n",
      "train loss:0.5543131899316415\n",
      "train loss:0.6276470481938795\n",
      "train loss:0.4671638292051396\n",
      "train loss:0.6723776291926354\n",
      "train loss:0.4693139829259653\n",
      "train loss:0.5792672008030807\n",
      "train loss:0.7510593994712821\n",
      "train loss:0.830612280240346\n",
      "train loss:0.47477771987734274\n",
      "train loss:0.6557791240552268\n",
      "train loss:0.5381341041391461\n",
      "train loss:0.6880655215786431\n",
      "train loss:0.7294276212463593\n",
      "train loss:0.4860717851684715\n",
      "train loss:0.6086056344356818\n",
      "train loss:0.7795390298177433\n",
      "train loss:0.45095739114164096\n",
      "train loss:0.5619230585430712\n",
      "train loss:0.598589242158187\n",
      "train loss:0.47715822614278797\n",
      "=== epoch:10, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.8046516400094541\n",
      "train loss:0.601050472218055\n",
      "train loss:0.7496714331395693\n",
      "train loss:0.7998721205051293\n",
      "train loss:0.5136706530655056\n",
      "train loss:0.456085430892347\n",
      "train loss:0.4464843554194503\n",
      "train loss:0.4345341581947674\n",
      "train loss:0.6794732005719938\n",
      "train loss:0.43361243252257076\n",
      "train loss:0.5371846887561671\n",
      "train loss:0.9213503434972667\n",
      "train loss:0.637336321891697\n",
      "train loss:0.47251178582622916\n",
      "train loss:0.6897307289518249\n",
      "train loss:0.5894061304319168\n",
      "train loss:0.5158098644431267\n",
      "train loss:0.34101811540726246\n",
      "train loss:0.5327867623666814\n",
      "train loss:1.0403312701664098\n",
      "train loss:0.8762645386695986\n",
      "train loss:0.6117990469567922\n",
      "train loss:0.5188773333595219\n",
      "train loss:0.5537186279172613\n",
      "train loss:0.4372925555186947\n",
      "train loss:0.4736119751411831\n",
      "train loss:0.6027990157579997\n",
      "train loss:0.6088567916575403\n",
      "train loss:0.6114186190391442\n",
      "train loss:0.5937450979744361\n",
      "train loss:0.6730121757763888\n",
      "train loss:0.8559390958873667\n",
      "train loss:0.699430534160201\n",
      "train loss:0.5587505072038496\n",
      "train loss:0.40206084866189196\n",
      "train loss:0.4030077171503323\n",
      "train loss:0.5859632334256816\n",
      "train loss:0.9024361571340052\n",
      "train loss:0.567844237871898\n",
      "train loss:0.8106426242081295\n",
      "train loss:0.5292399832638666\n",
      "train loss:0.6413075479771356\n",
      "train loss:0.7746313876734557\n",
      "train loss:0.6481797780464821\n",
      "train loss:0.6872130650849076\n",
      "train loss:0.7295765548806459\n",
      "train loss:0.6219025184448987\n",
      "train loss:0.6814764683557935\n",
      "train loss:0.5373581616876002\n",
      "train loss:0.538426863486768\n",
      "train loss:0.6346867164264817\n",
      "train loss:0.6565805677200203\n",
      "train loss:0.666709846664648\n",
      "train loss:0.5198438218752968\n",
      "train loss:0.6385046778492933\n",
      "train loss:0.6929888774365982\n",
      "train loss:0.540145114569411\n",
      "train loss:0.5646165861564874\n",
      "train loss:0.7329112670760949\n",
      "train loss:0.41301756885276053\n",
      "train loss:0.6842074169049943\n",
      "train loss:0.461030487567956\n",
      "train loss:0.5767772646580311\n",
      "train loss:0.624191281592197\n",
      "train loss:0.5463644570088764\n",
      "train loss:0.6592864113222097\n",
      "train loss:0.6129372196743168\n",
      "train loss:0.5580260043740106\n",
      "train loss:0.25085291988607994\n",
      "train loss:0.49804304475380945\n",
      "train loss:0.5723467049067222\n",
      "train loss:0.5209377048725241\n",
      "train loss:0.7468956723186677\n",
      "train loss:0.8515494080919345\n",
      "train loss:0.4987998519708598\n",
      "train loss:0.4710360143373438\n",
      "train loss:1.0540054893087605\n",
      "train loss:0.3557900230075153\n",
      "train loss:0.36587002090911225\n",
      "train loss:0.5988075638606558\n",
      "train loss:0.46496949631862955\n",
      "train loss:0.6905021118633381\n",
      "train loss:0.579424104249006\n",
      "train loss:0.41387338828124315\n",
      "train loss:0.6836150826422223\n",
      "train loss:0.370177717280783\n",
      "train loss:0.3847856956997488\n",
      "train loss:0.44759356013716156\n",
      "train loss:0.38333331613811367\n",
      "train loss:0.7197799249863908\n",
      "train loss:0.4939702249850729\n",
      "train loss:0.5086827413532838\n",
      "train loss:0.6741981824018825\n",
      "train loss:0.6451466512889689\n",
      "train loss:0.7947962186549581\n",
      "train loss:0.7540069541583783\n",
      "train loss:0.5581109392721505\n",
      "train loss:0.4970008827838247\n",
      "train loss:0.6606147858804213\n",
      "train loss:0.6104172408141131\n",
      "train loss:0.5436429313686093\n",
      "train loss:0.6026986757576362\n",
      "train loss:0.5382752317821669\n",
      "train loss:0.5698880574982441\n",
      "train loss:0.6107797204039015\n",
      "train loss:0.5208942856187855\n",
      "train loss:0.6078816821060538\n",
      "train loss:0.5328524110898816\n",
      "train loss:0.6785129055243276\n",
      "train loss:0.5034958653442497\n",
      "train loss:0.7471391056500337\n",
      "train loss:0.7301979256432796\n",
      "train loss:0.6496458498273583\n",
      "train loss:0.6895282829477204\n",
      "train loss:0.6361726659840181\n",
      "train loss:0.7484771334501559\n",
      "train loss:0.684108718661268\n",
      "train loss:0.6351283087378465\n",
      "train loss:0.6869594720514856\n",
      "train loss:0.592716588372739\n",
      "train loss:0.6844378421751515\n",
      "train loss:0.6362511296595879\n",
      "train loss:0.6376652070345914\n",
      "train loss:0.5571162861695715\n",
      "train loss:0.5326352222329392\n",
      "train loss:0.6127011285729113\n",
      "train loss:0.6190698592609791\n",
      "train loss:0.6193687134392559\n",
      "train loss:0.5387607893755608\n",
      "train loss:0.6264144046946372\n",
      "train loss:0.5161601167913348\n",
      "train loss:0.6511423636567274\n",
      "train loss:0.5965202412925109\n",
      "train loss:0.7136367594056324\n",
      "train loss:0.5355434827137893\n",
      "train loss:0.7817084252902768\n",
      "train loss:0.5322576049575588\n",
      "train loss:0.5007960140303861\n",
      "train loss:0.4331970124662806\n",
      "train loss:0.34453998465832647\n",
      "train loss:0.9693724566850193\n",
      "train loss:0.5109150312444082\n",
      "train loss:0.5869284717012129\n",
      "train loss:0.47902289822777017\n",
      "train loss:0.2986219742676509\n",
      "train loss:0.24301502432179994\n",
      "train loss:0.6681724682738287\n",
      "train loss:0.46912286681854526\n",
      "train loss:0.3800225951200471\n",
      "train loss:0.5431074041742682\n",
      "train loss:0.5101185699146311\n",
      "train loss:0.5051320860930246\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5529411764705883\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=200, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0ecbc17-fa5d-4450-b5b7-a60000b7d825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.292513318314815\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:2.2792742494364946\n",
      "train loss:2.250934249313369\n",
      "train loss:2.196822325673635\n",
      "train loss:2.0925206092216646\n",
      "train loss:1.9976171239244676\n",
      "train loss:1.7731526652313225\n",
      "train loss:1.5243969717187453\n",
      "train loss:1.2092282930764253\n",
      "train loss:1.014182654540482\n",
      "train loss:0.9082855028758127\n",
      "train loss:0.9461802480187387\n",
      "train loss:0.39627725001126274\n",
      "train loss:0.5440063476862593\n",
      "train loss:0.7657904779949696\n",
      "train loss:0.6810212057674485\n",
      "train loss:0.6670300931989182\n",
      "train loss:0.47206536489358\n",
      "train loss:0.6339301663810212\n",
      "train loss:0.512650615635484\n",
      "train loss:0.5453063624844312\n",
      "train loss:0.5563189137293926\n",
      "train loss:0.5179742720283669\n",
      "train loss:0.39063244077528314\n",
      "train loss:0.6701979059251977\n",
      "train loss:0.7191208600683839\n",
      "train loss:0.5267160028865491\n",
      "train loss:0.4936216573621567\n",
      "train loss:0.33245561351013453\n",
      "train loss:0.6170667142242425\n",
      "train loss:0.7666090451389039\n",
      "train loss:0.6824224997758869\n",
      "train loss:0.6582841708083684\n",
      "train loss:0.680176922894179\n",
      "train loss:0.6894938389160308\n",
      "train loss:0.6878922634764473\n",
      "train loss:0.6909858549296158\n",
      "train loss:0.6916870813461542\n",
      "train loss:0.5923713141128688\n",
      "train loss:0.3976218484410836\n",
      "train loss:0.6957074651667234\n",
      "train loss:0.7057872210317577\n",
      "train loss:0.6832333314182312\n",
      "train loss:0.7279887615500893\n",
      "train loss:0.9278664292175517\n",
      "train loss:0.5910862000575248\n",
      "train loss:0.3855222499922572\n",
      "train loss:0.4362750964140719\n",
      "train loss:0.698387763428741\n",
      "train loss:0.6914211357138957\n",
      "train loss:0.6730827331646003\n",
      "train loss:0.5730633752759207\n",
      "train loss:0.487184615270976\n",
      "train loss:0.5072871213459949\n",
      "train loss:0.7380782330938771\n",
      "train loss:0.8839158245392532\n",
      "train loss:0.5058543957016896\n",
      "train loss:0.5382898585177213\n",
      "train loss:0.5140819040669807\n",
      "train loss:0.6120351106134004\n",
      "train loss:0.6313900377112125\n",
      "train loss:0.5128836722342536\n",
      "train loss:0.6489809015709709\n",
      "train loss:0.6329051421926388\n",
      "train loss:0.6100540570142423\n",
      "train loss:0.4223015233995908\n",
      "train loss:0.5079500551253561\n",
      "train loss:0.6318121952400246\n",
      "train loss:0.6014739241348157\n",
      "train loss:0.4991583700164603\n",
      "train loss:0.777490711181331\n",
      "train loss:0.6220874885270012\n",
      "train loss:0.5244553327452418\n",
      "train loss:0.506236089631428\n",
      "train loss:0.8366085563100659\n",
      "train loss:0.6241253270550093\n",
      "train loss:0.6791921064281096\n",
      "train loss:0.6236487556082256\n",
      "train loss:0.6336780469556733\n",
      "train loss:0.6260711472200042\n",
      "train loss:0.6791310549840177\n",
      "train loss:0.5454652975530629\n",
      "train loss:0.6376872133476653\n",
      "train loss:0.691895907168558\n",
      "train loss:0.6917702385688203\n",
      "train loss:0.6149530624236798\n",
      "train loss:0.698850267074223\n",
      "train loss:0.4388427431496168\n",
      "train loss:0.5213522437628305\n",
      "train loss:0.6004528508810386\n",
      "train loss:0.3753796502105783\n",
      "train loss:0.6300041007038948\n",
      "train loss:0.7829889334024587\n",
      "train loss:0.9310750625863038\n",
      "train loss:0.5068772875949497\n",
      "train loss:0.7385614940047425\n",
      "train loss:0.6328555250814889\n",
      "train loss:0.5407921867279935\n",
      "train loss:0.6230084121829683\n",
      "train loss:0.5651662191074814\n",
      "train loss:0.609823792023283\n",
      "train loss:0.6825577289469459\n",
      "train loss:0.4274653697015748\n",
      "train loss:0.28462368855797204\n",
      "train loss:0.5261469647907975\n",
      "train loss:0.8348596630410562\n",
      "train loss:0.5286874075633705\n",
      "train loss:0.8287143635793388\n",
      "train loss:0.35305340240515326\n",
      "train loss:0.6448852355660686\n",
      "train loss:0.6877730795796262\n",
      "train loss:0.6773719848608216\n",
      "train loss:0.5209399433890783\n",
      "train loss:0.8000204489844084\n",
      "train loss:0.537274900842882\n",
      "train loss:0.49527479216966297\n",
      "train loss:0.6335053583500169\n",
      "train loss:0.6186732562452558\n",
      "train loss:0.6325279831269939\n",
      "train loss:0.6175741047147886\n",
      "train loss:0.6088201142369931\n",
      "train loss:0.6823167486570079\n",
      "train loss:0.4116521765709161\n",
      "train loss:0.43533378821441404\n",
      "train loss:0.7134897059939895\n",
      "train loss:0.2756022190064724\n",
      "train loss:0.6437628552570467\n",
      "train loss:0.5437962094193057\n",
      "train loss:0.9725259294221484\n",
      "train loss:0.6703250905060483\n",
      "train loss:0.7651903149874844\n",
      "train loss:0.7242443441437665\n",
      "train loss:0.41660716390005675\n",
      "train loss:0.5050914768796304\n",
      "train loss:0.6201686890048472\n",
      "train loss:0.4229738259217539\n",
      "train loss:0.4298177167197991\n",
      "train loss:0.6107936665010205\n",
      "train loss:0.6971624174404706\n",
      "train loss:0.6117143889035841\n",
      "train loss:0.4362398933020148\n",
      "train loss:0.29545648495190757\n",
      "train loss:0.5016940967692036\n",
      "train loss:0.7606345696389598\n",
      "train loss:0.4899459865634892\n",
      "train loss:0.3652529031929879\n",
      "train loss:0.7430758601059116\n",
      "train loss:0.7270216141645995\n",
      "train loss:0.6471463954624641\n",
      "train loss:0.37815347477576267\n",
      "train loss:0.641676007494425\n",
      "train loss:0.5076513514126126\n",
      "train loss:0.8463043472631453\n",
      "train loss:0.7221809997167399\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6119422006507456\n",
      "train loss:0.6760823274173066\n",
      "train loss:0.6160068142067151\n",
      "train loss:0.6323599818922314\n",
      "train loss:0.6526871431947823\n",
      "train loss:0.662994441084031\n",
      "train loss:0.641536739754102\n",
      "train loss:0.5870908142169116\n",
      "train loss:0.6718158435548308\n",
      "train loss:0.5425718814302193\n",
      "train loss:0.5700217881501666\n",
      "train loss:0.6887729721528504\n",
      "train loss:0.6742488886166779\n",
      "train loss:0.5267630117073618\n",
      "train loss:0.6070401825215908\n",
      "train loss:0.40812180413824695\n",
      "train loss:0.7052062007059313\n",
      "train loss:0.5410357570429742\n",
      "train loss:0.5141924749090419\n",
      "train loss:0.5169524830312424\n",
      "train loss:0.6072499260534193\n",
      "train loss:0.5197014069431658\n",
      "train loss:0.7978972648512992\n",
      "train loss:0.7623530811056538\n",
      "train loss:0.4963482914042244\n",
      "train loss:0.49054387636910846\n",
      "train loss:0.7222483174623349\n",
      "train loss:0.5208946715991093\n",
      "train loss:0.5201865909613572\n",
      "train loss:0.5259466277504908\n",
      "train loss:0.4130189904741381\n",
      "train loss:0.5037067075729171\n",
      "train loss:0.39670850487077197\n",
      "train loss:0.5255578725887355\n",
      "train loss:0.5249939850808825\n",
      "train loss:0.37375514461845305\n",
      "train loss:0.6603951154376184\n",
      "train loss:0.34830197057639295\n",
      "train loss:0.33018954731975536\n",
      "train loss:0.9995782047986177\n",
      "train loss:0.6588279808394413\n",
      "train loss:0.3473804190044748\n",
      "train loss:0.5396225215707439\n",
      "train loss:0.7375704533626165\n",
      "train loss:0.5157821817361741\n",
      "train loss:0.7415153650788636\n",
      "train loss:0.5107475243156455\n",
      "train loss:0.7235888783883496\n",
      "train loss:0.8403466971114085\n",
      "train loss:0.7909705835149453\n",
      "train loss:0.7216539893776431\n",
      "train loss:0.7076770230480041\n",
      "train loss:0.6924931711910183\n",
      "train loss:0.7077688298812558\n",
      "train loss:0.7040470126644844\n",
      "train loss:0.6976041670181724\n",
      "train loss:0.7843086588815639\n",
      "train loss:0.7135147156959541\n",
      "train loss:0.7389167224280337\n",
      "train loss:0.747756481020666\n",
      "train loss:0.7082149751347611\n",
      "train loss:0.7111316712517991\n",
      "train loss:0.7154231753531264\n",
      "train loss:0.7128621848871475\n",
      "train loss:0.6995161708620478\n",
      "train loss:0.6789769526337602\n",
      "train loss:0.6360931659842348\n",
      "train loss:0.7052067626831049\n",
      "train loss:0.6347925746408872\n",
      "train loss:0.674455845515597\n",
      "train loss:0.6110936276882952\n",
      "train loss:0.5359260170193636\n",
      "train loss:0.5986684389768051\n",
      "train loss:0.7291584424363399\n",
      "train loss:0.630742660253736\n",
      "train loss:0.6289993556444811\n",
      "train loss:0.8585480095495773\n",
      "train loss:0.4978230951384625\n",
      "train loss:0.6138327155162436\n",
      "train loss:0.8297600045064204\n",
      "train loss:0.6090851948359683\n",
      "train loss:0.6888608298175729\n",
      "train loss:0.5336297430418752\n",
      "train loss:0.5424264032808078\n",
      "train loss:0.6936602839705412\n",
      "train loss:0.6811111073429266\n",
      "train loss:0.6836099920077883\n",
      "train loss:0.5601763252258277\n",
      "train loss:0.7363829093180385\n",
      "train loss:0.562526854090379\n",
      "train loss:0.5045627048617155\n",
      "train loss:0.5570393349084113\n",
      "train loss:0.4707480173080971\n",
      "train loss:0.4485130849381351\n",
      "train loss:0.7097922603505686\n",
      "train loss:0.6358043560114806\n",
      "train loss:0.6185129779067584\n",
      "train loss:0.7344748790272189\n",
      "train loss:0.7242774597690959\n",
      "train loss:0.7194496912111972\n",
      "train loss:0.6483816969626895\n",
      "train loss:0.721707996207157\n",
      "train loss:0.4617373145157081\n",
      "train loss:0.7643791312607987\n",
      "train loss:0.4580740902596848\n",
      "train loss:0.6203503454785686\n",
      "train loss:0.6232003458368216\n",
      "train loss:0.5312261046136735\n",
      "train loss:0.614443402911663\n",
      "train loss:0.6117006072519736\n",
      "train loss:0.5352443595426271\n",
      "train loss:0.4301247190094505\n",
      "train loss:0.5178922160453443\n",
      "train loss:0.6151839625322875\n",
      "train loss:0.9262262539220248\n",
      "train loss:0.7051602456124675\n",
      "train loss:0.5251399544661512\n",
      "train loss:0.7079834963104443\n",
      "train loss:0.4344653072385893\n",
      "train loss:0.5091118628560347\n",
      "train loss:0.49992512625346847\n",
      "train loss:0.6224446796963955\n",
      "train loss:0.3942961549319496\n",
      "train loss:0.3725158848130775\n",
      "train loss:0.6455514573367622\n",
      "train loss:0.7887476912403629\n",
      "train loss:0.3746435379305953\n",
      "train loss:0.48441528445677917\n",
      "train loss:0.8828880530314578\n",
      "train loss:0.773524948496145\n",
      "train loss:0.7189713890688986\n",
      "train loss:0.7024879186424229\n",
      "train loss:0.6248956423065251\n",
      "train loss:0.684262003063236\n",
      "train loss:0.5084592586324458\n",
      "train loss:0.6743977511238789\n",
      "train loss:0.670522119940566\n",
      "train loss:0.5505269064106842\n",
      "train loss:0.6389492924149699\n",
      "train loss:0.6341555078140816\n",
      "train loss:0.5818978572882719\n",
      "train loss:0.6254380974928105\n",
      "train loss:0.5628600871044915\n",
      "train loss:0.611992070784656\n",
      "train loss:0.5457263802514218\n",
      "train loss:0.5249787203788204\n",
      "train loss:0.4140489573212056\n",
      "train loss:0.38567375834750317\n",
      "train loss:0.6445708175976557\n",
      "train loss:0.6451044239609592\n",
      "train loss:0.3482297435349247\n",
      "train loss:0.8546850234261225\n",
      "train loss:0.8109420830464545\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7921869045620276\n",
      "train loss:0.6381825882212434\n",
      "train loss:0.7342684203977916\n",
      "train loss:0.7081584267014372\n",
      "train loss:0.4334034478219344\n",
      "train loss:0.6937496400664822\n",
      "train loss:0.6856857627491612\n",
      "train loss:0.6804979472392755\n",
      "train loss:0.5814038313213743\n",
      "train loss:0.7272676337395414\n",
      "train loss:0.591862449849927\n",
      "train loss:0.5979434871614251\n",
      "train loss:0.6811788833986491\n",
      "train loss:0.7229900583845074\n",
      "train loss:0.6322441401649022\n",
      "train loss:0.6753602815229736\n",
      "train loss:0.5875879521358699\n",
      "train loss:0.6328386225811531\n",
      "train loss:0.730126817862312\n",
      "train loss:0.5670645457197185\n",
      "train loss:0.7308039827147923\n",
      "train loss:0.5541217086526309\n",
      "train loss:0.5517114287395624\n",
      "train loss:0.6842089495219217\n",
      "train loss:0.4420256159421516\n",
      "train loss:0.3224184524406833\n",
      "train loss:0.830299151855171\n",
      "train loss:0.7287358071221541\n",
      "train loss:0.8334406507270147\n",
      "train loss:0.6107787315312265\n",
      "train loss:0.7172221588277037\n",
      "train loss:0.513360014326269\n",
      "train loss:0.7008195857853293\n",
      "train loss:0.4392736977945032\n",
      "train loss:0.6156029470397791\n",
      "train loss:0.5179447921288667\n",
      "train loss:0.6118331650996559\n",
      "train loss:0.5176720570427491\n",
      "train loss:0.5136241471953603\n",
      "train loss:0.7023655886272332\n",
      "train loss:0.6979145569962854\n",
      "train loss:0.6085091047878979\n",
      "train loss:0.695521563020141\n",
      "train loss:0.6947722756498315\n",
      "train loss:0.9516931334655606\n",
      "train loss:0.5004101018077602\n",
      "train loss:0.7174634635590504\n",
      "train loss:0.5775174862340576\n",
      "train loss:0.640813756316159\n",
      "train loss:0.6700645752638594\n",
      "train loss:0.673593346761904\n",
      "train loss:0.5653508904286522\n",
      "train loss:0.6351277799970445\n",
      "train loss:0.6282844920774816\n",
      "train loss:0.7205210605889568\n",
      "train loss:0.5179350027603862\n",
      "train loss:0.5584474658389968\n",
      "train loss:0.4586758919259918\n",
      "train loss:0.6003110963221111\n",
      "train loss:0.7032102656048627\n",
      "train loss:0.6266317189727693\n",
      "train loss:0.3970969404149787\n",
      "train loss:0.5085986017717319\n",
      "train loss:0.9014890429002398\n",
      "train loss:0.36090011182818305\n",
      "train loss:0.7855701224853857\n",
      "train loss:0.7672043709212526\n",
      "train loss:0.6217605835389709\n",
      "train loss:0.5127846171656131\n",
      "train loss:0.400862228647593\n",
      "train loss:0.7197864474308893\n",
      "train loss:0.797559468672559\n",
      "train loss:0.6099126225546622\n",
      "train loss:0.6126464492358732\n",
      "train loss:0.6836620133700302\n",
      "train loss:0.7428449529949793\n",
      "train loss:0.5056367341434489\n",
      "train loss:0.6251568665004738\n",
      "train loss:0.7318601048012814\n",
      "train loss:0.5177649385213601\n",
      "train loss:0.6236040709546852\n",
      "train loss:0.6279208291555923\n",
      "train loss:0.6794200280895865\n",
      "train loss:0.5704173289263965\n",
      "train loss:0.5452856104648518\n",
      "train loss:0.5433869372492452\n",
      "train loss:0.6885686801775421\n",
      "train loss:0.6896221967922361\n",
      "train loss:0.5252198174453313\n",
      "train loss:0.6950116533880452\n",
      "train loss:0.7066300988060265\n",
      "train loss:0.5298067466851999\n",
      "train loss:0.6139173177923049\n",
      "train loss:0.7107401812951165\n",
      "train loss:0.7023383390980333\n",
      "train loss:0.8541485368156172\n",
      "train loss:0.537473328477972\n",
      "train loss:0.6134319194953306\n",
      "train loss:0.7366752403630604\n",
      "train loss:0.7810405171581329\n",
      "train loss:0.5781411023682377\n",
      "train loss:0.6330613672877188\n",
      "train loss:0.6752718874043673\n",
      "train loss:0.6327210602893962\n",
      "train loss:0.5071181494427859\n",
      "train loss:0.5284846293879415\n",
      "train loss:0.6725056232635593\n",
      "train loss:0.5513805479052654\n",
      "train loss:0.6175093911815666\n",
      "train loss:0.5284010967089381\n",
      "train loss:0.698088008321899\n",
      "train loss:0.6135066848246644\n",
      "train loss:0.7119433718139292\n",
      "train loss:0.5198318226665307\n",
      "train loss:0.619285636914322\n",
      "train loss:0.7294426719707899\n",
      "train loss:0.6167413739123795\n",
      "train loss:0.6235522870072819\n",
      "train loss:0.7177578809738488\n",
      "train loss:0.8752984155275072\n",
      "train loss:0.6141609169968251\n",
      "train loss:0.6156018718090939\n",
      "train loss:0.6758505016492069\n",
      "train loss:0.6228346359535026\n",
      "train loss:0.6251118800789737\n",
      "train loss:0.6811499022993284\n",
      "train loss:0.5837684395942823\n",
      "train loss:0.6794674330587396\n",
      "train loss:0.7229962594078478\n",
      "train loss:0.7536618080714097\n",
      "train loss:0.5627145504499492\n",
      "train loss:0.6756219037892219\n",
      "train loss:0.6774344679467996\n",
      "train loss:0.63703779499125\n",
      "train loss:0.6768934665460717\n",
      "train loss:0.673849343963209\n",
      "train loss:0.5898893781973159\n",
      "train loss:0.6245898663468554\n",
      "train loss:0.510046377546299\n",
      "train loss:0.5520895922283197\n",
      "train loss:0.6139825008804396\n",
      "train loss:0.4381432781484936\n",
      "train loss:0.609998138876444\n",
      "train loss:0.3827633039074413\n",
      "train loss:0.365585351509327\n",
      "train loss:0.9512236668552774\n",
      "train loss:0.6589668062036768\n",
      "train loss:0.6470372770067484\n",
      "train loss:0.6392148807002878\n",
      "train loss:0.7660676906623375\n",
      "train loss:0.38707036454519306\n",
      "train loss:0.7411354162644068\n",
      "train loss:0.7215376173409002\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7004444820015742\n",
      "train loss:0.5312084123131904\n",
      "train loss:0.5421703884188285\n",
      "train loss:0.7451864619861428\n",
      "train loss:0.6202471799786793\n",
      "train loss:0.5696916634185284\n",
      "train loss:0.5111110206046203\n",
      "train loss:0.4394543369596368\n",
      "train loss:0.5490382113904476\n",
      "train loss:0.535672843222219\n",
      "train loss:0.6137800493800168\n",
      "train loss:0.7042670001149185\n",
      "train loss:0.41784822577330794\n",
      "train loss:0.6181082743986405\n",
      "train loss:0.3859342537499682\n",
      "train loss:0.7480233327944606\n",
      "train loss:0.500217190951311\n",
      "train loss:0.6311269727025228\n",
      "train loss:0.7676949061569781\n",
      "train loss:0.36575457710219916\n",
      "train loss:0.88685997463988\n",
      "train loss:0.37986562709029204\n",
      "train loss:0.732359849206394\n",
      "train loss:0.7231016046779569\n",
      "train loss:0.6088286891404743\n",
      "train loss:0.6110010297429256\n",
      "train loss:0.5274317133229526\n",
      "train loss:0.6142774512909019\n",
      "train loss:0.6126639762922785\n",
      "train loss:0.46079147371461493\n",
      "train loss:0.6134466113427307\n",
      "train loss:0.6896757005698247\n",
      "train loss:0.6107642668958608\n",
      "train loss:0.620750159657058\n",
      "train loss:0.5383608686077583\n",
      "train loss:0.7683164239404593\n",
      "train loss:0.683733383486621\n",
      "train loss:0.5399245596427\n",
      "train loss:0.6850009449310092\n",
      "train loss:0.6888490692054797\n",
      "train loss:0.6850269558280244\n",
      "train loss:0.6193948762046751\n",
      "train loss:0.6184329475288353\n",
      "train loss:0.8513838326209621\n",
      "train loss:0.5078808079601262\n",
      "train loss:0.6222465721474376\n",
      "train loss:0.7307365046823936\n",
      "train loss:0.621463781559\n",
      "train loss:0.6727786295355825\n",
      "train loss:0.7253767637232768\n",
      "train loss:0.5777136122197094\n",
      "train loss:0.6265638463245564\n",
      "train loss:0.6770627642575642\n",
      "train loss:0.7246100824818128\n",
      "train loss:0.5801128309621522\n",
      "train loss:0.5689732528700375\n",
      "train loss:0.6204752406395766\n",
      "train loss:0.61691712804491\n",
      "train loss:0.40464277111262337\n",
      "train loss:0.6111681314768778\n",
      "train loss:0.7027264023539836\n",
      "train loss:0.6135143022914071\n",
      "train loss:0.6111723115716592\n",
      "train loss:0.39776206250299173\n",
      "train loss:1.0376876099420131\n",
      "train loss:0.6177413522383655\n",
      "train loss:0.7044892802770695\n",
      "train loss:0.5180354508727381\n",
      "train loss:0.8687941835577421\n",
      "train loss:0.6061608570227941\n",
      "train loss:0.6820667773787041\n",
      "train loss:0.615291338673236\n",
      "train loss:0.6776574454131457\n",
      "train loss:0.569857592775039\n",
      "train loss:0.4613935023761219\n",
      "train loss:0.5626571007122304\n",
      "train loss:0.5526567992490857\n",
      "train loss:0.46742105521567867\n",
      "train loss:0.5282098036993893\n",
      "train loss:0.7023854209837644\n",
      "train loss:0.41270616315252273\n",
      "train loss:0.7232873034611504\n",
      "train loss:0.7300588485613307\n",
      "train loss:0.6214997922296861\n",
      "train loss:0.5015208018063573\n",
      "train loss:0.5004487837122996\n",
      "train loss:0.24020862471760024\n",
      "train loss:0.35096387415578945\n",
      "train loss:0.8208094613131228\n",
      "train loss:0.500513982831701\n",
      "train loss:0.7798031313130731\n",
      "train loss:0.36046333000130637\n",
      "train loss:0.5007447782452101\n",
      "train loss:0.5042475268818234\n",
      "train loss:0.36100982783673075\n",
      "train loss:0.7872432180367351\n",
      "train loss:0.8744157608988822\n",
      "train loss:0.8308744143304452\n",
      "train loss:0.7058866077594604\n",
      "train loss:0.617453926206348\n",
      "train loss:0.8044708298541551\n",
      "train loss:0.6818074651888455\n",
      "train loss:0.6424705580848924\n",
      "train loss:0.6156665794993653\n",
      "train loss:0.6924574618527964\n",
      "train loss:0.632181315327119\n",
      "train loss:0.6899143049217417\n",
      "train loss:0.6314937968738867\n",
      "train loss:0.7121302924954691\n",
      "train loss:0.6510773564737004\n",
      "train loss:0.7867429431816162\n",
      "train loss:0.5669238399358454\n",
      "train loss:0.5765340992419595\n",
      "train loss:0.598155960325564\n",
      "train loss:0.717901642151283\n",
      "train loss:0.6759180907155088\n",
      "train loss:0.5120177621458489\n",
      "train loss:0.5540623326237405\n",
      "train loss:0.5405918765341066\n",
      "train loss:0.6142343059158352\n",
      "train loss:0.7960749773756578\n",
      "train loss:0.8131768332209003\n",
      "train loss:0.41450619695266866\n",
      "train loss:0.39894323966749656\n",
      "train loss:0.37902696764575905\n",
      "train loss:0.631214181674701\n",
      "train loss:0.496298015760261\n",
      "train loss:0.7796252620643211\n",
      "train loss:0.515412101821546\n",
      "train loss:0.4956597640276027\n",
      "train loss:0.6533131698926076\n",
      "train loss:0.4986810998950566\n",
      "train loss:0.7622399536518841\n",
      "train loss:0.6279490755451931\n",
      "train loss:0.7197012848601624\n",
      "train loss:0.6046970919114599\n",
      "train loss:0.6027570308355625\n",
      "train loss:0.3768428353609143\n",
      "train loss:0.8252781817452515\n",
      "train loss:0.7429686919367327\n",
      "train loss:0.5603230117536867\n",
      "train loss:0.5633153336224762\n",
      "train loss:0.6217049947291726\n",
      "train loss:0.4520989455635413\n",
      "train loss:0.5515699621324195\n",
      "train loss:0.6872283848510177\n",
      "train loss:0.5432545252675987\n",
      "train loss:0.6142936987472214\n",
      "train loss:0.6147110816681073\n",
      "train loss:0.5244406986688566\n",
      "train loss:0.5238909586193707\n",
      "train loss:0.5155850637185687\n",
      "train loss:0.719142453816222\n",
      "=== epoch:5, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7316192213688714\n",
      "train loss:0.7276956717587822\n",
      "train loss:0.6180967261514534\n",
      "train loss:0.7129335882422272\n",
      "train loss:0.4228268583690097\n",
      "train loss:0.7041501324796792\n",
      "train loss:0.6152044595751514\n",
      "train loss:0.4326435178194513\n",
      "train loss:0.6136582795281113\n",
      "train loss:0.6959360198097699\n",
      "train loss:0.6093289694986697\n",
      "train loss:0.6956253586153157\n",
      "train loss:0.6898270243111113\n",
      "train loss:0.7563789032875612\n",
      "train loss:0.7441871010037492\n",
      "train loss:0.6223867882147075\n",
      "train loss:0.6252646196435765\n",
      "train loss:0.7276705433319334\n",
      "train loss:0.6283130511514421\n",
      "train loss:0.8064478569566311\n",
      "train loss:0.5587968219060191\n",
      "train loss:0.5998950796507926\n",
      "train loss:0.636112661540492\n",
      "train loss:0.6751208546161658\n",
      "train loss:0.6332409846253817\n",
      "train loss:0.6741849193896889\n",
      "train loss:0.5772425527243386\n",
      "train loss:0.5148522135480139\n",
      "train loss:0.5576917250198056\n",
      "train loss:0.5443094786779316\n",
      "train loss:0.6123112663941878\n",
      "train loss:0.5190534185207405\n",
      "train loss:0.5105349431995678\n",
      "train loss:0.6184357272928133\n",
      "train loss:0.6269255143229662\n",
      "train loss:0.3612810976128535\n",
      "train loss:0.5029636000804694\n",
      "train loss:0.6581539910665668\n",
      "train loss:0.34464896371962617\n",
      "train loss:0.5047659983966937\n",
      "train loss:0.834430438907621\n",
      "train loss:0.3483715021146364\n",
      "train loss:0.8159168789469984\n",
      "train loss:0.6355019990937261\n",
      "train loss:0.5020737074554599\n",
      "train loss:0.6194737264420066\n",
      "train loss:0.8226451129934554\n",
      "train loss:0.6978206900958112\n",
      "train loss:0.45738169919485056\n",
      "train loss:0.3863975811159583\n",
      "train loss:0.6134635419953313\n",
      "train loss:0.6824754253851599\n",
      "train loss:0.5474994120595059\n",
      "train loss:0.4045614092977403\n",
      "train loss:0.4571836759576618\n",
      "train loss:0.770420720410627\n",
      "train loss:0.6932066461312878\n",
      "train loss:0.5269485138667875\n",
      "train loss:0.6982218473215009\n",
      "train loss:0.6976274047680688\n",
      "train loss:0.5221491335276774\n",
      "train loss:0.6996238377254179\n",
      "train loss:0.6159363328190464\n",
      "train loss:0.5177163853605967\n",
      "train loss:0.7824770272475345\n",
      "train loss:0.8518053582387985\n",
      "train loss:0.757733490350881\n",
      "train loss:0.4838173414752608\n",
      "train loss:0.6157653736673463\n",
      "train loss:0.6186373645283647\n",
      "train loss:0.6778317966752445\n",
      "train loss:0.561338081285351\n",
      "train loss:0.49489749470816313\n",
      "train loss:0.618911865615469\n",
      "train loss:0.5470400714897796\n",
      "train loss:0.38051307273241547\n",
      "train loss:0.6103569061772646\n",
      "train loss:0.5130043578610355\n",
      "train loss:0.398809871197778\n",
      "train loss:0.7445554164188207\n",
      "train loss:0.5014221816136304\n",
      "train loss:0.49948264195388054\n",
      "train loss:0.898752527948709\n",
      "train loss:0.8869228810391278\n",
      "train loss:0.6220557719579789\n",
      "train loss:0.8266948134516614\n",
      "train loss:0.6039167719734804\n",
      "train loss:0.687119855391088\n",
      "train loss:0.5312445477499614\n",
      "train loss:0.5438931842137344\n",
      "train loss:0.6175213072556289\n",
      "train loss:0.5510122903199448\n",
      "train loss:0.8021985757943415\n",
      "train loss:0.5618677007702452\n",
      "train loss:0.62360109292569\n",
      "train loss:0.619644201680331\n",
      "train loss:0.562042716446056\n",
      "train loss:0.7925891235193407\n",
      "train loss:0.6798248066674042\n",
      "train loss:0.5032964004413983\n",
      "train loss:0.6182386023933342\n",
      "train loss:0.4854582271333128\n",
      "train loss:0.542543359656173\n",
      "train loss:0.4472135204717677\n",
      "train loss:0.5138477470133138\n",
      "train loss:0.8079370340605674\n",
      "train loss:0.5121732057428906\n",
      "train loss:0.5065118892740221\n",
      "train loss:0.9527240300085034\n",
      "train loss:0.720635076582634\n",
      "train loss:0.8087480412423927\n",
      "train loss:0.6051455378502395\n",
      "train loss:0.42982551289828236\n",
      "train loss:0.4286404437473245\n",
      "train loss:0.4204748353428499\n",
      "train loss:0.7062858390608724\n",
      "train loss:0.7057671591748449\n",
      "train loss:0.6168820748977957\n",
      "train loss:0.43065708196471464\n",
      "train loss:0.51027232569528\n",
      "train loss:0.5130122083277833\n",
      "train loss:1.0026686848308368\n",
      "train loss:0.6149754676295391\n",
      "train loss:0.6111971586474747\n",
      "train loss:0.5133486038744438\n",
      "train loss:0.4281355254304671\n",
      "train loss:0.6038565594198851\n",
      "train loss:0.6897080282076437\n",
      "train loss:0.6130680174213987\n",
      "train loss:0.6991906581190619\n",
      "train loss:0.7869395588470316\n",
      "train loss:0.4499416075795998\n",
      "train loss:0.6162914941582676\n",
      "train loss:0.689794036226973\n",
      "train loss:0.46057962514419215\n",
      "train loss:0.45336444329584175\n",
      "train loss:0.6958475941821232\n",
      "train loss:0.6093782295040118\n",
      "train loss:0.5253760657145812\n",
      "train loss:0.7727428705842209\n",
      "train loss:0.6965552677320761\n",
      "train loss:0.7005840945896912\n",
      "train loss:0.6185970857210421\n",
      "train loss:0.45183426304967755\n",
      "train loss:0.6055611835938144\n",
      "train loss:0.5223021138931203\n",
      "train loss:0.6936139155341488\n",
      "train loss:0.5157952630171765\n",
      "train loss:0.778695840735867\n",
      "train loss:0.5298467293570244\n",
      "train loss:0.7784095570842384\n",
      "train loss:0.6094539891683926\n",
      "train loss:0.6233935422942583\n",
      "=== epoch:6, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7547707441522029\n",
      "train loss:0.6863893040493716\n",
      "train loss:0.5527202781990861\n",
      "train loss:0.5535778340019997\n",
      "train loss:0.5522439328278914\n",
      "train loss:0.5459676394831509\n",
      "train loss:0.6744027868301271\n",
      "train loss:0.5392864106515127\n",
      "train loss:0.6971084587881771\n",
      "train loss:0.6828696839027601\n",
      "train loss:0.4520330499243387\n",
      "train loss:0.5272249918311254\n",
      "train loss:0.5225294944078496\n",
      "train loss:0.62185979995428\n",
      "train loss:0.41067385052922073\n",
      "train loss:0.5114488366763703\n",
      "train loss:0.5061355674389364\n",
      "train loss:0.6421592891786265\n",
      "train loss:0.22145705593840423\n",
      "train loss:0.5019823060500122\n",
      "train loss:0.7958928673849834\n",
      "train loss:0.6472504769803126\n",
      "train loss:0.48958577702395417\n",
      "train loss:0.5108594679738703\n",
      "train loss:0.5008311299985854\n",
      "train loss:0.5013895847602795\n",
      "train loss:0.4958373786010011\n",
      "train loss:0.3614344795413117\n",
      "train loss:0.49531440231889956\n",
      "train loss:0.5025978281127863\n",
      "train loss:0.4981550068710031\n",
      "train loss:0.6407401945674409\n",
      "train loss:0.633558357953653\n",
      "train loss:0.7312408693599831\n",
      "train loss:0.5042503481268256\n",
      "train loss:0.6174037716629733\n",
      "train loss:0.5253425035659464\n",
      "train loss:0.7841301863458301\n",
      "train loss:0.44005134169981874\n",
      "train loss:0.6931265502469376\n",
      "train loss:0.6885228167648275\n",
      "train loss:0.6198907434421056\n",
      "train loss:0.6819178715401544\n",
      "train loss:0.6203885347386404\n",
      "train loss:0.6759798261090709\n",
      "train loss:0.5079766042391626\n",
      "train loss:0.6241037812945628\n",
      "train loss:0.5634141784635405\n",
      "train loss:0.6169016762958905\n",
      "train loss:0.548883769211577\n",
      "train loss:0.5449678317202807\n",
      "train loss:0.5366521150973956\n",
      "train loss:0.8540652055220279\n",
      "train loss:0.694894722318168\n",
      "train loss:0.6125136799589186\n",
      "train loss:0.4422968153447953\n",
      "train loss:0.5209465817517963\n",
      "train loss:0.5141090315869563\n",
      "train loss:0.6112145840845791\n",
      "train loss:0.7136087001049913\n",
      "train loss:0.6125988610592424\n",
      "train loss:0.6142500453084254\n",
      "train loss:0.5091693453645133\n",
      "train loss:0.6182755106910128\n",
      "train loss:0.40092728820274404\n",
      "train loss:0.2612864910976874\n",
      "train loss:0.7478925593665031\n",
      "train loss:0.6276495847067665\n",
      "train loss:0.8638718349251686\n",
      "train loss:0.6141396754600954\n",
      "train loss:0.5108627597024598\n",
      "train loss:0.5067224412219604\n",
      "train loss:0.50010522764543\n",
      "train loss:0.5121067329595389\n",
      "train loss:0.49709806730936795\n",
      "train loss:0.8102890229203006\n",
      "train loss:0.5132129913607341\n",
      "train loss:0.607220546577043\n",
      "train loss:0.40860508779671534\n",
      "train loss:0.8104667077330211\n",
      "train loss:0.5191926786362003\n",
      "train loss:0.5135690390149197\n",
      "train loss:0.5081597268780824\n",
      "train loss:1.0468688895559564\n",
      "train loss:0.43937289098577237\n",
      "train loss:0.6922064289409897\n",
      "train loss:0.6867319279214218\n",
      "train loss:0.5456401963736203\n",
      "train loss:0.6044238495090034\n",
      "train loss:0.5458686374655594\n",
      "train loss:0.6155297845495136\n",
      "train loss:0.7484161959522264\n",
      "train loss:0.5425041177451959\n",
      "train loss:0.5465342825534691\n",
      "train loss:0.6813992624612876\n",
      "train loss:0.5380303401426845\n",
      "train loss:0.6059563503721789\n",
      "train loss:0.6921161688712504\n",
      "train loss:0.6117699768809827\n",
      "train loss:0.6110732101133237\n",
      "train loss:0.5273213420199914\n",
      "train loss:0.6042959513557297\n",
      "train loss:0.5107928407887885\n",
      "train loss:0.40971507988354616\n",
      "train loss:0.4942277963664109\n",
      "train loss:0.621829235361614\n",
      "train loss:0.506178045609581\n",
      "train loss:0.6187569260772767\n",
      "train loss:0.4973973556012538\n",
      "train loss:0.6345551056237622\n",
      "train loss:0.641489041101865\n",
      "train loss:0.3576752639198102\n",
      "train loss:0.7606863890767399\n",
      "train loss:0.4992806652417647\n",
      "train loss:0.7417131745430627\n",
      "train loss:0.5132751978194414\n",
      "train loss:0.5109866978053067\n",
      "train loss:0.7075171186440727\n",
      "train loss:0.5190829356007989\n",
      "train loss:0.42798200600495645\n",
      "train loss:0.5216463324418192\n",
      "train loss:0.7055960625053322\n",
      "train loss:0.5215982247291661\n",
      "train loss:0.6111385163901866\n",
      "train loss:0.6056593181700669\n",
      "train loss:0.6114637734759067\n",
      "train loss:0.5994559348354136\n",
      "train loss:0.32761397565336736\n",
      "train loss:0.517340207171274\n",
      "train loss:0.5033119370349183\n",
      "train loss:0.37959565066562384\n",
      "train loss:0.8478949859581417\n",
      "train loss:0.6309063364714531\n",
      "train loss:0.6229305814829191\n",
      "train loss:0.7345926063486634\n",
      "train loss:0.7248691403041407\n",
      "train loss:0.5161870610112673\n",
      "train loss:0.6131798822433818\n",
      "train loss:0.7802210660496997\n",
      "train loss:0.3554841234421096\n",
      "train loss:0.4441034739467831\n",
      "train loss:0.6850875821599284\n",
      "train loss:0.5253003937533581\n",
      "train loss:0.6062059384415575\n",
      "train loss:0.6885598966834945\n",
      "train loss:0.6828008211747422\n",
      "train loss:0.7646087206504049\n",
      "train loss:0.690021749119465\n",
      "train loss:0.6127069116120071\n",
      "train loss:0.6805839824640518\n",
      "train loss:0.6172322324183666\n",
      "train loss:0.4989179689859071\n",
      "train loss:0.5555088071338738\n",
      "=== epoch:7, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6070805665638783\n",
      "train loss:0.6878829483740216\n",
      "train loss:0.5382608367740909\n",
      "train loss:0.6191092561818058\n",
      "train loss:0.5119649175349306\n",
      "train loss:0.9318322456176296\n",
      "train loss:0.5990912865207036\n",
      "train loss:0.6785674039214595\n",
      "train loss:0.6199266670025224\n",
      "train loss:0.5293294724102477\n",
      "train loss:0.6021413907179728\n",
      "train loss:0.4369189919491368\n",
      "train loss:0.7716130141889934\n",
      "train loss:0.673712908132783\n",
      "train loss:0.6151990483088594\n",
      "train loss:0.3475682087226338\n",
      "train loss:0.4222772487845995\n",
      "train loss:0.8175683869120803\n",
      "train loss:0.5115684895899014\n",
      "train loss:0.3989107362783299\n",
      "train loss:0.4991255807286702\n",
      "train loss:0.5032820365715887\n",
      "train loss:0.7465134678257533\n",
      "train loss:0.6362950755470973\n",
      "train loss:0.6329589448554623\n",
      "train loss:0.6399084178153408\n",
      "train loss:0.9212937107956123\n",
      "train loss:0.6100189077400638\n",
      "train loss:0.596857926443958\n",
      "train loss:0.7781553177398495\n",
      "train loss:0.6798243219797684\n",
      "train loss:0.6216335134242974\n",
      "train loss:0.5190106839114637\n",
      "train loss:0.530566887137549\n",
      "train loss:0.6621203084499632\n",
      "train loss:0.6671560217784089\n",
      "train loss:0.6813073880217903\n",
      "train loss:0.5766900850331007\n",
      "train loss:0.6694797611499459\n",
      "train loss:0.5621171159869458\n",
      "train loss:0.5135989924758254\n",
      "train loss:0.7446961448044808\n",
      "train loss:0.7424962887183006\n",
      "train loss:0.5983115318893529\n",
      "train loss:0.7525679428290049\n",
      "train loss:0.4768833647066105\n",
      "train loss:0.6863340023600936\n",
      "train loss:0.5376363219160687\n",
      "train loss:0.511889049572745\n",
      "train loss:0.859247924482459\n",
      "train loss:0.5988605549252087\n",
      "train loss:0.6097988595330441\n",
      "train loss:0.5874134727540599\n",
      "train loss:0.4333995854760134\n",
      "train loss:0.3188911397501692\n",
      "train loss:0.608483134668204\n",
      "train loss:0.5015401597048306\n",
      "train loss:0.6457992255028444\n",
      "train loss:0.5052525119886911\n",
      "train loss:0.7753488791917271\n",
      "train loss:0.9705317845398627\n",
      "train loss:0.6236209376781869\n",
      "train loss:0.6029995536879674\n",
      "train loss:0.685585028633259\n",
      "train loss:0.5156940616500696\n",
      "train loss:0.44425089958595143\n",
      "train loss:0.6210469922964459\n",
      "train loss:0.6900995147106497\n",
      "train loss:0.5368488962150137\n",
      "train loss:0.6097447636171505\n",
      "train loss:0.6102432770287376\n",
      "train loss:0.6106306869598566\n",
      "train loss:0.5158024392483727\n",
      "train loss:0.6173829528438662\n",
      "train loss:0.4438486552413793\n",
      "train loss:0.51954675891747\n",
      "train loss:0.30125908239731997\n",
      "train loss:0.5097386912434587\n",
      "train loss:0.7334132346052147\n",
      "train loss:0.8584675614866109\n",
      "train loss:0.6079351644243467\n",
      "train loss:0.5660660431268846\n",
      "train loss:0.5014483929005952\n",
      "train loss:0.5185633805741913\n",
      "train loss:0.7367072815641553\n",
      "train loss:0.8191885252158121\n",
      "train loss:0.525271073435293\n",
      "train loss:0.5153074871262227\n",
      "train loss:0.5965423467128717\n",
      "train loss:0.5251895652144004\n",
      "train loss:0.573059988584102\n",
      "train loss:0.5059438318970114\n",
      "train loss:0.4363112135171268\n",
      "train loss:0.49875359803497865\n",
      "train loss:0.6051382369768972\n",
      "train loss:0.5807792446179615\n",
      "train loss:0.5072370392019593\n",
      "train loss:0.49124240875767394\n",
      "train loss:0.8467877928481503\n",
      "train loss:0.6193378937916929\n",
      "train loss:0.38820739397505527\n",
      "train loss:0.7244390282702942\n",
      "train loss:0.8927294582982472\n",
      "train loss:0.40953502772715283\n",
      "train loss:0.7050482403128224\n",
      "train loss:0.5395747522326428\n",
      "train loss:0.5368201413087862\n",
      "train loss:0.5199125295552651\n",
      "train loss:0.6716200431124741\n",
      "train loss:0.5803492782144136\n",
      "train loss:0.5465653784393452\n",
      "train loss:0.5900306696819075\n",
      "train loss:0.3298967430639636\n",
      "train loss:0.6374020606539432\n",
      "train loss:0.7260550129914931\n",
      "train loss:0.3713046317735423\n",
      "train loss:0.5111806844510259\n",
      "train loss:0.8464293964710127\n",
      "train loss:0.6314672496913449\n",
      "train loss:0.510494093908359\n",
      "train loss:0.6109078904960727\n",
      "train loss:0.4843461810664378\n",
      "train loss:0.8266829829960216\n",
      "train loss:0.6124283429914168\n",
      "train loss:0.614137404529049\n",
      "train loss:0.4234367409059726\n",
      "train loss:0.717427758738124\n",
      "train loss:0.6069594947304576\n",
      "train loss:0.5279345199622444\n",
      "train loss:0.6109014241054966\n",
      "train loss:0.5324658940392766\n",
      "train loss:0.8061497158793571\n",
      "train loss:0.7168684348439489\n",
      "train loss:0.6300334299060768\n",
      "train loss:0.6656321259289254\n",
      "train loss:0.669443908665033\n",
      "train loss:0.7341185506648163\n",
      "train loss:0.6221471315891882\n",
      "train loss:0.6256088298150243\n",
      "train loss:0.7494529908648793\n",
      "train loss:0.6085115880072131\n",
      "train loss:0.6346970749648102\n",
      "train loss:0.6390603597821938\n",
      "train loss:0.6437169856417362\n",
      "train loss:0.6309437073381002\n",
      "train loss:0.5893226103453557\n",
      "train loss:0.6793083461609513\n",
      "train loss:0.6469763182673041\n",
      "train loss:0.4867043950499843\n",
      "train loss:0.4770688110392157\n",
      "train loss:0.5441325196308703\n",
      "train loss:0.4191827940007779\n",
      "train loss:0.8273096633115106\n",
      "=== epoch:8, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.4873730763329651\n",
      "train loss:0.6411289603047585\n",
      "train loss:0.8755175235431307\n",
      "train loss:0.6266921939245511\n",
      "train loss:0.5057620357665558\n",
      "train loss:0.8120944071644063\n",
      "train loss:0.7319002336339687\n",
      "train loss:0.7025015673005767\n",
      "train loss:0.5183520447764043\n",
      "train loss:0.6949980677062915\n",
      "train loss:0.6804465962981034\n",
      "train loss:0.5533870107427505\n",
      "train loss:0.7014306154715356\n",
      "train loss:0.6227836172600343\n",
      "train loss:0.6786872569073952\n",
      "train loss:0.5757431013684162\n",
      "train loss:0.5632779792094303\n",
      "train loss:0.6203752022481961\n",
      "train loss:0.4980787199564488\n",
      "train loss:0.623303764491322\n",
      "train loss:0.615790555145755\n",
      "train loss:0.45950042713648276\n",
      "train loss:0.7445937620159743\n",
      "train loss:0.7853917225929732\n",
      "train loss:0.5363323253006576\n",
      "train loss:0.5177941431961379\n",
      "train loss:0.517724935582015\n",
      "train loss:0.4913327685029124\n",
      "train loss:0.8187752059062892\n",
      "train loss:0.2709126153934544\n",
      "train loss:0.6937271919972986\n",
      "train loss:0.5035439941773303\n",
      "train loss:0.729827219920727\n",
      "train loss:0.2545375658919766\n",
      "train loss:0.7566145221142703\n",
      "train loss:0.7409108358365566\n",
      "train loss:0.5087257712599415\n",
      "train loss:0.7149227718685937\n",
      "train loss:0.5165055983726197\n",
      "train loss:0.6094002360576313\n",
      "train loss:0.5108100175418498\n",
      "train loss:0.5479799450897674\n",
      "train loss:0.69058708200729\n",
      "train loss:0.6124144728893154\n",
      "train loss:0.6120470629330841\n",
      "train loss:0.5204661858197155\n",
      "train loss:0.3640084700990441\n",
      "train loss:0.7451842852389141\n",
      "train loss:0.6906284619949231\n",
      "train loss:0.41958738450119826\n",
      "train loss:0.5590812231178423\n",
      "train loss:0.7011573681813936\n",
      "train loss:0.31621374761703847\n",
      "train loss:0.5703228584099573\n",
      "train loss:0.6080242245042186\n",
      "train loss:0.7833171418075004\n",
      "train loss:0.7119148346503372\n",
      "train loss:0.6850504702865907\n",
      "train loss:0.4569586764162061\n",
      "train loss:0.5198470255108087\n",
      "train loss:0.6873333608522012\n",
      "train loss:0.4213142959132405\n",
      "train loss:0.4961951332878399\n",
      "train loss:0.5018406405491221\n",
      "train loss:0.5033126823740071\n",
      "train loss:0.49296499101238134\n",
      "train loss:0.6834594635899962\n",
      "train loss:0.8310205314162602\n",
      "train loss:0.5117567008854504\n",
      "train loss:0.7150683575249747\n",
      "train loss:0.4242942674921054\n",
      "train loss:0.361647119883276\n",
      "train loss:0.7322188864200847\n",
      "train loss:0.6191010586410421\n",
      "train loss:0.7343661895344679\n",
      "train loss:0.4271202623221395\n",
      "train loss:0.7818901624221359\n",
      "train loss:0.6119583780361828\n",
      "train loss:0.7872857695237017\n",
      "train loss:0.6428701841048046\n",
      "train loss:0.6723383265310765\n",
      "train loss:0.5550776874667781\n",
      "train loss:0.7389863330481816\n",
      "train loss:0.6717581853850703\n",
      "train loss:0.6147162765204002\n",
      "train loss:0.640690137024938\n",
      "train loss:0.6281216898054293\n",
      "train loss:0.6439279096585272\n",
      "train loss:0.7264350948744128\n",
      "train loss:0.5842026543296179\n",
      "train loss:0.5596155189459054\n",
      "train loss:0.4232953831100958\n",
      "train loss:0.6251122112222356\n",
      "train loss:0.4719509517432468\n",
      "train loss:0.7196662224027249\n",
      "train loss:0.5032847959049785\n",
      "train loss:0.621129283071961\n",
      "train loss:0.798434819450603\n",
      "train loss:0.629256004889481\n",
      "train loss:0.5174145080710852\n",
      "train loss:0.4515323758523896\n",
      "train loss:0.6972059101608742\n",
      "train loss:0.5761293515257864\n",
      "train loss:0.6354600726547608\n",
      "train loss:0.5204422757890178\n",
      "train loss:0.4967743263437764\n",
      "train loss:0.6165027859450197\n",
      "train loss:0.5844274517576149\n",
      "train loss:0.5177432866937722\n",
      "train loss:0.6074019425508668\n",
      "train loss:0.6068201439560654\n",
      "train loss:0.9026403554055206\n",
      "train loss:0.5172769672627298\n",
      "train loss:0.7313694452354579\n",
      "train loss:0.709822316223965\n",
      "train loss:0.6617968000652206\n",
      "train loss:0.790632762492573\n",
      "train loss:0.5200894704227906\n",
      "train loss:0.5028042520908498\n",
      "train loss:0.6174849184928765\n",
      "train loss:0.7195741633383959\n",
      "train loss:0.44611602943469497\n",
      "train loss:0.5984021386319909\n",
      "train loss:0.5581461245579014\n",
      "train loss:0.5326244182784527\n",
      "train loss:0.6623584773929632\n",
      "train loss:0.5086730332340215\n",
      "train loss:0.49498658460192946\n",
      "train loss:0.5182249989534374\n",
      "train loss:0.7240029187020326\n",
      "train loss:0.7282553694584528\n",
      "train loss:0.7057634172690899\n",
      "train loss:0.7574757622222164\n",
      "train loss:0.6762422716222958\n",
      "train loss:0.5840136573245726\n",
      "train loss:0.6282532695316543\n",
      "train loss:0.5864435232512648\n",
      "train loss:0.5660138065563861\n",
      "train loss:0.6936528117880668\n",
      "train loss:0.7418891090480728\n",
      "train loss:0.7000645115465959\n",
      "train loss:0.6087277238350182\n",
      "train loss:0.6265525590990115\n",
      "train loss:0.5574476335087943\n",
      "train loss:0.6018182383443877\n",
      "train loss:0.5909170805071684\n",
      "train loss:0.47523283427917085\n",
      "train loss:0.4743502471144506\n",
      "train loss:0.49242116978943173\n",
      "train loss:0.7057828581156693\n",
      "train loss:0.41024916378738874\n",
      "train loss:0.3980909774159929\n",
      "train loss:0.7362633517555086\n",
      "=== epoch:9, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.501923085930931\n",
      "train loss:1.0218960627927445\n",
      "train loss:0.7645520974554898\n",
      "train loss:0.5541545510224383\n",
      "train loss:0.666266292046418\n",
      "train loss:0.6343606838882376\n",
      "train loss:0.6127183020210225\n",
      "train loss:0.35933305065837706\n",
      "train loss:0.4429572504777866\n",
      "train loss:0.5403521675844092\n",
      "train loss:0.682964282441129\n",
      "train loss:0.5991964506994607\n",
      "train loss:0.42671680472313545\n",
      "train loss:0.41406161119465024\n",
      "train loss:0.6008890852213998\n",
      "train loss:0.5882632962504669\n",
      "train loss:0.3881544563512343\n",
      "train loss:0.24100686349834705\n",
      "train loss:0.30067749960767937\n",
      "train loss:0.4915285046647745\n",
      "train loss:0.5224833987621983\n",
      "train loss:0.7466539246779444\n",
      "train loss:0.34411555706416624\n",
      "train loss:0.31752878364380865\n",
      "train loss:0.8713313143162621\n",
      "train loss:0.3198678477140314\n",
      "train loss:0.6508989312132502\n",
      "train loss:0.6171358546217622\n",
      "train loss:0.5949725504042204\n",
      "train loss:0.7574924615852511\n",
      "train loss:0.8093889622655672\n",
      "train loss:0.5773401613014669\n",
      "train loss:0.5917955210301432\n",
      "train loss:0.6009757681753126\n",
      "train loss:0.6079066985689667\n",
      "train loss:0.7909787293881807\n",
      "train loss:0.6560455151086413\n",
      "train loss:0.6028090827408009\n",
      "train loss:0.6353708310433769\n",
      "train loss:0.5599490826603917\n",
      "train loss:0.5651246166745364\n",
      "train loss:0.5914889779139021\n",
      "train loss:0.6093150513656358\n",
      "train loss:0.549584468071094\n",
      "train loss:0.6578503694632908\n",
      "train loss:0.8702535118154568\n",
      "train loss:0.5206505205773284\n",
      "train loss:0.5628002875285751\n",
      "train loss:0.6200668002994223\n",
      "train loss:0.8044415153215242\n",
      "train loss:0.6379756705182991\n",
      "train loss:0.596923174970849\n",
      "train loss:0.616873153317907\n",
      "train loss:0.6082405541243281\n",
      "train loss:0.4311285052960992\n",
      "train loss:0.5786614969454864\n",
      "train loss:0.6073595733849928\n",
      "train loss:0.6385371105567164\n",
      "train loss:0.6173008829430872\n",
      "train loss:0.6484069786198714\n",
      "train loss:0.38734757615763676\n",
      "train loss:0.4937593305443969\n",
      "train loss:0.985519853270387\n",
      "train loss:0.2817372193396678\n",
      "train loss:0.7867317864868848\n",
      "train loss:0.7771205286105662\n",
      "train loss:0.5292596638168254\n",
      "train loss:0.6119044592104921\n",
      "train loss:0.5419459312806961\n",
      "train loss:0.6586584075567269\n",
      "train loss:0.5303028094213474\n",
      "train loss:0.527920958503045\n",
      "train loss:0.6558177730693362\n",
      "train loss:0.6395268686349732\n",
      "train loss:0.6767828688511299\n",
      "train loss:0.613748517977559\n",
      "train loss:0.3635122862087736\n",
      "train loss:0.4532464375073282\n",
      "train loss:0.725009551026122\n",
      "train loss:0.3531416460317536\n",
      "train loss:0.3072657023269695\n",
      "train loss:0.345555136983539\n",
      "train loss:0.3316766815149204\n",
      "train loss:0.32090962475492285\n",
      "train loss:0.6013280059752174\n",
      "train loss:1.1674647483914335\n",
      "train loss:0.7311919979405959\n",
      "train loss:0.5997547012600936\n",
      "train loss:0.8129099477137551\n",
      "train loss:0.3318498448774497\n",
      "train loss:0.7563772683909097\n",
      "train loss:0.43228514524166195\n",
      "train loss:0.5181087435172763\n",
      "train loss:0.6852324688542498\n",
      "train loss:0.5929077426049562\n",
      "train loss:0.5597863197395216\n",
      "train loss:0.46974241504650616\n",
      "train loss:0.7209949975080441\n",
      "train loss:0.6506273805999754\n",
      "train loss:0.6210662766222441\n",
      "train loss:0.8044428083524267\n",
      "train loss:0.5964323547094224\n",
      "train loss:0.502052685713261\n",
      "train loss:0.5848705860836304\n",
      "train loss:0.6088407138874814\n",
      "train loss:0.6394829021687796\n",
      "train loss:0.41884662969795006\n",
      "train loss:0.5288640587738765\n",
      "train loss:0.6987020990959717\n",
      "train loss:0.5057005594839031\n",
      "train loss:0.8322782004128338\n",
      "train loss:0.6180061538326507\n",
      "train loss:0.6250302093874537\n",
      "train loss:0.39576428090754023\n",
      "train loss:0.5073231064724522\n",
      "train loss:0.7462284688302994\n",
      "train loss:0.7633323927478349\n",
      "train loss:0.613894351359723\n",
      "train loss:0.8213526894789901\n",
      "train loss:0.594591666606996\n",
      "train loss:0.6124970387614538\n",
      "train loss:0.6202898050487997\n",
      "train loss:0.5445741124579052\n",
      "train loss:0.582083066833021\n",
      "train loss:0.6065553672666338\n",
      "train loss:0.6150485639178181\n",
      "train loss:0.7205630067908491\n",
      "train loss:0.6816855315996075\n",
      "train loss:0.4764034443695734\n",
      "train loss:0.7874030097603134\n",
      "train loss:0.5700208228251988\n",
      "train loss:0.504401894888049\n",
      "train loss:0.6983161811517427\n",
      "train loss:0.5910941656324604\n",
      "train loss:0.7054764616565021\n",
      "train loss:0.4053384512846566\n",
      "train loss:0.7287658029944103\n",
      "train loss:0.7775084945531178\n",
      "train loss:0.5455010124377617\n",
      "train loss:0.6712679505525747\n",
      "train loss:0.6040817594728691\n",
      "train loss:0.6759944950582314\n",
      "train loss:0.4800369722595814\n",
      "train loss:0.4466063456974988\n",
      "train loss:0.5071424738585766\n",
      "train loss:0.46695263142488336\n",
      "train loss:0.5845759110737453\n",
      "train loss:0.919747940679326\n",
      "train loss:0.39462274961526966\n",
      "train loss:0.6171259456037446\n",
      "train loss:0.903414124199268\n",
      "train loss:0.8549985573456189\n",
      "train loss:0.3337438414360564\n",
      "=== epoch:10, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5101882284541153\n",
      "train loss:0.6583678856314734\n",
      "train loss:0.770325690499732\n",
      "train loss:0.4925200357793099\n",
      "train loss:0.4728835532874731\n",
      "train loss:0.5383129825364239\n",
      "train loss:0.3589331766487221\n",
      "train loss:0.5617946233602836\n",
      "train loss:0.6056678839644536\n",
      "train loss:0.45168043432778304\n",
      "train loss:0.7062091480363615\n",
      "train loss:0.6667455381555765\n",
      "train loss:0.4178240713710643\n",
      "train loss:0.39906646005622565\n",
      "train loss:0.8265591611744127\n",
      "train loss:0.4924226655040518\n",
      "train loss:0.4103989830243352\n",
      "train loss:0.5634860933271645\n",
      "train loss:0.6762222293987178\n",
      "train loss:0.42396604515415676\n",
      "train loss:0.46169612834881457\n",
      "train loss:0.37214556453175074\n",
      "train loss:0.6973374684553738\n",
      "train loss:0.6702737742967171\n",
      "train loss:0.7533454511530998\n",
      "train loss:0.6017374033140824\n",
      "train loss:0.5595739818647517\n",
      "train loss:0.5669151872933422\n",
      "train loss:0.5184492686844301\n",
      "train loss:0.6089246879123493\n",
      "train loss:0.5266168544433252\n",
      "train loss:0.4843291324082247\n",
      "train loss:0.5488070240083947\n",
      "train loss:0.6290678735033602\n",
      "train loss:0.7673060584611067\n",
      "train loss:0.5632017943538399\n",
      "train loss:0.7328133870005027\n",
      "train loss:0.7477318943770974\n",
      "train loss:0.6538408626460391\n",
      "train loss:0.4210908059994044\n",
      "train loss:0.8576430356404083\n",
      "train loss:0.6307070489771177\n",
      "train loss:0.6406904525142073\n",
      "train loss:0.6675870481003237\n",
      "train loss:0.7062497509696711\n",
      "train loss:0.6079563013716609\n",
      "train loss:0.5680224860909735\n",
      "train loss:0.5828093821979528\n",
      "train loss:0.6007949558172918\n",
      "train loss:0.6294044379123978\n",
      "train loss:0.5378574871115113\n",
      "train loss:0.816990740530527\n",
      "train loss:0.5800367868886622\n",
      "train loss:0.6176495475333429\n",
      "train loss:0.6224870617964691\n",
      "train loss:0.6437832469931155\n",
      "train loss:0.5771985078646694\n",
      "train loss:0.5089060382878793\n",
      "train loss:0.4921732365727623\n",
      "train loss:0.6260804862728032\n",
      "train loss:0.5130285129285024\n",
      "train loss:0.417023947793232\n",
      "train loss:0.5326656170776305\n",
      "train loss:0.8357754205712048\n",
      "train loss:0.3590486194493136\n",
      "train loss:0.3600190925750799\n",
      "train loss:0.3473665904264489\n",
      "train loss:0.4977737336172809\n",
      "train loss:1.1259701702644622\n",
      "train loss:0.48206658239144656\n",
      "train loss:0.4804755499987084\n",
      "train loss:0.6069491237649973\n",
      "train loss:0.5998481402697153\n",
      "train loss:0.3910103790034214\n",
      "train loss:0.5269278463492301\n",
      "train loss:0.6010373789943455\n",
      "train loss:0.39979501990045885\n",
      "train loss:0.6184962318558271\n",
      "train loss:0.5732830789398989\n",
      "train loss:0.8906303335938981\n",
      "train loss:0.47625884835506815\n",
      "train loss:0.5796588961730628\n",
      "train loss:0.8012098465471083\n",
      "train loss:0.4677608910864988\n",
      "train loss:0.6329759248327183\n",
      "train loss:0.7645205177399581\n",
      "train loss:0.5863301960413319\n",
      "train loss:0.7386138863020848\n",
      "train loss:0.5643410444774968\n",
      "train loss:0.614549463214093\n",
      "train loss:0.5926855941518349\n",
      "train loss:0.4677714116801435\n",
      "train loss:0.44713476822297304\n",
      "train loss:0.5982790415822158\n",
      "train loss:0.6712092260778819\n",
      "train loss:0.7813564310099066\n",
      "train loss:0.5725171775465313\n",
      "train loss:0.5178835922011771\n",
      "train loss:0.6819020066748929\n",
      "train loss:0.7875652928332684\n",
      "train loss:0.7474859661581837\n",
      "train loss:0.7273931083592396\n",
      "train loss:0.4785462713161321\n",
      "train loss:0.6589216112214165\n",
      "train loss:0.7022296693041079\n",
      "train loss:0.5231866762813209\n",
      "train loss:0.5816345488570366\n",
      "train loss:0.5735214393641684\n",
      "train loss:0.5579065373346903\n",
      "train loss:0.4725033735464793\n",
      "train loss:0.48579510239846924\n",
      "train loss:0.38009177799502736\n",
      "train loss:0.35026649308643437\n",
      "train loss:0.7429526844259088\n",
      "train loss:0.43955309908080736\n",
      "train loss:0.7641183444775749\n",
      "train loss:0.49928779645081994\n",
      "train loss:0.34332681765094414\n",
      "train loss:0.2846785343566349\n",
      "train loss:0.7343628105669273\n",
      "train loss:0.8008694883249424\n",
      "train loss:0.5731828965499259\n",
      "train loss:0.22013176131490267\n",
      "train loss:0.8419147999535816\n",
      "train loss:0.5616447310466861\n",
      "train loss:0.3646594266632368\n",
      "train loss:0.3539371768535968\n",
      "train loss:0.5748544451824067\n",
      "train loss:0.530276111290316\n",
      "train loss:0.7907778331549142\n",
      "train loss:0.5485374464624885\n",
      "train loss:0.723165427059176\n",
      "train loss:0.5008537497498812\n",
      "train loss:0.8657463811916841\n",
      "train loss:0.6030240090059887\n",
      "train loss:0.7469680570303489\n",
      "train loss:0.4302402072445399\n",
      "train loss:0.509904504854369\n",
      "train loss:0.6414386265873668\n",
      "train loss:0.6784985000608883\n",
      "train loss:0.5684537071004312\n",
      "train loss:0.6726223332069826\n",
      "train loss:0.5397910818286825\n",
      "train loss:0.6166944876200601\n",
      "train loss:0.5660232107289526\n",
      "train loss:0.6093200426323829\n",
      "train loss:0.5772464130666247\n",
      "train loss:0.5805920745745343\n",
      "train loss:0.4317012910957354\n",
      "train loss:0.5271355404022546\n",
      "train loss:0.7084811501915956\n",
      "train loss:0.6850558816304132\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5490196078431373\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=400, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46344efe-e947-45a5-94d6-a0e473a9ae02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2794080069160065\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:2.2424703261298697\n",
      "train loss:2.14955597367159\n",
      "train loss:1.9927190711443015\n",
      "train loss:1.833550754861264\n",
      "train loss:1.7158363159691568\n",
      "train loss:1.2208479633594413\n",
      "train loss:1.0197192884251771\n",
      "train loss:0.721341907432486\n",
      "train loss:0.531215337389711\n",
      "train loss:1.0970378201524553\n",
      "train loss:0.8428356094815861\n",
      "train loss:0.613973482149439\n",
      "train loss:0.5216443441456508\n",
      "train loss:0.6266810760879788\n",
      "train loss:0.6956142047474093\n",
      "train loss:0.5709550312132694\n",
      "train loss:0.6049725969746842\n",
      "train loss:0.8261295263418683\n",
      "train loss:0.48729506491452373\n",
      "train loss:0.7311326602475925\n",
      "train loss:0.6121988278941356\n",
      "train loss:0.6847808223680005\n",
      "train loss:0.41921257393996647\n",
      "train loss:0.504302606995542\n",
      "train loss:0.5214033766757946\n",
      "train loss:0.7347638736905588\n",
      "train loss:0.9916072757896753\n",
      "train loss:0.4079067475379836\n",
      "train loss:0.29538909436854305\n",
      "train loss:0.6062532296309794\n",
      "train loss:0.759777164790995\n",
      "train loss:0.6354919447349141\n",
      "train loss:0.6570801786659201\n",
      "train loss:0.650603934153893\n",
      "train loss:0.5896431693578638\n",
      "train loss:0.5504064593825682\n",
      "train loss:0.6198639292248755\n",
      "train loss:0.9035507439165947\n",
      "train loss:0.7299560305327163\n",
      "train loss:0.6596563787348558\n",
      "train loss:0.6793654133010951\n",
      "train loss:0.6947436990141328\n",
      "train loss:0.6910809491999064\n",
      "train loss:0.6846712797665668\n",
      "train loss:0.6578068534301057\n",
      "train loss:0.6812665169662357\n",
      "train loss:0.40799540267229795\n",
      "train loss:0.8188314622418756\n",
      "train loss:0.7703381029562678\n",
      "train loss:0.7356951149603974\n",
      "train loss:0.7080195313729394\n",
      "train loss:0.5652634271060066\n",
      "train loss:0.6769119069866846\n",
      "train loss:0.6930695700287417\n",
      "train loss:0.6000534985610095\n",
      "train loss:0.6814342756620971\n",
      "train loss:0.4750475009209615\n",
      "train loss:0.4204714063010894\n",
      "train loss:0.7262304154562967\n",
      "train loss:0.8585826970937489\n",
      "train loss:0.6421457925469227\n",
      "train loss:0.30890684596356216\n",
      "train loss:0.6791154987099156\n",
      "train loss:0.6287227018771422\n",
      "train loss:0.7466442788716041\n",
      "train loss:0.6993688350229037\n",
      "train loss:0.6714011648992424\n",
      "train loss:0.6602043366942459\n",
      "train loss:0.6981125976722312\n",
      "train loss:0.7089043697049962\n",
      "train loss:0.6993591740987611\n",
      "train loss:0.6353920187787871\n",
      "train loss:0.6714817714627587\n",
      "train loss:0.5567805017731629\n",
      "train loss:0.6057407877121903\n",
      "train loss:0.6374305851517166\n",
      "train loss:0.46886402358462387\n",
      "train loss:0.35344641517574094\n",
      "train loss:0.8242857391595526\n",
      "train loss:0.9366675443453708\n",
      "train loss:0.6870972461049752\n",
      "train loss:0.4973388987764732\n",
      "train loss:0.49182450654645926\n",
      "train loss:0.7363446202352271\n",
      "train loss:0.3092459133628145\n",
      "train loss:0.7028522071099016\n",
      "train loss:0.6718785464570279\n",
      "train loss:0.8136257890850038\n",
      "train loss:0.6235076746641761\n",
      "train loss:0.5390335149413534\n",
      "train loss:0.5946957005604905\n",
      "train loss:0.5820318501135064\n",
      "train loss:0.510148052501618\n",
      "train loss:0.5566440925740785\n",
      "train loss:0.4532988100556027\n",
      "train loss:0.504228883370117\n",
      "train loss:0.7697908276449082\n",
      "train loss:0.7607435912860712\n",
      "train loss:0.38630401140677656\n",
      "train loss:0.6323483912435429\n",
      "train loss:0.5100330481041572\n",
      "train loss:0.7820805474709331\n",
      "train loss:0.8549372248906592\n",
      "train loss:0.7266804306163059\n",
      "train loss:0.704872397955459\n",
      "train loss:0.6434707572129085\n",
      "train loss:0.6110898893762602\n",
      "train loss:0.73795083118086\n",
      "train loss:0.7075146458555108\n",
      "train loss:0.6317488730200785\n",
      "train loss:0.6581848523088413\n",
      "train loss:0.6214326992225832\n",
      "train loss:0.5968752294326585\n",
      "train loss:0.5580624241338503\n",
      "train loss:0.7316160959113541\n",
      "train loss:0.5287575520403245\n",
      "train loss:0.5573220684309504\n",
      "train loss:0.47054107629862657\n",
      "train loss:0.7217608247298515\n",
      "train loss:0.4194240971986566\n",
      "train loss:0.6341116904647341\n",
      "train loss:0.7496882359642844\n",
      "train loss:0.49596287690226476\n",
      "train loss:0.5166389469423914\n",
      "train loss:0.7951419702653955\n",
      "train loss:0.5350516941391487\n",
      "train loss:0.7540923863708203\n",
      "train loss:0.7262935981668319\n",
      "train loss:0.4825214981918789\n",
      "train loss:0.6059771696063484\n",
      "train loss:0.6824924543507759\n",
      "train loss:0.6813585841485438\n",
      "train loss:0.7474973168339887\n",
      "train loss:0.6220455773541784\n",
      "train loss:0.672360393411758\n",
      "train loss:0.6756965650834801\n",
      "train loss:0.5806587621751549\n",
      "train loss:0.7033661480149347\n",
      "train loss:0.6851526568626779\n",
      "train loss:0.6031759491608412\n",
      "train loss:0.6780685585991397\n",
      "train loss:0.5683420215953329\n",
      "train loss:0.6763061505478015\n",
      "train loss:0.6699728195026828\n",
      "train loss:0.5690698421006235\n",
      "train loss:0.7502233786034657\n",
      "train loss:0.6150679744765608\n",
      "train loss:0.5575616453597916\n",
      "train loss:0.3607338258300993\n",
      "train loss:0.5894784196234655\n",
      "train loss:0.38983507649187304\n",
      "train loss:0.8954114169056524\n",
      "train loss:0.4980011899305232\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.3429962689838441\n",
      "train loss:0.6465276066404887\n",
      "train loss:0.619723736743633\n",
      "train loss:0.48837926268266907\n",
      "train loss:0.4959769611645439\n",
      "train loss:0.9310759065195547\n",
      "train loss:0.6177910517545102\n",
      "train loss:0.5727374108172385\n",
      "train loss:0.5139854929056004\n",
      "train loss:0.7036418996446498\n",
      "train loss:0.7946587915461776\n",
      "train loss:0.623541546471919\n",
      "train loss:0.6272760177276014\n",
      "train loss:0.6699013399685743\n",
      "train loss:0.6767461878867778\n",
      "train loss:0.7168771556864244\n",
      "train loss:0.6611392262073321\n",
      "train loss:0.6639783519246139\n",
      "train loss:0.6166551044517476\n",
      "train loss:0.5944865843436203\n",
      "train loss:0.6064946623251537\n",
      "train loss:0.63185155598351\n",
      "train loss:0.5597345567481526\n",
      "train loss:0.6860383539763248\n",
      "train loss:0.4283561695997008\n",
      "train loss:0.5997342935426928\n",
      "train loss:0.5096671427462227\n",
      "train loss:0.35120442884295594\n",
      "train loss:0.5246360725374581\n",
      "train loss:0.9979110854526139\n",
      "train loss:0.48621260783241427\n",
      "train loss:0.5184670031074237\n",
      "train loss:0.337098695564747\n",
      "train loss:0.3797008749624884\n",
      "train loss:0.32184124080035226\n",
      "train loss:0.8205509651018323\n",
      "train loss:0.49766106048138925\n",
      "train loss:0.48158734980647033\n",
      "train loss:1.1178610630476546\n",
      "train loss:0.6348436973039963\n",
      "train loss:0.33309704487959846\n",
      "train loss:0.7086025438884963\n",
      "train loss:0.7467736342878193\n",
      "train loss:0.5466704504849836\n",
      "train loss:0.625873388807118\n",
      "train loss:0.5185175382836512\n",
      "train loss:0.6242458048613108\n",
      "train loss:0.5133320832159936\n",
      "train loss:0.5447943541746242\n",
      "train loss:0.4685464127413276\n",
      "train loss:0.6889643940292092\n",
      "train loss:0.52855236881407\n",
      "train loss:0.48222828611973567\n",
      "train loss:0.39224982886438287\n",
      "train loss:0.4688586604558376\n",
      "train loss:0.5302575685737408\n",
      "train loss:0.6514649867230414\n",
      "train loss:0.6586888569069875\n",
      "train loss:0.845118420330141\n",
      "train loss:0.32857974873543827\n",
      "train loss:0.799993419863885\n",
      "train loss:0.35299152256631944\n",
      "train loss:0.6130806898491684\n",
      "train loss:0.761690145080786\n",
      "train loss:0.5106418079261225\n",
      "train loss:0.4276717971367396\n",
      "train loss:0.7779598153782991\n",
      "train loss:0.821803515499133\n",
      "train loss:0.5532384307859499\n",
      "train loss:0.625086127623022\n",
      "train loss:0.7559945936046935\n",
      "train loss:0.6710998243387913\n",
      "train loss:0.6815192249768307\n",
      "train loss:0.6978597379766855\n",
      "train loss:0.6852478817356308\n",
      "train loss:0.6740348869012063\n",
      "train loss:0.7079067422796634\n",
      "train loss:0.6962150771216512\n",
      "train loss:0.6952160866941558\n",
      "train loss:0.6881384749498554\n",
      "train loss:0.6850839449625015\n",
      "train loss:0.6687017992663338\n",
      "train loss:0.673043662178524\n",
      "train loss:0.649211916979423\n",
      "train loss:0.7093927696277952\n",
      "train loss:0.6333864575053614\n",
      "train loss:0.5300439719656761\n",
      "train loss:0.6914890412089848\n",
      "train loss:0.6260806040406826\n",
      "train loss:0.6032597403469305\n",
      "train loss:0.5274208807005841\n",
      "train loss:0.6391274610139454\n",
      "train loss:0.510196196297337\n",
      "train loss:0.3538762739893045\n",
      "train loss:0.7688907359245108\n",
      "train loss:0.8955416741362715\n",
      "train loss:0.8709933792474661\n",
      "train loss:0.818937389923881\n",
      "train loss:0.5140391505658586\n",
      "train loss:0.43651232597125367\n",
      "train loss:0.7531114275506785\n",
      "train loss:0.5365030313809784\n",
      "train loss:0.6304804380153619\n",
      "train loss:0.7462892158730405\n",
      "train loss:0.42066407609210454\n",
      "train loss:0.7655457419517181\n",
      "train loss:0.6713947296378829\n",
      "train loss:0.6681426466743583\n",
      "train loss:0.6337344416525765\n",
      "train loss:0.6253327454174855\n",
      "train loss:0.6490863083542164\n",
      "train loss:0.617879562469634\n",
      "train loss:0.678386853461811\n",
      "train loss:0.6324846244614184\n",
      "train loss:0.6671837430575851\n",
      "train loss:0.6253364035338844\n",
      "train loss:0.6115011779808993\n",
      "train loss:0.5491875230506909\n",
      "train loss:0.6847455820445074\n",
      "train loss:0.4433872881675562\n",
      "train loss:0.6336619342230503\n",
      "train loss:0.6019540401445023\n",
      "train loss:0.70991066689452\n",
      "train loss:0.8456420990614296\n",
      "train loss:0.6116033047332483\n",
      "train loss:0.5330837797465972\n",
      "train loss:0.44071639845669336\n",
      "train loss:0.8635213757214913\n",
      "train loss:0.5413009902293469\n",
      "train loss:0.4368601331977273\n",
      "train loss:0.6055611933270488\n",
      "train loss:0.6025541514092168\n",
      "train loss:0.5952720291979221\n",
      "train loss:0.6277116808941374\n",
      "train loss:0.796483953055571\n",
      "train loss:0.5944741024891445\n",
      "train loss:0.5995465328544043\n",
      "train loss:0.5281789172286839\n",
      "train loss:0.4360631225535346\n",
      "train loss:0.6075730946430535\n",
      "train loss:0.7095593663563311\n",
      "train loss:0.4199025695227407\n",
      "train loss:0.5249308379007297\n",
      "train loss:0.5703466082821379\n",
      "train loss:0.6245256047418045\n",
      "train loss:0.5045009930885813\n",
      "train loss:0.6520265974842873\n",
      "train loss:0.3784941876818593\n",
      "train loss:0.3668226141856329\n",
      "train loss:0.8958376471165606\n",
      "train loss:0.6363712213234589\n",
      "train loss:0.6185105239380498\n",
      "train loss:0.6180314696954515\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7002767567684358\n",
      "train loss:0.5261419971453642\n",
      "train loss:0.6064271354677394\n",
      "train loss:0.5428820943430978\n",
      "train loss:0.660508821461454\n",
      "train loss:0.5476579828610575\n",
      "train loss:0.5354839430407543\n",
      "train loss:0.37643377360264985\n",
      "train loss:0.4212952899154782\n",
      "train loss:0.8028564968704428\n",
      "train loss:0.7918837441064859\n",
      "train loss:0.6112366753348304\n",
      "train loss:0.8939574242493211\n",
      "train loss:0.514576990677153\n",
      "train loss:0.6871411829904934\n",
      "train loss:0.5252170889813468\n",
      "train loss:0.7532718269251581\n",
      "train loss:0.7323126320302442\n",
      "train loss:0.5668619739438581\n",
      "train loss:0.6791497750292474\n",
      "train loss:0.6658092821486198\n",
      "train loss:0.6830841289402677\n",
      "train loss:0.5444125641016189\n",
      "train loss:0.5844817717608792\n",
      "train loss:0.5169011673923696\n",
      "train loss:0.6629889397266486\n",
      "train loss:0.6157443818228451\n",
      "train loss:0.6891198676097363\n",
      "train loss:0.6882629836938154\n",
      "train loss:0.531689466927455\n",
      "train loss:0.6122851242881016\n",
      "train loss:0.7900662121300245\n",
      "train loss:0.7607522704119443\n",
      "train loss:0.7263342433811275\n",
      "train loss:0.46300852846341617\n",
      "train loss:0.8116738459333485\n",
      "train loss:0.48734339609162836\n",
      "train loss:0.5507206793665824\n",
      "train loss:0.6131136558860742\n",
      "train loss:0.6725955778654086\n",
      "train loss:0.7035725544554293\n",
      "train loss:0.6105338935577327\n",
      "train loss:0.6030848466330381\n",
      "train loss:0.6861200186312268\n",
      "train loss:0.4635680210686971\n",
      "train loss:0.8281468384101982\n",
      "train loss:0.6298145241614461\n",
      "train loss:0.5628029601220643\n",
      "train loss:0.8701917594665136\n",
      "train loss:0.6689855510755887\n",
      "train loss:0.6475514634970487\n",
      "train loss:0.6329942443048041\n",
      "train loss:0.7046212276296347\n",
      "train loss:0.6257664715905888\n",
      "train loss:0.7123052131328593\n",
      "train loss:0.5526314971442268\n",
      "train loss:0.7139949076850608\n",
      "train loss:0.668865920166413\n",
      "train loss:0.5776988683461945\n",
      "train loss:0.5252201397388502\n",
      "train loss:0.5610297828372965\n",
      "train loss:0.6679355859597094\n",
      "train loss:0.6974303118864482\n",
      "train loss:0.688312345453326\n",
      "train loss:0.685551871267131\n",
      "train loss:0.6823249417196349\n",
      "train loss:0.5967537986702149\n",
      "train loss:0.5156587341926595\n",
      "train loss:0.6045461603314118\n",
      "train loss:0.5135668366394378\n",
      "train loss:0.6126330388242607\n",
      "train loss:0.6295755533107712\n",
      "train loss:0.7888736869454066\n",
      "train loss:0.4062158271783013\n",
      "train loss:0.6461703005975237\n",
      "train loss:0.6340046880541312\n",
      "train loss:0.38848384407778935\n",
      "train loss:0.5937262399173747\n",
      "train loss:0.6542886411854236\n",
      "train loss:0.6072982608934157\n",
      "train loss:0.8025741615778876\n",
      "train loss:0.5149618440505688\n",
      "train loss:0.599353246662133\n",
      "train loss:0.524981844404773\n",
      "train loss:0.7642125317227076\n",
      "train loss:0.6983407046388532\n",
      "train loss:0.6148228849629536\n",
      "train loss:0.671825397196744\n",
      "train loss:0.6151528886892571\n",
      "train loss:0.6138126829116859\n",
      "train loss:0.6666688791089981\n",
      "train loss:0.6327822440784752\n",
      "train loss:0.6709738415696207\n",
      "train loss:0.5810578260512982\n",
      "train loss:0.6684398921960434\n",
      "train loss:0.6286376496465441\n",
      "train loss:0.6041451540882556\n",
      "train loss:0.6849019661192239\n",
      "train loss:0.6140911441521295\n",
      "train loss:0.6785553121409449\n",
      "train loss:0.5176687948822067\n",
      "train loss:0.5461429523010705\n",
      "train loss:0.5944890004802555\n",
      "train loss:0.6078639033762583\n",
      "train loss:0.4949618550937035\n",
      "train loss:0.40848796176360447\n",
      "train loss:0.4807376998483326\n",
      "train loss:1.0017271062235797\n",
      "train loss:0.49263718628339\n",
      "train loss:0.4891766187068103\n",
      "train loss:0.38552930357876714\n",
      "train loss:0.6394040864788316\n",
      "train loss:0.6132287371448605\n",
      "train loss:0.6282541966014221\n",
      "train loss:0.4932118456708358\n",
      "train loss:0.6429053370015971\n",
      "train loss:0.5145889313688424\n",
      "train loss:0.47868371463114096\n",
      "train loss:0.38350335980062356\n",
      "train loss:0.6324050415579041\n",
      "train loss:0.5111260373795556\n",
      "train loss:0.5979889763717934\n",
      "train loss:0.7435214655557943\n",
      "train loss:0.39865767011665837\n",
      "train loss:0.48817218931917783\n",
      "train loss:0.6149458313698689\n",
      "train loss:0.6241619819108245\n",
      "train loss:0.7190307756255082\n",
      "train loss:0.5960447280456393\n",
      "train loss:0.6921748717693854\n",
      "train loss:0.5279088171926808\n",
      "train loss:0.6103647107514777\n",
      "train loss:0.6774321284972895\n",
      "train loss:0.7458217116664918\n",
      "train loss:0.6266360847784551\n",
      "train loss:0.4480427319754446\n",
      "train loss:0.49202903566191747\n",
      "train loss:0.6765890933783061\n",
      "train loss:0.7412496877117108\n",
      "train loss:0.692881377180594\n",
      "train loss:0.5965750028218584\n",
      "train loss:0.6519706475869745\n",
      "train loss:0.6192247507369714\n",
      "train loss:0.6016353554155282\n",
      "train loss:0.6804669294473663\n",
      "train loss:0.6934435976028062\n",
      "train loss:0.6838462081773132\n",
      "train loss:0.6700152699068715\n",
      "train loss:0.5423781477964462\n",
      "train loss:0.5507647044228492\n",
      "train loss:0.5505731055729706\n",
      "train loss:0.5316655450374287\n",
      "train loss:0.6897315173872198\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.49973378919225053\n",
      "train loss:0.492278008238049\n",
      "train loss:0.46375754274309366\n",
      "train loss:0.5094616046571374\n",
      "train loss:0.5018021649285157\n",
      "train loss:0.6228819275716685\n",
      "train loss:0.38140624379993515\n",
      "train loss:0.5283815908367838\n",
      "train loss:0.47181817797185144\n",
      "train loss:0.6673769963851076\n",
      "train loss:0.3215706471670638\n",
      "train loss:0.7805460390595146\n",
      "train loss:0.4833008901673625\n",
      "train loss:0.6398915750822513\n",
      "train loss:0.6462421605142569\n",
      "train loss:0.39493971886559925\n",
      "train loss:0.3861307186838851\n",
      "train loss:0.8004633188564851\n",
      "train loss:0.5877802782654534\n",
      "train loss:0.6925662464414344\n",
      "train loss:0.7426469024177166\n",
      "train loss:0.6153394752274813\n",
      "train loss:0.5029016149042393\n",
      "train loss:0.544247915819257\n",
      "train loss:0.6072704508606745\n",
      "train loss:0.6124982210849116\n",
      "train loss:0.5340350123516118\n",
      "train loss:0.5285339631448444\n",
      "train loss:0.5947415437064368\n",
      "train loss:0.7057461290959861\n",
      "train loss:0.5262924121114122\n",
      "train loss:0.7266325250058508\n",
      "train loss:0.6153390185667229\n",
      "train loss:0.7043504569916418\n",
      "train loss:0.5191798097917267\n",
      "train loss:0.4011473920191662\n",
      "train loss:0.7382162974120042\n",
      "train loss:0.8908904397064397\n",
      "train loss:0.3280905661144268\n",
      "train loss:0.8001548011949797\n",
      "train loss:0.7588418128227744\n",
      "train loss:0.7203920590803303\n",
      "train loss:0.7198141605937878\n",
      "train loss:0.6320024290202955\n",
      "train loss:0.641819264240741\n",
      "train loss:0.6375683540428106\n",
      "train loss:0.5820764634332332\n",
      "train loss:0.5283287027790363\n",
      "train loss:0.6331525629233458\n",
      "train loss:0.5706978066283909\n",
      "train loss:0.5785164947872826\n",
      "train loss:0.5512354881567697\n",
      "train loss:0.8399798004825803\n",
      "train loss:0.5167079550019601\n",
      "train loss:0.5994163074436706\n",
      "train loss:0.3742078187599464\n",
      "train loss:0.6060414987256901\n",
      "train loss:0.5080395227091836\n",
      "train loss:0.2032638433529974\n",
      "train loss:0.3323088635380666\n",
      "train loss:0.2927980449013906\n",
      "train loss:1.1017659916277691\n",
      "train loss:0.5134248134968666\n",
      "train loss:0.6697457513503007\n",
      "train loss:0.48340071246158967\n",
      "train loss:0.6661488002325818\n",
      "train loss:0.48233011822385485\n",
      "train loss:0.38708278435844307\n",
      "train loss:0.5210323908876151\n",
      "train loss:0.77362723111845\n",
      "train loss:0.6231286262582528\n",
      "train loss:0.6942826002870915\n",
      "train loss:0.5444946397715802\n",
      "train loss:0.6539721921792937\n",
      "train loss:0.40864779433347637\n",
      "train loss:0.5608229194716485\n",
      "train loss:0.6932076907521683\n",
      "train loss:0.5574733905219599\n",
      "train loss:0.6133728323248818\n",
      "train loss:0.6070069296369031\n",
      "train loss:0.5158990974274417\n",
      "train loss:0.42370695430380867\n",
      "train loss:0.5100140341526338\n",
      "train loss:0.4952711215884936\n",
      "train loss:0.8724968213139999\n",
      "train loss:0.7290901033810522\n",
      "train loss:0.6433013823390704\n",
      "train loss:0.5082639524501917\n",
      "train loss:0.6875854267651833\n",
      "train loss:0.5107422063552326\n",
      "train loss:0.6215556850777493\n",
      "train loss:0.5189348138376102\n",
      "train loss:0.622266455240303\n",
      "train loss:0.44630780578452856\n",
      "train loss:0.6841075632630365\n",
      "train loss:0.5015973249188592\n",
      "train loss:0.5665335880069868\n",
      "train loss:0.6924805226213135\n",
      "train loss:0.6894885698331352\n",
      "train loss:0.523774344517625\n",
      "train loss:0.4945268663476485\n",
      "train loss:0.5358010648529714\n",
      "train loss:0.8447773262833668\n",
      "train loss:0.6170378855478089\n",
      "train loss:0.6176987945680916\n",
      "train loss:0.531409057672913\n",
      "train loss:0.4437731855921264\n",
      "train loss:0.6504346040727554\n",
      "train loss:0.6328570923801777\n",
      "train loss:0.49605726323720134\n",
      "train loss:0.5855278957869784\n",
      "train loss:0.5481301109875649\n",
      "train loss:0.6837659323706695\n",
      "train loss:0.5620249570557374\n",
      "train loss:0.5617141808589025\n",
      "train loss:0.6791689059054501\n",
      "train loss:0.5369913425806744\n",
      "train loss:0.7722196579654629\n",
      "train loss:0.5065688587628199\n",
      "train loss:0.5145611986024967\n",
      "train loss:0.6624937568705282\n",
      "train loss:0.551291754410995\n",
      "train loss:0.6851536247274896\n",
      "train loss:0.5726334128785706\n",
      "train loss:0.5671860958034023\n",
      "train loss:0.6690619382011057\n",
      "train loss:0.5082336401255058\n",
      "train loss:0.6024667725022715\n",
      "train loss:0.6890060045101474\n",
      "train loss:0.4975398418180938\n",
      "train loss:0.6079625789938781\n",
      "train loss:0.6023117485225884\n",
      "train loss:0.5205652919693046\n",
      "train loss:0.2494062836359888\n",
      "train loss:0.5113864513495966\n",
      "train loss:0.5683472273358736\n",
      "train loss:1.1839615537859638\n",
      "train loss:0.38889530557692653\n",
      "train loss:0.7077789626127099\n",
      "train loss:0.6329752354819338\n",
      "train loss:0.575778074958872\n",
      "train loss:0.5639326249127691\n",
      "train loss:0.5183229659815792\n",
      "train loss:0.7290571182764602\n",
      "train loss:0.6588500798496697\n",
      "train loss:0.8141796770851426\n",
      "train loss:0.7103690431584833\n",
      "train loss:0.7034838363849346\n",
      "train loss:0.552233620307237\n",
      "train loss:0.5851387236472178\n",
      "train loss:0.6413103286699237\n",
      "train loss:0.6451887158234397\n",
      "train loss:0.5966881847670626\n",
      "=== epoch:5, train acc:0.73, test acc:0.68 ===\n",
      "train loss:0.5309337724329988\n",
      "train loss:0.6511739760452433\n",
      "train loss:0.6649077046699825\n",
      "train loss:0.6083149386008055\n",
      "train loss:0.5562141812486152\n",
      "train loss:0.2507455169374285\n",
      "train loss:0.3417534090329199\n",
      "train loss:0.8436466258734601\n",
      "train loss:0.8103828541684827\n",
      "train loss:0.5837412837224191\n",
      "train loss:0.7244658292231273\n",
      "train loss:0.689321802468147\n",
      "train loss:0.39472656158166775\n",
      "train loss:0.41803101853188274\n",
      "train loss:0.5925703170279271\n",
      "train loss:0.47680396324794605\n",
      "train loss:0.41853421400450797\n",
      "train loss:0.6032172404496992\n",
      "train loss:0.6332335076524241\n",
      "train loss:0.7284060097259913\n",
      "train loss:0.7539032818382971\n",
      "train loss:0.5309346099078033\n",
      "train loss:0.520970054316949\n",
      "train loss:0.684309656867075\n",
      "train loss:0.7810876547249268\n",
      "train loss:0.6689719202960133\n",
      "train loss:0.5268017439823767\n",
      "train loss:0.4771979642804814\n",
      "train loss:0.6151868121388421\n",
      "train loss:0.6216635515730417\n",
      "train loss:0.5866640110798542\n",
      "train loss:0.6843539122120185\n",
      "train loss:0.6648506224035937\n",
      "train loss:0.53631709891048\n",
      "train loss:0.5762369228581408\n",
      "train loss:0.3255214791048047\n",
      "train loss:0.4946687416219957\n",
      "train loss:0.6309281246634384\n",
      "train loss:0.7312738340652027\n",
      "train loss:0.67265204615737\n",
      "train loss:0.721982903550391\n",
      "train loss:0.48710822490492334\n",
      "train loss:0.7058104415947161\n",
      "train loss:0.7045928236556815\n",
      "train loss:0.5240935980442828\n",
      "train loss:0.7258796694277241\n",
      "train loss:0.5398281771983228\n",
      "train loss:0.5529663106041445\n",
      "train loss:0.5889348586557224\n",
      "train loss:0.5156462538444951\n",
      "train loss:0.5126567238162865\n",
      "train loss:0.7089126269546286\n",
      "train loss:0.6824577418036847\n",
      "train loss:0.5196128143707602\n",
      "train loss:0.6815718473188213\n",
      "train loss:0.7030320410634163\n",
      "train loss:0.6554895360368167\n",
      "train loss:0.6045783138771936\n",
      "train loss:0.6892819673965421\n",
      "train loss:0.46663073246830783\n",
      "train loss:0.5161178581777106\n",
      "train loss:0.7518909406176706\n",
      "train loss:0.5114857656272705\n",
      "train loss:0.7545766675383386\n",
      "train loss:0.5372794075606414\n",
      "train loss:0.5652324450110895\n",
      "train loss:0.4821053666650469\n",
      "train loss:0.7466356326400589\n",
      "train loss:0.6467839131843652\n",
      "train loss:0.4065068658684078\n",
      "train loss:0.5814066174421513\n",
      "train loss:0.710891253882655\n",
      "train loss:0.621399834127962\n",
      "train loss:0.5074678676395059\n",
      "train loss:0.6276236722349204\n",
      "train loss:0.6926611768342062\n",
      "train loss:0.7114240048120454\n",
      "train loss:0.5523350461441894\n",
      "train loss:0.5887860639541558\n",
      "train loss:0.5778963599422954\n",
      "train loss:0.7691753737183074\n",
      "train loss:0.5354060887692011\n",
      "train loss:0.43791410841786105\n",
      "train loss:0.4306371768513061\n",
      "train loss:0.741858958642543\n",
      "train loss:0.5728930265685149\n",
      "train loss:0.518522925626977\n",
      "train loss:0.7282711191501245\n",
      "train loss:0.44666248896379857\n",
      "train loss:0.4638641141485344\n",
      "train loss:0.7935763391963573\n",
      "train loss:0.3513005576442655\n",
      "train loss:0.6621127688026258\n",
      "train loss:0.49975117821900084\n",
      "train loss:0.6202653099160004\n",
      "train loss:0.4177012015064202\n",
      "train loss:0.6542994005270341\n",
      "train loss:0.5726932677117242\n",
      "train loss:0.5465302963175873\n",
      "train loss:0.6892403276445033\n",
      "train loss:0.9119891553275499\n",
      "train loss:0.6947681702917032\n",
      "train loss:0.6779161701585139\n",
      "train loss:0.5306725142176927\n",
      "train loss:0.7614452741316784\n",
      "train loss:0.7204740886732012\n",
      "train loss:0.6251142942542804\n",
      "train loss:0.53793133444608\n",
      "train loss:0.5781580676494166\n",
      "train loss:0.6904038446330903\n",
      "train loss:0.6076654126992089\n",
      "train loss:0.5715157800435244\n",
      "train loss:0.6834853227923123\n",
      "train loss:0.48231155662617037\n",
      "train loss:0.8676347725551098\n",
      "train loss:0.5033701251719777\n",
      "train loss:0.7138461900991128\n",
      "train loss:0.8234382835291678\n",
      "train loss:0.7544355051630403\n",
      "train loss:0.6226881894608194\n",
      "train loss:0.6449282453585319\n",
      "train loss:0.7947325679145003\n",
      "train loss:0.6657278336814249\n",
      "train loss:0.5695659300816496\n",
      "train loss:0.6262016490897289\n",
      "train loss:0.5309562950691677\n",
      "train loss:0.6155422106897225\n",
      "train loss:0.49797858872759937\n",
      "train loss:0.6119464547977072\n",
      "train loss:0.6217114798006683\n",
      "train loss:0.6235370401067106\n",
      "train loss:0.5894379329735794\n",
      "train loss:0.3487639391028294\n",
      "train loss:0.6403626872436317\n",
      "train loss:0.7912032405321947\n",
      "train loss:0.5591535131523028\n",
      "train loss:0.4850647120836909\n",
      "train loss:0.6751593152771125\n",
      "train loss:0.613997600719903\n",
      "train loss:0.4970306391244919\n",
      "train loss:0.599575832645171\n",
      "train loss:0.59485665429315\n",
      "train loss:0.5860409729107012\n",
      "train loss:0.7467559893892023\n",
      "train loss:0.501840148716948\n",
      "train loss:0.5426572771383398\n",
      "train loss:0.7617628890729502\n",
      "train loss:0.5904582902717033\n",
      "train loss:0.47052751734931625\n",
      "train loss:0.5343218586528453\n",
      "train loss:0.5456152275364593\n",
      "train loss:0.6124343080092662\n",
      "train loss:0.5898535219697522\n",
      "=== epoch:6, train acc:0.71, test acc:0.69 ===\n",
      "train loss:0.6078372978295243\n",
      "train loss:0.36778324024592124\n",
      "train loss:0.5468982370697186\n",
      "train loss:0.5187478204996852\n",
      "train loss:0.5518945104619041\n",
      "train loss:0.4760137657736485\n",
      "train loss:0.16867467162767893\n",
      "train loss:0.8369429937002227\n",
      "train loss:0.8758446397940858\n",
      "train loss:0.7419883285197447\n",
      "train loss:0.3094436569680497\n",
      "train loss:0.8549959496731709\n",
      "train loss:0.573760704936995\n",
      "train loss:0.5294315803259984\n",
      "train loss:0.4911108505902268\n",
      "train loss:0.48163440092790893\n",
      "train loss:0.4248338598148978\n",
      "train loss:0.5394699497572302\n",
      "train loss:0.35845633980751546\n",
      "train loss:0.62821209858649\n",
      "train loss:0.7017346182878748\n",
      "train loss:0.4639337339225908\n",
      "train loss:0.49278411284013285\n",
      "train loss:0.7779850022461081\n",
      "train loss:0.728919388944732\n",
      "train loss:0.7342278703577751\n",
      "train loss:0.5136063914041182\n",
      "train loss:0.3735332857613559\n",
      "train loss:0.8708878313386483\n",
      "train loss:0.558959138126234\n",
      "train loss:0.6994660740471792\n",
      "train loss:0.5915702848237923\n",
      "train loss:0.5740664394420224\n",
      "train loss:0.7231294994923075\n",
      "train loss:0.6055789604564097\n",
      "train loss:0.5955041842055637\n",
      "train loss:0.6164582428823056\n",
      "train loss:0.548334584629496\n",
      "train loss:0.7168347911527801\n",
      "train loss:0.6577579692707664\n",
      "train loss:0.7416261005513569\n",
      "train loss:0.6449004409029296\n",
      "train loss:0.6090843357443602\n",
      "train loss:0.5856025530679714\n",
      "train loss:0.6690047703920794\n",
      "train loss:0.6299092845710733\n",
      "train loss:0.6717671397690634\n",
      "train loss:0.6898044673069261\n",
      "train loss:0.5359846626772786\n",
      "train loss:0.7694347350433123\n",
      "train loss:0.5432105762679151\n",
      "train loss:0.5306872686825205\n",
      "train loss:0.3665449690617028\n",
      "train loss:0.5915959320045804\n",
      "train loss:0.43669882200940735\n",
      "train loss:0.9146903632786634\n",
      "train loss:0.4777667204897583\n",
      "train loss:0.3584892544692078\n",
      "train loss:0.5867913071978541\n",
      "train loss:0.5367810522322412\n",
      "train loss:0.9310982261022012\n",
      "train loss:0.4168253029308312\n",
      "train loss:0.6026011621470388\n",
      "train loss:0.6165817127158113\n",
      "train loss:0.35515465049598743\n",
      "train loss:0.7023189107539914\n",
      "train loss:0.5524196122530365\n",
      "train loss:0.9207310465667374\n",
      "train loss:0.5825315066564837\n",
      "train loss:0.4756330106743956\n",
      "train loss:0.46307726376607555\n",
      "train loss:0.7416292405859208\n",
      "train loss:0.7655312736856839\n",
      "train loss:0.6379372324972513\n",
      "train loss:0.5363506322558778\n",
      "train loss:0.6137889289197536\n",
      "train loss:0.5458558707235188\n",
      "train loss:0.4833677027110853\n",
      "train loss:0.5618657362571675\n",
      "train loss:0.5179500676351422\n",
      "train loss:0.6809644079056623\n",
      "train loss:0.52106515883133\n",
      "train loss:0.5237971650025892\n",
      "train loss:0.6156439884823828\n",
      "train loss:0.5778581327830594\n",
      "train loss:0.5307512602358142\n",
      "train loss:0.8573283531543956\n",
      "train loss:0.5820739130519248\n",
      "train loss:0.6697883333314029\n",
      "train loss:0.3450357194334564\n",
      "train loss:0.5840902122268854\n",
      "train loss:0.6138094373681736\n",
      "train loss:0.3599630194137383\n",
      "train loss:0.5898379318013767\n",
      "train loss:0.1923904002880691\n",
      "train loss:0.5275037593675654\n",
      "train loss:0.5047638483310132\n",
      "train loss:0.5735744160148355\n",
      "train loss:0.3922858719673345\n",
      "train loss:0.4261482421149011\n",
      "train loss:0.3540121087507101\n",
      "train loss:0.7179390630189397\n",
      "train loss:0.5623012755414803\n",
      "train loss:0.6230167973453776\n",
      "train loss:0.5173152175469017\n",
      "train loss:0.521444619710671\n",
      "train loss:0.626783830424215\n",
      "train loss:0.41993565101788094\n",
      "train loss:0.5682912509376605\n",
      "train loss:0.7531872547515818\n",
      "train loss:0.4301281500428157\n",
      "train loss:0.5363751013954254\n",
      "train loss:0.6416199389074042\n",
      "train loss:0.42154504932861\n",
      "train loss:0.5005090623108913\n",
      "train loss:0.6434649594562105\n",
      "train loss:0.5097600641872724\n",
      "train loss:0.5590491504108333\n",
      "train loss:0.8203833221451535\n",
      "train loss:0.4312815135061984\n",
      "train loss:0.6969728759554557\n",
      "train loss:0.4642211807844735\n",
      "train loss:0.4577669172072321\n",
      "train loss:0.5930723757207046\n",
      "train loss:0.5398309909678914\n",
      "train loss:0.5149240803512477\n",
      "train loss:0.5087486439933616\n",
      "train loss:0.5118863112337365\n",
      "train loss:0.5318348101802329\n",
      "train loss:0.5208340138389062\n",
      "train loss:0.21674157505229105\n",
      "train loss:0.5303779617300866\n",
      "train loss:0.38814427170825916\n",
      "train loss:0.651591802904693\n",
      "train loss:0.6335491346116306\n",
      "train loss:0.4139508613399797\n",
      "train loss:0.5487355706728266\n",
      "train loss:0.7466418253649789\n",
      "train loss:0.6072177825658117\n",
      "train loss:0.6011167528370476\n",
      "train loss:0.43416054703389645\n",
      "train loss:0.7251654050694848\n",
      "train loss:0.3435865145040945\n",
      "train loss:0.5860742619102126\n",
      "train loss:0.38021941237405715\n",
      "train loss:0.6103076647972562\n",
      "train loss:0.6936437489404478\n",
      "train loss:0.6023796785355293\n",
      "train loss:0.3888920572855254\n",
      "train loss:0.26331461087932356\n",
      "train loss:0.7366210336269908\n",
      "train loss:0.5885756405701579\n",
      "train loss:0.9578314470785448\n",
      "=== epoch:7, train acc:0.74, test acc:0.7 ===\n",
      "train loss:0.6518646887968108\n",
      "train loss:0.5948219977933233\n",
      "train loss:0.6045684244365859\n",
      "train loss:0.5179658673102443\n",
      "train loss:0.5830530548220414\n",
      "train loss:0.5039813759104372\n",
      "train loss:0.5804438379771936\n",
      "train loss:0.5508183383761086\n",
      "train loss:0.44142575759405134\n",
      "train loss:0.8565427209332277\n",
      "train loss:0.5535293733479163\n",
      "train loss:0.571058029289297\n",
      "train loss:0.5056501065765573\n",
      "train loss:0.5047671355278227\n",
      "train loss:0.6761572781428763\n",
      "train loss:0.5079960565717412\n",
      "train loss:0.48786415449505477\n",
      "train loss:0.371593446283358\n",
      "train loss:0.793987710993626\n",
      "train loss:0.6437570597865174\n",
      "train loss:0.6364672717949301\n",
      "train loss:0.7852763570562911\n",
      "train loss:0.6207258094873557\n",
      "train loss:0.44542928149528177\n",
      "train loss:0.6407281483782246\n",
      "train loss:0.6154280661091316\n",
      "train loss:0.5366438793054503\n",
      "train loss:0.5686292781505292\n",
      "train loss:0.5962912532880298\n",
      "train loss:0.6613197060531264\n",
      "train loss:0.6852880740358502\n",
      "train loss:0.5502979873928364\n",
      "train loss:0.5196201957981963\n",
      "train loss:0.6367006069267699\n",
      "train loss:0.500609579647614\n",
      "train loss:0.6702353145533185\n",
      "train loss:0.3398378888145449\n",
      "train loss:0.467897305862851\n",
      "train loss:0.6320533020478526\n",
      "train loss:0.5212442507187908\n",
      "train loss:0.7559076640804603\n",
      "train loss:0.7164720741584663\n",
      "train loss:0.3250683906764146\n",
      "train loss:0.653300302066094\n",
      "train loss:0.4530378947249288\n",
      "train loss:0.8788760998225287\n",
      "train loss:0.373731974453397\n",
      "train loss:0.42154892647795383\n",
      "train loss:0.6221358689163549\n",
      "train loss:0.567152681528074\n",
      "train loss:0.589110459945654\n",
      "train loss:0.5722179097141719\n",
      "train loss:0.5577440920395413\n",
      "train loss:0.48888625806402375\n",
      "train loss:0.7259826762205034\n",
      "train loss:0.49053860110339687\n",
      "train loss:0.5158610506677614\n",
      "train loss:0.48135640945589386\n",
      "train loss:0.5887462604976154\n",
      "train loss:0.5048685280577712\n",
      "train loss:0.5664019085183218\n",
      "train loss:0.5601997857320278\n",
      "train loss:0.6520332592629532\n",
      "train loss:0.7073100796065945\n",
      "train loss:0.5282114862710655\n",
      "train loss:0.41043068275628025\n",
      "train loss:0.5196990237984507\n",
      "train loss:0.4260742162963035\n",
      "train loss:0.4895410148096773\n",
      "train loss:0.3828206106402459\n",
      "train loss:0.6383252219106204\n",
      "train loss:0.8061746109039902\n",
      "train loss:0.5604716739269325\n",
      "train loss:0.6214923365677016\n",
      "train loss:0.3530498238824327\n",
      "train loss:0.5254851275697926\n",
      "train loss:0.5156688212048877\n",
      "train loss:0.6115883400140144\n",
      "train loss:0.16971575305109537\n",
      "train loss:0.269970777922358\n",
      "train loss:0.7533306635866619\n",
      "train loss:0.5884190466250605\n",
      "train loss:0.9254745246488485\n",
      "train loss:0.7262990746109356\n",
      "train loss:0.4822069573538945\n",
      "train loss:0.3934441985938747\n",
      "train loss:0.5765411931671233\n",
      "train loss:0.6182080008453459\n",
      "train loss:0.4704146448187375\n",
      "train loss:0.5654578695390025\n",
      "train loss:0.542560839430861\n",
      "train loss:0.5113622300456464\n",
      "train loss:0.622018481480288\n",
      "train loss:0.48905804927678054\n",
      "train loss:0.5041009511075155\n",
      "train loss:0.806385968794103\n",
      "train loss:0.5563787512030539\n",
      "train loss:0.8688286176932107\n",
      "train loss:0.6754439824987479\n",
      "train loss:0.5390098112129749\n",
      "train loss:0.5238569593710817\n",
      "train loss:0.5573995823940224\n",
      "train loss:0.5302882357137453\n",
      "train loss:0.6262610063773181\n",
      "train loss:0.6143561095994169\n",
      "train loss:0.6128405672759139\n",
      "train loss:0.7477137603999152\n",
      "train loss:0.7422699204047879\n",
      "train loss:0.6333709413229304\n",
      "train loss:0.5540726363924129\n",
      "train loss:0.6551496951246492\n",
      "train loss:0.5027479067149184\n",
      "train loss:0.7224297010030194\n",
      "train loss:0.5777578475586342\n",
      "train loss:0.4134610308020169\n",
      "train loss:0.5231408063807063\n",
      "train loss:0.6114127119724438\n",
      "train loss:0.671492110799951\n",
      "train loss:0.24732837665164312\n",
      "train loss:0.5090736903406129\n",
      "train loss:0.8792085548103135\n",
      "train loss:0.2385248307910115\n",
      "train loss:0.6297207454559635\n",
      "train loss:0.7012817231732533\n",
      "train loss:0.49802723147509614\n",
      "train loss:0.5104065457568943\n",
      "train loss:0.7729958536444295\n",
      "train loss:0.4778637898415231\n",
      "train loss:0.4514357431241723\n",
      "train loss:0.7796366057156607\n",
      "train loss:0.3942067073928668\n",
      "train loss:0.5642872986758743\n",
      "train loss:0.7293475169560969\n",
      "train loss:0.573813505824355\n",
      "train loss:0.5327431048780593\n",
      "train loss:0.3808287342016636\n",
      "train loss:0.5688132367089418\n",
      "train loss:0.6257199414579342\n",
      "train loss:0.6012763520039954\n",
      "train loss:0.45579668989867805\n",
      "train loss:0.5739051000181576\n",
      "train loss:0.8941409202208355\n",
      "train loss:0.2363529771055551\n",
      "train loss:0.44626289993115453\n",
      "train loss:0.2791803591457435\n",
      "train loss:0.48960141099889415\n",
      "train loss:0.3215096683777559\n",
      "train loss:0.3774123763395167\n",
      "train loss:0.9706930703310181\n",
      "train loss:0.5100794430062343\n",
      "train loss:0.4293672477211875\n",
      "train loss:0.7492513767505498\n",
      "train loss:0.7477363185103455\n",
      "=== epoch:8, train acc:0.75, test acc:0.7 ===\n",
      "train loss:0.6379849810956252\n",
      "train loss:0.5018281146683773\n",
      "train loss:0.6480321382756584\n",
      "train loss:0.6338841160582891\n",
      "train loss:0.7216539317272105\n",
      "train loss:0.536888004950842\n",
      "train loss:0.5540201037841783\n",
      "train loss:0.573984867987904\n",
      "train loss:0.5496590550423397\n",
      "train loss:0.40784077976416777\n",
      "train loss:0.6514434421735513\n",
      "train loss:0.5812514465156443\n",
      "train loss:0.6508474635618791\n",
      "train loss:0.7216627893709602\n",
      "train loss:0.6652467955349483\n",
      "train loss:0.6597358150402378\n",
      "train loss:0.6838550972022486\n",
      "train loss:0.6239210354753781\n",
      "train loss:0.5195613898685962\n",
      "train loss:0.5081988586695673\n",
      "train loss:0.4968297281568247\n",
      "train loss:0.5369924034874597\n",
      "train loss:0.7078111732625633\n",
      "train loss:0.6707831518868443\n",
      "train loss:0.5721114171603645\n",
      "train loss:0.5945931739017101\n",
      "train loss:0.5348558080579215\n",
      "train loss:0.528927760926116\n",
      "train loss:0.5243688033389573\n",
      "train loss:0.4744407840418118\n",
      "train loss:0.24870750383443746\n",
      "train loss:0.4845623311437889\n",
      "train loss:0.6941168820463867\n",
      "train loss:0.6039855425821754\n",
      "train loss:0.7137980970966621\n",
      "train loss:0.4957363416177391\n",
      "train loss:0.4023560558449624\n",
      "train loss:0.3757917043877849\n",
      "train loss:0.8336644403055311\n",
      "train loss:0.5011778286351162\n",
      "train loss:0.6017363514446896\n",
      "train loss:0.713184076892775\n",
      "train loss:0.45941535691821517\n",
      "train loss:0.3514727298736221\n",
      "train loss:0.6616856664692439\n",
      "train loss:0.6589161600963954\n",
      "train loss:0.4049146730330714\n",
      "train loss:0.33593229361544624\n",
      "train loss:0.5005151382759951\n",
      "train loss:0.49806055356744394\n",
      "train loss:0.5979699559304109\n",
      "train loss:0.6254985146978481\n",
      "train loss:0.4933122968207888\n",
      "train loss:0.6866349631501085\n",
      "train loss:0.7333143227655428\n",
      "train loss:0.44343897662313914\n",
      "train loss:0.4677260197149402\n",
      "train loss:0.6563163480779309\n",
      "train loss:0.6932317690028432\n",
      "train loss:0.5066558676952182\n",
      "train loss:0.8819705362777215\n",
      "train loss:0.5720958693620756\n",
      "train loss:0.4577805428171932\n",
      "train loss:0.6218887934545683\n",
      "train loss:0.6976849008888034\n",
      "train loss:0.6372552133034134\n",
      "train loss:0.694094180447797\n",
      "train loss:0.6239590588108781\n",
      "train loss:0.6037469483982597\n",
      "train loss:0.5934185842385091\n",
      "train loss:0.6557102988855412\n",
      "train loss:0.4757458633602976\n",
      "train loss:0.4597484213843453\n",
      "train loss:0.5287316813796088\n",
      "train loss:0.5707688635241014\n",
      "train loss:0.581852629409745\n",
      "train loss:0.4724352662318642\n",
      "train loss:0.5939590055346439\n",
      "train loss:0.44195014121931697\n",
      "train loss:0.48660924693402086\n",
      "train loss:0.5365373981443022\n",
      "train loss:0.5009460592166384\n",
      "train loss:0.6201885600780469\n",
      "train loss:0.47898547885585446\n",
      "train loss:0.8683209372205003\n",
      "train loss:0.4830096538591938\n",
      "train loss:0.677848108903045\n",
      "train loss:0.4792554331313955\n",
      "train loss:0.5839900277768952\n",
      "train loss:0.4119602549681443\n",
      "train loss:0.4813270276613203\n",
      "train loss:0.4765359717936845\n",
      "train loss:0.3145987181094202\n",
      "train loss:0.7404053229264431\n",
      "train loss:0.43825486123101476\n",
      "train loss:0.3185510842373983\n",
      "train loss:0.46209155308770755\n",
      "train loss:0.26401965067601596\n",
      "train loss:0.8756916411482416\n",
      "train loss:0.7167019438117215\n",
      "train loss:0.3629449722466308\n",
      "train loss:0.18697617161782676\n",
      "train loss:0.5186611903840407\n",
      "train loss:0.6259647440620457\n",
      "train loss:0.5377754805815171\n",
      "train loss:0.4930786109905366\n",
      "train loss:0.7903605184556893\n",
      "train loss:0.6461524364312068\n",
      "train loss:0.9770397415137324\n",
      "train loss:0.6220717644622721\n",
      "train loss:0.556012792484584\n",
      "train loss:0.614050985689252\n",
      "train loss:0.4821595842591588\n",
      "train loss:0.5584528373180686\n",
      "train loss:0.5558720341089783\n",
      "train loss:0.5437202122899043\n",
      "train loss:0.5301954756879094\n",
      "train loss:0.6070614480271415\n",
      "train loss:0.6045804280633919\n",
      "train loss:0.7703119826431064\n",
      "train loss:0.6748220449826701\n",
      "train loss:0.5221276095016302\n",
      "train loss:0.524104501633926\n",
      "train loss:0.728685210362803\n",
      "train loss:0.4488936721664666\n",
      "train loss:0.6671671464081024\n",
      "train loss:0.43641316549388665\n",
      "train loss:0.37610188935203764\n",
      "train loss:0.6768834896511666\n",
      "train loss:0.7030379567613954\n",
      "train loss:0.40656366651952885\n",
      "train loss:0.6660340986212896\n",
      "train loss:0.41747723679729487\n",
      "train loss:0.6871146266397936\n",
      "train loss:0.6257801847010566\n",
      "train loss:0.8507056302374136\n",
      "train loss:0.49624519721526184\n",
      "train loss:0.7462400560071505\n",
      "train loss:0.46168303675647043\n",
      "train loss:0.5935115681387212\n",
      "train loss:0.5942920650067001\n",
      "train loss:0.4530473245857517\n",
      "train loss:0.5678592194334785\n",
      "train loss:0.4137779017297959\n",
      "train loss:0.3525463500281266\n",
      "train loss:0.3268964323995861\n",
      "train loss:0.22810855212426356\n",
      "train loss:0.49479799953291204\n",
      "train loss:0.6462585479963047\n",
      "train loss:0.8220857292286601\n",
      "train loss:0.7726461595999856\n",
      "train loss:0.8146601856515862\n",
      "train loss:0.6682987198606819\n",
      "=== epoch:9, train acc:0.76, test acc:0.7 ===\n",
      "train loss:0.45658979382419124\n",
      "train loss:0.7082915624582312\n",
      "train loss:0.46252929293663997\n",
      "train loss:0.655426494682418\n",
      "train loss:0.48747777845255225\n",
      "train loss:0.6502307461803973\n",
      "train loss:0.6203686707409299\n",
      "train loss:0.6435386068838677\n",
      "train loss:0.7489024992404901\n",
      "train loss:0.5344884427513141\n",
      "train loss:0.6587607673586475\n",
      "train loss:0.5183333245603762\n",
      "train loss:0.6994814783614623\n",
      "train loss:0.4836081228598507\n",
      "train loss:0.4648622964004277\n",
      "train loss:0.4888055825384677\n",
      "train loss:0.4915199131937116\n",
      "train loss:0.6354904035654034\n",
      "train loss:0.540275858202101\n",
      "train loss:0.5443454952185599\n",
      "train loss:0.7118204223024313\n",
      "train loss:0.37808459630082114\n",
      "train loss:0.6516781706843783\n",
      "train loss:0.7837819567043561\n",
      "train loss:0.6710016381871065\n",
      "train loss:0.6873980303517554\n",
      "train loss:0.4179372455713599\n",
      "train loss:0.5101082505322195\n",
      "train loss:0.6776465348774304\n",
      "train loss:0.5748728651669314\n",
      "train loss:0.7906469141022958\n",
      "train loss:0.5222714267098124\n",
      "train loss:0.5491273043200436\n",
      "train loss:0.7753012152840117\n",
      "train loss:0.6585766654150631\n",
      "train loss:0.689060573918691\n",
      "train loss:0.7034839911218813\n",
      "train loss:0.6661979548896236\n",
      "train loss:0.4918918168840066\n",
      "train loss:0.68475353350322\n",
      "train loss:0.6576124516413346\n",
      "train loss:0.5898814279703596\n",
      "train loss:0.5806736577556132\n",
      "train loss:0.48775291957098166\n",
      "train loss:0.6382303934111124\n",
      "train loss:0.5566850467967818\n",
      "train loss:0.6485306984553202\n",
      "train loss:0.6677638533747601\n",
      "train loss:0.5180177282795829\n",
      "train loss:0.6584914216568651\n",
      "train loss:0.5557260356515927\n",
      "train loss:0.4536639590859803\n",
      "train loss:0.48647685722998163\n",
      "train loss:0.6400172330405982\n",
      "train loss:0.46334750307925876\n",
      "train loss:0.6020682687438417\n",
      "train loss:0.6370427003485847\n",
      "train loss:0.20692994417083863\n",
      "train loss:0.6791063101005242\n",
      "train loss:0.33797927096453717\n",
      "train loss:0.3238871957408976\n",
      "train loss:0.3788209384168912\n",
      "train loss:0.8491958588252105\n",
      "train loss:0.33073802823351167\n",
      "train loss:0.37196191697485825\n",
      "train loss:0.3463749701980297\n",
      "train loss:0.5380331414867016\n",
      "train loss:0.6034676481300498\n",
      "train loss:0.6088966757451255\n",
      "train loss:0.6563105978033678\n",
      "train loss:0.7885885424267511\n",
      "train loss:0.6579656736967017\n",
      "train loss:0.49765396600400713\n",
      "train loss:0.44502272997240827\n",
      "train loss:0.7204956282447691\n",
      "train loss:0.752202796461954\n",
      "train loss:0.5367913481192848\n",
      "train loss:0.5162175599202563\n",
      "train loss:0.4882166222022038\n",
      "train loss:0.5497014295078948\n",
      "train loss:0.5708611249390769\n",
      "train loss:0.5547424106276742\n",
      "train loss:0.5812533088729421\n",
      "train loss:0.4925961926971684\n",
      "train loss:0.5600755009152566\n",
      "train loss:0.8207183074344874\n",
      "train loss:0.47777388628437595\n",
      "train loss:0.4470322628161427\n",
      "train loss:0.6357336656148137\n",
      "train loss:0.43938726569165565\n",
      "train loss:0.5801782195723183\n",
      "train loss:0.34084178027857925\n",
      "train loss:0.5697238911303751\n",
      "train loss:0.49935105064081575\n",
      "train loss:0.5759891538510739\n",
      "train loss:0.4999164905456587\n",
      "train loss:0.4271119093655976\n",
      "train loss:0.46815901688769535\n",
      "train loss:0.4446514201980586\n",
      "train loss:0.4984571579670319\n",
      "train loss:0.3219921773089613\n",
      "train loss:0.45664243898723156\n",
      "train loss:0.6080144499745544\n",
      "train loss:0.7283973121969737\n",
      "train loss:0.4588252047886515\n",
      "train loss:0.5527665943951516\n",
      "train loss:0.323109817071089\n",
      "train loss:0.7172230210014312\n",
      "train loss:0.7025400619775822\n",
      "train loss:0.6777833269908609\n",
      "train loss:0.5411175674067416\n",
      "train loss:0.6438294171988522\n",
      "train loss:0.6345253571913264\n",
      "train loss:0.5827681720564032\n",
      "train loss:0.5362080540125358\n",
      "train loss:0.5201620112972825\n",
      "train loss:0.6316187259583299\n",
      "train loss:0.6287653388490326\n",
      "train loss:0.6111510098657083\n",
      "train loss:0.6321509112094946\n",
      "train loss:0.38652092908398367\n",
      "train loss:0.47337742182523057\n",
      "train loss:0.72639912364574\n",
      "train loss:0.7135998688354357\n",
      "train loss:0.37681441723340126\n",
      "train loss:0.5764931711497386\n",
      "train loss:0.6103947359042374\n",
      "train loss:0.6294129458603\n",
      "train loss:0.5873443296123082\n",
      "train loss:0.5820444028049017\n",
      "train loss:0.35071807154215806\n",
      "train loss:0.6532630612071197\n",
      "train loss:0.5755517605352685\n",
      "train loss:0.6044620232712452\n",
      "train loss:0.4919542615759836\n",
      "train loss:0.7203232268425385\n",
      "train loss:0.4968840578366257\n",
      "train loss:0.44671545865381984\n",
      "train loss:0.46935336658513077\n",
      "train loss:0.9147549389947767\n",
      "train loss:0.7414850270594394\n",
      "train loss:0.3452429708590448\n",
      "train loss:0.788443444172408\n",
      "train loss:0.5125522123814612\n",
      "train loss:0.4906902783241346\n",
      "train loss:0.6287638993790821\n",
      "train loss:0.4563196537506647\n",
      "train loss:0.6741483944167752\n",
      "train loss:0.5784273653662425\n",
      "train loss:0.7012020704968162\n",
      "train loss:0.5568386238435158\n",
      "train loss:0.414937861830355\n",
      "train loss:0.6361046291371292\n",
      "=== epoch:10, train acc:0.73, test acc:0.7 ===\n",
      "train loss:0.42721059379766907\n",
      "train loss:0.43646359161443404\n",
      "train loss:0.35380358829062747\n",
      "train loss:0.5665835080172074\n",
      "train loss:0.7526822684260417\n",
      "train loss:0.49137512181323706\n",
      "train loss:0.6403799465349806\n",
      "train loss:0.5377636660450105\n",
      "train loss:0.636154543960542\n",
      "train loss:0.7177051365241188\n",
      "train loss:0.5998258771744919\n",
      "train loss:0.44031213888224674\n",
      "train loss:0.5083625101322825\n",
      "train loss:0.4050995211909402\n",
      "train loss:0.514151154131469\n",
      "train loss:0.6293218707974184\n",
      "train loss:0.4599717817327306\n",
      "train loss:0.37933095870138545\n",
      "train loss:0.35363336288655883\n",
      "train loss:0.4561832895400305\n",
      "train loss:0.34409830199413793\n",
      "train loss:0.6919568557502405\n",
      "train loss:0.4623017132314833\n",
      "train loss:0.7767103494481857\n",
      "train loss:0.644125055349049\n",
      "train loss:0.6108756676038103\n",
      "train loss:0.44109860894631636\n",
      "train loss:0.6658754831926071\n",
      "train loss:0.5540802868655124\n",
      "train loss:0.39727363033003044\n",
      "train loss:0.8033332786836468\n",
      "train loss:0.5961028507660062\n",
      "train loss:0.423934805722452\n",
      "train loss:0.4526638453232404\n",
      "train loss:0.5721719803359852\n",
      "train loss:0.363143961908173\n",
      "train loss:0.7082464344795668\n",
      "train loss:0.41069695236726017\n",
      "train loss:0.467392664128099\n",
      "train loss:0.5311580515955233\n",
      "train loss:0.5167484661319299\n",
      "train loss:0.5129788258191244\n",
      "train loss:0.6276845466144717\n",
      "train loss:0.521560442201439\n",
      "train loss:0.482114920287275\n",
      "train loss:0.4321241897343075\n",
      "train loss:0.7411272777147261\n",
      "train loss:0.5305800273534913\n",
      "train loss:0.6999495089998129\n",
      "train loss:0.4317521803913117\n",
      "train loss:0.47523508488928956\n",
      "train loss:0.5686433205076498\n",
      "train loss:0.43262742767306034\n",
      "train loss:0.488534372076393\n",
      "train loss:0.47946785878625997\n",
      "train loss:0.5024420929442363\n",
      "train loss:0.5624794009698216\n",
      "train loss:0.3083325112339385\n",
      "train loss:0.5464775080685905\n",
      "train loss:0.37672694575193477\n",
      "train loss:0.6716836060930359\n",
      "train loss:0.3847932797777637\n",
      "train loss:0.4779651535234698\n",
      "train loss:0.5157249422040268\n",
      "train loss:0.5175698243206219\n",
      "train loss:0.30184292447637817\n",
      "train loss:0.7879829134002596\n",
      "train loss:0.46020278527649267\n",
      "train loss:0.387462546782265\n",
      "train loss:0.36958903661277376\n",
      "train loss:0.5571675607843884\n",
      "train loss:0.5854801711964289\n",
      "train loss:0.6693577612141702\n",
      "train loss:0.5079428323497204\n",
      "train loss:0.5260310033372725\n",
      "train loss:0.46510782350110685\n",
      "train loss:0.5944367433694088\n",
      "train loss:0.40794733373567754\n",
      "train loss:0.39553224037756973\n",
      "train loss:0.6247689560603285\n",
      "train loss:0.6906150184939726\n",
      "train loss:0.7471652439671892\n",
      "train loss:0.5816440614856614\n",
      "train loss:0.2882219574899998\n",
      "train loss:0.46708787819053654\n",
      "train loss:0.485060720318378\n",
      "train loss:0.3487603635135682\n",
      "train loss:0.45675762728411423\n",
      "train loss:0.44960779881146273\n",
      "train loss:0.9847985498683627\n",
      "train loss:0.2638793748672311\n",
      "train loss:0.9951863563984178\n",
      "train loss:0.6882455993665826\n",
      "train loss:0.7309182539563942\n",
      "train loss:0.5275617633392005\n",
      "train loss:0.6089674205333109\n",
      "train loss:0.5305795279182275\n",
      "train loss:0.6130471980774792\n",
      "train loss:0.6744597239782444\n",
      "train loss:0.5156992455037114\n",
      "train loss:0.5165943494322518\n",
      "train loss:0.45423624687768693\n",
      "train loss:0.4653218686741658\n",
      "train loss:0.761862986864209\n",
      "train loss:0.6225642409073446\n",
      "train loss:0.4912533676563098\n",
      "train loss:0.574829397540714\n",
      "train loss:0.5040761708904439\n",
      "train loss:0.60338399012445\n",
      "train loss:0.6095294135898218\n",
      "train loss:0.49871241945954586\n",
      "train loss:0.4848309356713071\n",
      "train loss:0.5662619464904065\n",
      "train loss:0.5914663073617771\n",
      "train loss:0.4289996216246279\n",
      "train loss:0.4888781577047248\n",
      "train loss:0.3696883208354794\n",
      "train loss:0.6875989727059701\n",
      "train loss:0.4922387250917393\n",
      "train loss:0.40830501803506936\n",
      "train loss:0.45202796691642816\n",
      "train loss:0.515295097596969\n",
      "train loss:0.6433954792411293\n",
      "train loss:0.44615826329293506\n",
      "train loss:0.4991039369905911\n",
      "train loss:0.49377786464204537\n",
      "train loss:0.30960128552312544\n",
      "train loss:0.9562887059848583\n",
      "train loss:0.5691813216187206\n",
      "train loss:0.1322178427971728\n",
      "train loss:0.6534600282475849\n",
      "train loss:0.31984493612591985\n",
      "train loss:0.48308044626513513\n",
      "train loss:0.9114007463392978\n",
      "train loss:0.34973154756625624\n",
      "train loss:0.8403835207433692\n",
      "train loss:0.7721179777790488\n",
      "train loss:0.5248216698463702\n",
      "train loss:0.513298547487385\n",
      "train loss:0.7642348699384567\n",
      "train loss:0.7811353263833795\n",
      "train loss:0.4145346889982684\n",
      "train loss:0.4669691383680171\n",
      "train loss:0.5560648933657066\n",
      "train loss:0.7057047940933483\n",
      "train loss:0.5866530033188995\n",
      "train loss:0.4483741629510264\n",
      "train loss:0.546171103774911\n",
      "train loss:0.53712203936468\n",
      "train loss:0.6306619169286005\n",
      "train loss:0.47362205270878405\n",
      "train loss:0.6577059435139667\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5450980392156862\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=800, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8176b565-bcd9-4adc-8d44-67c4ce8028e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.300936351513168\n",
      "=== epoch:1, train acc:0.29, test acc:0.31 ===\n",
      "train loss:2.2991809705886346\n",
      "train loss:2.297396478486835\n",
      "train loss:2.295414605095741\n",
      "train loss:2.293144494213417\n",
      "train loss:2.2904360113182833\n",
      "train loss:2.287101797267558\n",
      "train loss:2.283421241958398\n",
      "train loss:2.2783970400417735\n",
      "train loss:2.2731574718267202\n",
      "train loss:2.2678440036460086\n",
      "train loss:2.259025840890818\n",
      "train loss:2.2489293338014082\n",
      "train loss:2.235726388030277\n",
      "train loss:2.22755420709388\n",
      "train loss:2.2108268792601065\n",
      "train loss:2.193832297422564\n",
      "train loss:2.178347838818064\n",
      "train loss:2.1507280786418836\n",
      "train loss:2.116697015420104\n",
      "train loss:2.0673321829361133\n",
      "train loss:2.06210934157894\n",
      "train loss:2.0184264946067945\n",
      "train loss:1.9425535203781084\n",
      "train loss:1.9374556492579402\n",
      "train loss:1.863719567045461\n",
      "train loss:1.8047809724786035\n",
      "train loss:1.7827131302957937\n",
      "train loss:1.6823326416784794\n",
      "train loss:1.625225360084676\n",
      "train loss:1.5001965197676472\n",
      "train loss:1.480952438871578\n",
      "train loss:1.3281099599038142\n",
      "train loss:1.3116378922753977\n",
      "train loss:1.2381480175512618\n",
      "train loss:1.1715083746393766\n",
      "train loss:1.0483776634754773\n",
      "train loss:1.0071069382595421\n",
      "train loss:0.9195365872076848\n",
      "train loss:0.8194250294837626\n",
      "train loss:0.7825195978113448\n",
      "train loss:0.8014459543330673\n",
      "train loss:0.8226338023125394\n",
      "train loss:0.7915350838072441\n",
      "train loss:0.7494158562378156\n",
      "train loss:0.8053721355792962\n",
      "train loss:0.7993484040412272\n",
      "train loss:0.5721157628784616\n",
      "train loss:0.4810339636761431\n",
      "train loss:0.927313183715032\n",
      "train loss:0.5490509349005266\n",
      "train loss:0.7124705693466077\n",
      "train loss:0.7925262438696153\n",
      "train loss:0.626452340360687\n",
      "train loss:0.5367045043919265\n",
      "train loss:0.536954938589439\n",
      "train loss:0.8599274267383474\n",
      "train loss:0.5314888567246769\n",
      "train loss:0.5456123567856946\n",
      "train loss:0.43646017719867025\n",
      "train loss:0.5153058313160435\n",
      "train loss:0.6085207148329232\n",
      "train loss:0.807409814272075\n",
      "train loss:0.3227922152772363\n",
      "train loss:0.49803883102157637\n",
      "train loss:0.5181360062502287\n",
      "train loss:0.621729597177467\n",
      "train loss:0.5099123406864916\n",
      "train loss:0.6209010439058631\n",
      "train loss:0.6132465636488672\n",
      "train loss:0.6124472144541333\n",
      "train loss:0.7487428884844347\n",
      "train loss:0.4235028045810113\n",
      "train loss:0.8524122908702919\n",
      "train loss:0.3820372475531466\n",
      "train loss:0.6271524862273031\n",
      "train loss:0.5267867282558076\n",
      "train loss:0.6050473471531364\n",
      "train loss:0.2936711105133279\n",
      "train loss:0.4761920743969517\n",
      "train loss:0.7390354489849236\n",
      "train loss:0.7095454994154606\n",
      "train loss:0.6905225323008475\n",
      "train loss:0.6301804429438892\n",
      "train loss:0.6283487934062004\n",
      "train loss:0.5703921740690231\n",
      "train loss:0.5368021316649019\n",
      "train loss:0.4054866747967128\n",
      "train loss:0.6033619185150351\n",
      "train loss:0.5477762120054472\n",
      "train loss:0.8890683341869888\n",
      "train loss:0.42409385670403676\n",
      "train loss:0.8040056070481771\n",
      "train loss:0.33969405322757873\n",
      "train loss:0.4206900385359214\n",
      "train loss:0.506151979846422\n",
      "train loss:0.7252899615866856\n",
      "train loss:0.6260087710049423\n",
      "train loss:0.4784182396275341\n",
      "train loss:0.7163580772709031\n",
      "train loss:0.38373119107942794\n",
      "train loss:0.2546426881858038\n",
      "train loss:0.610820528550887\n",
      "train loss:0.6491875605023086\n",
      "train loss:0.7464775882930679\n",
      "train loss:0.7392183568311144\n",
      "train loss:0.7683142976666584\n",
      "train loss:0.511405662728307\n",
      "train loss:0.8235578767590439\n",
      "train loss:0.5205463594347935\n",
      "train loss:0.4382631715472406\n",
      "train loss:0.7206786257562551\n",
      "train loss:0.6229456522680465\n",
      "train loss:0.7880517128109068\n",
      "train loss:0.7755024471935353\n",
      "train loss:0.5451316921703436\n",
      "train loss:0.6098681898968349\n",
      "train loss:0.6258633355505195\n",
      "train loss:0.6940839464169993\n",
      "train loss:0.5620985997961442\n",
      "train loss:0.7224676198023271\n",
      "train loss:0.7305319211836092\n",
      "train loss:0.6285588534248306\n",
      "train loss:0.6759140183544821\n",
      "train loss:0.6260052109465027\n",
      "train loss:0.6874684699375141\n",
      "train loss:0.6866925752556325\n",
      "train loss:0.605309390469656\n",
      "train loss:0.6750037609230095\n",
      "train loss:0.5912810040684627\n",
      "train loss:0.679196198733238\n",
      "train loss:0.626097263415043\n",
      "train loss:0.7843264071549292\n",
      "train loss:0.7711796388522253\n",
      "train loss:0.6264312324641905\n",
      "train loss:0.5849795210698441\n",
      "train loss:0.6339864927063711\n",
      "train loss:0.4734996351040566\n",
      "train loss:0.6881589051831697\n",
      "train loss:0.5586691054152262\n",
      "train loss:0.5341159630724126\n",
      "train loss:0.4578919279153137\n",
      "train loss:0.6385853414227516\n",
      "train loss:0.4236760656456983\n",
      "train loss:0.7417176802818959\n",
      "train loss:0.7458264377902906\n",
      "train loss:0.3756629110173949\n",
      "train loss:0.5056877349621415\n",
      "train loss:0.6280924882292929\n",
      "train loss:0.5077603857144799\n",
      "train loss:0.48975388339160525\n",
      "train loss:0.37341658721864435\n",
      "train loss:0.5155845897472833\n",
      "train loss:0.1961508824781632\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6166734575615246\n",
      "train loss:0.35348278385279003\n",
      "train loss:0.6517036576854343\n",
      "train loss:0.3913997424450673\n",
      "train loss:0.8380127877602783\n",
      "train loss:0.647037712666301\n",
      "train loss:0.8699602617762523\n",
      "train loss:0.6472541340090607\n",
      "train loss:0.4689382343725555\n",
      "train loss:0.7646620055824515\n",
      "train loss:0.7717670919368425\n",
      "train loss:0.4905729174003624\n",
      "train loss:0.49989296784887916\n",
      "train loss:0.5103297074596321\n",
      "train loss:0.6335295779113401\n",
      "train loss:0.42556426166838196\n",
      "train loss:0.7016101716569147\n",
      "train loss:0.3462728851145689\n",
      "train loss:0.5139046989969294\n",
      "train loss:0.689632831164889\n",
      "train loss:0.614762281882389\n",
      "train loss:0.517652626265195\n",
      "train loss:0.7825504016862418\n",
      "train loss:0.5411675957340119\n",
      "train loss:0.605179437367706\n",
      "train loss:0.6197471918949146\n",
      "train loss:0.8774559758949272\n",
      "train loss:0.4548784401547417\n",
      "train loss:0.6197233430150371\n",
      "train loss:0.46221823138136153\n",
      "train loss:0.7853854620936083\n",
      "train loss:0.3834044641748157\n",
      "train loss:0.4276754840504971\n",
      "train loss:0.5953874500641724\n",
      "train loss:0.5357931945029027\n",
      "train loss:0.6266091283355797\n",
      "train loss:0.7231659283689961\n",
      "train loss:0.6269177140607642\n",
      "train loss:0.5388021342349452\n",
      "train loss:0.7602890549273139\n",
      "train loss:0.7223258773466859\n",
      "train loss:0.7082727635328581\n",
      "train loss:0.6188741334870838\n",
      "train loss:0.5106864952309782\n",
      "train loss:0.6183516079927003\n",
      "train loss:0.528938805284457\n",
      "train loss:0.5286306307796739\n",
      "train loss:0.5154330417861928\n",
      "train loss:0.6127017016950752\n",
      "train loss:0.8167878211499007\n",
      "train loss:0.6164429086420172\n",
      "train loss:0.6007018294174663\n",
      "train loss:0.5016586233185213\n",
      "train loss:0.5402851980157444\n",
      "train loss:0.7736097616466009\n",
      "train loss:0.560259779177935\n",
      "train loss:0.6130349010828946\n",
      "train loss:0.5318599269615223\n",
      "train loss:0.451399781303758\n",
      "train loss:0.6104342997989061\n",
      "train loss:0.44898226858194185\n",
      "train loss:0.7303541417600248\n",
      "train loss:0.5358529710460491\n",
      "train loss:0.72312504506404\n",
      "train loss:0.6852246009247761\n",
      "train loss:0.8304655895248256\n",
      "train loss:0.6990777099189767\n",
      "train loss:0.5254110228899916\n",
      "train loss:0.6838353941111802\n",
      "train loss:0.5286814930541611\n",
      "train loss:0.5381120300470584\n",
      "train loss:0.6722543084712593\n",
      "train loss:0.529454255970374\n",
      "train loss:0.6085076826615733\n",
      "train loss:0.6112588005025096\n",
      "train loss:0.6903798434927323\n",
      "train loss:0.5415442680931464\n",
      "train loss:0.4442466175303389\n",
      "train loss:0.521433525043114\n",
      "train loss:0.558000849342691\n",
      "train loss:0.9075661018824259\n",
      "train loss:0.807245541149389\n",
      "train loss:0.5245234343713495\n",
      "train loss:0.9995442025421835\n",
      "train loss:0.5392475098889264\n",
      "train loss:0.5949632039194556\n",
      "train loss:0.564243378619273\n",
      "train loss:0.4656907521285471\n",
      "train loss:0.5427587061301632\n",
      "train loss:0.5556320841808523\n",
      "train loss:0.6683474851390117\n",
      "train loss:0.6107146151141706\n",
      "train loss:0.5299944779895843\n",
      "train loss:0.44749068730936664\n",
      "train loss:0.7105433266613829\n",
      "train loss:0.7089355416182295\n",
      "train loss:0.4010136882718906\n",
      "train loss:0.6232181256678262\n",
      "train loss:0.292715883168952\n",
      "train loss:0.6205321975680597\n",
      "train loss:0.6067485827463708\n",
      "train loss:0.750803901763142\n",
      "train loss:0.6015312914331054\n",
      "train loss:0.8352832215887768\n",
      "train loss:0.8474529448239736\n",
      "train loss:0.6313754252985552\n",
      "train loss:0.6459299727466289\n",
      "train loss:0.7744760601306814\n",
      "train loss:0.7827111252126533\n",
      "train loss:0.5401702581500814\n",
      "train loss:0.6222075946787499\n",
      "train loss:0.49394270468295065\n",
      "train loss:0.5363673261925659\n",
      "train loss:0.8110940209725715\n",
      "train loss:0.5001130183908841\n",
      "train loss:0.6722610683794703\n",
      "train loss:0.6808605002707313\n",
      "train loss:0.5500096329934105\n",
      "train loss:0.6761930735093508\n",
      "train loss:0.49048301307296677\n",
      "train loss:0.6816062035171055\n",
      "train loss:0.4858668810853965\n",
      "train loss:0.857370370754912\n",
      "train loss:0.5392950904880752\n",
      "train loss:0.3962195055578131\n",
      "train loss:0.3725318190016578\n",
      "train loss:0.6139574682267676\n",
      "train loss:0.4982754036092918\n",
      "train loss:0.7191552865458715\n",
      "train loss:0.5118537632156659\n",
      "train loss:0.9296940894445932\n",
      "train loss:0.5991337993504668\n",
      "train loss:0.5448738399246862\n",
      "train loss:0.5278483817103895\n",
      "train loss:0.4099588856925093\n",
      "train loss:0.40526444203700446\n",
      "train loss:0.7358267364336772\n",
      "train loss:0.6451695614558115\n",
      "train loss:0.39018149923756335\n",
      "train loss:0.7370371836546981\n",
      "train loss:0.5142513412566865\n",
      "train loss:0.9947383629463122\n",
      "train loss:0.6095998333567288\n",
      "train loss:0.7383787969231385\n",
      "train loss:0.8931297180275122\n",
      "train loss:0.8268453807044139\n",
      "train loss:0.4415971122970487\n",
      "train loss:0.7652877617626228\n",
      "train loss:0.5568701778560149\n",
      "train loss:0.6161729053345839\n",
      "train loss:0.5630498722423002\n",
      "train loss:0.624492648433564\n",
      "train loss:0.5751601220457085\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6357448272587423\n",
      "train loss:0.559216217531887\n",
      "train loss:0.618067285544249\n",
      "train loss:0.5489834434618097\n",
      "train loss:0.6361366817627176\n",
      "train loss:0.7411117329245859\n",
      "train loss:0.39931403531207343\n",
      "train loss:0.5402018642259706\n",
      "train loss:0.5521505815174631\n",
      "train loss:0.7597716942917883\n",
      "train loss:0.6712560053480001\n",
      "train loss:0.6075770684534969\n",
      "train loss:0.44141319313875\n",
      "train loss:0.5983405618245151\n",
      "train loss:0.58837945416179\n",
      "train loss:0.8169349649664095\n",
      "train loss:0.4939287384898655\n",
      "train loss:0.9102446876792882\n",
      "train loss:0.5003373717690532\n",
      "train loss:0.4437201322786004\n",
      "train loss:0.31854908737799087\n",
      "train loss:0.738013754521171\n",
      "train loss:0.5061189899151414\n",
      "train loss:0.5756915942673682\n",
      "train loss:0.5140947908013015\n",
      "train loss:0.7158083203724162\n",
      "train loss:0.4831173352036015\n",
      "train loss:0.5187816946182309\n",
      "train loss:0.5088642339573538\n",
      "train loss:0.6319331674587565\n",
      "train loss:0.49430561330563094\n",
      "train loss:0.5096330761160861\n",
      "train loss:0.7416514872752965\n",
      "train loss:0.6333172374618776\n",
      "train loss:0.727554371959229\n",
      "train loss:0.7621886844877096\n",
      "train loss:0.4927264387089097\n",
      "train loss:0.5231430924420319\n",
      "train loss:0.5033516659839856\n",
      "train loss:0.7945682494741465\n",
      "train loss:0.5261203893134299\n",
      "train loss:0.4361393551179121\n",
      "train loss:0.7224888296885691\n",
      "train loss:0.8060442306687445\n",
      "train loss:0.5139781253784814\n",
      "train loss:0.6862413156701971\n",
      "train loss:0.6760189681658316\n",
      "train loss:0.5476466447070286\n",
      "train loss:0.4448079664061725\n",
      "train loss:0.6966342519637247\n",
      "train loss:0.5269250774858565\n",
      "train loss:0.76483554229406\n",
      "train loss:0.5307194260479691\n",
      "train loss:0.6922850094030176\n",
      "train loss:0.5370419542753828\n",
      "train loss:0.46513042438262886\n",
      "train loss:0.7902985422261227\n",
      "train loss:0.5369837139540745\n",
      "train loss:0.6319538930986549\n",
      "train loss:0.5356148577598473\n",
      "train loss:0.7790440255703045\n",
      "train loss:0.6090166304345455\n",
      "train loss:0.5409070211507949\n",
      "train loss:0.6312647811442358\n",
      "train loss:0.6135132791688545\n",
      "train loss:0.44052370588184503\n",
      "train loss:0.6296455899004705\n",
      "train loss:0.6272084279288488\n",
      "train loss:0.6252447604166277\n",
      "train loss:0.6991888207196112\n",
      "train loss:0.6044175014655333\n",
      "train loss:0.6272653219302253\n",
      "train loss:0.6164648015959472\n",
      "train loss:0.4153374334915192\n",
      "train loss:0.5988295209240164\n",
      "train loss:0.510510978716963\n",
      "train loss:0.5075850820281402\n",
      "train loss:0.7178684074102678\n",
      "train loss:0.7209557529412904\n",
      "train loss:0.5487346072675127\n",
      "train loss:0.9106175710643191\n",
      "train loss:0.5938888823033748\n",
      "train loss:0.6206035564994046\n",
      "train loss:0.7848026567082769\n",
      "train loss:0.5453817534536637\n",
      "train loss:0.5296632621788001\n",
      "train loss:0.6108428677944809\n",
      "train loss:0.6999868222571629\n",
      "train loss:0.5370400817688894\n",
      "train loss:0.5493148500157388\n",
      "train loss:0.6161490519234366\n",
      "train loss:0.518537394878951\n",
      "train loss:0.6546167959584938\n",
      "train loss:0.6295393899079741\n",
      "train loss:0.7042583858751874\n",
      "train loss:0.6934473419550471\n",
      "train loss:0.44062042734329915\n",
      "train loss:0.6791911123357298\n",
      "train loss:0.8361621045274191\n",
      "train loss:0.45755948925420853\n",
      "train loss:0.6241833099557169\n",
      "train loss:0.5346035629679\n",
      "train loss:0.5069535300024655\n",
      "train loss:0.5346483241449993\n",
      "train loss:0.7857760546744331\n",
      "train loss:0.5993705379808226\n",
      "train loss:0.5195860604154497\n",
      "train loss:0.5325364338983474\n",
      "train loss:0.5149430434411603\n",
      "train loss:0.625984711002176\n",
      "train loss:0.3074823359240383\n",
      "train loss:0.5982309461729598\n",
      "train loss:0.7285945662516242\n",
      "train loss:0.5235375477965905\n",
      "train loss:0.6234015406399499\n",
      "train loss:0.6268543593218754\n",
      "train loss:0.5183576276453271\n",
      "train loss:0.6035797021028392\n",
      "train loss:0.7565364747408635\n",
      "train loss:0.5189813710492571\n",
      "train loss:0.4667934702672011\n",
      "train loss:0.9551488262083886\n",
      "train loss:0.6245474519631447\n",
      "train loss:0.8376611026697803\n",
      "train loss:0.6925535809507695\n",
      "train loss:0.5228241393384596\n",
      "train loss:0.6266000526380266\n",
      "train loss:0.627824890948132\n",
      "train loss:0.6170399825281798\n",
      "train loss:0.5592498218500397\n",
      "train loss:0.6316771513291265\n",
      "train loss:0.7498126702188085\n",
      "train loss:0.5022040769097404\n",
      "train loss:0.6298121366024259\n",
      "train loss:0.6265922723576944\n",
      "train loss:0.6191521531238113\n",
      "train loss:0.5524916211002344\n",
      "train loss:0.617104914565516\n",
      "train loss:0.6202389869583863\n",
      "train loss:0.5345433627186489\n",
      "train loss:0.6212891409716768\n",
      "train loss:0.6063176781580835\n",
      "train loss:0.838244611080151\n",
      "train loss:0.6090770653801815\n",
      "train loss:0.6465744562699823\n",
      "train loss:0.6184354200032494\n",
      "train loss:0.6940845529106255\n",
      "train loss:0.5457846196325222\n",
      "train loss:0.6118065960509208\n",
      "train loss:0.5234027983597062\n",
      "train loss:0.6096656519108212\n",
      "train loss:0.4379637342449806\n",
      "train loss:0.7200466694283609\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6025103147752838\n",
      "train loss:0.5821304711416441\n",
      "train loss:0.6936107166172999\n",
      "train loss:0.9068222775767645\n",
      "train loss:0.51015836865077\n",
      "train loss:0.502499623536802\n",
      "train loss:0.684103079165322\n",
      "train loss:0.5169054951932258\n",
      "train loss:0.6879208128628072\n",
      "train loss:0.5288195804414824\n",
      "train loss:0.612993531666132\n",
      "train loss:0.6147117395873416\n",
      "train loss:0.6862266068177133\n",
      "train loss:0.5910620501842824\n",
      "train loss:0.6551444819439286\n",
      "train loss:0.6802992255351273\n",
      "train loss:0.6959442413298957\n",
      "train loss:0.7070012146565209\n",
      "train loss:0.6333452587477774\n",
      "train loss:0.5226717646497085\n",
      "train loss:0.6106096126540048\n",
      "train loss:0.5708855195059549\n",
      "train loss:0.616549123061381\n",
      "train loss:0.6180538547978885\n",
      "train loss:0.6098376914921622\n",
      "train loss:0.6119790057881671\n",
      "train loss:0.7091775768921071\n",
      "train loss:0.6198049341708084\n",
      "train loss:0.6380987196155986\n",
      "train loss:0.5478361834785261\n",
      "train loss:0.7562828959733578\n",
      "train loss:0.5593569028703164\n",
      "train loss:0.6195837732506762\n",
      "train loss:0.5454615578491113\n",
      "train loss:0.7486357446890882\n",
      "train loss:0.6278494656401491\n",
      "train loss:0.7568172809540987\n",
      "train loss:0.653845106208897\n",
      "train loss:0.8154579556834884\n",
      "train loss:0.6223930844254858\n",
      "train loss:0.43691455307518784\n",
      "train loss:0.6047470775352461\n",
      "train loss:0.4157569851941819\n",
      "train loss:0.6034493733367696\n",
      "train loss:0.6897233346644606\n",
      "train loss:0.45576025561961153\n",
      "train loss:0.4673455986740514\n",
      "train loss:0.6834439818228244\n",
      "train loss:0.4154444948584525\n",
      "train loss:0.3976306652277524\n",
      "train loss:0.7275449118531881\n",
      "train loss:0.6160666605763175\n",
      "train loss:0.7313403090665547\n",
      "train loss:0.73716804217219\n",
      "train loss:0.6161801099095983\n",
      "train loss:0.9558269432135663\n",
      "train loss:0.5330424059102544\n",
      "train loss:0.49289806072825904\n",
      "train loss:0.6884539747343982\n",
      "train loss:0.5292293166790673\n",
      "train loss:0.41220558534616936\n",
      "train loss:0.4203335333610455\n",
      "train loss:0.43020877036871524\n",
      "train loss:0.38523062019786425\n",
      "train loss:0.5044129056596833\n",
      "train loss:0.3928627942918337\n",
      "train loss:0.6144318307856003\n",
      "train loss:0.8656350120953082\n",
      "train loss:0.626249529515109\n",
      "train loss:0.36291539455900584\n",
      "train loss:0.5333902081615484\n",
      "train loss:0.3675044374605564\n",
      "train loss:0.3747893115165536\n",
      "train loss:0.3959321958017129\n",
      "train loss:0.6353816095828151\n",
      "train loss:0.954987438828856\n",
      "train loss:0.35720425981807125\n",
      "train loss:0.3840687756262633\n",
      "train loss:0.4648546049032092\n",
      "train loss:0.48771041302542334\n",
      "train loss:0.9119286103972003\n",
      "train loss:0.5524846979415314\n",
      "train loss:0.652204074071434\n",
      "train loss:0.4636570328043982\n",
      "train loss:0.6630038210080442\n",
      "train loss:0.7347831321334592\n",
      "train loss:0.7263147011043821\n",
      "train loss:0.619425902287356\n",
      "train loss:0.5191900928833688\n",
      "train loss:0.5189021309869448\n",
      "train loss:0.514652584160927\n",
      "train loss:0.5989851171196287\n",
      "train loss:0.7988321690733693\n",
      "train loss:0.5339501311897564\n",
      "train loss:0.6425537827353016\n",
      "train loss:0.6760727493003504\n",
      "train loss:0.4663299609099186\n",
      "train loss:0.6825628062868973\n",
      "train loss:0.5994381893415467\n",
      "train loss:0.4753587415557859\n",
      "train loss:0.4539986707799802\n",
      "train loss:0.5298244588489032\n",
      "train loss:0.6266487662678419\n",
      "train loss:0.6931945831820825\n",
      "train loss:0.5081323190860931\n",
      "train loss:0.514542459732036\n",
      "train loss:0.3195319272763313\n",
      "train loss:0.8289396775734588\n",
      "train loss:0.49752313194635445\n",
      "train loss:0.5037954098532234\n",
      "train loss:0.6192423073623029\n",
      "train loss:0.5128663916549117\n",
      "train loss:0.978724544107742\n",
      "train loss:0.37479335111393375\n",
      "train loss:0.3796423865432391\n",
      "train loss:0.7258805429440379\n",
      "train loss:0.7093392377642734\n",
      "train loss:0.5279131299941529\n",
      "train loss:0.8179905985435468\n",
      "train loss:0.5167953185851335\n",
      "train loss:0.523818821247094\n",
      "train loss:0.6035470878608198\n",
      "train loss:0.69333269642053\n",
      "train loss:0.49310068224753445\n",
      "train loss:0.6328629260019593\n",
      "train loss:0.6216887235402492\n",
      "train loss:0.5071970811042817\n",
      "train loss:0.6356885139292187\n",
      "train loss:0.42988583490318816\n",
      "train loss:0.5138396749986706\n",
      "train loss:0.619668117889943\n",
      "train loss:0.6040240230133389\n",
      "train loss:0.6039041404764101\n",
      "train loss:0.6449123195845865\n",
      "train loss:0.6274748639091459\n",
      "train loss:0.519050815935635\n",
      "train loss:0.7062951801850049\n",
      "train loss:0.5340845953866603\n",
      "train loss:0.7038206051288818\n",
      "train loss:0.7064453836523333\n",
      "train loss:0.5214704289707817\n",
      "train loss:0.639290425381595\n",
      "train loss:0.5102064115336342\n",
      "train loss:0.5468244761335778\n",
      "train loss:0.6055698277895636\n",
      "train loss:0.5214357000893413\n",
      "train loss:0.9085123504096886\n",
      "train loss:0.8480180081918547\n",
      "train loss:0.5192983383742424\n",
      "train loss:0.5618754954891956\n",
      "train loss:0.6308415518649079\n",
      "train loss:0.6155155011384827\n",
      "train loss:0.5322427760429153\n",
      "=== epoch:5, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7614074464420062\n",
      "train loss:0.8133722392361173\n",
      "train loss:0.6160983231842836\n",
      "train loss:0.6281000111495212\n",
      "train loss:0.5521042879774496\n",
      "train loss:0.5635056247897711\n",
      "train loss:0.6273107783338414\n",
      "train loss:0.8002417742950648\n",
      "train loss:0.4333695405173853\n",
      "train loss:0.5465967698653759\n",
      "train loss:0.5353347125839155\n",
      "train loss:0.5240912779373452\n",
      "train loss:0.606780450220629\n",
      "train loss:0.5280695681622961\n",
      "train loss:0.6097405468269046\n",
      "train loss:0.6094336014025397\n",
      "train loss:0.6144663078681232\n",
      "train loss:0.815453077196721\n",
      "train loss:0.6187252977213914\n",
      "train loss:0.7276295757470915\n",
      "train loss:0.41810750340579084\n",
      "train loss:0.6490972416662728\n",
      "train loss:0.6964002187367899\n",
      "train loss:0.5071804448853262\n",
      "train loss:0.5068599346286499\n",
      "train loss:0.6934975721949124\n",
      "train loss:0.41612580850409325\n",
      "train loss:0.6231140556034319\n",
      "train loss:0.6020519859544009\n",
      "train loss:0.7258847343781045\n",
      "train loss:0.5311003087987736\n",
      "train loss:0.6136442865243505\n",
      "train loss:0.8990160514683445\n",
      "train loss:0.7793007324320403\n",
      "train loss:0.5043459475703446\n",
      "train loss:0.6948299022947921\n",
      "train loss:0.5598679960250788\n",
      "train loss:0.3624066209618616\n",
      "train loss:0.831595478289672\n",
      "train loss:0.6171186562408363\n",
      "train loss:0.7518472764686601\n",
      "train loss:0.6136290202492328\n",
      "train loss:0.6687149890904757\n",
      "train loss:0.6089200353210658\n",
      "train loss:0.5603272338032869\n",
      "train loss:0.6822160232301115\n",
      "train loss:0.6792194730456015\n",
      "train loss:0.5027177381677209\n",
      "train loss:0.6191765784839285\n",
      "train loss:0.7930303798739755\n",
      "train loss:0.5627186514382465\n",
      "train loss:0.5557135493894082\n",
      "train loss:0.39435175997462346\n",
      "train loss:0.7522006081142614\n",
      "train loss:0.6809614649346865\n",
      "train loss:0.44729370330344675\n",
      "train loss:0.44746231892003846\n",
      "train loss:0.6291951850438362\n",
      "train loss:0.7881431970849035\n",
      "train loss:0.7144951066827339\n",
      "train loss:0.537044706085643\n",
      "train loss:0.7301206955609227\n",
      "train loss:0.6148973409471101\n",
      "train loss:0.7185658269308781\n",
      "train loss:0.7026197918999487\n",
      "train loss:0.5093441558497925\n",
      "train loss:0.6171943632647232\n",
      "train loss:0.6279186300340018\n",
      "train loss:0.5275647946129741\n",
      "train loss:0.8032900552929301\n",
      "train loss:0.5490506558454665\n",
      "train loss:0.6984795348232382\n",
      "train loss:0.6698925165789694\n",
      "train loss:0.6480184364497286\n",
      "train loss:0.5561341627256577\n",
      "train loss:0.5299438434576242\n",
      "train loss:0.6074282741022529\n",
      "train loss:0.5587568314858885\n",
      "train loss:0.6170354006895206\n",
      "train loss:0.6231903787475188\n",
      "train loss:0.5289170155913512\n",
      "train loss:0.5473427875934063\n",
      "train loss:0.5253577479469017\n",
      "train loss:0.6061177977099105\n",
      "train loss:0.6164639901796674\n",
      "train loss:0.5968676691567706\n",
      "train loss:0.6962160836206752\n",
      "train loss:0.6085599283827864\n",
      "train loss:0.7813914400369065\n",
      "train loss:0.4159276415687813\n",
      "train loss:0.30090134510306943\n",
      "train loss:0.7368177654587205\n",
      "train loss:0.5156985698592271\n",
      "train loss:0.6344747557130974\n",
      "train loss:0.6044689521411082\n",
      "train loss:0.614848022780945\n",
      "train loss:0.6162358674536502\n",
      "train loss:0.61067503465447\n",
      "train loss:0.5012470273345007\n",
      "train loss:0.6170096540054947\n",
      "train loss:0.9556412870269686\n",
      "train loss:0.511490830790924\n",
      "train loss:0.6006281790688129\n",
      "train loss:0.9056391127637928\n",
      "train loss:0.7730394055630941\n",
      "train loss:0.5085881754019193\n",
      "train loss:0.5316025072869732\n",
      "train loss:0.6762773909668655\n",
      "train loss:0.4993084341457468\n",
      "train loss:0.9085953695612838\n",
      "train loss:0.6742344932220645\n",
      "train loss:0.726413774872632\n",
      "train loss:0.6295419363618007\n",
      "train loss:0.7194359669382768\n",
      "train loss:0.5169575614138745\n",
      "train loss:0.742365824837956\n",
      "train loss:0.5471349516961039\n",
      "train loss:0.5997289629749343\n",
      "train loss:0.6327429147989216\n",
      "train loss:0.6324498595901253\n",
      "train loss:0.7650689647450833\n",
      "train loss:0.6792623136299086\n",
      "train loss:0.6248878278403294\n",
      "train loss:0.5201646287317654\n",
      "train loss:0.6191347076516008\n",
      "train loss:0.6011122095989878\n",
      "train loss:0.5539714782288215\n",
      "train loss:0.5973397322309921\n",
      "train loss:0.6885190975368514\n",
      "train loss:0.6076715418736999\n",
      "train loss:0.6114893897749241\n",
      "train loss:0.43489554486090787\n",
      "train loss:0.6377354487149669\n",
      "train loss:0.49681484715725127\n",
      "train loss:0.8289610371936685\n",
      "train loss:0.3057504269366265\n",
      "train loss:0.8159514679725051\n",
      "train loss:0.7151630484063044\n",
      "train loss:0.7606147809824222\n",
      "train loss:0.7024061849032116\n",
      "train loss:0.6418584888749193\n",
      "train loss:0.541849969733242\n",
      "train loss:0.4907925852893819\n",
      "train loss:0.7020756235697977\n",
      "train loss:0.6253246107031398\n",
      "train loss:0.6462806969945237\n",
      "train loss:0.6185702633266317\n",
      "train loss:0.7019008130170493\n",
      "train loss:0.5340204998986942\n",
      "train loss:0.5272268655429299\n",
      "train loss:0.681849399769796\n",
      "train loss:0.7807367390917948\n",
      "train loss:0.743567978786033\n",
      "=== epoch:6, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6218656567705886\n",
      "train loss:0.5456804726826364\n",
      "train loss:0.5610423750473128\n",
      "train loss:0.6234976848571484\n",
      "train loss:0.694232718514219\n",
      "train loss:0.623341826669014\n",
      "train loss:0.6842239249430693\n",
      "train loss:0.6756797413803466\n",
      "train loss:0.4794921410849778\n",
      "train loss:0.5433846533571755\n",
      "train loss:0.6269524179585617\n",
      "train loss:0.8362714940468093\n",
      "train loss:0.6694388911528175\n",
      "train loss:0.7332837568320935\n",
      "train loss:0.6240293412595608\n",
      "train loss:0.6099819091345898\n",
      "train loss:0.6157026812247269\n",
      "train loss:0.6743061963399031\n",
      "train loss:0.4929791695345601\n",
      "train loss:0.5480130180092064\n",
      "train loss:0.8176106190033767\n",
      "train loss:0.6078074908246972\n",
      "train loss:0.472070280915388\n",
      "train loss:0.3915542974016181\n",
      "train loss:0.7011193498637749\n",
      "train loss:0.44499820383293776\n",
      "train loss:0.8701403859421577\n",
      "train loss:0.5973081536277414\n",
      "train loss:0.6102678248711305\n",
      "train loss:0.5129518671478135\n",
      "train loss:0.517046191026185\n",
      "train loss:0.8859877682680851\n",
      "train loss:0.8151320377355091\n",
      "train loss:0.5311840340382357\n",
      "train loss:0.6067173157659566\n",
      "train loss:0.610317733873725\n",
      "train loss:0.42090565803308444\n",
      "train loss:0.6393616092873288\n",
      "train loss:0.6202948467214564\n",
      "train loss:0.717425585882375\n",
      "train loss:0.7040332317871626\n",
      "train loss:0.6098416172886618\n",
      "train loss:0.3593786232212171\n",
      "train loss:0.6841247352755747\n",
      "train loss:0.4472803910274748\n",
      "train loss:0.6966605321489565\n",
      "train loss:0.7747103465217332\n",
      "train loss:0.7959082391420407\n",
      "train loss:0.6140794972334126\n",
      "train loss:0.4454688476345261\n",
      "train loss:0.6221961199776095\n",
      "train loss:0.44312806884665434\n",
      "train loss:0.5373672178896916\n",
      "train loss:0.5165110571148046\n",
      "train loss:0.4306893585698875\n",
      "train loss:0.5205455152872939\n",
      "train loss:0.5020078984846669\n",
      "train loss:0.4967092506945991\n",
      "train loss:0.6174172620668863\n",
      "train loss:0.6057145789107885\n",
      "train loss:0.38995467532839595\n",
      "train loss:0.38894447679613686\n",
      "train loss:0.5105928848127455\n",
      "train loss:0.6409524204531734\n",
      "train loss:0.6826183683524872\n",
      "train loss:0.4850161630727155\n",
      "train loss:0.5197672358638948\n",
      "train loss:0.6702536480060597\n",
      "train loss:0.36511720548457205\n",
      "train loss:0.5021264332560174\n",
      "train loss:0.6611895467531215\n",
      "train loss:0.20745321335288175\n",
      "train loss:0.49006161936520476\n",
      "train loss:0.5015929589164556\n",
      "train loss:0.8272047297347502\n",
      "train loss:0.5058903172665987\n",
      "train loss:0.5123375507153912\n",
      "train loss:0.516158590039838\n",
      "train loss:0.6574762475477831\n",
      "train loss:0.38294888359910423\n",
      "train loss:0.4747643403612106\n",
      "train loss:0.36251594163509276\n",
      "train loss:0.3718489627376103\n",
      "train loss:0.7596195143218748\n",
      "train loss:0.7723883529620756\n",
      "train loss:0.4825696564749406\n",
      "train loss:0.7338716846485893\n",
      "train loss:0.74497210824732\n",
      "train loss:0.7126074374405781\n",
      "train loss:0.5056265007243141\n",
      "train loss:0.4106285437447861\n",
      "train loss:0.29659903912188085\n",
      "train loss:0.5304048669039085\n",
      "train loss:0.526426190138878\n",
      "train loss:0.5317579438515945\n",
      "train loss:0.4240026450713949\n",
      "train loss:0.8202979943166377\n",
      "train loss:0.508464139132659\n",
      "train loss:0.41965959647324214\n",
      "train loss:0.2811226858174406\n",
      "train loss:0.4006875885272316\n",
      "train loss:0.5314891000199747\n",
      "train loss:1.1053441549772467\n",
      "train loss:0.7513436861728375\n",
      "train loss:0.25746087651250604\n",
      "train loss:0.623589401884681\n",
      "train loss:0.757982824569281\n",
      "train loss:0.6255290537582519\n",
      "train loss:0.6326724412334825\n",
      "train loss:0.8469429115101204\n",
      "train loss:0.9150766562984382\n",
      "train loss:0.5799446022586879\n",
      "train loss:0.6127697270024235\n",
      "train loss:0.38409808715019306\n",
      "train loss:0.6003222206825797\n",
      "train loss:0.6045154861156534\n",
      "train loss:0.5442790637191488\n",
      "train loss:0.6092218036852876\n",
      "train loss:0.720292681042465\n",
      "train loss:0.47551718054781345\n",
      "train loss:0.46763979516461057\n",
      "train loss:0.6833701981645909\n",
      "train loss:0.770964337341241\n",
      "train loss:0.6130823177498536\n",
      "train loss:0.6079325761075258\n",
      "train loss:0.6916227291659847\n",
      "train loss:0.6175227559449895\n",
      "train loss:0.7450067982278435\n",
      "train loss:0.6050541087938119\n",
      "train loss:0.6123961021339923\n",
      "train loss:0.5528441908698953\n",
      "train loss:0.6714093786558827\n",
      "train loss:0.6785249480627726\n",
      "train loss:0.7523290117282168\n",
      "train loss:0.611063300971769\n",
      "train loss:0.6199571263907542\n",
      "train loss:0.6836152744774681\n",
      "train loss:0.551655867233793\n",
      "train loss:0.5371281397950064\n",
      "train loss:0.47992854963935816\n",
      "train loss:0.5319042603800932\n",
      "train loss:0.6863861735058842\n",
      "train loss:0.7833522500289114\n",
      "train loss:0.6885406957630724\n",
      "train loss:0.5507115773365661\n",
      "train loss:0.536381655993709\n",
      "train loss:0.5224116142720014\n",
      "train loss:0.8643554706604304\n",
      "train loss:0.452833764307464\n",
      "train loss:0.6854528315396184\n",
      "train loss:0.3529429098066107\n",
      "train loss:0.6144551054410117\n",
      "train loss:0.5692717118778445\n",
      "=== epoch:7, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5188090129543241\n",
      "train loss:0.614630289443129\n",
      "train loss:0.6421456576963537\n",
      "train loss:0.7944273249624741\n",
      "train loss:0.7299193564689765\n",
      "train loss:0.7253679500682245\n",
      "train loss:0.5029072984332335\n",
      "train loss:0.7904740181635267\n",
      "train loss:0.682982484331579\n",
      "train loss:0.5993212752933641\n",
      "train loss:0.7762074492548572\n",
      "train loss:0.8302726453806359\n",
      "train loss:0.5468167717012586\n",
      "train loss:0.4182034382553347\n",
      "train loss:0.47934748286858275\n",
      "train loss:0.4807581394854606\n",
      "train loss:0.6665707802377334\n",
      "train loss:0.6783188543910664\n",
      "train loss:0.5506202860472321\n",
      "train loss:0.5484504955930573\n",
      "train loss:0.4633282763868435\n",
      "train loss:0.7085607645197392\n",
      "train loss:0.5492321753099426\n",
      "train loss:0.5230838774199545\n",
      "train loss:0.7150338075344634\n",
      "train loss:0.7918678916716155\n",
      "train loss:0.6295089312727287\n",
      "train loss:0.6066087440530377\n",
      "train loss:0.4219335039835196\n",
      "train loss:0.5360646303828125\n",
      "train loss:0.9018202615951421\n",
      "train loss:0.6934269034298348\n",
      "train loss:0.7753486403936762\n",
      "train loss:0.4421564445265692\n",
      "train loss:0.538827721506677\n",
      "train loss:0.5018899366928382\n",
      "train loss:0.6860407149083543\n",
      "train loss:0.43451924193876224\n",
      "train loss:0.6149842814132029\n",
      "train loss:0.8847051296930015\n",
      "train loss:0.6943220930192305\n",
      "train loss:0.7002323370570502\n",
      "train loss:0.5342038600831504\n",
      "train loss:0.6936412063850443\n",
      "train loss:0.7557634332458054\n",
      "train loss:0.6972093175376319\n",
      "train loss:0.5435633029420744\n",
      "train loss:0.6719533246380471\n",
      "train loss:0.7404530354133645\n",
      "train loss:0.6213275155324798\n",
      "train loss:0.6790505189912746\n",
      "train loss:0.5074981325695023\n",
      "train loss:0.7261718902944575\n",
      "train loss:0.6822108557998503\n",
      "train loss:0.6161454010836434\n",
      "train loss:0.6799870406610362\n",
      "train loss:0.774446770472889\n",
      "train loss:0.7297655199758338\n",
      "train loss:0.580272551862276\n",
      "train loss:0.5883628180420579\n",
      "train loss:0.6409477166942725\n",
      "train loss:0.5358830232924663\n",
      "train loss:0.7669341186139902\n",
      "train loss:0.6737382455822973\n",
      "train loss:0.5847767644580759\n",
      "train loss:0.5773620674785966\n",
      "train loss:0.6748143603635908\n",
      "train loss:0.6697874382351661\n",
      "train loss:0.7302103590747573\n",
      "train loss:0.6288294367658391\n",
      "train loss:0.558145080718509\n",
      "train loss:0.6190991263133727\n",
      "train loss:0.5453384722267434\n",
      "train loss:0.6874207668341414\n",
      "train loss:0.6876158448106098\n",
      "train loss:0.5381098416649226\n",
      "train loss:0.7579154102163157\n",
      "train loss:0.4436149967961499\n",
      "train loss:0.5408331486583462\n",
      "train loss:0.5292990851421426\n",
      "train loss:0.61834847555242\n",
      "train loss:0.6177057181459112\n",
      "train loss:0.5959119210914409\n",
      "train loss:0.5053671615267257\n",
      "train loss:0.6123383101286328\n",
      "train loss:0.619455328825086\n",
      "train loss:0.8455583349617235\n",
      "train loss:0.6055368193693509\n",
      "train loss:0.5265186589815851\n",
      "train loss:0.5220469224561964\n",
      "train loss:0.7246570645764598\n",
      "train loss:0.7042225422790992\n",
      "train loss:0.6236088881988296\n",
      "train loss:0.7097543930426085\n",
      "train loss:0.606639531088747\n",
      "train loss:0.5275123791117716\n",
      "train loss:0.7736148835805109\n",
      "train loss:0.7553781934521102\n",
      "train loss:0.6972838813270352\n",
      "train loss:0.6927398906659233\n",
      "train loss:0.6821200300499931\n",
      "train loss:0.6766687870798297\n",
      "train loss:0.6810649589523659\n",
      "train loss:0.63211444176384\n",
      "train loss:0.7643277665143959\n",
      "train loss:0.7522345139114066\n",
      "train loss:0.5980852876908649\n",
      "train loss:0.619403903149329\n",
      "train loss:0.6774619074418962\n",
      "train loss:0.6772371372548471\n",
      "train loss:0.5883891179664441\n",
      "train loss:0.7061019999098666\n",
      "train loss:0.7103726000782323\n",
      "train loss:0.5721084917961392\n",
      "train loss:0.668955842517389\n",
      "train loss:0.6750464308302595\n",
      "train loss:0.5917502240893718\n",
      "train loss:0.6305659856015222\n",
      "train loss:0.6725579292514265\n",
      "train loss:0.6792584170838867\n",
      "train loss:0.6161707434416639\n",
      "train loss:0.770094305451287\n",
      "train loss:0.6287972586245407\n",
      "train loss:0.6813623745130639\n",
      "train loss:0.6730999494799775\n",
      "train loss:0.43688977423512443\n",
      "train loss:0.48551325288229447\n",
      "train loss:0.5402855491930467\n",
      "train loss:0.679486186822637\n",
      "train loss:0.7691053937432306\n",
      "train loss:0.5473403498813093\n",
      "train loss:0.4297428136030426\n",
      "train loss:0.6158022987841971\n",
      "train loss:0.42799205865912116\n",
      "train loss:0.620167139598957\n",
      "train loss:0.727291481779923\n",
      "train loss:0.7312699849403007\n",
      "train loss:0.29822376429932984\n",
      "train loss:0.6113522512212904\n",
      "train loss:0.4983091705790228\n",
      "train loss:0.6484536839483549\n",
      "train loss:0.5972639990799158\n",
      "train loss:0.6195552402944016\n",
      "train loss:0.8700237378337341\n",
      "train loss:0.5149008160333899\n",
      "train loss:0.3871151421050759\n",
      "train loss:0.3796343916491162\n",
      "train loss:0.6025779204024848\n",
      "train loss:0.8830805072947768\n",
      "train loss:0.7185900807518507\n",
      "train loss:0.39963904161229064\n",
      "train loss:0.3889204284409378\n",
      "train loss:0.6202966882154097\n",
      "=== epoch:8, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5247658807725453\n",
      "train loss:0.5014228976386133\n",
      "train loss:0.3959077671170672\n",
      "train loss:0.6163259300853012\n",
      "train loss:0.5298390674539925\n",
      "train loss:0.5996435290248806\n",
      "train loss:0.8413440378800173\n",
      "train loss:0.6164780805521295\n",
      "train loss:0.42491733512020546\n",
      "train loss:0.6187115249811811\n",
      "train loss:0.5037564402935181\n",
      "train loss:0.7332863616524067\n",
      "train loss:1.0042620932773423\n",
      "train loss:0.5153631022408418\n",
      "train loss:0.5090551216995187\n",
      "train loss:0.6217036021398071\n",
      "train loss:0.36000801273110017\n",
      "train loss:0.43114251195548353\n",
      "train loss:0.5124895496759994\n",
      "train loss:0.6002422991695588\n",
      "train loss:0.43394041887220264\n",
      "train loss:0.7910651147740295\n",
      "train loss:0.4236238689709402\n",
      "train loss:0.41035653405578715\n",
      "train loss:0.8221781505055589\n",
      "train loss:0.6164072147750049\n",
      "train loss:0.7061653772104683\n",
      "train loss:0.6152768433136621\n",
      "train loss:0.829278524275907\n",
      "train loss:0.4163996236544968\n",
      "train loss:0.593447088558509\n",
      "train loss:0.8950654983592375\n",
      "train loss:0.7094758474680674\n",
      "train loss:0.8052844380475251\n",
      "train loss:0.6859986814567585\n",
      "train loss:0.6144798038626434\n",
      "train loss:0.6198216438330357\n",
      "train loss:0.5456768809883764\n",
      "train loss:0.5596978450420906\n",
      "train loss:0.5525235355960977\n",
      "train loss:0.672845180980368\n",
      "train loss:0.6744888948851413\n",
      "train loss:0.4928584320550762\n",
      "train loss:0.6158183118696801\n",
      "train loss:0.5477320075739271\n",
      "train loss:0.6152238516951283\n",
      "train loss:0.62123898050116\n",
      "train loss:0.7623291209351581\n",
      "train loss:0.6350667729623327\n",
      "train loss:0.4626612126121453\n",
      "train loss:0.46388554018501366\n",
      "train loss:0.7683323847380659\n",
      "train loss:0.6052167099056971\n",
      "train loss:0.35379897247806275\n",
      "train loss:0.7042102322200819\n",
      "train loss:0.7119432759559334\n",
      "train loss:0.6247652371993089\n",
      "train loss:0.50608744241028\n",
      "train loss:0.6954508209447415\n",
      "train loss:0.6145746955797221\n",
      "train loss:0.41085757608574786\n",
      "train loss:0.7364638856662001\n",
      "train loss:0.31511434930064314\n",
      "train loss:0.37965298951338766\n",
      "train loss:0.4967821596010881\n",
      "train loss:0.6134454112968604\n",
      "train loss:0.3855309331044128\n",
      "train loss:0.6384380559801504\n",
      "train loss:0.619906951224586\n",
      "train loss:0.6109992971028574\n",
      "train loss:0.7381336093246273\n",
      "train loss:0.6357291407574135\n",
      "train loss:0.8466676891728202\n",
      "train loss:0.6102910968469913\n",
      "train loss:0.6165912003051128\n",
      "train loss:0.4980554494016311\n",
      "train loss:0.6240688342187088\n",
      "train loss:0.6999888455648329\n",
      "train loss:0.7115994685249062\n",
      "train loss:0.631987731072295\n",
      "train loss:0.712231109287168\n",
      "train loss:0.6124314519135394\n",
      "train loss:0.5314218333132225\n",
      "train loss:0.4593063855933643\n",
      "train loss:0.7663361494137295\n",
      "train loss:0.7593105642257651\n",
      "train loss:0.5528161237791753\n",
      "train loss:0.4771656843937886\n",
      "train loss:0.6975906427768382\n",
      "train loss:0.6129505901936306\n",
      "train loss:0.6301034135458605\n",
      "train loss:0.6778593549265488\n",
      "train loss:0.7413795495113702\n",
      "train loss:0.5463355131348633\n",
      "train loss:0.8006369216987126\n",
      "train loss:0.5584301422372897\n",
      "train loss:0.7359977037387921\n",
      "train loss:0.6277091996457171\n",
      "train loss:0.6158807155190303\n",
      "train loss:0.6713795736750251\n",
      "train loss:0.622943047652352\n",
      "train loss:0.7347058395161319\n",
      "train loss:0.7228065010034481\n",
      "train loss:0.582871123768397\n",
      "train loss:0.5820936692160072\n",
      "train loss:0.6571624505332656\n",
      "train loss:0.6210210692983638\n",
      "train loss:0.6244964120356535\n",
      "train loss:0.6244901614009328\n",
      "train loss:0.5598897642949445\n",
      "train loss:0.7458127903238417\n",
      "train loss:0.6246590837609748\n",
      "train loss:0.6100867520027815\n",
      "train loss:0.552279489919614\n",
      "train loss:0.6144510249566772\n",
      "train loss:0.5441540170163607\n",
      "train loss:0.6141314551722166\n",
      "train loss:0.6976899366580274\n",
      "train loss:0.5212013154679599\n",
      "train loss:0.7056696220028004\n",
      "train loss:0.7759521964473622\n",
      "train loss:0.6869543913859874\n",
      "train loss:0.7239291272924648\n",
      "train loss:0.6995087457027114\n",
      "train loss:0.6881975150905794\n",
      "train loss:0.6067924169027272\n",
      "train loss:0.6047765413944983\n",
      "train loss:0.5499332533650614\n",
      "train loss:0.5530841908228017\n",
      "train loss:0.5390203654666288\n",
      "train loss:0.6150592457546827\n",
      "train loss:0.8353361074397248\n",
      "train loss:0.538305926451881\n",
      "train loss:0.6014899759413467\n",
      "train loss:0.6208075826062986\n",
      "train loss:0.45007361639772786\n",
      "train loss:0.614918846845754\n",
      "train loss:0.6091642024509341\n",
      "train loss:0.3524361657112555\n",
      "train loss:0.711604252169872\n",
      "train loss:0.7699841339511692\n",
      "train loss:0.5893676831853379\n",
      "train loss:0.6021937389122329\n",
      "train loss:0.616024475620208\n",
      "train loss:0.5467192300937209\n",
      "train loss:0.4171263604220842\n",
      "train loss:0.5135534726765133\n",
      "train loss:0.4906913576502224\n",
      "train loss:0.5061486603088756\n",
      "train loss:0.40774975122929275\n",
      "train loss:0.3966187555203513\n",
      "train loss:0.24157742949496647\n",
      "train loss:0.4917084907323658\n",
      "=== epoch:9, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.9211556581429111\n",
      "train loss:0.35465352429857694\n",
      "train loss:0.4983210447923729\n",
      "train loss:0.6628057455714444\n",
      "train loss:0.6333364943376267\n",
      "train loss:0.3609087175636413\n",
      "train loss:0.6423930780476683\n",
      "train loss:0.6325008823476272\n",
      "train loss:0.7655393880775259\n",
      "train loss:0.619705302010034\n",
      "train loss:0.7685234464389925\n",
      "train loss:0.47793564037532965\n",
      "train loss:0.8547505772168902\n",
      "train loss:0.4941994455801302\n",
      "train loss:0.41008903045557776\n",
      "train loss:0.5039310268158832\n",
      "train loss:0.5052352386515957\n",
      "train loss:0.30615603393040536\n",
      "train loss:0.6011975944344823\n",
      "train loss:0.6221997057469625\n",
      "train loss:0.41206438216682945\n",
      "train loss:0.7278928420347381\n",
      "train loss:0.5231042363229\n",
      "train loss:0.6155979104827353\n",
      "train loss:0.7498289128220683\n",
      "train loss:0.9078631330245678\n",
      "train loss:0.4146388312367565\n",
      "train loss:0.3144147179423379\n",
      "train loss:0.5890882452392907\n",
      "train loss:0.5890660908005662\n",
      "train loss:0.4386298031281715\n",
      "train loss:0.7172303708765011\n",
      "train loss:0.5111801656421208\n",
      "train loss:0.41091125657132704\n",
      "train loss:0.6133567617894183\n",
      "train loss:0.4170666607697665\n",
      "train loss:0.5073325081054615\n",
      "train loss:0.7491487297784258\n",
      "train loss:0.8164976999539484\n",
      "train loss:0.5021719181732862\n",
      "train loss:0.41324842448210486\n",
      "train loss:0.6285902947658938\n",
      "train loss:0.6071929177736457\n",
      "train loss:0.7115009870066842\n",
      "train loss:0.609875011910652\n",
      "train loss:0.706783318245455\n",
      "train loss:0.5531893485803184\n",
      "train loss:0.5235742629093492\n",
      "train loss:0.3050283749313801\n",
      "train loss:0.5043118895931704\n",
      "train loss:0.5085549527706059\n",
      "train loss:0.9317320443666022\n",
      "train loss:0.524874165172956\n",
      "train loss:0.4935984870968985\n",
      "train loss:0.3912022567392963\n",
      "train loss:0.4139955476944938\n",
      "train loss:0.6095611614299246\n",
      "train loss:0.7127054847369549\n",
      "train loss:0.631044796671257\n",
      "train loss:0.6216658130274116\n",
      "train loss:0.6131648646428395\n",
      "train loss:0.9878559561599664\n",
      "train loss:0.41714788450321283\n",
      "train loss:0.40348305878958834\n",
      "train loss:0.49826951459020696\n",
      "train loss:0.7310732421105687\n",
      "train loss:0.609400934647598\n",
      "train loss:0.6191888024104679\n",
      "train loss:0.692428844365444\n",
      "train loss:0.692638695102261\n",
      "train loss:0.6264266047777381\n",
      "train loss:1.0207258276794187\n",
      "train loss:0.3931457249958332\n",
      "train loss:0.6969734048853875\n",
      "train loss:0.8130103372236437\n",
      "train loss:0.7388342043364137\n",
      "train loss:0.6213141948615524\n",
      "train loss:0.6230559982977915\n",
      "train loss:0.6404648609203004\n",
      "train loss:0.5862227960436246\n",
      "train loss:0.6225834441677277\n",
      "train loss:0.5354517860037696\n",
      "train loss:0.5793238881951182\n",
      "train loss:0.6706387412745631\n",
      "train loss:0.7759747921579342\n",
      "train loss:0.621608303057805\n",
      "train loss:0.5625648487100074\n",
      "train loss:0.6159694283995358\n",
      "train loss:0.6221359788260996\n",
      "train loss:0.49936971851414125\n",
      "train loss:0.8705465448940573\n",
      "train loss:0.6246944251409834\n",
      "train loss:0.7485261868598757\n",
      "train loss:0.4149228382239767\n",
      "train loss:0.6203260837698851\n",
      "train loss:0.47051150078121734\n",
      "train loss:0.5396483908839338\n",
      "train loss:0.5419713744714821\n",
      "train loss:0.6177919631991265\n",
      "train loss:0.7116813375758106\n",
      "train loss:0.43021714999994626\n",
      "train loss:0.6120135143138331\n",
      "train loss:0.4036247296435196\n",
      "train loss:0.6163578260999519\n",
      "train loss:0.6661832511874655\n",
      "train loss:0.5307956264133191\n",
      "train loss:0.613539150840007\n",
      "train loss:0.6287770194650186\n",
      "train loss:0.844650092233244\n",
      "train loss:0.7243713810830888\n",
      "train loss:0.8447071858863971\n",
      "train loss:0.7026164524267471\n",
      "train loss:0.7108545396949303\n",
      "train loss:0.5335389958943859\n",
      "train loss:0.5950187624492213\n",
      "train loss:0.5993251403724412\n",
      "train loss:0.7751399177852868\n",
      "train loss:0.47199762660758326\n",
      "train loss:0.6070709265515118\n",
      "train loss:0.6907328215856351\n",
      "train loss:0.8097446691911303\n",
      "train loss:0.7521446146211206\n",
      "train loss:0.5073586492006713\n",
      "train loss:0.5635475008379877\n",
      "train loss:0.5722691962139806\n",
      "train loss:0.6262768660340243\n",
      "train loss:0.4328795108457341\n",
      "train loss:0.7294738523650274\n",
      "train loss:0.5507757985900454\n",
      "train loss:0.7385059983659256\n",
      "train loss:0.548620164893267\n",
      "train loss:0.6137286682726548\n",
      "train loss:0.5389475165394564\n",
      "train loss:0.6197132109189532\n",
      "train loss:0.6841029081123563\n",
      "train loss:0.7648505956529008\n",
      "train loss:0.613030249331584\n",
      "train loss:0.6812543104929443\n",
      "train loss:0.5395065993147734\n",
      "train loss:0.4480712802564225\n",
      "train loss:0.6091273546751823\n",
      "train loss:0.6856416353701016\n",
      "train loss:0.6146886765511963\n",
      "train loss:0.6988936239316786\n",
      "train loss:0.7918357931389076\n",
      "train loss:0.697715881617227\n",
      "train loss:0.36165316152258536\n",
      "train loss:0.6917593030468285\n",
      "train loss:0.5261485005411566\n",
      "train loss:0.5338809646684675\n",
      "train loss:0.422882529790581\n",
      "train loss:0.4266663830740803\n",
      "train loss:0.7962847677569792\n",
      "=== epoch:10, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7173900693849785\n",
      "train loss:0.6287407592406156\n",
      "train loss:0.6052003188438\n",
      "train loss:0.6926060529127447\n",
      "train loss:0.5131739395875368\n",
      "train loss:0.7093600466477894\n",
      "train loss:0.6437481727993287\n",
      "train loss:0.7130831524434738\n",
      "train loss:0.8878320029386005\n",
      "train loss:0.6080589415234582\n",
      "train loss:0.5425162081574247\n",
      "train loss:0.5370857260603941\n",
      "train loss:0.6957115514726222\n",
      "train loss:0.5341541052553515\n",
      "train loss:0.6102304775342778\n",
      "train loss:0.6085654242772408\n",
      "train loss:0.46101537029825596\n",
      "train loss:0.6065098988928717\n",
      "train loss:0.6018852546804568\n",
      "train loss:0.6113433713381738\n",
      "train loss:0.77356477173647\n",
      "train loss:0.670107513647726\n",
      "train loss:0.8566826999057007\n",
      "train loss:0.5425566011375946\n",
      "train loss:0.4749421356066824\n",
      "train loss:0.5301109403037171\n",
      "train loss:0.6988964266324527\n",
      "train loss:0.5122335188994839\n",
      "train loss:0.755182672620082\n",
      "train loss:0.5266878963133352\n",
      "train loss:0.3685266953290899\n",
      "train loss:0.7781104608918742\n",
      "train loss:0.7794450251198013\n",
      "train loss:0.5289936898807602\n",
      "train loss:0.3623934445559026\n",
      "train loss:0.5315075066779386\n",
      "train loss:0.42591750597140443\n",
      "train loss:0.611977506684711\n",
      "train loss:0.500314266309236\n",
      "train loss:0.6071182855397657\n",
      "train loss:0.6080036084086429\n",
      "train loss:0.6362728353084369\n",
      "train loss:0.602962877489272\n",
      "train loss:0.5066887363072136\n",
      "train loss:0.5024349084036246\n",
      "train loss:0.49602226176457975\n",
      "train loss:0.7348840931360463\n",
      "train loss:0.4980691991168113\n",
      "train loss:0.3847725326750102\n",
      "train loss:0.503561533437936\n",
      "train loss:0.5105489713367606\n",
      "train loss:0.650666113858455\n",
      "train loss:0.6374078625882924\n",
      "train loss:0.7579198363347259\n",
      "train loss:0.37403252512995866\n",
      "train loss:0.8919104148848194\n",
      "train loss:0.6231522179857698\n",
      "train loss:0.49852620299678635\n",
      "train loss:0.6037710687372055\n",
      "train loss:0.39109592746432364\n",
      "train loss:0.6144723792117053\n",
      "train loss:0.8224619525145214\n",
      "train loss:0.6114169268669695\n",
      "train loss:0.8096112708128291\n",
      "train loss:0.6130808845887239\n",
      "train loss:0.7848714806521299\n",
      "train loss:0.6946149855999254\n",
      "train loss:0.6138778133320095\n",
      "train loss:0.6839020296011099\n",
      "train loss:0.5516435332218628\n",
      "train loss:0.5649002837446874\n",
      "train loss:0.6742118066566274\n",
      "train loss:0.6216616772016963\n",
      "train loss:0.6779751533194622\n",
      "train loss:0.68091190249522\n",
      "train loss:0.6386682781176767\n",
      "train loss:0.5746870010562215\n",
      "train loss:0.6265767202666838\n",
      "train loss:0.5772368427668149\n",
      "train loss:0.6250237625184958\n",
      "train loss:0.5659227359813488\n",
      "train loss:0.6233076253246459\n",
      "train loss:0.8032336953658297\n",
      "train loss:0.6191848822212508\n",
      "train loss:0.6245737467708544\n",
      "train loss:0.5438104163278027\n",
      "train loss:0.48011766059914346\n",
      "train loss:0.6840400433998765\n",
      "train loss:0.4600417349332714\n",
      "train loss:0.778220655984627\n",
      "train loss:0.5997186251668418\n",
      "train loss:0.43370737917728225\n",
      "train loss:0.6128490170271008\n",
      "train loss:0.7099207700003171\n",
      "train loss:0.4348240721559945\n",
      "train loss:0.5096740156661748\n",
      "train loss:0.5111952677068755\n",
      "train loss:0.7184806646038311\n",
      "train loss:0.5148092173215367\n",
      "train loss:0.2776203417356967\n",
      "train loss:0.49696268380712294\n",
      "train loss:0.8388238130897196\n",
      "train loss:0.6271829861134849\n",
      "train loss:0.757837383497941\n",
      "train loss:0.3817765451338426\n",
      "train loss:0.5015356798872477\n",
      "train loss:0.751558743576293\n",
      "train loss:0.525231103393545\n",
      "train loss:0.8567752619470262\n",
      "train loss:0.5063238910915865\n",
      "train loss:0.41930745449815143\n",
      "train loss:0.2580451629371312\n",
      "train loss:0.3786736947916533\n",
      "train loss:0.6153516620624997\n",
      "train loss:0.7599121295281769\n",
      "train loss:0.37803936491153584\n",
      "train loss:0.527957275716165\n",
      "train loss:0.8797781962230091\n",
      "train loss:0.6222636403540305\n",
      "train loss:0.6205790181859078\n",
      "train loss:0.5062632309338666\n",
      "train loss:0.5165090953694907\n",
      "train loss:0.39681007183806594\n",
      "train loss:0.6046669678578014\n",
      "train loss:0.628913659360221\n",
      "train loss:0.496129202128504\n",
      "train loss:0.8448461400681996\n",
      "train loss:0.6064635525938296\n",
      "train loss:0.5212460962170578\n",
      "train loss:0.30601487811567457\n",
      "train loss:0.4110464707569189\n",
      "train loss:0.6290261063695755\n",
      "train loss:0.5039393521589609\n",
      "train loss:0.39978199440040785\n",
      "train loss:0.6176151029947043\n",
      "train loss:0.5203971506002976\n",
      "train loss:0.8490251767220908\n",
      "train loss:0.712753728488483\n",
      "train loss:0.5072974573955484\n",
      "train loss:0.6092402057976889\n",
      "train loss:0.9496527604128016\n",
      "train loss:0.8265517172561371\n",
      "train loss:0.5111116885587588\n",
      "train loss:0.5186256801513449\n",
      "train loss:0.6184596180567715\n",
      "train loss:0.6045335441831708\n",
      "train loss:0.523552320588357\n",
      "train loss:0.6734768234574363\n",
      "train loss:0.5399520874189888\n",
      "train loss:0.6161340432618643\n",
      "train loss:0.9040401184578508\n",
      "train loss:0.6110263077905363\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5490196078431373\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=10, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8656edc-6088-4ed6-a5c6-c34e99d748da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3003261250057094\n",
      "=== epoch:1, train acc:0.73, test acc:0.69 ===\n",
      "train loss:2.296851492381972\n",
      "train loss:2.2922660548594704\n",
      "train loss:2.28514750307751\n",
      "train loss:2.272854429852919\n",
      "train loss:2.261872028297387\n",
      "train loss:2.2381201860297693\n",
      "train loss:2.2142352473977533\n",
      "train loss:2.17211477345898\n",
      "train loss:2.137161654434684\n",
      "train loss:2.05971383397923\n",
      "train loss:2.0077898074747584\n",
      "train loss:1.9473260169696895\n",
      "train loss:1.8245003286709238\n",
      "train loss:1.6998798842366643\n",
      "train loss:1.5987416592122032\n",
      "train loss:1.449021986153584\n",
      "train loss:1.324715003047123\n",
      "train loss:1.1805909117231994\n",
      "train loss:1.0181684675107978\n",
      "train loss:0.9201447032919958\n",
      "train loss:0.7931227792583831\n",
      "train loss:0.6707622494440384\n",
      "train loss:0.6706803315823174\n",
      "train loss:0.4135033690454886\n",
      "train loss:0.39740560701383426\n",
      "train loss:0.1688032173887566\n",
      "train loss:0.778656005311898\n",
      "train loss:1.4902918304188222\n",
      "train loss:0.9867459374620833\n",
      "train loss:0.36676868852148853\n",
      "train loss:0.3365539044252469\n",
      "train loss:0.9150299061363288\n",
      "train loss:0.7193565094217421\n",
      "train loss:0.6366768729195955\n",
      "train loss:1.1698591549323676\n",
      "train loss:0.7432988025624014\n",
      "train loss:0.7775155521365769\n",
      "train loss:0.4098812018094272\n",
      "train loss:0.6899551501830891\n",
      "train loss:0.644119407608531\n",
      "train loss:0.696466326053739\n",
      "train loss:0.6681968499014295\n",
      "train loss:0.6908734196234306\n",
      "train loss:0.6815841631489719\n",
      "train loss:0.6944084124566275\n",
      "train loss:0.6877655197828508\n",
      "train loss:0.6932474732014436\n",
      "train loss:0.6422261409585353\n",
      "train loss:0.5494779423869846\n",
      "train loss:0.5731325268647752\n",
      "train loss:0.5476833548510516\n",
      "train loss:0.7747357931252876\n",
      "train loss:0.626970179142356\n",
      "train loss:0.7331628776725589\n",
      "train loss:0.49623742018628264\n",
      "train loss:0.7561744794292695\n",
      "train loss:0.6235708817911213\n",
      "train loss:0.6454811183817354\n",
      "train loss:0.5948280723202779\n",
      "train loss:0.4107975578531435\n",
      "train loss:0.6289399382327565\n",
      "train loss:0.7072121063879919\n",
      "train loss:0.6325348455282758\n",
      "train loss:0.26601316880091275\n",
      "train loss:0.3665558374570904\n",
      "train loss:0.7785931174205463\n",
      "train loss:0.7633120425837628\n",
      "train loss:0.5848374471430577\n",
      "train loss:0.7044967909069413\n",
      "train loss:0.5156195477058999\n",
      "train loss:0.41423184011599207\n",
      "train loss:0.48992767260250736\n",
      "train loss:0.6061964712431898\n",
      "train loss:0.4177587589095603\n",
      "train loss:0.789049133634349\n",
      "train loss:0.7858073801031021\n",
      "train loss:0.6039394940130685\n",
      "train loss:0.7469770234355707\n",
      "train loss:0.6164727592521337\n",
      "train loss:0.689230017706575\n",
      "train loss:0.6151757175923415\n",
      "train loss:0.5686704475162108\n",
      "train loss:0.43090656706344477\n",
      "train loss:0.6240394568801786\n",
      "train loss:0.6821862210643271\n",
      "train loss:0.630827245660169\n",
      "train loss:0.8672468674260472\n",
      "train loss:0.731469082456816\n",
      "train loss:0.6219106152480225\n",
      "train loss:0.556484150639607\n",
      "train loss:0.5690578688916975\n",
      "train loss:0.5566543432334992\n",
      "train loss:0.6196448506463276\n",
      "train loss:0.6257226702116722\n",
      "train loss:0.6760662608369292\n",
      "train loss:0.8933122918021162\n",
      "train loss:0.8054000967130712\n",
      "train loss:0.6219349241035315\n",
      "train loss:0.6173255453660864\n",
      "train loss:0.6218924389724809\n",
      "train loss:0.5863485822306658\n",
      "train loss:0.6712649522132963\n",
      "train loss:0.7182813507297509\n",
      "train loss:0.5911798260276335\n",
      "train loss:0.6296327089709977\n",
      "train loss:0.6258569458127504\n",
      "train loss:0.4952660649466222\n",
      "train loss:0.5334197084194312\n",
      "train loss:0.7082615376954439\n",
      "train loss:0.6606194978437789\n",
      "train loss:0.41857467246842894\n",
      "train loss:0.602226218222062\n",
      "train loss:0.5018675363738008\n",
      "train loss:0.7335375619010949\n",
      "train loss:0.5930477142877901\n",
      "train loss:0.6040649531816183\n",
      "train loss:0.5194784500400301\n",
      "train loss:0.3409620290160697\n",
      "train loss:0.746438997057526\n",
      "train loss:0.8351064528118581\n",
      "train loss:0.4907704005399712\n",
      "train loss:0.6330600757121776\n",
      "train loss:0.7161293159764334\n",
      "train loss:0.5068120631458883\n",
      "train loss:0.475278266056094\n",
      "train loss:0.633526085625245\n",
      "train loss:0.5107640083659595\n",
      "train loss:0.5227999501069975\n",
      "train loss:0.7752439724870965\n",
      "train loss:0.5181033176563472\n",
      "train loss:0.5311100121378299\n",
      "train loss:0.4305450915749217\n",
      "train loss:0.600380781444436\n",
      "train loss:0.6171917836571159\n",
      "train loss:0.5896969084002709\n",
      "train loss:0.7155759268619232\n",
      "train loss:0.7143767597460946\n",
      "train loss:0.45323644396062435\n",
      "train loss:0.5207466301549761\n",
      "train loss:0.5995951737371129\n",
      "train loss:0.5417475693288616\n",
      "train loss:0.3860441097670829\n",
      "train loss:0.5766382829006572\n",
      "train loss:0.41696582102953766\n",
      "train loss:0.5332179230023718\n",
      "train loss:0.782274627422349\n",
      "train loss:0.7798871897319424\n",
      "train loss:0.9149869941358227\n",
      "train loss:0.7369365212015676\n",
      "train loss:0.6800410328038937\n",
      "train loss:0.5402581872192187\n",
      "train loss:0.6207566604713458\n",
      "train loss:0.663916464535214\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.572916886035227\n",
      "train loss:0.6930843139376446\n",
      "train loss:0.6300788640958275\n",
      "train loss:0.6199408624900136\n",
      "train loss:0.7299026315704699\n",
      "train loss:0.6382469855921371\n",
      "train loss:0.6710240102109404\n",
      "train loss:0.6721357731659785\n",
      "train loss:0.5924758456161916\n",
      "train loss:0.6315460095852281\n",
      "train loss:0.5702171401669053\n",
      "train loss:0.6287191863731685\n",
      "train loss:0.6265534874519342\n",
      "train loss:0.7046493042496327\n",
      "train loss:0.616171979465036\n",
      "train loss:0.5189238373353031\n",
      "train loss:0.5016562599225917\n",
      "train loss:0.6361822891072004\n",
      "train loss:0.5270095976584944\n",
      "train loss:0.8132302132837619\n",
      "train loss:0.9464695127525967\n",
      "train loss:0.5418936810803437\n",
      "train loss:0.5934748455228561\n",
      "train loss:0.6992457374057366\n",
      "train loss:0.5974704927586447\n",
      "train loss:0.6163359456328844\n",
      "train loss:0.5412601267339131\n",
      "train loss:0.38474915780329455\n",
      "train loss:0.6117251341279213\n",
      "train loss:0.6195056501228271\n",
      "train loss:0.4210094763880979\n",
      "train loss:0.7746850902655665\n",
      "train loss:0.7084496301233714\n",
      "train loss:0.6226309801877652\n",
      "train loss:0.6056436225785371\n",
      "train loss:0.8822418098321613\n",
      "train loss:0.6258760187055895\n",
      "train loss:0.5577133907882216\n",
      "train loss:0.5609189198777232\n",
      "train loss:0.6854987651735813\n",
      "train loss:0.5250982230021877\n",
      "train loss:0.39481424210767696\n",
      "train loss:0.6092677356574262\n",
      "train loss:0.4377231475008916\n",
      "train loss:0.5396206207348495\n",
      "train loss:0.7052610898244731\n",
      "train loss:1.0111730524095104\n",
      "train loss:0.623776733834758\n",
      "train loss:0.5381172898413674\n",
      "train loss:0.40115827613863814\n",
      "train loss:0.9873168949894836\n",
      "train loss:0.6842984201513018\n",
      "train loss:0.6722956379068303\n",
      "train loss:0.6191437493084373\n",
      "train loss:0.6880775576195262\n",
      "train loss:0.6612350536640307\n",
      "train loss:0.6173131868702126\n",
      "train loss:0.5016293814525945\n",
      "train loss:0.5684732283515764\n",
      "train loss:0.4894557996313423\n",
      "train loss:0.617759039328307\n",
      "train loss:0.5285069027610131\n",
      "train loss:0.606558743759253\n",
      "train loss:0.528493957332207\n",
      "train loss:0.6095771107237028\n",
      "train loss:0.8102116753849069\n",
      "train loss:0.8114901173049706\n",
      "train loss:0.39008316064454734\n",
      "train loss:0.7315801296094101\n",
      "train loss:0.6082566927723461\n",
      "train loss:0.5158679715795638\n",
      "train loss:0.5957962985382808\n",
      "train loss:0.592492752317008\n",
      "train loss:0.5256402144425938\n",
      "train loss:0.6104489535179414\n",
      "train loss:0.28040220921864656\n",
      "train loss:0.6317066611982335\n",
      "train loss:0.6124685529760814\n",
      "train loss:0.624931760387472\n",
      "train loss:0.4009127309559439\n",
      "train loss:0.7508764438649698\n",
      "train loss:0.5037034318202063\n",
      "train loss:0.6409001487955116\n",
      "train loss:0.832051475853884\n",
      "train loss:0.6160620806011174\n",
      "train loss:0.60323882203568\n",
      "train loss:0.7126827003733961\n",
      "train loss:0.5951000541375346\n",
      "train loss:0.545209842737848\n",
      "train loss:0.6146211904386346\n",
      "train loss:0.6415819179522005\n",
      "train loss:0.5460411275459902\n",
      "train loss:0.5989315158610756\n",
      "train loss:0.5646557322943728\n",
      "train loss:0.7634922563128402\n",
      "train loss:0.5537034297063491\n",
      "train loss:0.6812751205214922\n",
      "train loss:0.47405958141097343\n",
      "train loss:0.6085901973723351\n",
      "train loss:0.6944244891666151\n",
      "train loss:0.34458740989724124\n",
      "train loss:0.6108500186635715\n",
      "train loss:0.8453357362759835\n",
      "train loss:0.6165809353379446\n",
      "train loss:0.4037462148618462\n",
      "train loss:0.9117038347854723\n",
      "train loss:0.6446927912923112\n",
      "train loss:0.7003433653363943\n",
      "train loss:0.584081011896842\n",
      "train loss:0.6894354300103874\n",
      "train loss:0.6380006566272182\n",
      "train loss:0.6159871734943587\n",
      "train loss:0.540641897983283\n",
      "train loss:0.6250451381932578\n",
      "train loss:0.7976004265740133\n",
      "train loss:0.7204909523971673\n",
      "train loss:0.636606807400195\n",
      "train loss:0.6796502423147512\n",
      "train loss:0.5817868397493431\n",
      "train loss:0.675459090647766\n",
      "train loss:0.6674536420911298\n",
      "train loss:0.527250352899467\n",
      "train loss:0.6771359397015073\n",
      "train loss:0.6270502313542583\n",
      "train loss:0.5552442505753328\n",
      "train loss:0.6793121645815969\n",
      "train loss:0.7223198076310442\n",
      "train loss:0.6737865339434869\n",
      "train loss:0.5410145774762762\n",
      "train loss:0.5443090025253106\n",
      "train loss:0.6180340229956705\n",
      "train loss:0.51082653452084\n",
      "train loss:0.4133472304914708\n",
      "train loss:0.5129873601635119\n",
      "train loss:0.6633414147385549\n",
      "train loss:0.7679345766887722\n",
      "train loss:0.4913582454517117\n",
      "train loss:0.7522166184661697\n",
      "train loss:0.6224384548718855\n",
      "train loss:0.62995203250502\n",
      "train loss:0.6042047092979155\n",
      "train loss:0.7486767146344746\n",
      "train loss:0.9714890511997396\n",
      "train loss:0.5301440321101942\n",
      "train loss:0.5309314937154943\n",
      "train loss:0.6248936179845799\n",
      "train loss:0.6102275249152312\n",
      "train loss:0.5567819181307084\n",
      "train loss:0.5002139309712093\n",
      "train loss:0.572547935167152\n",
      "train loss:0.8107125379642618\n",
      "train loss:0.5403835440460976\n",
      "train loss:0.7997873910901282\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.48699012185363866\n",
      "train loss:0.3980921884933909\n",
      "train loss:0.5311902946833401\n",
      "train loss:0.6409202378385352\n",
      "train loss:0.6240667628569623\n",
      "train loss:0.841998544050018\n",
      "train loss:0.6400288582217544\n",
      "train loss:0.3059347892155348\n",
      "train loss:0.5346047791511606\n",
      "train loss:0.27755316163260757\n",
      "train loss:0.7315626012817124\n",
      "train loss:0.6464138881150561\n",
      "train loss:0.8877963582437591\n",
      "train loss:0.23940147570444906\n",
      "train loss:1.2204607340731368\n",
      "train loss:0.5966223260400925\n",
      "train loss:0.7160215374965864\n",
      "train loss:0.6207911562393164\n",
      "train loss:0.8828251738959582\n",
      "train loss:0.7711462456836882\n",
      "train loss:0.5772770842669186\n",
      "train loss:0.6704973680137882\n",
      "train loss:0.703678103088465\n",
      "train loss:0.6647180408326536\n",
      "train loss:0.6906494807650188\n",
      "train loss:0.6888207149900756\n",
      "train loss:0.6899494739230873\n",
      "train loss:0.6803195240994292\n",
      "train loss:0.6947883561983589\n",
      "train loss:0.6766166987164088\n",
      "train loss:0.676160441948541\n",
      "train loss:0.663482280945923\n",
      "train loss:0.7046568439550334\n",
      "train loss:0.5950408228609629\n",
      "train loss:0.6038955840919438\n",
      "train loss:0.6216610299217484\n",
      "train loss:0.5723339320326872\n",
      "train loss:0.718290066337594\n",
      "train loss:0.5477885926852983\n",
      "train loss:0.618210415239318\n",
      "train loss:0.6954649014451422\n",
      "train loss:0.6159831022930067\n",
      "train loss:0.5193989060229962\n",
      "train loss:0.3837854561807445\n",
      "train loss:0.6346033192025551\n",
      "train loss:0.7306404689813666\n",
      "train loss:0.6434452442196736\n",
      "train loss:0.613964880838857\n",
      "train loss:0.36267011791270537\n",
      "train loss:0.8993578774762374\n",
      "train loss:0.34733749242770107\n",
      "train loss:0.625070570443732\n",
      "train loss:0.382585387262124\n",
      "train loss:0.6108301575718514\n",
      "train loss:0.37335013709214754\n",
      "train loss:0.48655103695525864\n",
      "train loss:0.5015014296346539\n",
      "train loss:0.6772174564994443\n",
      "train loss:0.49161059234252696\n",
      "train loss:0.6362555518363783\n",
      "train loss:0.4873375658459058\n",
      "train loss:0.6861711305419285\n",
      "train loss:0.49185933953010375\n",
      "train loss:0.5209900970173095\n",
      "train loss:0.7482687402064784\n",
      "train loss:0.7278896416732218\n",
      "train loss:0.5195922934682841\n",
      "train loss:0.8109119035157221\n",
      "train loss:0.6973610597706437\n",
      "train loss:0.5284628492425409\n",
      "train loss:0.8355354615957952\n",
      "train loss:0.6129742992530542\n",
      "train loss:0.7488382643679572\n",
      "train loss:0.7347297173333212\n",
      "train loss:0.6718741214963299\n",
      "train loss:0.6772954178950565\n",
      "train loss:0.6455784414428806\n",
      "train loss:0.6555439791896819\n",
      "train loss:0.6295251080672377\n",
      "train loss:0.6806649724119469\n",
      "train loss:0.6595345558313277\n",
      "train loss:0.6523715889565367\n",
      "train loss:0.6722923180219935\n",
      "train loss:0.6740015537978131\n",
      "train loss:0.691285650350535\n",
      "train loss:0.5991999173497526\n",
      "train loss:0.6385345937120117\n",
      "train loss:0.7286785941957781\n",
      "train loss:0.6641160641912623\n",
      "train loss:0.626383653649023\n",
      "train loss:0.44728949614098157\n",
      "train loss:0.4719629164424258\n",
      "train loss:0.6727138266814765\n",
      "train loss:0.6180342734809937\n",
      "train loss:0.43194627627485477\n",
      "train loss:0.419475626509717\n",
      "train loss:0.648479580851786\n",
      "train loss:0.7306983812899903\n",
      "train loss:0.6091339707122512\n",
      "train loss:0.786946672865408\n",
      "train loss:0.86453431803159\n",
      "train loss:0.9030942274740982\n",
      "train loss:0.6913960126893471\n",
      "train loss:0.5128857306827407\n",
      "train loss:0.5199469039708788\n",
      "train loss:0.6187622261503644\n",
      "train loss:0.7731179813099166\n",
      "train loss:0.6836691052344996\n",
      "train loss:0.6196780416248974\n",
      "train loss:0.6792602465980531\n",
      "train loss:0.5630782938676012\n",
      "train loss:0.7330405187866897\n",
      "train loss:0.6166844790302762\n",
      "train loss:0.6726299798086832\n",
      "train loss:0.5353788575734132\n",
      "train loss:0.57545458549025\n",
      "train loss:0.5252748426022738\n",
      "train loss:0.5110641371589317\n",
      "train loss:0.38925606479886676\n",
      "train loss:0.429124830463348\n",
      "train loss:0.5227185773763806\n",
      "train loss:0.7321821745376089\n",
      "train loss:0.6337687765871943\n",
      "train loss:0.6389130941653703\n",
      "train loss:0.6222524104631668\n",
      "train loss:0.3802075446050665\n",
      "train loss:0.6372866844213846\n",
      "train loss:0.6713303672519755\n",
      "train loss:0.3660842329751107\n",
      "train loss:0.6441699297942487\n",
      "train loss:0.6637705219933081\n",
      "train loss:0.2191867804900205\n",
      "train loss:0.7999657758572943\n",
      "train loss:0.34073699891759984\n",
      "train loss:0.4895660943680083\n",
      "train loss:0.783813072922365\n",
      "train loss:0.4746010468157721\n",
      "train loss:0.37410474749205896\n",
      "train loss:0.6028886765733799\n",
      "train loss:0.5040321723714887\n",
      "train loss:0.48753173840959907\n",
      "train loss:0.6176690314651567\n",
      "train loss:0.6199114728676316\n",
      "train loss:0.5041053585622515\n",
      "train loss:0.697454538986734\n",
      "train loss:0.5278680831800509\n",
      "train loss:0.4025122090168639\n",
      "train loss:1.129403939594605\n",
      "train loss:0.6949129858628517\n",
      "train loss:0.6097773987420918\n",
      "train loss:0.5392153604674024\n",
      "train loss:0.6235682718229818\n",
      "train loss:0.5282967631589257\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.554317997871351\n",
      "train loss:0.6676531933908258\n",
      "train loss:0.6151548919367633\n",
      "train loss:0.7457958288303039\n",
      "train loss:0.5600022859555743\n",
      "train loss:0.6749903804655334\n",
      "train loss:0.6289775326167254\n",
      "train loss:0.740031361383947\n",
      "train loss:0.49884157138979307\n",
      "train loss:0.5641211553562038\n",
      "train loss:0.6812960513847841\n",
      "train loss:0.6798325820469447\n",
      "train loss:0.5534537318307241\n",
      "train loss:0.6935152491849168\n",
      "train loss:0.6854639093285158\n",
      "train loss:0.46954194531814314\n",
      "train loss:0.532098974750858\n",
      "train loss:0.6448887287615003\n",
      "train loss:0.6117141076210876\n",
      "train loss:0.6080171248941111\n",
      "train loss:0.5980822272666808\n",
      "train loss:0.5988374180247589\n",
      "train loss:0.3100832209529317\n",
      "train loss:0.8136861506481992\n",
      "train loss:0.6249796023560751\n",
      "train loss:0.7224340278453962\n",
      "train loss:0.282996569744867\n",
      "train loss:0.4105890373641462\n",
      "train loss:0.7502243506510871\n",
      "train loss:0.7282528752887717\n",
      "train loss:0.5951140681629481\n",
      "train loss:0.5057514971849565\n",
      "train loss:0.6083186095770916\n",
      "train loss:0.6245931546961145\n",
      "train loss:0.5973627272466587\n",
      "train loss:0.7681760531247962\n",
      "train loss:0.5080552327440965\n",
      "train loss:0.7955904787490297\n",
      "train loss:0.500696849033803\n",
      "train loss:0.6179870011345364\n",
      "train loss:0.4376287278349141\n",
      "train loss:0.5391023438565672\n",
      "train loss:0.340993290242273\n",
      "train loss:0.6197830023652818\n",
      "train loss:0.5234727665683147\n",
      "train loss:0.41003141794741627\n",
      "train loss:0.7182792389480569\n",
      "train loss:0.614367475499525\n",
      "train loss:0.7194362755138213\n",
      "train loss:0.7488365500038114\n",
      "train loss:0.7155020556538567\n",
      "train loss:0.3917541870322091\n",
      "train loss:0.8091733799588013\n",
      "train loss:0.6176645852307929\n",
      "train loss:0.606220391701354\n",
      "train loss:0.6932674166641859\n",
      "train loss:0.6983255717831887\n",
      "train loss:0.703664145910811\n",
      "train loss:0.5471886631440506\n",
      "train loss:0.5562606530485752\n",
      "train loss:0.8286977779062799\n",
      "train loss:0.67970110504231\n",
      "train loss:0.5163172057563905\n",
      "train loss:0.68049210318693\n",
      "train loss:0.6211016345086069\n",
      "train loss:0.585104855696767\n",
      "train loss:0.7702504808180836\n",
      "train loss:0.5845269184826559\n",
      "train loss:0.6724372956633662\n",
      "train loss:0.6851802855449969\n",
      "train loss:0.6807930341069366\n",
      "train loss:0.6127598850137171\n",
      "train loss:0.7267421445427488\n",
      "train loss:0.6278568271653026\n",
      "train loss:0.6656402670780056\n",
      "train loss:0.6748475448038386\n",
      "train loss:0.5767318619805464\n",
      "train loss:0.6778663482780435\n",
      "train loss:0.6275456484537012\n",
      "train loss:0.6721247207677091\n",
      "train loss:0.5663631886235103\n",
      "train loss:0.6092686072505151\n",
      "train loss:0.5217246219364542\n",
      "train loss:0.6113131725946962\n",
      "train loss:0.5362950985658241\n",
      "train loss:0.44252536446108637\n",
      "train loss:0.5227155801317668\n",
      "train loss:0.4188882580067613\n",
      "train loss:0.6133167917565006\n",
      "train loss:0.6523131701783846\n",
      "train loss:0.7543624780925078\n",
      "train loss:0.4900587855220885\n",
      "train loss:0.8711977751552424\n",
      "train loss:0.35496319858422476\n",
      "train loss:0.6511051191578952\n",
      "train loss:0.503990726214503\n",
      "train loss:0.3657034890362074\n",
      "train loss:0.6218642772848288\n",
      "train loss:0.4755443616039889\n",
      "train loss:0.7666214404610847\n",
      "train loss:0.6227555410140104\n",
      "train loss:0.7304358180962963\n",
      "train loss:0.5296063965548192\n",
      "train loss:0.5246605620226992\n",
      "train loss:0.5630481181699173\n",
      "train loss:0.5066236749753567\n",
      "train loss:0.4148731299535843\n",
      "train loss:0.503468056043573\n",
      "train loss:0.3978019016208586\n",
      "train loss:0.626602099476694\n",
      "train loss:0.2823987959062446\n",
      "train loss:0.3954821324023611\n",
      "train loss:0.7778300594219877\n",
      "train loss:0.49954157236179475\n",
      "train loss:0.5985809921472411\n",
      "train loss:0.8924264590311648\n",
      "train loss:0.6727965731026274\n",
      "train loss:0.37150819297007265\n",
      "train loss:0.5001453043281667\n",
      "train loss:0.6236780971023507\n",
      "train loss:0.2790817445282482\n",
      "train loss:0.6136676090756925\n",
      "train loss:0.8440456854299597\n",
      "train loss:0.7162139261481667\n",
      "train loss:0.9282935273148956\n",
      "train loss:0.607347193951097\n",
      "train loss:0.536487234994736\n",
      "train loss:0.7573122277332979\n",
      "train loss:0.673632377856457\n",
      "train loss:0.552554345027518\n",
      "train loss:0.5690737072711176\n",
      "train loss:0.5049998227375532\n",
      "train loss:0.6750649033702112\n",
      "train loss:0.5681768514108569\n",
      "train loss:0.6187825510051476\n",
      "train loss:0.5607510988792301\n",
      "train loss:0.4745443557683009\n",
      "train loss:0.6182589886921193\n",
      "train loss:0.5293483633117837\n",
      "train loss:0.6081235816193898\n",
      "train loss:0.5952837742496572\n",
      "train loss:0.7096729905768423\n",
      "train loss:0.41329520600271846\n",
      "train loss:0.8384670576539728\n",
      "train loss:0.6433945907048699\n",
      "train loss:0.6139969908798188\n",
      "train loss:0.7524169783734029\n",
      "train loss:0.5104356751743298\n",
      "train loss:0.5202740998984803\n",
      "train loss:0.8183950157548272\n",
      "train loss:0.7131079504096686\n",
      "train loss:0.7739452904058596\n",
      "train loss:0.782941413840887\n",
      "=== epoch:5, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5648571587699409\n",
      "train loss:0.6254801307280081\n",
      "train loss:0.6865634678319973\n",
      "train loss:0.6197320863950957\n",
      "train loss:0.7293267091812322\n",
      "train loss:0.6299014772616516\n",
      "train loss:0.6381578383834405\n",
      "train loss:0.6353628593334276\n",
      "train loss:0.5875572237011876\n",
      "train loss:0.6813950227065632\n",
      "train loss:0.6405030501839486\n",
      "train loss:0.8092707870580982\n",
      "train loss:0.544186808918372\n",
      "train loss:0.6216758131147782\n",
      "train loss:0.7266056735034226\n",
      "train loss:0.5648950133466709\n",
      "train loss:0.6692189564704425\n",
      "train loss:0.5173536717717476\n",
      "train loss:0.6720809464811273\n",
      "train loss:0.6230612584849708\n",
      "train loss:0.5449549714412141\n",
      "train loss:0.5450335010959458\n",
      "train loss:0.5197582662729752\n",
      "train loss:0.7069235028250402\n",
      "train loss:0.41684306228108026\n",
      "train loss:0.5040396502651163\n",
      "train loss:0.5180866096614536\n",
      "train loss:0.35777920596431656\n",
      "train loss:0.7928880162388026\n",
      "train loss:0.6370612400925799\n",
      "train loss:0.7745238547446716\n",
      "train loss:0.5230586799696343\n",
      "train loss:0.7374835493725032\n",
      "train loss:0.5030941796804083\n",
      "train loss:0.3608041269433631\n",
      "train loss:0.4984603322950042\n",
      "train loss:1.0050669527514653\n",
      "train loss:0.5957425844766802\n",
      "train loss:0.8207797296119466\n",
      "train loss:0.696076953899635\n",
      "train loss:0.43634627445643304\n",
      "train loss:0.5306921981320702\n",
      "train loss:0.4552091348287721\n",
      "train loss:0.6097560010694941\n",
      "train loss:0.6199682123036265\n",
      "train loss:0.617230627010381\n",
      "train loss:0.6183720340674388\n",
      "train loss:0.45867062059137514\n",
      "train loss:0.4462821609239878\n",
      "train loss:0.6316546898888318\n",
      "train loss:0.5138741923692481\n",
      "train loss:0.6821906210862168\n",
      "train loss:0.6010940704885164\n",
      "train loss:0.40404120347884065\n",
      "train loss:0.7193320601746096\n",
      "train loss:0.6233506004136867\n",
      "train loss:0.9484178793577083\n",
      "train loss:0.8088074482683266\n",
      "train loss:0.7232376255660165\n",
      "train loss:0.5092230375566313\n",
      "train loss:0.598507975441697\n",
      "train loss:0.43468927204190055\n",
      "train loss:0.7711457555412666\n",
      "train loss:0.6169776287350838\n",
      "train loss:0.6295863342610044\n",
      "train loss:0.6097788221246738\n",
      "train loss:0.5511047170326584\n",
      "train loss:0.68610290774127\n",
      "train loss:0.6669665385125441\n",
      "train loss:0.5606460082915612\n",
      "train loss:0.5748164710265723\n",
      "train loss:0.6303102975900089\n",
      "train loss:0.5342169285643303\n",
      "train loss:0.45518541261233086\n",
      "train loss:0.6843525233124195\n",
      "train loss:0.5945341587152597\n",
      "train loss:0.7954490216077504\n",
      "train loss:0.6047572978238007\n",
      "train loss:0.783545410662166\n",
      "train loss:0.6158520905216353\n",
      "train loss:0.5331666001115472\n",
      "train loss:0.43886960964076255\n",
      "train loss:0.5355820799198521\n",
      "train loss:0.5373793300535069\n",
      "train loss:0.8059707484136271\n",
      "train loss:0.689550102764166\n",
      "train loss:0.8106589252999721\n",
      "train loss:0.5278320749534953\n",
      "train loss:0.6258441210227665\n",
      "train loss:0.8560763236141821\n",
      "train loss:0.5390646098508954\n",
      "train loss:0.5481839581638965\n",
      "train loss:0.5343010006461526\n",
      "train loss:0.5382980981467309\n",
      "train loss:0.5367486131885644\n",
      "train loss:0.4479941440608609\n",
      "train loss:0.6145935528805895\n",
      "train loss:0.7039821634844488\n",
      "train loss:0.6029291440217628\n",
      "train loss:0.6178030462762464\n",
      "train loss:0.7208155742740339\n",
      "train loss:0.609721797770747\n",
      "train loss:0.6027834423708314\n",
      "train loss:0.5318990741076001\n",
      "train loss:0.7197479156394209\n",
      "train loss:0.6025824718034132\n",
      "train loss:0.6896430890796605\n",
      "train loss:0.6191601541591691\n",
      "train loss:0.7162020824938307\n",
      "train loss:0.4365892397124395\n",
      "train loss:0.44458287085534093\n",
      "train loss:0.5136416117768651\n",
      "train loss:0.7843579015406441\n",
      "train loss:0.6057311303345307\n",
      "train loss:0.6955106619461383\n",
      "train loss:0.5396012747471775\n",
      "train loss:0.4252383182708207\n",
      "train loss:0.5834314868488946\n",
      "train loss:0.6038980553155708\n",
      "train loss:0.6193597297114785\n",
      "train loss:0.6097955835560688\n",
      "train loss:0.7246308425380004\n",
      "train loss:0.6414379726867868\n",
      "train loss:0.5066418012812652\n",
      "train loss:0.634221518967864\n",
      "train loss:0.6991668526864725\n",
      "train loss:0.6035085367527637\n",
      "train loss:0.43873671427830774\n",
      "train loss:0.7860760475440698\n",
      "train loss:0.6044209528497902\n",
      "train loss:0.6010650266438703\n",
      "train loss:0.5339318351664492\n",
      "train loss:0.607244566543389\n",
      "train loss:0.44604795326111424\n",
      "train loss:0.7912937148988893\n",
      "train loss:0.7766747757497614\n",
      "train loss:0.7088758981677815\n",
      "train loss:0.6894052536754696\n",
      "train loss:0.47965543625019136\n",
      "train loss:0.5480111718558611\n",
      "train loss:0.5406437493680396\n",
      "train loss:0.7697296516145746\n",
      "train loss:0.47482963452019683\n",
      "train loss:0.6011766915747434\n",
      "train loss:0.6748211064460016\n",
      "train loss:0.6122804105835525\n",
      "train loss:0.6902745046762027\n",
      "train loss:0.533684973611739\n",
      "train loss:0.5229258373267291\n",
      "train loss:0.8590947681835601\n",
      "train loss:0.6079809167805361\n",
      "train loss:0.6896561027044167\n",
      "train loss:0.4464678242681714\n",
      "=== epoch:6, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6934255456014635\n",
      "train loss:0.6901877744982811\n",
      "train loss:0.7027716538618625\n",
      "train loss:0.7574515522097219\n",
      "train loss:0.6797888581086483\n",
      "train loss:0.5675251772161903\n",
      "train loss:0.7338906783512462\n",
      "train loss:0.7295400700233368\n",
      "train loss:0.5800705406325052\n",
      "train loss:0.5764813505550686\n",
      "train loss:0.5236061203937661\n",
      "train loss:0.6200905368393065\n",
      "train loss:0.625818672846019\n",
      "train loss:0.625599101070043\n",
      "train loss:0.4784764235710627\n",
      "train loss:0.6025798088279304\n",
      "train loss:0.5256884500541161\n",
      "train loss:0.43352421164427185\n",
      "train loss:0.6176538315296061\n",
      "train loss:0.5177030901403047\n",
      "train loss:0.9528312337662301\n",
      "train loss:0.5002352354645017\n",
      "train loss:0.5122235312017436\n",
      "train loss:0.4953222447116776\n",
      "train loss:0.5589368596422533\n",
      "train loss:0.867070147444367\n",
      "train loss:0.85899796067383\n",
      "train loss:0.5162166458330313\n",
      "train loss:0.9197537617049415\n",
      "train loss:0.5956025390041674\n",
      "train loss:0.6204155884776321\n",
      "train loss:0.4579282422794633\n",
      "train loss:0.531325521777813\n",
      "train loss:0.8197227870283637\n",
      "train loss:0.5604006828290069\n",
      "train loss:0.6190087290530524\n",
      "train loss:0.49364607005440797\n",
      "train loss:0.6787238057323643\n",
      "train loss:0.50128281301608\n",
      "train loss:0.7502654230023714\n",
      "train loss:0.616496661268765\n",
      "train loss:0.5446900529502618\n",
      "train loss:0.6025647327919231\n",
      "train loss:0.5299920298553962\n",
      "train loss:0.6181817917016537\n",
      "train loss:0.8399058945476\n",
      "train loss:0.608352017420782\n",
      "train loss:0.5351379734024075\n",
      "train loss:0.6944387483095309\n",
      "train loss:0.6900573635068615\n",
      "train loss:0.3617815206016638\n",
      "train loss:0.6850058692166365\n",
      "train loss:0.4278404870346524\n",
      "train loss:0.6220674505892804\n",
      "train loss:0.6960801708675797\n",
      "train loss:0.5133208787476666\n",
      "train loss:0.5095542051708861\n",
      "train loss:0.2915847067026581\n",
      "train loss:0.5030097198753949\n",
      "train loss:0.7412687671801802\n",
      "train loss:0.8900707659333715\n",
      "train loss:0.8552676088509239\n",
      "train loss:0.5072013885452462\n",
      "train loss:0.39355813193214717\n",
      "train loss:0.7162626840078634\n",
      "train loss:0.809327876967451\n",
      "train loss:0.5172574353816232\n",
      "train loss:0.6923501584996667\n",
      "train loss:0.7883159755254162\n",
      "train loss:0.7482888894001211\n",
      "train loss:0.5364953257756878\n",
      "train loss:0.6815835707250215\n",
      "train loss:0.6749558086452563\n",
      "train loss:0.7261211631133777\n",
      "train loss:0.5359603641522495\n",
      "train loss:0.5377891097781305\n",
      "train loss:0.5786212189908068\n",
      "train loss:0.5710777522355043\n",
      "train loss:0.7839857491581194\n",
      "train loss:0.5042992332719914\n",
      "train loss:0.6271394363924283\n",
      "train loss:0.614278826129974\n",
      "train loss:0.6811634081443103\n",
      "train loss:0.4746436714137299\n",
      "train loss:0.8283995546866516\n",
      "train loss:0.7594917982141682\n",
      "train loss:0.5433313260333823\n",
      "train loss:0.7643230185770145\n",
      "train loss:0.46543523662169706\n",
      "train loss:0.5349669901296352\n",
      "train loss:0.5182966541540297\n",
      "train loss:0.6939579630214937\n",
      "train loss:0.624424043552324\n",
      "train loss:0.8582271243040314\n",
      "train loss:0.5314171273969571\n",
      "train loss:0.8623606824261494\n",
      "train loss:0.6915663437216717\n",
      "train loss:0.6164868114875043\n",
      "train loss:0.5425025999092614\n",
      "train loss:0.5434621453693027\n",
      "train loss:0.6051392825977067\n",
      "train loss:0.6133926245149025\n",
      "train loss:0.6853187267351463\n",
      "train loss:0.3882737998403815\n",
      "train loss:0.6073014210817923\n",
      "train loss:0.523810518763053\n",
      "train loss:0.4290269320528175\n",
      "train loss:0.5198947776769108\n",
      "train loss:0.6312052192407642\n",
      "train loss:0.3989387119725542\n",
      "train loss:0.6231264153604721\n",
      "train loss:0.37437452348603223\n",
      "train loss:0.8903673301427764\n",
      "train loss:1.0055369248435089\n",
      "train loss:0.7411273817782953\n",
      "train loss:0.4947825284152826\n",
      "train loss:0.4941568501604035\n",
      "train loss:0.40577561806275464\n",
      "train loss:0.7230358010666459\n",
      "train loss:0.8128905685751358\n",
      "train loss:0.5152536895233744\n",
      "train loss:0.5229130207109681\n",
      "train loss:0.7812621891658109\n",
      "train loss:0.5294560674178165\n",
      "train loss:0.6967762956862699\n",
      "train loss:0.4619832040340734\n",
      "train loss:0.6854625808701617\n",
      "train loss:0.6784350791292311\n",
      "train loss:0.46699593121625826\n",
      "train loss:0.4676597712213453\n",
      "train loss:0.7613949066251054\n",
      "train loss:0.7539349426896896\n",
      "train loss:0.7604700597719342\n",
      "train loss:0.6780330465662463\n",
      "train loss:0.7002683539829004\n",
      "train loss:0.48927075495448397\n",
      "train loss:0.7340390769128973\n",
      "train loss:0.6187386691809735\n",
      "train loss:0.6769076276043101\n",
      "train loss:0.43683306347982287\n",
      "train loss:0.4895747225361756\n",
      "train loss:0.4792756556818568\n",
      "train loss:0.6765196928106676\n",
      "train loss:0.7575036888322574\n",
      "train loss:0.4448940216445926\n",
      "train loss:0.5204577168879584\n",
      "train loss:0.5052042532942312\n",
      "train loss:0.5000690293251683\n",
      "train loss:0.6186672888265852\n",
      "train loss:0.4918030749183496\n",
      "train loss:0.3789007575994018\n",
      "train loss:0.4904676372912163\n",
      "train loss:0.2205784683165828\n",
      "=== epoch:7, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.35153448279021643\n",
      "train loss:0.5032424626672763\n",
      "train loss:0.3327716736461728\n",
      "train loss:0.9224907614749476\n",
      "train loss:0.9218682028834652\n",
      "train loss:0.8574556930692232\n",
      "train loss:0.5265259729703466\n",
      "train loss:0.6407397335045861\n",
      "train loss:0.7805658940416496\n",
      "train loss:0.6153307718025978\n",
      "train loss:0.3924122495355988\n",
      "train loss:0.6118486563858887\n",
      "train loss:0.6086445887987139\n",
      "train loss:0.6876379778444084\n",
      "train loss:0.6035008699275406\n",
      "train loss:0.6918429904204372\n",
      "train loss:0.5629190644592693\n",
      "train loss:0.6204282032144992\n",
      "train loss:0.5009141625847046\n",
      "train loss:0.849110987619978\n",
      "train loss:0.5627828737378704\n",
      "train loss:0.5743208967465858\n",
      "train loss:0.6247006168370477\n",
      "train loss:0.6235625699594082\n",
      "train loss:0.5570255700416626\n",
      "train loss:0.7343697671048158\n",
      "train loss:0.7396151045743952\n",
      "train loss:0.564202655082812\n",
      "train loss:0.49624971917232497\n",
      "train loss:0.6915727911998907\n",
      "train loss:0.7542106079901054\n",
      "train loss:0.7426619815168656\n",
      "train loss:0.6890429039543454\n",
      "train loss:0.6738005566058465\n",
      "train loss:0.7473746034729928\n",
      "train loss:0.5656565349266656\n",
      "train loss:0.6232068667033038\n",
      "train loss:0.6172368794568232\n",
      "train loss:0.5609528525551397\n",
      "train loss:0.5578499010656326\n",
      "train loss:0.6143818934997246\n",
      "train loss:0.7515924208432233\n",
      "train loss:0.5449081257536604\n",
      "train loss:0.8204172650138709\n",
      "train loss:0.6823914015669511\n",
      "train loss:0.47173472950094747\n",
      "train loss:0.39701046885740465\n",
      "train loss:0.6051259997151187\n",
      "train loss:0.4313358408878341\n",
      "train loss:0.43611649036584066\n",
      "train loss:0.6161903993846123\n",
      "train loss:0.5075946117378324\n",
      "train loss:0.9539534173460368\n",
      "train loss:0.8488487901995905\n",
      "train loss:0.3901249558837151\n",
      "train loss:0.6137923650299196\n",
      "train loss:0.7172484850839239\n",
      "train loss:0.7249739862048754\n",
      "train loss:0.612710167124549\n",
      "train loss:0.6136269132546234\n",
      "train loss:0.4239686621741344\n",
      "train loss:0.5256466728957647\n",
      "train loss:0.6988363311534229\n",
      "train loss:0.6104050242161342\n",
      "train loss:0.8678167775803811\n",
      "train loss:0.6133638473169453\n",
      "train loss:0.6236745625749325\n",
      "train loss:0.3849728441777339\n",
      "train loss:0.5501037574081075\n",
      "train loss:0.461135815124517\n",
      "train loss:0.770990273852496\n",
      "train loss:0.4457176256529838\n",
      "train loss:0.6985538264728806\n",
      "train loss:0.5220106772981289\n",
      "train loss:0.6159295927304529\n",
      "train loss:0.875778404546647\n",
      "train loss:0.6947265388391031\n",
      "train loss:0.5274330863878062\n",
      "train loss:0.6078001335424202\n",
      "train loss:0.5346062638140905\n",
      "train loss:0.6043043394978674\n",
      "train loss:0.5258316979821289\n",
      "train loss:0.6098399131711998\n",
      "train loss:0.7018031819955418\n",
      "train loss:0.5225277991881203\n",
      "train loss:0.3423755416539748\n",
      "train loss:0.5181647330884209\n",
      "train loss:0.705920723874536\n",
      "train loss:0.4007017644464009\n",
      "train loss:0.6085966864861472\n",
      "train loss:0.39055107078439405\n",
      "train loss:0.7457229826891538\n",
      "train loss:0.7420532136856552\n",
      "train loss:0.5096223910892832\n",
      "train loss:0.6245370976784983\n",
      "train loss:0.7591408723642575\n",
      "train loss:0.3791299206125246\n",
      "train loss:0.5118030446436331\n",
      "train loss:0.7396565306187292\n",
      "train loss:0.6650232670561518\n",
      "train loss:0.4030368009828845\n",
      "train loss:0.9017415294208435\n",
      "train loss:0.6056457066284896\n",
      "train loss:0.6095028480532059\n",
      "train loss:0.692104671488165\n",
      "train loss:0.4539593382504714\n",
      "train loss:0.679540358260074\n",
      "train loss:0.6119822360569606\n",
      "train loss:0.6883914604395865\n",
      "train loss:0.7587507874893386\n",
      "train loss:0.6749182441072868\n",
      "train loss:0.6732545227159648\n",
      "train loss:0.5742654541645781\n",
      "train loss:0.6240685232092503\n",
      "train loss:0.6267523871634397\n",
      "train loss:0.5705687755010167\n",
      "train loss:0.6773620785597535\n",
      "train loss:0.5715818720050787\n",
      "train loss:0.5669263093406058\n",
      "train loss:0.49757265291939295\n",
      "train loss:0.5431923766925866\n",
      "train loss:0.6893605329589424\n",
      "train loss:0.6268652462553834\n",
      "train loss:0.5249618347209847\n",
      "train loss:0.6089639636822859\n",
      "train loss:0.7907663907367571\n",
      "train loss:0.8766649578875423\n",
      "train loss:0.6116451957539524\n",
      "train loss:0.6035474415480511\n",
      "train loss:0.6214680533853147\n",
      "train loss:0.44666776789706447\n",
      "train loss:0.6131974375443203\n",
      "train loss:0.6108156536598809\n",
      "train loss:0.5249550138683633\n",
      "train loss:0.6999007260141211\n",
      "train loss:0.6143861124018483\n",
      "train loss:0.6148930823627845\n",
      "train loss:0.51150379375198\n",
      "train loss:0.8750296532861993\n",
      "train loss:0.6921605903520719\n",
      "train loss:0.5318585338066785\n",
      "train loss:0.6896481242973151\n",
      "train loss:0.45304134748822056\n",
      "train loss:0.5290823505299557\n",
      "train loss:0.8388628089341832\n",
      "train loss:0.8336940338776616\n",
      "train loss:0.465669533923304\n",
      "train loss:0.8186845700867889\n",
      "train loss:0.6165082960880881\n",
      "train loss:0.491827973776644\n",
      "train loss:0.5476039744475188\n",
      "train loss:0.4764599155176753\n",
      "train loss:0.6162093934855735\n",
      "=== epoch:8, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6855695750812856\n",
      "train loss:0.6101126522412634\n",
      "train loss:0.4508193146269971\n",
      "train loss:0.8525050059672216\n",
      "train loss:0.5248369474687704\n",
      "train loss:0.5224623335554759\n",
      "train loss:0.7006060513445813\n",
      "train loss:0.4372954917701294\n",
      "train loss:0.41750267113609685\n",
      "train loss:0.623238418761032\n",
      "train loss:0.5089349951678439\n",
      "train loss:0.38863275648562745\n",
      "train loss:0.7368091462862689\n",
      "train loss:0.5058801601034715\n",
      "train loss:0.9821757842921105\n",
      "train loss:0.7385691057403259\n",
      "train loss:0.7244558644723671\n",
      "train loss:0.5117109287074008\n",
      "train loss:0.29682444837655736\n",
      "train loss:0.617274248052088\n",
      "train loss:0.5172057628139681\n",
      "train loss:0.6171851073280396\n",
      "train loss:0.7061521665738583\n",
      "train loss:0.4111475005036806\n",
      "train loss:0.4017883368878071\n",
      "train loss:0.6154073605803955\n",
      "train loss:0.6138099164152939\n",
      "train loss:0.5028460843044344\n",
      "train loss:0.4957675572031587\n",
      "train loss:0.5089646888333963\n",
      "train loss:0.5145754160815776\n",
      "train loss:0.37930357080788796\n",
      "train loss:0.7395771814258227\n",
      "train loss:0.9852623500049441\n",
      "train loss:0.6176499420005976\n",
      "train loss:0.4974402824711922\n",
      "train loss:0.611312038501917\n",
      "train loss:0.6999836728215448\n",
      "train loss:0.4062910728599099\n",
      "train loss:0.9023650382185238\n",
      "train loss:0.6877897195080629\n",
      "train loss:0.689078803570475\n",
      "train loss:0.5418098625535862\n",
      "train loss:0.682944030404048\n",
      "train loss:0.5518045225237264\n",
      "train loss:0.6756296066218243\n",
      "train loss:0.6760257535391895\n",
      "train loss:0.5671669741778941\n",
      "train loss:0.5595912206149728\n",
      "train loss:0.5586793157864673\n",
      "train loss:0.6159054238880348\n",
      "train loss:0.6148216671230443\n",
      "train loss:0.6813706239381437\n",
      "train loss:0.5499768753845281\n",
      "train loss:0.623276931603893\n",
      "train loss:0.5328225520620397\n",
      "train loss:0.5383419107512133\n",
      "train loss:0.684844386570812\n",
      "train loss:0.4397018477253617\n",
      "train loss:0.6021722543885776\n",
      "train loss:0.5010624670143995\n",
      "train loss:1.0153644287189794\n",
      "train loss:0.41609310257248966\n",
      "train loss:0.6120128927366142\n",
      "train loss:0.5028266377651776\n",
      "train loss:0.5000144167986537\n",
      "train loss:0.7268457232855997\n",
      "train loss:0.6147454756389434\n",
      "train loss:0.4902860065055822\n",
      "train loss:0.7247127491421457\n",
      "train loss:0.4018815255872889\n",
      "train loss:0.49842158026443145\n",
      "train loss:0.6337636915024164\n",
      "train loss:0.6184493833927482\n",
      "train loss:0.7292415926222267\n",
      "train loss:0.6111273917591133\n",
      "train loss:0.504064044467716\n",
      "train loss:0.6114146439108034\n",
      "train loss:0.5165245880225495\n",
      "train loss:0.4081507928943794\n",
      "train loss:0.6158380792020323\n",
      "train loss:0.7080417562824136\n",
      "train loss:0.5106504648096489\n",
      "train loss:0.6186033525628054\n",
      "train loss:0.637044963599439\n",
      "train loss:0.7081662024335303\n",
      "train loss:0.5154637610570784\n",
      "train loss:0.621451301004461\n",
      "train loss:0.5238387431984076\n",
      "train loss:0.5973801043096475\n",
      "train loss:0.42950331257466934\n",
      "train loss:0.6095769481017863\n",
      "train loss:0.7977034572416167\n",
      "train loss:0.6111879594972043\n",
      "train loss:0.6023116420289686\n",
      "train loss:0.6949069637404612\n",
      "train loss:0.6087857277301708\n",
      "train loss:0.5274016218628418\n",
      "train loss:0.6938215990168818\n",
      "train loss:0.7739852405988872\n",
      "train loss:0.45294975301701523\n",
      "train loss:0.6161192970510414\n",
      "train loss:0.6817657383191114\n",
      "train loss:0.6140562044953368\n",
      "train loss:0.6780319424899216\n",
      "train loss:0.6212936779563976\n",
      "train loss:0.6806690668487121\n",
      "train loss:0.5511108018055453\n",
      "train loss:0.616718014679871\n",
      "train loss:0.6082845069117216\n",
      "train loss:0.5337552441972302\n",
      "train loss:0.5296763533703794\n",
      "train loss:0.5184150172954155\n",
      "train loss:0.6978591155063562\n",
      "train loss:0.7028919836109958\n",
      "train loss:0.5195818063629483\n",
      "train loss:0.7108053914585174\n",
      "train loss:0.5256806410448582\n",
      "train loss:0.5159150347397945\n",
      "train loss:0.61350400099398\n",
      "train loss:0.5130676263060926\n",
      "train loss:0.5184032963469802\n",
      "train loss:0.6159365897127491\n",
      "train loss:0.4976136669273849\n",
      "train loss:0.38468027065170834\n",
      "train loss:0.2288560605049743\n",
      "train loss:0.1954097178489413\n",
      "train loss:0.663743936806302\n",
      "train loss:0.6809021457301973\n",
      "train loss:0.3237526916269896\n",
      "train loss:0.5313249014470922\n",
      "train loss:0.6990024815127952\n",
      "train loss:0.5029958464005256\n",
      "train loss:0.83233766041749\n",
      "train loss:0.5018753017411823\n",
      "train loss:0.8850240186147149\n",
      "train loss:0.6143309924066347\n",
      "train loss:0.89285940831681\n",
      "train loss:0.6120150194001392\n",
      "train loss:0.616447828548861\n",
      "train loss:0.563547426493541\n",
      "train loss:0.5690074288053715\n",
      "train loss:0.5266398997233633\n",
      "train loss:0.5312338493538428\n",
      "train loss:0.6770186077791374\n",
      "train loss:0.5756808256801512\n",
      "train loss:0.5177937833968971\n",
      "train loss:0.623604012764264\n",
      "train loss:0.6166529430348033\n",
      "train loss:0.7368442061943139\n",
      "train loss:0.6198900193896086\n",
      "train loss:0.6159915107027468\n",
      "train loss:0.5405426415887399\n",
      "=== epoch:9, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.382547475589129\n",
      "train loss:0.4406794503511106\n",
      "train loss:0.6980885594718904\n",
      "train loss:0.609354536370576\n",
      "train loss:0.607393472754427\n",
      "train loss:0.5089424497032764\n",
      "train loss:0.6163322629933254\n",
      "train loss:0.5042603840238432\n",
      "train loss:0.6189988517407443\n",
      "train loss:0.6276056229315483\n",
      "train loss:0.6373563391071629\n",
      "train loss:0.6225023679977546\n",
      "train loss:0.6080451028162033\n",
      "train loss:0.6276333873089281\n",
      "train loss:0.5012818297644888\n",
      "train loss:0.6170166869693048\n",
      "train loss:0.7370822954325135\n",
      "train loss:0.5110200799168172\n",
      "train loss:0.6131732454134123\n",
      "train loss:0.7005798542088287\n",
      "train loss:0.5199567434614936\n",
      "train loss:0.8691422416751369\n",
      "train loss:0.6092268352131655\n",
      "train loss:0.7586419426742395\n",
      "train loss:0.5400142564122616\n",
      "train loss:0.5450417671353451\n",
      "train loss:0.6133485938129463\n",
      "train loss:0.6844195785621847\n",
      "train loss:0.5563485668681727\n",
      "train loss:0.7427709303734378\n",
      "train loss:0.6197977561117953\n",
      "train loss:0.6208981007021167\n",
      "train loss:0.6211010732189249\n",
      "train loss:0.6206159003597996\n",
      "train loss:0.5520917696484281\n",
      "train loss:0.6180365325198125\n",
      "train loss:0.6778978291780501\n",
      "train loss:0.7360701895205226\n",
      "train loss:0.6874741094900563\n",
      "train loss:0.5457437069095605\n",
      "train loss:0.683041839972147\n",
      "train loss:0.6077386402145902\n",
      "train loss:0.6043081292331457\n",
      "train loss:0.6830769436781374\n",
      "train loss:0.6197442316715398\n",
      "train loss:0.5417039891171758\n",
      "train loss:0.692329029629551\n",
      "train loss:0.7560298912480118\n",
      "train loss:0.607840925849029\n",
      "train loss:0.7526518163513825\n",
      "train loss:0.5486843870204934\n",
      "train loss:0.6714479066616272\n",
      "train loss:0.6181083073817419\n",
      "train loss:0.6108459227277534\n",
      "train loss:0.5415862855816879\n",
      "train loss:0.8026138239346793\n",
      "train loss:0.4801969915723392\n",
      "train loss:0.803043299209403\n",
      "train loss:0.5358597946631234\n",
      "train loss:0.47106436659261774\n",
      "train loss:0.5436255798380247\n",
      "train loss:0.7666197981842771\n",
      "train loss:0.6129851428197067\n",
      "train loss:0.6925973676081031\n",
      "train loss:0.5311052908511433\n",
      "train loss:0.7618935156650124\n",
      "train loss:0.689150959757974\n",
      "train loss:0.7497461881348602\n",
      "train loss:0.5411678705971228\n",
      "train loss:0.5342088838064387\n",
      "train loss:0.4595624183559699\n",
      "train loss:0.4530833843662915\n",
      "train loss:0.5196620143364049\n",
      "train loss:0.5960611095052502\n",
      "train loss:0.7042505762743225\n",
      "train loss:0.6161620564639565\n",
      "train loss:0.40521330602666883\n",
      "train loss:0.3845324077383141\n",
      "train loss:0.7394776961703222\n",
      "train loss:0.7352468384555254\n",
      "train loss:0.7428673679202106\n",
      "train loss:0.9391258755505932\n",
      "train loss:0.6065489472233699\n",
      "train loss:0.31583059339475683\n",
      "train loss:0.5155060453719253\n",
      "train loss:0.6074840083275372\n",
      "train loss:0.7998599256353855\n",
      "train loss:0.6059128243783439\n",
      "train loss:0.4297216330281735\n",
      "train loss:0.6959573976921525\n",
      "train loss:0.7596112308392954\n",
      "train loss:0.5221357802524118\n",
      "train loss:0.46101065258541435\n",
      "train loss:0.5168086742906776\n",
      "train loss:0.5139029667089168\n",
      "train loss:0.441031404133156\n",
      "train loss:0.613956158802887\n",
      "train loss:0.5091053784955791\n",
      "train loss:0.5145607854059379\n",
      "train loss:0.7214498046844546\n",
      "train loss:0.7329440093052939\n",
      "train loss:0.6063571762815526\n",
      "train loss:0.49988983984961777\n",
      "train loss:0.49104906050504615\n",
      "train loss:0.5122568679726773\n",
      "train loss:0.8203370501634873\n",
      "train loss:0.9279575900444856\n",
      "train loss:0.6146703757903705\n",
      "train loss:0.5312801215086238\n",
      "train loss:0.528288301274595\n",
      "train loss:0.6077028057566694\n",
      "train loss:0.6854645329326672\n",
      "train loss:0.5322189546008707\n",
      "train loss:0.6896257951293787\n",
      "train loss:0.5380518771552046\n",
      "train loss:0.46320245515114145\n",
      "train loss:0.4538862217323623\n",
      "train loss:0.543162939879095\n",
      "train loss:0.6107445597864685\n",
      "train loss:0.5174476410084641\n",
      "train loss:0.3147377596779357\n",
      "train loss:0.7351373840215025\n",
      "train loss:0.3927308634889605\n",
      "train loss:0.4938859663880578\n",
      "train loss:0.3663300528779934\n",
      "train loss:0.4919104204865633\n",
      "train loss:0.6375047721483854\n",
      "train loss:0.6446882724915795\n",
      "train loss:0.5010667876961711\n",
      "train loss:0.7944165045996904\n",
      "train loss:0.502679969992584\n",
      "train loss:0.490321193944588\n",
      "train loss:0.2048066912606578\n",
      "train loss:0.9011748285502208\n",
      "train loss:0.7763564243797421\n",
      "train loss:0.6181557433692632\n",
      "train loss:0.7439377392850062\n",
      "train loss:0.4004232908338431\n",
      "train loss:0.5107302500877313\n",
      "train loss:0.5002710894007556\n",
      "train loss:0.6161904809148974\n",
      "train loss:0.5171417358075219\n",
      "train loss:0.6801819676162596\n",
      "train loss:0.6112591815010241\n",
      "train loss:0.5971414112372699\n",
      "train loss:0.6157795937050509\n",
      "train loss:0.4375979795244496\n",
      "train loss:0.4329360807194063\n",
      "train loss:0.42751842673769025\n",
      "train loss:0.6179478034784742\n",
      "train loss:0.5243521971018258\n",
      "train loss:0.7035895713291522\n",
      "train loss:0.596957359791153\n",
      "=== epoch:10, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5017487156450705\n",
      "train loss:0.7177830341604899\n",
      "train loss:0.6059328643779931\n",
      "train loss:0.3928289657842866\n",
      "train loss:0.822292332854419\n",
      "train loss:0.5155866379728307\n",
      "train loss:0.619363758400145\n",
      "train loss:0.5157853529103278\n",
      "train loss:0.6198653226782652\n",
      "train loss:0.6127233485074692\n",
      "train loss:0.5113108380588152\n",
      "train loss:0.3048806170104349\n",
      "train loss:0.6116009570247442\n",
      "train loss:0.6245138750786997\n",
      "train loss:0.3927710678708995\n",
      "train loss:0.4938561599336896\n",
      "train loss:0.6055971626098469\n",
      "train loss:0.3768986583847861\n",
      "train loss:0.871596588637612\n",
      "train loss:0.8623836867708146\n",
      "train loss:0.39025536039838965\n",
      "train loss:0.5018155191765498\n",
      "train loss:0.48798753924260374\n",
      "train loss:0.387750138937852\n",
      "train loss:0.49300106854052894\n",
      "train loss:0.4983252401592537\n",
      "train loss:0.5067047037285743\n",
      "train loss:0.6152355323127257\n",
      "train loss:0.608824988975279\n",
      "train loss:0.5027454981047805\n",
      "train loss:0.6145519096339543\n",
      "train loss:0.47880674781971677\n",
      "train loss:0.8654974653994163\n",
      "train loss:0.5007905796843349\n",
      "train loss:0.6198703854721532\n",
      "train loss:0.3946457469958306\n",
      "train loss:0.5023912591675473\n",
      "train loss:0.3948783332057305\n",
      "train loss:0.5046047267885879\n",
      "train loss:0.512480848330136\n",
      "train loss:0.38890919283729014\n",
      "train loss:0.49910338782034\n",
      "train loss:0.6225885999404901\n",
      "train loss:0.22894163424987773\n",
      "train loss:0.7456618918912497\n",
      "train loss:0.32915634982897235\n",
      "train loss:0.7917363146300707\n",
      "train loss:0.4938971201882435\n",
      "train loss:0.493634263921715\n",
      "train loss:0.6554429092733673\n",
      "train loss:0.6154325514287506\n",
      "train loss:0.753934900599093\n",
      "train loss:0.5117584593692204\n",
      "train loss:0.8516212814987856\n",
      "train loss:0.49831069219117763\n",
      "train loss:0.3013513373652052\n",
      "train loss:0.40179548118136504\n",
      "train loss:0.5803763462966633\n",
      "train loss:0.7077759047620421\n",
      "train loss:0.71560311574662\n",
      "train loss:0.620224020655904\n",
      "train loss:0.7121303922365194\n",
      "train loss:0.5257589658962653\n",
      "train loss:0.6880382148275532\n",
      "train loss:0.44548857190419006\n",
      "train loss:0.5255120970011331\n",
      "train loss:0.6131463521702047\n",
      "train loss:0.5932520336155621\n",
      "train loss:0.8460345930561731\n",
      "train loss:0.692229516849357\n",
      "train loss:0.536462528994484\n",
      "train loss:0.7492775969256169\n",
      "train loss:0.5522740442536745\n",
      "train loss:0.47120518663069044\n",
      "train loss:0.597396174759186\n",
      "train loss:0.6227631513556701\n",
      "train loss:0.7440409874099925\n",
      "train loss:0.6770206536153274\n",
      "train loss:0.6111934188262813\n",
      "train loss:0.7603818361809164\n",
      "train loss:0.7549930451306855\n",
      "train loss:0.8089835924268449\n",
      "train loss:0.6155805894707149\n",
      "train loss:0.5627772320028381\n",
      "train loss:0.6640230189564548\n",
      "train loss:0.5033672444899912\n",
      "train loss:0.6275302347437672\n",
      "train loss:0.665597120978824\n",
      "train loss:0.6731324614005972\n",
      "train loss:0.681679449046692\n",
      "train loss:0.5614788938894218\n",
      "train loss:0.7279281775215823\n",
      "train loss:0.6788373941166126\n",
      "train loss:0.5030143653054717\n",
      "train loss:0.620846163242045\n",
      "train loss:0.6175378622086731\n",
      "train loss:0.622930024180768\n",
      "train loss:0.475848227395156\n",
      "train loss:0.6034673437442283\n",
      "train loss:0.5087247993285297\n",
      "train loss:0.4328053137633489\n",
      "train loss:0.5038138542936326\n",
      "train loss:0.7088458028599371\n",
      "train loss:0.7366505070735713\n",
      "train loss:0.7345467820370123\n",
      "train loss:0.6149268103949602\n",
      "train loss:0.38140638468729166\n",
      "train loss:0.2618990402965506\n",
      "train loss:0.9850362589830031\n",
      "train loss:0.6249904126502096\n",
      "train loss:0.4905130794593856\n",
      "train loss:0.3889158923373678\n",
      "train loss:0.38101975189172194\n",
      "train loss:0.6105492741766557\n",
      "train loss:0.3742758604934985\n",
      "train loss:0.9784600143468504\n",
      "train loss:0.6351038038632582\n",
      "train loss:0.4912279876505565\n",
      "train loss:0.5078941145239126\n",
      "train loss:1.0267629803819251\n",
      "train loss:0.5972539329858522\n",
      "train loss:0.6830306562264488\n",
      "train loss:0.6212069344079435\n",
      "train loss:0.669799477080383\n",
      "train loss:0.6060650607712353\n",
      "train loss:0.6858644367446762\n",
      "train loss:0.5496183160164084\n",
      "train loss:0.5578864789913689\n",
      "train loss:0.6061527687078687\n",
      "train loss:0.6619689092665129\n",
      "train loss:0.5564462225203642\n",
      "train loss:0.434340835636542\n",
      "train loss:0.49254629041206127\n",
      "train loss:0.6937888960759727\n",
      "train loss:0.46972683902771184\n",
      "train loss:0.7699584469086587\n",
      "train loss:0.6652962143669386\n",
      "train loss:0.4514757874315004\n",
      "train loss:0.6203450881263992\n",
      "train loss:0.6016978555365005\n",
      "train loss:0.6280972413644401\n",
      "train loss:0.6275965702799184\n",
      "train loss:0.7094806732583476\n",
      "train loss:0.7117511749482597\n",
      "train loss:0.42187279816499645\n",
      "train loss:0.8100367558428372\n",
      "train loss:0.5906106341921435\n",
      "train loss:0.6160131667678188\n",
      "train loss:0.6054117610864541\n",
      "train loss:0.6913553664707738\n",
      "train loss:0.6117433434319957\n",
      "train loss:0.6063290337258799\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5490196078431373\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=50, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1eb19e02-0b85-4e5e-b897-1e47b36bff79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6915285859618538\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6712874218279228\n",
      "train loss:0.6930089718227419\n",
      "train loss:0.6573711284883116\n",
      "train loss:0.619618406751931\n",
      "train loss:0.7291805605492454\n",
      "train loss:0.7426553164854911\n",
      "train loss:0.6635932375872062\n",
      "train loss:0.6302734678812467\n",
      "train loss:0.6767755604795154\n",
      "train loss:0.5896515493065783\n",
      "train loss:0.5628306969062276\n",
      "train loss:0.522675610533059\n",
      "train loss:0.6855303627175213\n",
      "train loss:0.45903694304905535\n",
      "train loss:0.3565530684145682\n",
      "train loss:0.68145310027813\n",
      "train loss:0.3406646027479913\n",
      "train loss:0.8681427861238074\n",
      "train loss:0.6424057359867591\n",
      "train loss:0.6976845919709276\n",
      "train loss:0.24439990616006088\n",
      "train loss:0.7485616760353639\n",
      "train loss:0.6022187434569496\n",
      "train loss:0.4228569096567976\n",
      "train loss:0.4340476547636383\n",
      "train loss:0.6887205768351352\n",
      "train loss:0.552364542083206\n",
      "train loss:0.6088776406460253\n",
      "train loss:0.4361076473214151\n",
      "train loss:0.5002955057862055\n",
      "train loss:0.5772268009805217\n",
      "train loss:0.6255345878740106\n",
      "train loss:0.47280415962198347\n",
      "train loss:0.5079232143068173\n",
      "train loss:0.6206614326695882\n",
      "train loss:0.7430053541024602\n",
      "train loss:0.5084328271133269\n",
      "train loss:0.5133035926469761\n",
      "train loss:0.7279662190841453\n",
      "train loss:0.7342198750053825\n",
      "train loss:0.8527660181128963\n",
      "train loss:0.669302263354702\n",
      "train loss:0.5664350145443714\n",
      "train loss:0.5457401406778961\n",
      "train loss:0.6414890328555426\n",
      "train loss:0.6851764522362715\n",
      "train loss:0.7411403489525075\n",
      "train loss:0.6483512060975981\n",
      "train loss:0.544522306251632\n",
      "train loss:0.6251587567574011\n",
      "train loss:0.5811965775698589\n",
      "train loss:0.5764952345506373\n",
      "train loss:0.5373330371626539\n",
      "train loss:0.5174004026067454\n",
      "train loss:0.598564206007054\n",
      "train loss:0.517291667664167\n",
      "train loss:0.8852725521352524\n",
      "train loss:0.4730847204867298\n",
      "train loss:0.7084407098753839\n",
      "train loss:0.3690478598873404\n",
      "train loss:0.7503618430676118\n",
      "train loss:0.9603117831039327\n",
      "train loss:0.7492343862913817\n",
      "train loss:0.5031624988383564\n",
      "train loss:0.5042764507746144\n",
      "train loss:0.7849239711824275\n",
      "train loss:0.5896846437335224\n",
      "train loss:0.6192727201420529\n",
      "train loss:0.6423450957016833\n",
      "train loss:0.6381497739440156\n",
      "train loss:0.644010338332567\n",
      "train loss:0.6351595715331219\n",
      "train loss:0.5854026003946264\n",
      "train loss:0.7640201859564147\n",
      "train loss:0.530418895360274\n",
      "train loss:0.6273168898739361\n",
      "train loss:0.6282819293883726\n",
      "train loss:0.7418792391950034\n",
      "train loss:0.5341611927858985\n",
      "train loss:0.6975047305735504\n",
      "train loss:0.6674431284221403\n",
      "train loss:0.484990214990691\n",
      "train loss:0.5139400380271628\n",
      "train loss:0.6260172243725268\n",
      "train loss:0.5975366176884045\n",
      "train loss:0.6844902113773215\n",
      "train loss:0.8463589652548947\n",
      "train loss:0.6140147556024405\n",
      "train loss:0.5823650009415858\n",
      "train loss:0.7324518885506117\n",
      "train loss:0.7269052086074773\n",
      "train loss:0.7074678734573114\n",
      "train loss:0.6369811261113074\n",
      "train loss:0.628278300317591\n",
      "train loss:0.5956037441815545\n",
      "train loss:0.6000431736205434\n",
      "train loss:0.6630561625920075\n",
      "train loss:0.5964681523868186\n",
      "train loss:0.5895460633597231\n",
      "train loss:0.576917456622445\n",
      "train loss:0.6622330539410224\n",
      "train loss:0.7348721782817905\n",
      "train loss:0.6609424619865951\n",
      "train loss:0.6686284265663994\n",
      "train loss:0.4239377501194637\n",
      "train loss:0.5445863461661248\n",
      "train loss:0.6021960795314396\n",
      "train loss:0.4034993399186084\n",
      "train loss:0.5658685651899809\n",
      "train loss:0.8465318227598806\n",
      "train loss:0.5416823923509543\n",
      "train loss:0.2943704779797684\n",
      "train loss:0.632945880157332\n",
      "train loss:0.6605205861382689\n",
      "train loss:0.7085811740673335\n",
      "train loss:0.35192831042525186\n",
      "train loss:0.46023981852904133\n",
      "train loss:0.6371914645694151\n",
      "train loss:0.7316623246447337\n",
      "train loss:0.8645931824140526\n",
      "train loss:0.5485898228362748\n",
      "train loss:0.629896949145616\n",
      "train loss:0.5630830211178564\n",
      "train loss:0.45444843946029023\n",
      "train loss:0.45249100734441405\n",
      "train loss:0.42293906662697606\n",
      "train loss:0.6043725993640215\n",
      "train loss:0.97096415665736\n",
      "train loss:0.5334900518300192\n",
      "train loss:0.28450251844679864\n",
      "train loss:0.47912136621403656\n",
      "train loss:0.7083000436552929\n",
      "train loss:0.5180658826302074\n",
      "train loss:0.5768801792232243\n",
      "train loss:0.5326636514650414\n",
      "train loss:0.5328537418412709\n",
      "train loss:0.8997805633932916\n",
      "train loss:0.2699157574791348\n",
      "train loss:0.48213622076928325\n",
      "train loss:0.6037007393488583\n",
      "train loss:0.6307171283893188\n",
      "train loss:0.8609099865879379\n",
      "train loss:0.5363196540651828\n",
      "train loss:0.42717617497706\n",
      "train loss:0.6574112635308026\n",
      "train loss:0.36463136452899436\n",
      "train loss:0.5328513586880824\n",
      "train loss:0.6859647656743366\n",
      "train loss:0.649399394439437\n",
      "train loss:0.5150308387191427\n",
      "train loss:0.57952688252582\n",
      "train loss:0.7551701678326139\n",
      "train loss:0.7020992534575939\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.4796586217054289\n",
      "train loss:0.6170138962745497\n",
      "train loss:0.6634630294830409\n",
      "train loss:0.4523151060471178\n",
      "train loss:0.7665847573023634\n",
      "train loss:0.4269796904527027\n",
      "train loss:0.6441161039624693\n",
      "train loss:0.6883905982449557\n",
      "train loss:0.6405776574825673\n",
      "train loss:0.603493794428302\n",
      "train loss:0.7037936690365744\n",
      "train loss:0.5902768622670803\n",
      "train loss:0.4825131512433531\n",
      "train loss:0.61357384930288\n",
      "train loss:0.7050202108942253\n",
      "train loss:0.5017662475017957\n",
      "train loss:0.5962197599557714\n",
      "train loss:0.40803402676197303\n",
      "train loss:0.2969095236476415\n",
      "train loss:0.4610392419371097\n",
      "train loss:0.5438808117020855\n",
      "train loss:0.4902993095206282\n",
      "train loss:0.5924694215607005\n",
      "train loss:0.516603026529728\n",
      "train loss:1.0311559305156641\n",
      "train loss:0.19491718762254825\n",
      "train loss:0.3333742991204472\n",
      "train loss:0.5021371494384858\n",
      "train loss:0.7372290105987513\n",
      "train loss:0.678169818271029\n",
      "train loss:0.38638799609940805\n",
      "train loss:0.5088030959535659\n",
      "train loss:0.42338538314612784\n",
      "train loss:0.6673020686906417\n",
      "train loss:0.3377359170207005\n",
      "train loss:0.4937088339032947\n",
      "train loss:0.35888611933455783\n",
      "train loss:0.39860354953644805\n",
      "train loss:0.5319323947032236\n",
      "train loss:0.7248755854438493\n",
      "train loss:0.708392497322097\n",
      "train loss:0.5599213293972335\n",
      "train loss:0.49338812274008903\n",
      "train loss:0.48000345387183146\n",
      "train loss:0.6081585834803489\n",
      "train loss:0.7270003271194255\n",
      "train loss:0.63882777174945\n",
      "train loss:0.48577392987309576\n",
      "train loss:0.5369661973361347\n",
      "train loss:0.5676670260596748\n",
      "train loss:0.6294784893371799\n",
      "train loss:0.5414007626712962\n",
      "train loss:0.47961023616255477\n",
      "train loss:0.6583581612127979\n",
      "train loss:0.5133087475354329\n",
      "train loss:0.5819528037520678\n",
      "train loss:0.6387860736247849\n",
      "train loss:0.4160906511841527\n",
      "train loss:0.49837260390056287\n",
      "train loss:0.652697373735623\n",
      "train loss:0.3561866042616778\n",
      "train loss:0.4980202534521232\n",
      "train loss:0.3486950606244846\n",
      "train loss:0.7520940886533585\n",
      "train loss:0.7937289162249777\n",
      "train loss:0.4978317403027502\n",
      "train loss:0.7000310245808405\n",
      "train loss:0.4531250048582054\n",
      "train loss:0.8213566244989641\n",
      "train loss:0.6738767514397918\n",
      "train loss:0.501995703238809\n",
      "train loss:0.7292212607042526\n",
      "train loss:0.5018486497842892\n",
      "train loss:0.7480846275595827\n",
      "train loss:0.7091001674378601\n",
      "train loss:0.5330512377871662\n",
      "train loss:0.5631571028523021\n",
      "train loss:0.6129244003446966\n",
      "train loss:0.5734436868753644\n",
      "train loss:0.6825966539659707\n",
      "train loss:0.6188905430081936\n",
      "train loss:0.6338930553423903\n",
      "train loss:0.6854699965390582\n",
      "train loss:0.5050090566222587\n",
      "train loss:0.4272706733513266\n",
      "train loss:0.5116379868076428\n",
      "train loss:0.5934848998681355\n",
      "train loss:0.7333937260685841\n",
      "train loss:0.7165011537298229\n",
      "train loss:0.5089643185352521\n",
      "train loss:0.45686352841642713\n",
      "train loss:0.621356586395447\n",
      "train loss:0.4609005280508526\n",
      "train loss:1.1250550539624204\n",
      "train loss:0.5225456842515649\n",
      "train loss:0.533887586264121\n",
      "train loss:0.6449830896335083\n",
      "train loss:0.6108202135621087\n",
      "train loss:0.48464750520637284\n",
      "train loss:0.620862794977777\n",
      "train loss:0.5189480045094006\n",
      "train loss:0.6502711620412372\n",
      "train loss:0.5159986048981298\n",
      "train loss:0.5904127931611473\n",
      "train loss:0.6745488407734965\n",
      "train loss:0.7195910153329571\n",
      "train loss:0.6121690658163437\n",
      "train loss:0.7361893403972343\n",
      "train loss:0.6002618643816303\n",
      "train loss:0.587341699530963\n",
      "train loss:0.6968023335590451\n",
      "train loss:0.6237111410626877\n",
      "train loss:0.5561607519059385\n",
      "train loss:0.6961004341198592\n",
      "train loss:0.5403482619533169\n",
      "train loss:0.4718010917495998\n",
      "train loss:0.751234697300123\n",
      "train loss:0.6125463788926184\n",
      "train loss:0.6383914487129112\n",
      "train loss:0.4414778540966632\n",
      "train loss:0.7091586067456422\n",
      "train loss:0.5996602432236027\n",
      "train loss:0.5321409584255132\n",
      "train loss:0.7099638902291932\n",
      "train loss:0.5629547425006473\n",
      "train loss:0.7734940993606925\n",
      "train loss:0.4750548980299463\n",
      "train loss:0.6602126655348691\n",
      "train loss:0.5218296611162019\n",
      "train loss:0.5394585366378302\n",
      "train loss:0.5978540527759104\n",
      "train loss:0.5497528617142284\n",
      "train loss:0.7198531395991685\n",
      "train loss:0.6682908337746137\n",
      "train loss:0.6233871490850087\n",
      "train loss:0.5026996187754402\n",
      "train loss:0.8351197013277712\n",
      "train loss:0.6085986587103343\n",
      "train loss:0.6008340212732899\n",
      "train loss:0.4119863918878533\n",
      "train loss:0.5602447791438045\n",
      "train loss:0.6136189070168583\n",
      "train loss:0.5886642284642172\n",
      "train loss:0.5337348948660378\n",
      "train loss:0.5214913102687043\n",
      "train loss:0.6217776337334838\n",
      "train loss:0.6814982609710875\n",
      "train loss:0.6086873256413621\n",
      "train loss:0.4925826068732736\n",
      "train loss:0.6424658399692962\n",
      "train loss:0.8030777377443113\n",
      "train loss:0.5843008308252657\n",
      "train loss:0.7892623223683474\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7656501381060519\n",
      "train loss:0.6314911835319185\n",
      "train loss:0.634986988616185\n",
      "train loss:0.6709029248317837\n",
      "train loss:0.5219092135649686\n",
      "train loss:0.5909024523712465\n",
      "train loss:0.641423973133712\n",
      "train loss:0.49768855220092095\n",
      "train loss:0.5201793618282939\n",
      "train loss:0.6064827255781707\n",
      "train loss:0.6464014962021702\n",
      "train loss:0.5940807724108292\n",
      "train loss:0.48406835396620884\n",
      "train loss:0.5800751333516924\n",
      "train loss:0.37889330846177965\n",
      "train loss:0.4937035966942197\n",
      "train loss:0.6500732665014135\n",
      "train loss:0.5262787657151318\n",
      "train loss:0.4611654385715581\n",
      "train loss:0.653056469524679\n",
      "train loss:0.6850645696073058\n",
      "train loss:0.9813186377014127\n",
      "train loss:0.6805937696157714\n",
      "train loss:0.6007698140194446\n",
      "train loss:0.5239501126404233\n",
      "train loss:0.5955694634969456\n",
      "train loss:0.40018910744171476\n",
      "train loss:0.5672063084094938\n",
      "train loss:0.36967077183117275\n",
      "train loss:0.4847166725456771\n",
      "train loss:0.5217678975255488\n",
      "train loss:0.5212452910563752\n",
      "train loss:0.6221208315742857\n",
      "train loss:0.5630419848911015\n",
      "train loss:0.3380026988956387\n",
      "train loss:0.6774296104058832\n",
      "train loss:0.4622537953910325\n",
      "train loss:0.5202472636835371\n",
      "train loss:0.5549280857502151\n",
      "train loss:0.5884254160881446\n",
      "train loss:0.6567098665399296\n",
      "train loss:0.3301322039974124\n",
      "train loss:0.7897193414944519\n",
      "train loss:0.644347117012193\n",
      "train loss:0.5579231816050605\n",
      "train loss:0.39171837240297125\n",
      "train loss:0.6279996988236387\n",
      "train loss:0.639959423052493\n",
      "train loss:0.6118810924377349\n",
      "train loss:0.5190155110652221\n",
      "train loss:0.7941069160961473\n",
      "train loss:0.6860675916899753\n",
      "train loss:0.6822107159866894\n",
      "train loss:0.5389091899997605\n",
      "train loss:0.6208978055087349\n",
      "train loss:0.5633658399325887\n",
      "train loss:0.576638272352252\n",
      "train loss:0.5914068879007854\n",
      "train loss:0.568262076622169\n",
      "train loss:0.496110862666401\n",
      "train loss:0.5627922696338552\n",
      "train loss:0.5698850238039027\n",
      "train loss:0.6106330356974606\n",
      "train loss:0.40827044872454127\n",
      "train loss:0.8650889213121514\n",
      "train loss:0.43216162082720644\n",
      "train loss:0.6085727415489298\n",
      "train loss:0.29543235739012064\n",
      "train loss:0.8350706092155471\n",
      "train loss:0.900172945791416\n",
      "train loss:0.37643558396794347\n",
      "train loss:0.7291704883524138\n",
      "train loss:0.5445362584715083\n",
      "train loss:0.5714268811889562\n",
      "train loss:0.6977178624183279\n",
      "train loss:0.5701104740697825\n",
      "train loss:0.5417645740532759\n",
      "train loss:0.8087631472322393\n",
      "train loss:0.5879702038043536\n",
      "train loss:0.4711777389178235\n",
      "train loss:0.6027664526275456\n",
      "train loss:0.6110357059462747\n",
      "train loss:0.6287491231337666\n",
      "train loss:0.5897723996514406\n",
      "train loss:0.5013818370970458\n",
      "train loss:0.5296620325052721\n",
      "train loss:0.5521545247783759\n",
      "train loss:0.5001016276196207\n",
      "train loss:0.3327490298145617\n",
      "train loss:0.6344537339464156\n",
      "train loss:0.5902410942027345\n",
      "train loss:0.5823634294184518\n",
      "train loss:0.8160508837818556\n",
      "train loss:0.5562478281399236\n",
      "train loss:0.5139240282076889\n",
      "train loss:0.48993590749621674\n",
      "train loss:0.7704528546151617\n",
      "train loss:0.5426600158285206\n",
      "train loss:0.565435749170916\n",
      "train loss:0.5027504001714855\n",
      "train loss:0.5200717642450547\n",
      "train loss:0.6337696334028429\n",
      "train loss:0.60627066908046\n",
      "train loss:0.6211557603239465\n",
      "train loss:0.5572454515391516\n",
      "train loss:0.6799484808802045\n",
      "train loss:0.5723470308503926\n",
      "train loss:0.5678536269412531\n",
      "train loss:0.472108883698193\n",
      "train loss:0.49515935638034747\n",
      "train loss:0.45729570958842025\n",
      "train loss:0.5309134945045828\n",
      "train loss:0.6098552757490291\n",
      "train loss:0.508623986163087\n",
      "train loss:0.39210609063148005\n",
      "train loss:0.5472505435840676\n",
      "train loss:0.3996154399621036\n",
      "train loss:0.5817686562951997\n",
      "train loss:0.8648551202403285\n",
      "train loss:0.33639686974329697\n",
      "train loss:0.7773393386900582\n",
      "train loss:0.4487761787673933\n",
      "train loss:0.4817242091590958\n",
      "train loss:0.5934831899801426\n",
      "train loss:0.6559520143215283\n",
      "train loss:0.5121592243853457\n",
      "train loss:0.5461550232902697\n",
      "train loss:0.42757810577398114\n",
      "train loss:0.4297485291358809\n",
      "train loss:0.5145440783184836\n",
      "train loss:0.6486743220473652\n",
      "train loss:0.6221312403561897\n",
      "train loss:0.6264849035989493\n",
      "train loss:0.7902188273914875\n",
      "train loss:0.38627581284393486\n",
      "train loss:0.45081067765571053\n",
      "train loss:0.6987931524681555\n",
      "train loss:0.7709635280382152\n",
      "train loss:0.5288488778388787\n",
      "train loss:0.4988451939040061\n",
      "train loss:0.450201581049644\n",
      "train loss:0.8056210730120089\n",
      "train loss:0.6442905582532823\n",
      "train loss:0.6221330621469491\n",
      "train loss:0.41890087291773925\n",
      "train loss:0.6641692313583014\n",
      "train loss:0.6211779800363344\n",
      "train loss:0.5300922640123431\n",
      "train loss:0.5840211053550866\n",
      "train loss:0.5211371711352112\n",
      "train loss:0.5209868544510565\n",
      "train loss:0.5125524046548262\n",
      "train loss:0.47842765941578236\n",
      "=== epoch:4, train acc:0.73, test acc:0.68 ===\n",
      "train loss:0.6969179477662852\n",
      "train loss:0.4316340929458657\n",
      "train loss:0.4747370893577094\n",
      "train loss:0.7948710435283093\n",
      "train loss:0.5264784914530928\n",
      "train loss:0.5995017710365602\n",
      "train loss:0.5045267755902827\n",
      "train loss:0.992775398773948\n",
      "train loss:0.588405093008519\n",
      "train loss:0.6263747861292706\n",
      "train loss:0.5433960694388934\n",
      "train loss:0.5526288769003934\n",
      "train loss:0.5963155421376137\n",
      "train loss:0.8065033527395608\n",
      "train loss:0.6820963972089619\n",
      "train loss:0.6402260440008885\n",
      "train loss:0.5477872092066514\n",
      "train loss:0.5961767861573637\n",
      "train loss:0.5591663126288668\n",
      "train loss:0.6846053320291667\n",
      "train loss:0.6525628104493546\n",
      "train loss:0.5784718323893093\n",
      "train loss:0.7365728319628188\n",
      "train loss:0.7403874187689222\n",
      "train loss:0.5421074545652028\n",
      "train loss:0.500605194961097\n",
      "train loss:0.6619835832258072\n",
      "train loss:0.6256424837449853\n",
      "train loss:0.623656303256793\n",
      "train loss:0.5812982869757207\n",
      "train loss:0.6766423999419606\n",
      "train loss:0.6151500055596486\n",
      "train loss:0.5143806000527819\n",
      "train loss:0.5364513102553772\n",
      "train loss:0.5960076242626084\n",
      "train loss:0.6338396780880833\n",
      "train loss:0.349194942118361\n",
      "train loss:0.6251694768372136\n",
      "train loss:0.5370357964216607\n",
      "train loss:0.7108228500349879\n",
      "train loss:0.6352791805914413\n",
      "train loss:0.6367776305525731\n",
      "train loss:0.6994028166444702\n",
      "train loss:0.5530938510315577\n",
      "train loss:0.651449208450885\n",
      "train loss:0.466522822911247\n",
      "train loss:0.7135754322307583\n",
      "train loss:0.6010709172116375\n",
      "train loss:0.5986220505630435\n",
      "train loss:0.428143097769259\n",
      "train loss:0.42783265489074973\n",
      "train loss:0.6734218975759495\n",
      "train loss:0.584159597551155\n",
      "train loss:0.5624650809898941\n",
      "train loss:0.5878686564617949\n",
      "train loss:0.572644823156014\n",
      "train loss:0.44823894823304417\n",
      "train loss:0.8472427101008293\n",
      "train loss:0.7012540063444241\n",
      "train loss:0.5568079391367142\n",
      "train loss:0.42495279199483155\n",
      "train loss:0.5074944715530968\n",
      "train loss:0.6966428584581964\n",
      "train loss:0.42802503016007476\n",
      "train loss:0.40892414786188996\n",
      "train loss:0.5990460280839571\n",
      "train loss:0.4978207483929104\n",
      "train loss:0.7742985908808044\n",
      "train loss:0.5135099621383717\n",
      "train loss:0.5992708891251209\n",
      "train loss:0.43339306539956324\n",
      "train loss:0.4228497422973131\n",
      "train loss:0.7397548383129929\n",
      "train loss:0.4597418123394445\n",
      "train loss:0.770636167807524\n",
      "train loss:0.7208885035888529\n",
      "train loss:0.2818593941403915\n",
      "train loss:0.7346101556449764\n",
      "train loss:0.6946423682401217\n",
      "train loss:0.4215922823146854\n",
      "train loss:0.4712506561216772\n",
      "train loss:0.6201116432783788\n",
      "train loss:0.5535001942486707\n",
      "train loss:0.6773851961027995\n",
      "train loss:0.6390987258146026\n",
      "train loss:0.42170048534121174\n",
      "train loss:0.48440520468752946\n",
      "train loss:0.62572840711109\n",
      "train loss:0.4008704271883956\n",
      "train loss:0.7060639286069078\n",
      "train loss:0.5302469183242896\n",
      "train loss:0.6416377324691556\n",
      "train loss:0.5330304985846155\n",
      "train loss:0.693070850960134\n",
      "train loss:0.3368917115705965\n",
      "train loss:0.371539759211631\n",
      "train loss:0.5995290362578507\n",
      "train loss:0.7032131104808541\n",
      "train loss:0.5143630301365049\n",
      "train loss:0.9169258134917516\n",
      "train loss:0.5108042229614252\n",
      "train loss:0.5858851919531405\n",
      "train loss:0.48881144638832447\n",
      "train loss:0.7743676682450105\n",
      "train loss:0.46484236288575775\n",
      "train loss:0.6687985604337812\n",
      "train loss:0.4754918325315966\n",
      "train loss:0.6439439473601898\n",
      "train loss:0.5486575000664065\n",
      "train loss:0.6293789649858986\n",
      "train loss:0.5949158757030564\n",
      "train loss:0.6017366818537073\n",
      "train loss:0.5674358587905274\n",
      "train loss:0.4990440678952058\n",
      "train loss:0.6434830558519133\n",
      "train loss:0.6122037172022432\n",
      "train loss:0.6843626899672102\n",
      "train loss:0.5762541018642109\n",
      "train loss:0.6931791256355874\n",
      "train loss:0.6232451601538804\n",
      "train loss:0.547477280436209\n",
      "train loss:0.5462723870109605\n",
      "train loss:0.708454353938736\n",
      "train loss:0.4780723287086355\n",
      "train loss:0.4482428451211957\n",
      "train loss:0.5648011945501377\n",
      "train loss:0.6257174088634162\n",
      "train loss:0.39817069139093064\n",
      "train loss:0.6416191260389498\n",
      "train loss:0.49372790059917226\n",
      "train loss:0.7277710579526245\n",
      "train loss:0.31294595256815927\n",
      "train loss:0.8268903120671715\n",
      "train loss:0.50510089220319\n",
      "train loss:0.6465845792613064\n",
      "train loss:0.49251264650975246\n",
      "train loss:0.3779708613304252\n",
      "train loss:0.7216518800303322\n",
      "train loss:0.9890915715587543\n",
      "train loss:0.5965822226442345\n",
      "train loss:0.46796494333434085\n",
      "train loss:0.43327945206872165\n",
      "train loss:0.7695792385744052\n",
      "train loss:0.5319081512350653\n",
      "train loss:0.6713926777161092\n",
      "train loss:0.5356769637178077\n",
      "train loss:0.3877669442342441\n",
      "train loss:0.6417013135028895\n",
      "train loss:0.48273293671139167\n",
      "train loss:0.6870020822939823\n",
      "train loss:0.4867222699522394\n",
      "train loss:0.4376162458324093\n",
      "train loss:0.4302815083556176\n",
      "=== epoch:5, train acc:0.76, test acc:0.69 ===\n",
      "train loss:0.4163062355136683\n",
      "train loss:0.557411940669669\n",
      "train loss:0.3329999967178499\n",
      "train loss:0.6937969217906493\n",
      "train loss:0.508691757050135\n",
      "train loss:0.7087736172605806\n",
      "train loss:0.43000620618976904\n",
      "train loss:0.6567373305635273\n",
      "train loss:0.6161592600510036\n",
      "train loss:0.40140693628874624\n",
      "train loss:0.7768726911659285\n",
      "train loss:0.5237343623226126\n",
      "train loss:0.6887042551960179\n",
      "train loss:0.3797692466895221\n",
      "train loss:0.7351133272410653\n",
      "train loss:0.5213873697067719\n",
      "train loss:0.5697761504658172\n",
      "train loss:0.5424849477189821\n",
      "train loss:0.5141029915357678\n",
      "train loss:0.5487569784201081\n",
      "train loss:0.3687195809127579\n",
      "train loss:0.5322554181151598\n",
      "train loss:0.9214119348685104\n",
      "train loss:0.7043126754266572\n",
      "train loss:0.5149334909163792\n",
      "train loss:0.7366899060997064\n",
      "train loss:0.625929761051211\n",
      "train loss:0.5625547132697662\n",
      "train loss:0.6649306231547205\n",
      "train loss:0.4952031577484807\n",
      "train loss:0.6854026926634098\n",
      "train loss:0.32770775103599076\n",
      "train loss:0.47133948109143653\n",
      "train loss:0.4805831750456063\n",
      "train loss:0.5250394665293703\n",
      "train loss:0.6222153708107847\n",
      "train loss:0.6048014060761295\n",
      "train loss:0.7262312762850296\n",
      "train loss:0.3859887752437984\n",
      "train loss:0.751557432643227\n",
      "train loss:0.6342706423491759\n",
      "train loss:0.4705429511963325\n",
      "train loss:0.49085878131433824\n",
      "train loss:0.6098768426683276\n",
      "train loss:0.46394513268999404\n",
      "train loss:0.47973550603695714\n",
      "train loss:0.521607038986408\n",
      "train loss:0.5444903533228341\n",
      "train loss:0.3966260526193735\n",
      "train loss:0.4812100977849731\n",
      "train loss:0.6214081368450419\n",
      "train loss:0.6026509635341869\n",
      "train loss:0.35019818955589205\n",
      "train loss:0.846508797518721\n",
      "train loss:0.6481863888933006\n",
      "train loss:0.7217748048428052\n",
      "train loss:0.7315362920326924\n",
      "train loss:0.4528971126175196\n",
      "train loss:0.8587457440694164\n",
      "train loss:0.6461700836555437\n",
      "train loss:0.5607786269895814\n",
      "train loss:0.6278599726174705\n",
      "train loss:0.5276270290384231\n",
      "train loss:0.5307796988406264\n",
      "train loss:0.6040612221160104\n",
      "train loss:0.4815315472831519\n",
      "train loss:0.5629134424278829\n",
      "train loss:0.6347625376486791\n",
      "train loss:0.5452281602825698\n",
      "train loss:0.6889713461352119\n",
      "train loss:0.4331993692554497\n",
      "train loss:0.5721186800051936\n",
      "train loss:0.6058679379189943\n",
      "train loss:0.662839297731247\n",
      "train loss:0.5319067576536358\n",
      "train loss:0.4718863896288399\n",
      "train loss:0.7054550572929793\n",
      "train loss:0.522328319468506\n",
      "train loss:0.562449094600261\n",
      "train loss:0.6291610218987865\n",
      "train loss:0.6793419868345756\n",
      "train loss:0.6307352615493448\n",
      "train loss:0.5477523379568091\n",
      "train loss:0.4966463330310608\n",
      "train loss:0.6527625761353091\n",
      "train loss:0.6758721263447157\n",
      "train loss:0.5994341876369165\n",
      "train loss:0.6962807547392651\n",
      "train loss:0.6191054929087916\n",
      "train loss:0.5492364023016102\n",
      "train loss:0.536929185110263\n",
      "train loss:0.6261103446475691\n",
      "train loss:0.5779303355573642\n",
      "train loss:0.6409185359362031\n",
      "train loss:0.768265241076894\n",
      "train loss:0.5388691788218827\n",
      "train loss:0.6684214291391122\n",
      "train loss:0.4309533253304739\n",
      "train loss:0.49372983781151036\n",
      "train loss:0.3812869819168724\n",
      "train loss:0.732756258908162\n",
      "train loss:0.580479928884069\n",
      "train loss:0.6957033798772138\n",
      "train loss:0.7158008240097044\n",
      "train loss:0.5269527861787079\n",
      "train loss:0.6845038545780364\n",
      "train loss:0.581158230273894\n",
      "train loss:0.6265069514922268\n",
      "train loss:0.5041447249706079\n",
      "train loss:0.480411820179934\n",
      "train loss:0.5566452485479162\n",
      "train loss:0.591385724023817\n",
      "train loss:0.5277082942439837\n",
      "train loss:0.4129472669918429\n",
      "train loss:0.22330650058159138\n",
      "train loss:0.19844796403795706\n",
      "train loss:0.5478391943233831\n",
      "train loss:0.8901956393523577\n",
      "train loss:0.35202581672938793\n",
      "train loss:0.40042143406760716\n",
      "train loss:0.37582904734001454\n",
      "train loss:0.8122016550659085\n",
      "train loss:0.9172887518667814\n",
      "train loss:0.22194909560685314\n",
      "train loss:0.7499673276148405\n",
      "train loss:0.7183012288150674\n",
      "train loss:0.6787451352327671\n",
      "train loss:0.9067987072339012\n",
      "train loss:0.48963282769824723\n",
      "train loss:0.533718779982455\n",
      "train loss:0.6407926609739834\n",
      "train loss:0.5935824485445356\n",
      "train loss:0.6660439671463428\n",
      "train loss:0.5174883319652739\n",
      "train loss:0.5177784849819649\n",
      "train loss:0.5438324304223686\n",
      "train loss:0.39837015366632955\n",
      "train loss:0.5742783396018389\n",
      "train loss:0.5050364991401657\n",
      "train loss:0.7310038661146429\n",
      "train loss:0.5466298867991982\n",
      "train loss:0.5019467959547292\n",
      "train loss:0.6004404968762497\n",
      "train loss:0.6707439518184477\n",
      "train loss:0.4923567112846345\n",
      "train loss:0.4291219009758332\n",
      "train loss:0.5150871988917786\n",
      "train loss:0.6332847253432479\n",
      "train loss:0.5871717769076209\n",
      "train loss:0.4804029773955715\n",
      "train loss:0.4423906855198586\n",
      "train loss:0.5694804862564574\n",
      "train loss:0.48472076659370994\n",
      "=== epoch:6, train acc:0.77, test acc:0.69 ===\n",
      "train loss:0.2644156888401488\n",
      "train loss:0.3357016868174806\n",
      "train loss:0.25858531731980233\n",
      "train loss:0.6094580563972849\n",
      "train loss:0.5574682305940106\n",
      "train loss:0.7084067888768966\n",
      "train loss:0.6681447714127801\n",
      "train loss:0.5214701748742261\n",
      "train loss:0.31716679566296524\n",
      "train loss:0.37221727083450634\n",
      "train loss:0.5414076217667866\n",
      "train loss:0.5416243838824923\n",
      "train loss:0.5508065892399399\n",
      "train loss:0.7604966867104792\n",
      "train loss:0.5530774935115604\n",
      "train loss:0.5622089248753959\n",
      "train loss:0.5416460998677287\n",
      "train loss:0.7010513252926727\n",
      "train loss:0.468391589255142\n",
      "train loss:0.7027158300592697\n",
      "train loss:0.4645486945748397\n",
      "train loss:0.4832428258096061\n",
      "train loss:0.708866755448861\n",
      "train loss:0.6896233399907487\n",
      "train loss:0.6802875041162257\n",
      "train loss:0.44267873990504986\n",
      "train loss:0.427323308230714\n",
      "train loss:0.5269740385785846\n",
      "train loss:0.4139116110044698\n",
      "train loss:0.7939322740433785\n",
      "train loss:0.6517953838244297\n",
      "train loss:0.5174500142989247\n",
      "train loss:0.8340275906595082\n",
      "train loss:0.48967999291377884\n",
      "train loss:0.47022683822183564\n",
      "train loss:0.7724813427219087\n",
      "train loss:0.48461332706780214\n",
      "train loss:0.5282737927394651\n",
      "train loss:0.5716759901436428\n",
      "train loss:0.6948119631361076\n",
      "train loss:0.6851664205522126\n",
      "train loss:0.7182071705824092\n",
      "train loss:0.529899199227932\n",
      "train loss:0.4661288390470781\n",
      "train loss:0.756127999418766\n",
      "train loss:0.5458943781915659\n",
      "train loss:0.4163353755100515\n",
      "train loss:0.7122680168233486\n",
      "train loss:0.47257004130496433\n",
      "train loss:0.7715458860160378\n",
      "train loss:0.47766988185763043\n",
      "train loss:0.6796198997819137\n",
      "train loss:0.577071554020904\n",
      "train loss:0.40920872996566116\n",
      "train loss:0.4861574639876701\n",
      "train loss:0.5798608469327475\n",
      "train loss:0.4615314683666062\n",
      "train loss:0.5592757468605974\n",
      "train loss:0.7527337454488828\n",
      "train loss:0.4686226996161378\n",
      "train loss:0.5365089678679847\n",
      "train loss:0.6153273618133008\n",
      "train loss:0.5100855810344483\n",
      "train loss:0.37376008984842335\n",
      "train loss:0.6577132099070425\n",
      "train loss:0.33542892570254945\n",
      "train loss:0.5899032415789378\n",
      "train loss:0.5161755554146839\n",
      "train loss:0.5043706091628457\n",
      "train loss:0.44178669311400603\n",
      "train loss:0.37579232767368725\n",
      "train loss:0.6981023501503862\n",
      "train loss:0.5275927122836718\n",
      "train loss:0.5032110418448418\n",
      "train loss:0.6486663961256547\n",
      "train loss:0.6013218096800488\n",
      "train loss:0.5787466833787388\n",
      "train loss:0.4976258283643671\n",
      "train loss:0.4705757730996103\n",
      "train loss:0.45613837323627904\n",
      "train loss:0.3403099685130186\n",
      "train loss:0.40066190183288636\n",
      "train loss:0.46728683697942053\n",
      "train loss:0.7010166350336723\n",
      "train loss:0.49970273826860423\n",
      "train loss:0.6314371284388394\n",
      "train loss:0.41157773131191766\n",
      "train loss:0.5697436583158888\n",
      "train loss:0.3905869039931786\n",
      "train loss:0.7431423452082764\n",
      "train loss:0.5450077570097751\n",
      "train loss:0.465525993419727\n",
      "train loss:0.31348915936846755\n",
      "train loss:0.5248296091351853\n",
      "train loss:0.3540501540859921\n",
      "train loss:0.3713204113564955\n",
      "train loss:0.3561594811221746\n",
      "train loss:0.6931429610666305\n",
      "train loss:0.2949088840086109\n",
      "train loss:0.639726338754916\n",
      "train loss:0.9804795072991059\n",
      "train loss:0.6753551777776654\n",
      "train loss:0.5708166037537978\n",
      "train loss:0.7079621867103358\n",
      "train loss:0.8414040327249696\n",
      "train loss:0.8225954082537326\n",
      "train loss:0.4561085253386582\n",
      "train loss:0.3928022392496848\n",
      "train loss:0.41430287345047756\n",
      "train loss:0.5275615176369873\n",
      "train loss:0.48879963539490906\n",
      "train loss:0.4700855584561305\n",
      "train loss:0.5646332351511926\n",
      "train loss:0.6349446166587531\n",
      "train loss:0.4283844872468361\n",
      "train loss:0.3718773316980736\n",
      "train loss:0.7779347164051634\n",
      "train loss:0.3471041228893297\n",
      "train loss:0.6513511610376613\n",
      "train loss:0.6534905811336386\n",
      "train loss:0.617040609395527\n",
      "train loss:0.5013172974229123\n",
      "train loss:0.46775773664979\n",
      "train loss:0.5202434182062243\n",
      "train loss:0.3623909281514335\n",
      "train loss:0.5648061325337339\n",
      "train loss:0.4639443633089317\n",
      "train loss:0.49013601705880827\n",
      "train loss:0.626400246300303\n",
      "train loss:0.7914932193534636\n",
      "train loss:0.5965429241670819\n",
      "train loss:0.3394736216149948\n",
      "train loss:0.5170938462503957\n",
      "train loss:0.5983955609789663\n",
      "train loss:0.6634046730765395\n",
      "train loss:0.2698063553336828\n",
      "train loss:0.6134941196842307\n",
      "train loss:0.6979944492877443\n",
      "train loss:0.7606287640843117\n",
      "train loss:0.5403216964374238\n",
      "train loss:0.534224737099932\n",
      "train loss:0.8935649714237115\n",
      "train loss:0.4733710455484374\n",
      "train loss:0.511113800531885\n",
      "train loss:0.511052397770565\n",
      "train loss:0.7223040103549077\n",
      "train loss:0.7737113751502954\n",
      "train loss:0.3623243985049263\n",
      "train loss:0.637777841470672\n",
      "train loss:0.30545317964380486\n",
      "train loss:0.36064034732718386\n",
      "train loss:0.48031950395251155\n",
      "train loss:0.42665197448699904\n",
      "=== epoch:7, train acc:0.77, test acc:0.71 ===\n",
      "train loss:0.7297634374460437\n",
      "train loss:0.4487036995577934\n",
      "train loss:0.7687376203733653\n",
      "train loss:0.5568740502666232\n",
      "train loss:0.6317431889338461\n",
      "train loss:0.5499953929135216\n",
      "train loss:0.5507228792533868\n",
      "train loss:0.4573184112568106\n",
      "train loss:0.7113176648105939\n",
      "train loss:0.37634788171942424\n",
      "train loss:0.6010968470984619\n",
      "train loss:0.525473057259743\n",
      "train loss:0.43191013918903776\n",
      "train loss:0.5589803492408243\n",
      "train loss:0.4465555039796845\n",
      "train loss:0.35299763991028643\n",
      "train loss:0.7584194418317154\n",
      "train loss:0.6865366660567906\n",
      "train loss:0.6613602401263348\n",
      "train loss:0.5356074027637792\n",
      "train loss:0.6090384820592748\n",
      "train loss:0.5320752027987348\n",
      "train loss:0.6409164121460854\n",
      "train loss:0.7571472582695027\n",
      "train loss:0.4097268913191864\n",
      "train loss:0.5951687053017543\n",
      "train loss:0.39164863667854793\n",
      "train loss:0.6466121601660672\n",
      "train loss:0.5070711500545246\n",
      "train loss:0.5226296332979106\n",
      "train loss:0.5409408266797503\n",
      "train loss:0.4789902530608095\n",
      "train loss:0.6387708560246275\n",
      "train loss:0.29707558699804953\n",
      "train loss:0.4340339201122082\n",
      "train loss:0.7212450964040246\n",
      "train loss:0.7487156998243156\n",
      "train loss:0.3841254135619736\n",
      "train loss:0.6952890228614136\n",
      "train loss:0.563426569678396\n",
      "train loss:0.7003866090094126\n",
      "train loss:0.5386494102201332\n",
      "train loss:0.6139378247044762\n",
      "train loss:0.5376967603140631\n",
      "train loss:0.5843156293817263\n",
      "train loss:0.3768188184130391\n",
      "train loss:0.5820411409270767\n",
      "train loss:0.4548690028870207\n",
      "train loss:0.7391789911404181\n",
      "train loss:0.6394492436854167\n",
      "train loss:0.40634418262035765\n",
      "train loss:0.475540244560395\n",
      "train loss:0.703972176673666\n",
      "train loss:0.4226503079369562\n",
      "train loss:0.40061983371390414\n",
      "train loss:0.4675485623972543\n",
      "train loss:0.5832592281223608\n",
      "train loss:0.707259492358485\n",
      "train loss:0.6895120732495216\n",
      "train loss:0.5783877264418826\n",
      "train loss:0.731490495934118\n",
      "train loss:0.6869480147739792\n",
      "train loss:0.4961566783850063\n",
      "train loss:0.4298715441684962\n",
      "train loss:0.5442158065629992\n",
      "train loss:0.6290757317094243\n",
      "train loss:0.5656342162448491\n",
      "train loss:0.33944631322735386\n",
      "train loss:0.37696031878186786\n",
      "train loss:0.6768874163736285\n",
      "train loss:0.5596392876252922\n",
      "train loss:0.32893384482883986\n",
      "train loss:0.5173842656545039\n",
      "train loss:0.41537689960550966\n",
      "train loss:0.2838745403727077\n",
      "train loss:0.6280554088646214\n",
      "train loss:0.41286317889329666\n",
      "train loss:0.47119867335464505\n",
      "train loss:0.5077979552368064\n",
      "train loss:0.3985959199246382\n",
      "train loss:0.5952980616138828\n",
      "train loss:0.5046065692188939\n",
      "train loss:0.6798421516282638\n",
      "train loss:0.5154589861552206\n",
      "train loss:0.29646559148555685\n",
      "train loss:0.6592878102860324\n",
      "train loss:0.5031074800626131\n",
      "train loss:0.5095897654684433\n",
      "train loss:0.6035445692993593\n",
      "train loss:0.2770241042417128\n",
      "train loss:0.7109707372198838\n",
      "train loss:0.6274387411924953\n",
      "train loss:0.5917655611674262\n",
      "train loss:0.3368799649770417\n",
      "train loss:0.4417806770519026\n",
      "train loss:0.550818303158388\n",
      "train loss:0.47657437064081504\n",
      "train loss:0.48615048780455716\n",
      "train loss:0.44797421260382997\n",
      "train loss:1.0162472387574542\n",
      "train loss:0.40677616277359563\n",
      "train loss:0.787857776034522\n",
      "train loss:0.5973518639573849\n",
      "train loss:0.7878899398653673\n",
      "train loss:0.540698520555277\n",
      "train loss:0.43770383389295786\n",
      "train loss:0.5046438994001363\n",
      "train loss:0.30135584399112614\n",
      "train loss:0.5621054288044792\n",
      "train loss:0.6353139995157419\n",
      "train loss:0.5330621885985197\n",
      "train loss:0.8101029460261827\n",
      "train loss:0.7388046524941572\n",
      "train loss:0.2866764826827949\n",
      "train loss:0.7892295960755272\n",
      "train loss:0.5540666946276726\n",
      "train loss:0.6644278718307209\n",
      "train loss:0.3251357375172879\n",
      "train loss:0.6817949195235998\n",
      "train loss:0.4674861032540777\n",
      "train loss:0.6040140857742271\n",
      "train loss:0.5039436523258518\n",
      "train loss:0.4586947984330405\n",
      "train loss:0.45359814183165115\n",
      "train loss:0.5846997490728656\n",
      "train loss:0.5018053769872604\n",
      "train loss:0.5291713135140588\n",
      "train loss:0.6173489748514739\n",
      "train loss:0.3452916679413121\n",
      "train loss:0.5884337095663011\n",
      "train loss:0.42685046934173415\n",
      "train loss:0.37178944754244514\n",
      "train loss:0.7251718387539732\n",
      "train loss:0.6149845654708612\n",
      "train loss:0.44752070774743624\n",
      "train loss:0.30135338982484766\n",
      "train loss:0.6253927785489212\n",
      "train loss:0.553518292360874\n",
      "train loss:0.5027179341992534\n",
      "train loss:0.2729278516045281\n",
      "train loss:0.33841201884969263\n",
      "train loss:0.44574043053172074\n",
      "train loss:0.44173838535613674\n",
      "train loss:0.5398021786397781\n",
      "train loss:0.47687866912060023\n",
      "train loss:0.45101720083315333\n",
      "train loss:0.665343755310061\n",
      "train loss:0.29172188077387384\n",
      "train loss:0.3570133237460408\n",
      "train loss:0.6168481788340242\n",
      "train loss:0.5200397942644427\n",
      "train loss:0.5366576875177406\n",
      "train loss:0.5382876999681893\n",
      "=== epoch:8, train acc:0.77, test acc:0.71 ===\n",
      "train loss:0.6136852511510966\n",
      "train loss:0.5769036513693219\n",
      "train loss:0.4246757164966712\n",
      "train loss:0.5790516006024579\n",
      "train loss:0.37265356545843054\n",
      "train loss:0.4518624966058364\n",
      "train loss:0.4661916186698252\n",
      "train loss:0.622991887644905\n",
      "train loss:0.5364539721687728\n",
      "train loss:0.601334750650957\n",
      "train loss:0.7861660678871708\n",
      "train loss:0.6164763801700128\n",
      "train loss:0.673588375618036\n",
      "train loss:0.3100356688663056\n",
      "train loss:0.553212572989361\n",
      "train loss:0.5234376811550598\n",
      "train loss:0.7264102711806087\n",
      "train loss:0.37982959457564686\n",
      "train loss:0.40800732584432564\n",
      "train loss:0.5069750411537345\n",
      "train loss:0.5797644019376424\n",
      "train loss:0.43321376777559867\n",
      "train loss:0.6282521228091198\n",
      "train loss:0.43917848377089974\n",
      "train loss:0.5435619262915196\n",
      "train loss:0.5811935358828377\n",
      "train loss:0.5880988565502601\n",
      "train loss:0.19930490365306305\n",
      "train loss:0.5200990800402293\n",
      "train loss:0.44195448482828814\n",
      "train loss:0.673506035810609\n",
      "train loss:0.573814872470035\n",
      "train loss:0.28081775965416383\n",
      "train loss:0.5588325979943176\n",
      "train loss:0.7757613310670266\n",
      "train loss:0.6598514447750603\n",
      "train loss:0.35254745412253763\n",
      "train loss:0.6185613443206258\n",
      "train loss:0.8148951810211218\n",
      "train loss:0.7384200225457842\n",
      "train loss:0.37919678783364924\n",
      "train loss:0.7370226907926255\n",
      "train loss:0.4711092491653047\n",
      "train loss:0.3886144305509715\n",
      "train loss:0.3752861198540204\n",
      "train loss:0.5181185326338695\n",
      "train loss:0.6311179276717248\n",
      "train loss:0.5700324080682873\n",
      "train loss:0.6324958462380934\n",
      "train loss:0.47801661226197456\n",
      "train loss:0.2731370457693163\n",
      "train loss:0.6643056917884229\n",
      "train loss:0.5178103835805883\n",
      "train loss:0.47507477303734086\n",
      "train loss:0.5003111401128197\n",
      "train loss:0.4873273401598621\n",
      "train loss:0.5468383447709642\n",
      "train loss:0.38881208991441685\n",
      "train loss:0.531162922512967\n",
      "train loss:0.7927674941106316\n",
      "train loss:0.4375151081410283\n",
      "train loss:0.6431140113067537\n",
      "train loss:0.44756101534749454\n",
      "train loss:0.42868345963163684\n",
      "train loss:0.563744713363584\n",
      "train loss:0.47733779968228074\n",
      "train loss:0.6322680567170241\n",
      "train loss:0.6108563128050533\n",
      "train loss:0.28688748364261196\n",
      "train loss:0.7738403499472546\n",
      "train loss:0.347640284337612\n",
      "train loss:0.3811846459897455\n",
      "train loss:0.7277784028475679\n",
      "train loss:0.6116118276406152\n",
      "train loss:0.4954963447689945\n",
      "train loss:0.5075968218727216\n",
      "train loss:0.6279135480245037\n",
      "train loss:0.5484216592985895\n",
      "train loss:0.6209674124796869\n",
      "train loss:0.419871898875515\n",
      "train loss:0.7452186070110551\n",
      "train loss:0.7344484175995454\n",
      "train loss:0.4532133423118666\n",
      "train loss:0.4432948189066204\n",
      "train loss:0.34963071477911234\n",
      "train loss:0.5664796852811541\n",
      "train loss:0.5609317239978744\n",
      "train loss:0.4599424110212788\n",
      "train loss:0.5224555458525912\n",
      "train loss:0.6285112865394892\n",
      "train loss:0.4458236178688638\n",
      "train loss:0.5354491049810679\n",
      "train loss:0.6292020123263572\n",
      "train loss:0.532133366453618\n",
      "train loss:0.31704473915487374\n",
      "train loss:0.35335378580901333\n",
      "train loss:0.6521326630889883\n",
      "train loss:0.6214500610029227\n",
      "train loss:0.5607447752653829\n",
      "train loss:0.49964322024202834\n",
      "train loss:0.512095081192326\n",
      "train loss:0.637764208269073\n",
      "train loss:0.5156654788134611\n",
      "train loss:0.8182955141170746\n",
      "train loss:0.3320329432962371\n",
      "train loss:0.2815450726916682\n",
      "train loss:0.35180973120997266\n",
      "train loss:0.461132832942846\n",
      "train loss:0.4899492204604359\n",
      "train loss:0.31983442060684164\n",
      "train loss:0.36629471457327883\n",
      "train loss:0.27727815181915233\n",
      "train loss:0.3798705608924463\n",
      "train loss:0.6156446440853649\n",
      "train loss:0.5939736248068381\n",
      "train loss:0.49882465775904017\n",
      "train loss:0.3017162364688412\n",
      "train loss:0.49117415980092105\n",
      "train loss:0.20251888793541517\n",
      "train loss:0.5721036834870907\n",
      "train loss:0.6407787004127861\n",
      "train loss:0.4969807248555015\n",
      "train loss:0.651470636944247\n",
      "train loss:0.6041340616372636\n",
      "train loss:0.6915106034718389\n",
      "train loss:0.5883720506465329\n",
      "train loss:0.3791361013634468\n",
      "train loss:0.4371397758794869\n",
      "train loss:0.42483227336935975\n",
      "train loss:0.5594900570416457\n",
      "train loss:0.4519383941109303\n",
      "train loss:0.452553487705803\n",
      "train loss:0.219194080802944\n",
      "train loss:0.4940598221657284\n",
      "train loss:0.5112514861040433\n",
      "train loss:0.36978422697530916\n",
      "train loss:0.34592883229738003\n",
      "train loss:0.5247312380201256\n",
      "train loss:0.6215634676231272\n",
      "train loss:0.5871813957622048\n",
      "train loss:0.6093530887652463\n",
      "train loss:0.47247919297537483\n",
      "train loss:0.46286231802785444\n",
      "train loss:0.5388560679630234\n",
      "train loss:0.7896101412842633\n",
      "train loss:0.47122368050394636\n",
      "train loss:0.8835001295438577\n",
      "train loss:0.5650749655102122\n",
      "train loss:0.6410550478959386\n",
      "train loss:0.5882560738375602\n",
      "train loss:0.5268125679729279\n",
      "train loss:0.3636085357000363\n",
      "train loss:0.5939412354976628\n",
      "=== epoch:9, train acc:0.77, test acc:0.7 ===\n",
      "train loss:0.520913054861267\n",
      "train loss:0.5173830234481167\n",
      "train loss:0.793435555392843\n",
      "train loss:0.6076335404459078\n",
      "train loss:0.515352000952737\n",
      "train loss:0.6512864063450884\n",
      "train loss:0.5539428563108002\n",
      "train loss:0.5142687116463946\n",
      "train loss:0.4411967857682691\n",
      "train loss:0.3711631319722219\n",
      "train loss:0.4840548478698846\n",
      "train loss:0.44260163134059977\n",
      "train loss:0.43761166671455093\n",
      "train loss:0.5727558936466097\n",
      "train loss:0.7945504516597519\n",
      "train loss:0.5856453864458967\n",
      "train loss:0.4948476002424445\n",
      "train loss:0.295560223863037\n",
      "train loss:0.5811832758433539\n",
      "train loss:0.6748416137098606\n",
      "train loss:0.4645370833517619\n",
      "train loss:0.6033963620452965\n",
      "train loss:0.4752407051172586\n",
      "train loss:0.5347233032951831\n",
      "train loss:0.3434864338890523\n",
      "train loss:0.46119941628319233\n",
      "train loss:0.3675985424048514\n",
      "train loss:0.6034980038020037\n",
      "train loss:0.3197742842392147\n",
      "train loss:0.5832919113192843\n",
      "train loss:0.5754717665938522\n",
      "train loss:0.5040269614061028\n",
      "train loss:0.36761041266419403\n",
      "train loss:0.5911339808944192\n",
      "train loss:0.47849077258313744\n",
      "train loss:0.35188663651161234\n",
      "train loss:0.44681330453868584\n",
      "train loss:0.5094155725890616\n",
      "train loss:0.6043425220432145\n",
      "train loss:0.3777368964522976\n",
      "train loss:0.35845192824377803\n",
      "train loss:0.4208739827021164\n",
      "train loss:0.6323796494911017\n",
      "train loss:0.586905519671282\n",
      "train loss:0.39778042632882527\n",
      "train loss:0.450745628693777\n",
      "train loss:0.45673286693451665\n",
      "train loss:0.6129064404433409\n",
      "train loss:0.38833558979834737\n",
      "train loss:0.5091528936301731\n",
      "train loss:0.6024590833022001\n",
      "train loss:0.4542917096315735\n",
      "train loss:0.504382174757654\n",
      "train loss:0.5446121684638915\n",
      "train loss:0.798306874910273\n",
      "train loss:0.36011230210604633\n",
      "train loss:0.20250432755723508\n",
      "train loss:0.6400083031194385\n",
      "train loss:0.35894570338224935\n",
      "train loss:0.5479569259512967\n",
      "train loss:0.4815689117994797\n",
      "train loss:0.5932752240504426\n",
      "train loss:0.6581572523649742\n",
      "train loss:0.5461191348517542\n",
      "train loss:0.5485086976660248\n",
      "train loss:0.5838339535551118\n",
      "train loss:0.7202778705912667\n",
      "train loss:0.4796511683534332\n",
      "train loss:0.879825205701642\n",
      "train loss:0.6410135994760795\n",
      "train loss:0.5333311961817323\n",
      "train loss:0.5821247822535726\n",
      "train loss:0.2532386141706248\n",
      "train loss:0.6378279078249658\n",
      "train loss:0.7050682601313223\n",
      "train loss:0.5644921565800336\n",
      "train loss:0.4939128347006476\n",
      "train loss:0.6499112136264565\n",
      "train loss:0.7261185019711256\n",
      "train loss:0.5263609942988369\n",
      "train loss:0.6455402000979448\n",
      "train loss:0.6329071503757516\n",
      "train loss:0.43927840745920815\n",
      "train loss:0.4108388241190153\n",
      "train loss:0.5009291322013475\n",
      "train loss:0.5310604680518789\n",
      "train loss:0.42948296367222305\n",
      "train loss:0.6957888483473553\n",
      "train loss:0.519744102626761\n",
      "train loss:0.6690508191613493\n",
      "train loss:0.7505530797795272\n",
      "train loss:0.6139083731694873\n",
      "train loss:0.4325968210425152\n",
      "train loss:0.6485148665049365\n",
      "train loss:0.25770873710566916\n",
      "train loss:0.47166962088790276\n",
      "train loss:0.37567271064494967\n",
      "train loss:0.3690053242886723\n",
      "train loss:0.3253604188508394\n",
      "train loss:0.3712957079806786\n",
      "train loss:0.4581522425669962\n",
      "train loss:0.6383514943892143\n",
      "train loss:0.323902753265003\n",
      "train loss:0.6061448161306716\n",
      "train loss:0.3412822845577695\n",
      "train loss:0.4636517351404893\n",
      "train loss:0.22263316958171328\n",
      "train loss:0.1922819247554292\n",
      "train loss:0.9564360579389268\n",
      "train loss:0.487149421459163\n",
      "train loss:0.5948930270723654\n",
      "train loss:0.7611403895611255\n",
      "train loss:0.4214801281583817\n",
      "train loss:0.5290178636393288\n",
      "train loss:0.5232498797339634\n",
      "train loss:0.248259041109123\n",
      "train loss:0.6459840953069469\n",
      "train loss:0.3544403216464928\n",
      "train loss:0.32726071133679635\n",
      "train loss:0.5121015776647252\n",
      "train loss:0.5317970622874366\n",
      "train loss:0.5797159704903877\n",
      "train loss:0.5029139735888105\n",
      "train loss:0.4298335187596793\n",
      "train loss:0.5847130020609923\n",
      "train loss:0.551210453004851\n",
      "train loss:0.9038879438565525\n",
      "train loss:0.49515795989223665\n",
      "train loss:0.6586081977911763\n",
      "train loss:0.5611926742537887\n",
      "train loss:0.7095391119864293\n",
      "train loss:0.6934797221669726\n",
      "train loss:0.7458241413984953\n",
      "train loss:0.5497725411634592\n",
      "train loss:0.46752641160117453\n",
      "train loss:0.678784226442095\n",
      "train loss:0.3972546418060844\n",
      "train loss:0.5518354019269462\n",
      "train loss:0.6372079649452003\n",
      "train loss:0.6860014530083784\n",
      "train loss:0.501862599762381\n",
      "train loss:0.3740038040752194\n",
      "train loss:0.6013460769748442\n",
      "train loss:0.4769065355895711\n",
      "train loss:0.4965220516870522\n",
      "train loss:0.48179584463110076\n",
      "train loss:0.6597929619716785\n",
      "train loss:0.6854082487473109\n",
      "train loss:0.3701768120886708\n",
      "train loss:0.6899066623916221\n",
      "train loss:0.4979182200542008\n",
      "train loss:0.3713284489934092\n",
      "train loss:0.4400581716091782\n",
      "=== epoch:10, train acc:0.76, test acc:0.7 ===\n",
      "train loss:0.5392702031442541\n",
      "train loss:0.2785636960271348\n",
      "train loss:0.5697885185733527\n",
      "train loss:0.5143561539260301\n",
      "train loss:0.34346729575324636\n",
      "train loss:0.4323726882782217\n",
      "train loss:0.5050236571938028\n",
      "train loss:0.28354553671087596\n",
      "train loss:0.3344286061396356\n",
      "train loss:0.31424491014192285\n",
      "train loss:0.8707944851280043\n",
      "train loss:0.5661783761909513\n",
      "train loss:0.26739215090273005\n",
      "train loss:0.4849899077258241\n",
      "train loss:0.28355034453026867\n",
      "train loss:0.4273003376741161\n",
      "train loss:0.36643642259227305\n",
      "train loss:0.5147187500081553\n",
      "train loss:0.5837507730920937\n",
      "train loss:0.6776197130489476\n",
      "train loss:0.6461133098014705\n",
      "train loss:0.4370203518622362\n",
      "train loss:0.3081233245511271\n",
      "train loss:0.3595974176759294\n",
      "train loss:0.48128245198484965\n",
      "train loss:0.49890977302077744\n",
      "train loss:0.7582688523658696\n",
      "train loss:0.6507843725495877\n",
      "train loss:0.4745804107983499\n",
      "train loss:0.5136417545724977\n",
      "train loss:0.5033667659005552\n",
      "train loss:0.444475605176658\n",
      "train loss:0.5807826968726506\n",
      "train loss:0.34414569974001674\n",
      "train loss:0.45538490901157036\n",
      "train loss:0.49126549355296756\n",
      "train loss:0.34668205509116223\n",
      "train loss:0.29401432191632043\n",
      "train loss:0.34782179429713933\n",
      "train loss:0.7730709593317637\n",
      "train loss:0.5358294700967984\n",
      "train loss:0.3226296960458447\n",
      "train loss:0.5578062322226509\n",
      "train loss:0.5212735402394475\n",
      "train loss:0.4360504001008131\n",
      "train loss:0.3133695590831163\n",
      "train loss:0.46365912086553324\n",
      "train loss:0.582378209151039\n",
      "train loss:0.47876214917903825\n",
      "train loss:0.5133349739355659\n",
      "train loss:0.33283934865769027\n",
      "train loss:0.6369278552564948\n",
      "train loss:0.43585355415028665\n",
      "train loss:0.5809630717768064\n",
      "train loss:0.5333906226645386\n",
      "train loss:0.7251096899939453\n",
      "train loss:0.5342832148814827\n",
      "train loss:0.4935160048420233\n",
      "train loss:0.5502446098692244\n",
      "train loss:0.5932848023669777\n",
      "train loss:0.3570063902335936\n",
      "train loss:0.8523702586199621\n",
      "train loss:0.3962159064757885\n",
      "train loss:0.5193786479611319\n",
      "train loss:0.35802989149926706\n",
      "train loss:0.40516710558054314\n",
      "train loss:0.38069104915031154\n",
      "train loss:0.7027517982556459\n",
      "train loss:0.6720225521677277\n",
      "train loss:0.6106061683688694\n",
      "train loss:0.5414155735225945\n",
      "train loss:0.7197666450808506\n",
      "train loss:0.42797926316762835\n",
      "train loss:0.56394166555247\n",
      "train loss:0.2603028686950103\n",
      "train loss:0.2716169275387008\n",
      "train loss:0.44263512358675483\n",
      "train loss:0.5609834757537786\n",
      "train loss:0.4892123396595838\n",
      "train loss:0.4489491020598564\n",
      "train loss:0.5008962154787961\n",
      "train loss:0.6099882694229161\n",
      "train loss:0.4135804495301275\n",
      "train loss:0.4041567803212855\n",
      "train loss:0.43174515449729467\n",
      "train loss:0.3732253633731288\n",
      "train loss:0.31261041116033805\n",
      "train loss:0.3985486334314756\n",
      "train loss:0.18539144744427793\n",
      "train loss:0.8886386380343527\n",
      "train loss:0.41398119131029787\n",
      "train loss:0.2430634228970566\n",
      "train loss:0.8418085650291385\n",
      "train loss:0.3500170472442344\n",
      "train loss:0.6145254635542681\n",
      "train loss:0.3634261536010034\n",
      "train loss:0.6768258915087022\n",
      "train loss:0.3065792586811025\n",
      "train loss:0.4005224801335988\n",
      "train loss:0.5861021426009184\n",
      "train loss:0.5033540732810046\n",
      "train loss:0.514565672079512\n",
      "train loss:0.6464365307159395\n",
      "train loss:0.7964319248468154\n",
      "train loss:0.51300111549038\n",
      "train loss:0.5769638002270598\n",
      "train loss:0.5863129187681119\n",
      "train loss:0.43303028001697824\n",
      "train loss:0.4434790436335418\n",
      "train loss:0.6292314386317611\n",
      "train loss:0.41926479156289914\n",
      "train loss:0.5528202912389485\n",
      "train loss:0.465488046197572\n",
      "train loss:0.4520896435702329\n",
      "train loss:0.5990678264476407\n",
      "train loss:0.5309204438893829\n",
      "train loss:0.46959031183269906\n",
      "train loss:0.4604714831093989\n",
      "train loss:0.2801945597191444\n",
      "train loss:0.4541907206855031\n",
      "train loss:0.48247090264095716\n",
      "train loss:0.510154535945193\n",
      "train loss:0.5458172582616471\n",
      "train loss:0.5168508005946254\n",
      "train loss:0.33036561900813044\n",
      "train loss:0.42725406110439135\n",
      "train loss:0.6559449869770245\n",
      "train loss:0.4227280440931224\n",
      "train loss:0.4986367734323802\n",
      "train loss:0.35699777640612057\n",
      "train loss:0.3305847017519571\n",
      "train loss:0.5241693470479504\n",
      "train loss:0.520015689606508\n",
      "train loss:0.19968509654114747\n",
      "train loss:0.22230984912048202\n",
      "train loss:0.4930462572704336\n",
      "train loss:0.6683589753992573\n",
      "train loss:0.24753427003288414\n",
      "train loss:0.3425337794805701\n",
      "train loss:0.2785468171720626\n",
      "train loss:0.4859423110847712\n",
      "train loss:0.6038822903926518\n",
      "train loss:0.5251024192555327\n",
      "train loss:0.8286073368490774\n",
      "train loss:0.4910361861367158\n",
      "train loss:0.20028720424643498\n",
      "train loss:0.8109301783380435\n",
      "train loss:0.37801135939103675\n",
      "train loss:0.4224608945142975\n",
      "train loss:0.6615319975009915\n",
      "train loss:0.21358320638554695\n",
      "train loss:0.5955909061704422\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5450980392156862\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=1600, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2992060d-08ee-42c3-be34-f459aed6cb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6892532351829087\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.69145826443175\n",
      "train loss:0.6588352181910816\n",
      "train loss:0.6628301596783277\n",
      "train loss:0.7069022246976209\n",
      "train loss:0.6480793516584941\n",
      "train loss:0.6340707151578339\n",
      "train loss:0.6737850235193757\n",
      "train loss:0.7010888718823131\n",
      "train loss:0.6649808207589925\n",
      "train loss:0.6297914510777591\n",
      "train loss:0.37211568345704105\n",
      "train loss:0.6973146577415352\n",
      "train loss:0.6000894242757904\n",
      "train loss:0.5058184414592856\n",
      "train loss:0.6086609850220278\n",
      "train loss:0.1989079093079176\n",
      "train loss:0.6419859935888382\n",
      "train loss:0.8201784520148152\n",
      "train loss:0.5179356024527799\n",
      "train loss:0.3693570727993506\n",
      "train loss:0.8001470086539836\n",
      "train loss:0.6896811236273049\n",
      "train loss:0.6935637965276749\n",
      "train loss:0.4657833618609731\n",
      "train loss:0.39636532828802074\n",
      "train loss:0.5330529481176288\n",
      "train loss:0.7556031826962211\n",
      "train loss:0.6788462590367572\n",
      "train loss:0.4760327640554903\n",
      "train loss:0.6140806398347445\n",
      "train loss:0.6149849949337469\n",
      "train loss:0.7034037070965239\n",
      "train loss:0.5411381330578413\n",
      "train loss:0.4409624033100119\n",
      "train loss:0.6822740827937113\n",
      "train loss:0.7901174394986097\n",
      "train loss:0.6278393782184862\n",
      "train loss:0.4175680322849984\n",
      "train loss:0.7260951053305152\n",
      "train loss:0.5357743236947945\n",
      "train loss:0.6064096541604169\n",
      "train loss:0.679431359416822\n",
      "train loss:0.7047483560732115\n",
      "train loss:0.7775664472629773\n",
      "train loss:0.4606250027706288\n",
      "train loss:0.4617445791799282\n",
      "train loss:0.6234754539878903\n",
      "train loss:0.530656386457691\n",
      "train loss:0.5948685515406306\n",
      "train loss:0.6282906341442882\n",
      "train loss:0.7770937183978281\n",
      "train loss:0.6804639829516981\n",
      "train loss:0.6117026202982405\n",
      "train loss:0.7461251546720267\n",
      "train loss:0.6791221522814586\n",
      "train loss:0.6738262116049275\n",
      "train loss:0.6768373253030432\n",
      "train loss:0.6755303511597167\n",
      "train loss:0.5530646084618801\n",
      "train loss:0.6323024008109549\n",
      "train loss:0.707084239538253\n",
      "train loss:0.5837883277773923\n",
      "train loss:0.6682906804684883\n",
      "train loss:0.6278381360816262\n",
      "train loss:0.6316153999836326\n",
      "train loss:0.5732235358965022\n",
      "train loss:0.721975177428116\n",
      "train loss:0.599121594122207\n",
      "train loss:0.4663342205859271\n",
      "train loss:0.5082805797898606\n",
      "train loss:0.7861705430107406\n",
      "train loss:0.38558030185759284\n",
      "train loss:0.6205445987173668\n",
      "train loss:0.845592033613979\n",
      "train loss:0.38234084502962185\n",
      "train loss:0.8385887788109558\n",
      "train loss:0.7257507067173237\n",
      "train loss:0.5922730380970582\n",
      "train loss:0.8226850217366588\n",
      "train loss:0.5070340564139293\n",
      "train loss:0.6141375820791561\n",
      "train loss:0.6825016557273959\n",
      "train loss:0.6288638504663394\n",
      "train loss:0.6596209976760796\n",
      "train loss:0.7025405669615996\n",
      "train loss:0.596934299744988\n",
      "train loss:0.671940470219072\n",
      "train loss:0.5892413756386049\n",
      "train loss:0.5499495595302413\n",
      "train loss:0.6303927363191374\n",
      "train loss:0.6275074654476966\n",
      "train loss:0.5685613912756716\n",
      "train loss:0.7550310757178121\n",
      "train loss:0.6793812165138609\n",
      "train loss:0.731444392439285\n",
      "train loss:0.7266193354426064\n",
      "train loss:0.6126722914311389\n",
      "train loss:0.5983654503697016\n",
      "train loss:0.622036609188057\n",
      "train loss:0.6244110370570938\n",
      "train loss:0.633319196410612\n",
      "train loss:0.8056619794122323\n",
      "train loss:0.5358611574626735\n",
      "train loss:0.6227368687696562\n",
      "train loss:0.67325388956327\n",
      "train loss:0.5267317562473806\n",
      "train loss:0.4572849901657473\n",
      "train loss:0.668202983396484\n",
      "train loss:0.5238200133423556\n",
      "train loss:0.43154859771056947\n",
      "train loss:0.280097342206249\n",
      "train loss:0.6368983195706717\n",
      "train loss:0.3126247572025067\n",
      "train loss:0.9611362974015659\n",
      "train loss:0.49772691319103746\n",
      "train loss:0.5252784944567004\n",
      "train loss:0.18942511552825225\n",
      "train loss:0.4997974937634317\n",
      "train loss:0.5241134327591315\n",
      "train loss:0.6762505287238714\n",
      "train loss:0.5128270876797637\n",
      "train loss:1.0503484389449123\n",
      "train loss:0.5259389631814366\n",
      "train loss:0.7665873939083185\n",
      "train loss:0.478376831542382\n",
      "train loss:0.6067940917461325\n",
      "train loss:0.5593682817724336\n",
      "train loss:0.5801185900668183\n",
      "train loss:0.6299762954618485\n",
      "train loss:0.6087079433170727\n",
      "train loss:0.530860014678922\n",
      "train loss:0.5131834078638818\n",
      "train loss:0.714872065813635\n",
      "train loss:0.6159949316926954\n",
      "train loss:0.6852259319459677\n",
      "train loss:0.6695027156214577\n",
      "train loss:0.4631520412292063\n",
      "train loss:0.8093825918578584\n",
      "train loss:0.6812366891239101\n",
      "train loss:0.546960853594846\n",
      "train loss:0.760536239958903\n",
      "train loss:0.5508677314856378\n",
      "train loss:0.7024925655061177\n",
      "train loss:0.46190644389822777\n",
      "train loss:0.5860962367437319\n",
      "train loss:0.6774645773994361\n",
      "train loss:0.5847638284215009\n",
      "train loss:0.49987415435706434\n",
      "train loss:0.7077137261303358\n",
      "train loss:0.4924974439523499\n",
      "train loss:0.5237123688952156\n",
      "train loss:0.27949584851697745\n",
      "train loss:0.9368441947012652\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6944511090495842\n",
      "train loss:0.6253539314809029\n",
      "train loss:0.5336119195113075\n",
      "train loss:0.6246107746019515\n",
      "train loss:0.8710489300852473\n",
      "train loss:0.5983095805275955\n",
      "train loss:0.5883233621928549\n",
      "train loss:0.46638068438243696\n",
      "train loss:0.48222490598752143\n",
      "train loss:0.5389236419214538\n",
      "train loss:0.6658706623301142\n",
      "train loss:0.4461209299232188\n",
      "train loss:0.766208606277653\n",
      "train loss:0.6077230838531932\n",
      "train loss:0.690920339604786\n",
      "train loss:0.6020689933218166\n",
      "train loss:0.4123858262785453\n",
      "train loss:0.6893519332528496\n",
      "train loss:0.5205436711457815\n",
      "train loss:0.690878512933535\n",
      "train loss:0.7631556376712993\n",
      "train loss:0.5708736681062793\n",
      "train loss:0.582991449894379\n",
      "train loss:0.3350327418732175\n",
      "train loss:0.5306740581831314\n",
      "train loss:0.3016205097010392\n",
      "train loss:0.5888877170435716\n",
      "train loss:0.508743129839039\n",
      "train loss:0.7508794537503314\n",
      "train loss:0.5326414862764504\n",
      "train loss:0.5227414073386163\n",
      "train loss:0.6915705274422218\n",
      "train loss:0.6564118654335326\n",
      "train loss:0.6455453530391464\n",
      "train loss:0.49516039084915464\n",
      "train loss:0.3970147367179378\n",
      "train loss:0.46903118673065336\n",
      "train loss:0.8276917269457564\n",
      "train loss:0.529835671549979\n",
      "train loss:0.5017781075294349\n",
      "train loss:0.427392730680519\n",
      "train loss:0.6078936677387793\n",
      "train loss:0.516156918114153\n",
      "train loss:0.7193806514086505\n",
      "train loss:0.5748892653429623\n",
      "train loss:0.7614820926575319\n",
      "train loss:0.6135438720151216\n",
      "train loss:0.45724372339756886\n",
      "train loss:0.43826867532354674\n",
      "train loss:0.8328479146698567\n",
      "train loss:0.6460446152632568\n",
      "train loss:0.8915546935900697\n",
      "train loss:0.6231854127449463\n",
      "train loss:0.6029818426860964\n",
      "train loss:0.55896354445022\n",
      "train loss:0.6828306509190418\n",
      "train loss:0.5087241493094291\n",
      "train loss:0.534634052086721\n",
      "train loss:0.6112824560421722\n",
      "train loss:0.6833844068090997\n",
      "train loss:0.5479870132239405\n",
      "train loss:0.5712051608429007\n",
      "train loss:0.5236247640548217\n",
      "train loss:0.602807404736278\n",
      "train loss:0.41461828719425664\n",
      "train loss:0.6240867359926625\n",
      "train loss:0.5821042825312737\n",
      "train loss:0.6332158707897153\n",
      "train loss:0.19993709186110734\n",
      "train loss:0.16566295667825695\n",
      "train loss:0.4775781898226632\n",
      "train loss:0.3358671107426904\n",
      "train loss:0.9050434807652911\n",
      "train loss:0.7431558353135042\n",
      "train loss:0.6241261203997426\n",
      "train loss:0.4952171297129103\n",
      "train loss:0.3589320912549775\n",
      "train loss:0.737427195308281\n",
      "train loss:0.6286146744696595\n",
      "train loss:0.6208265804567492\n",
      "train loss:0.6180757272962298\n",
      "train loss:0.9005444006183904\n",
      "train loss:0.5714334297082567\n",
      "train loss:0.5400046963128726\n",
      "train loss:0.7002323933681532\n",
      "train loss:0.5966798919536951\n",
      "train loss:0.6244169762627694\n",
      "train loss:0.587704644850987\n",
      "train loss:0.4853390948600465\n",
      "train loss:0.5332427283645266\n",
      "train loss:0.6479791228499866\n",
      "train loss:0.6135929446158693\n",
      "train loss:0.6009053763390095\n",
      "train loss:0.5155677052721447\n",
      "train loss:0.30470198272152327\n",
      "train loss:0.6309145193865127\n",
      "train loss:0.5185681990974997\n",
      "train loss:0.6185405207079294\n",
      "train loss:0.8079086000259361\n",
      "train loss:0.49899269979526667\n",
      "train loss:0.3581344203633363\n",
      "train loss:0.20852343297072032\n",
      "train loss:0.49150365266517176\n",
      "train loss:0.8406950641403972\n",
      "train loss:0.6434304148413478\n",
      "train loss:0.4693725965048146\n",
      "train loss:0.575688219255097\n",
      "train loss:0.672365740537089\n",
      "train loss:0.4270278721807646\n",
      "train loss:0.41823857983226276\n",
      "train loss:0.6986856706061739\n",
      "train loss:0.47010270305123525\n",
      "train loss:0.687739664985598\n",
      "train loss:0.5250481387435115\n",
      "train loss:0.694606142063559\n",
      "train loss:0.4982839844281585\n",
      "train loss:0.6923344330188244\n",
      "train loss:0.5177310822386681\n",
      "train loss:0.7393291697425963\n",
      "train loss:0.5842018763170965\n",
      "train loss:0.6832754486886772\n",
      "train loss:0.5945184959532636\n",
      "train loss:0.6593817345495079\n",
      "train loss:0.4990880025705793\n",
      "train loss:0.4372006450809371\n",
      "train loss:0.5213637980227407\n",
      "train loss:0.502156443242022\n",
      "train loss:0.5483705681082695\n",
      "train loss:0.8320349409830643\n",
      "train loss:0.5650313585735834\n",
      "train loss:0.5287295190043817\n",
      "train loss:0.830419848468589\n",
      "train loss:0.42234673157512637\n",
      "train loss:0.5154183051895684\n",
      "train loss:0.6481404153926945\n",
      "train loss:0.8734088770714769\n",
      "train loss:0.5864574609267583\n",
      "train loss:0.6228100189643885\n",
      "train loss:0.7361387514456347\n",
      "train loss:0.6174513602816581\n",
      "train loss:0.6559214917712176\n",
      "train loss:0.6074982976103792\n",
      "train loss:0.5819717535152594\n",
      "train loss:0.6780346577681843\n",
      "train loss:0.6257837299309272\n",
      "train loss:0.5278920775108954\n",
      "train loss:0.45303572113673185\n",
      "train loss:0.7239204783325905\n",
      "train loss:0.391689742786648\n",
      "train loss:0.6489353590190446\n",
      "train loss:0.7386244187927644\n",
      "train loss:0.33758483907506\n",
      "train loss:0.6579317145609724\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.730631449741949\n",
      "train loss:0.48246673963179043\n",
      "train loss:0.37811541866697473\n",
      "train loss:0.5016113430923398\n",
      "train loss:0.4900054374082831\n",
      "train loss:0.9710434374564356\n",
      "train loss:0.530627184234377\n",
      "train loss:0.6496251096920785\n",
      "train loss:0.4958710141610781\n",
      "train loss:0.5865018922402854\n",
      "train loss:0.5368922733178819\n",
      "train loss:0.5881050221256158\n",
      "train loss:0.6513372544020496\n",
      "train loss:0.456372953021947\n",
      "train loss:0.5021784693235559\n",
      "train loss:0.6110903672381934\n",
      "train loss:0.5421937656278502\n",
      "train loss:0.7282701518454593\n",
      "train loss:0.4145989178175079\n",
      "train loss:0.5305117100409411\n",
      "train loss:0.5171005615680283\n",
      "train loss:0.6879080446853816\n",
      "train loss:0.7000078347052071\n",
      "train loss:0.6301362255349853\n",
      "train loss:0.5869298010286\n",
      "train loss:0.6450197281821508\n",
      "train loss:0.485237769872371\n",
      "train loss:0.3148369049726013\n",
      "train loss:0.6404073343110085\n",
      "train loss:0.7147498759373008\n",
      "train loss:0.7063622594523562\n",
      "train loss:0.7217831203582876\n",
      "train loss:0.5234667737102219\n",
      "train loss:0.6638278656455333\n",
      "train loss:0.4970990936654037\n",
      "train loss:0.4948355525053555\n",
      "train loss:0.7662637738940193\n",
      "train loss:0.6364527137337304\n",
      "train loss:0.8019373966020262\n",
      "train loss:0.7301704089818462\n",
      "train loss:0.6127829923911905\n",
      "train loss:0.6661702528832373\n",
      "train loss:0.5265015393470385\n",
      "train loss:0.5235892484496434\n",
      "train loss:0.6607617035605102\n",
      "train loss:0.7002005051645029\n",
      "train loss:0.5866985474154711\n",
      "train loss:0.6971391558323138\n",
      "train loss:0.5127850858227999\n",
      "train loss:0.639031639043697\n",
      "train loss:0.756546489537384\n",
      "train loss:0.500593054537961\n",
      "train loss:0.5320472690724432\n",
      "train loss:0.6120374617847179\n",
      "train loss:0.7543623912311734\n",
      "train loss:0.5386054821348496\n",
      "train loss:0.790944218715313\n",
      "train loss:0.7506643542443593\n",
      "train loss:0.6006636540674334\n",
      "train loss:0.6781823640260202\n",
      "train loss:0.5220749780464773\n",
      "train loss:0.69797513506924\n",
      "train loss:0.6143295815409523\n",
      "train loss:0.5630756061510924\n",
      "train loss:0.5899213013907204\n",
      "train loss:0.5274493439940453\n",
      "train loss:0.4385109103687584\n",
      "train loss:0.5643847086024147\n",
      "train loss:0.4136206801815268\n",
      "train loss:0.7960482649150367\n",
      "train loss:0.5251412545876704\n",
      "train loss:0.740697166765355\n",
      "train loss:0.3970065199138345\n",
      "train loss:0.6060625256555274\n",
      "train loss:0.8473079499289481\n",
      "train loss:0.5776441667777571\n",
      "train loss:0.4136566800761381\n",
      "train loss:0.5652466689486504\n",
      "train loss:0.6123009584505151\n",
      "train loss:0.45768214434014187\n",
      "train loss:0.7013031531418487\n",
      "train loss:0.6109689444648977\n",
      "train loss:0.8648778969445026\n",
      "train loss:0.6534381936870364\n",
      "train loss:0.40369343282094183\n",
      "train loss:0.6941404339095679\n",
      "train loss:0.6491157829904897\n",
      "train loss:0.5505007663261907\n",
      "train loss:0.7849340335249142\n",
      "train loss:0.5843233017541605\n",
      "train loss:0.6651081594934215\n",
      "train loss:0.484720335980301\n",
      "train loss:0.5401243926468358\n",
      "train loss:0.659050776858006\n",
      "train loss:0.6780281763232633\n",
      "train loss:0.5987206905584059\n",
      "train loss:0.66955211586751\n",
      "train loss:0.651806586295945\n",
      "train loss:0.7008189895115196\n",
      "train loss:0.5719009033547394\n",
      "train loss:0.6384051159479405\n",
      "train loss:0.5668031833298355\n",
      "train loss:0.6636891124554275\n",
      "train loss:0.687010008790786\n",
      "train loss:0.42767091188593254\n",
      "train loss:0.7895551786121489\n",
      "train loss:0.7851512210970983\n",
      "train loss:0.6312949755704061\n",
      "train loss:0.6017018588909812\n",
      "train loss:0.5424142085303106\n",
      "train loss:0.61116713044227\n",
      "train loss:0.49340753341974375\n",
      "train loss:0.49855390258240745\n",
      "train loss:0.7556925237409431\n",
      "train loss:0.3711547765896569\n",
      "train loss:0.6009803500191175\n",
      "train loss:0.6413268203384859\n",
      "train loss:0.5109288602209395\n",
      "train loss:0.31727813682210415\n",
      "train loss:0.5119388030914368\n",
      "train loss:0.7303541612849473\n",
      "train loss:1.1029492324115235\n",
      "train loss:0.3253180765204871\n",
      "train loss:0.34871133040226054\n",
      "train loss:0.3779943371596787\n",
      "train loss:0.5219036732729648\n",
      "train loss:0.5399894465051696\n",
      "train loss:0.6951393155542778\n",
      "train loss:0.7279028437687047\n",
      "train loss:0.6379692455910198\n",
      "train loss:0.8571834729273503\n",
      "train loss:0.40035844521437003\n",
      "train loss:0.600009777912714\n",
      "train loss:0.7303447728500252\n",
      "train loss:0.6305140542026031\n",
      "train loss:0.6119322958133872\n",
      "train loss:0.5621608647504931\n",
      "train loss:0.7175922600614236\n",
      "train loss:0.6767050307095032\n",
      "train loss:0.6642051316553219\n",
      "train loss:0.6218934432439621\n",
      "train loss:0.5217810465194265\n",
      "train loss:0.5201276030957707\n",
      "train loss:0.49438495310842046\n",
      "train loss:0.6910451535461868\n",
      "train loss:0.5933968473062633\n",
      "train loss:0.5666319227357918\n",
      "train loss:0.7009731875118452\n",
      "train loss:0.4814767510613107\n",
      "train loss:0.6397018791240976\n",
      "train loss:0.6307873711186247\n",
      "train loss:0.5850560428452471\n",
      "train loss:0.621792206430341\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6112032706486108\n",
      "train loss:0.650013677542737\n",
      "train loss:0.4572474256036728\n",
      "train loss:0.5461278075343883\n",
      "train loss:0.374938723546446\n",
      "train loss:0.410992363554241\n",
      "train loss:0.5309795410182163\n",
      "train loss:0.46747567501167975\n",
      "train loss:0.6751026255473638\n",
      "train loss:0.6905720652185754\n",
      "train loss:0.9493043995737583\n",
      "train loss:0.5753024924626915\n",
      "train loss:0.7194741810179263\n",
      "train loss:0.3750154422264097\n",
      "train loss:0.3592948719710848\n",
      "train loss:0.9315795505896032\n",
      "train loss:0.5224749669633163\n",
      "train loss:0.6310929640498737\n",
      "train loss:0.619734706134388\n",
      "train loss:0.5832488423517164\n",
      "train loss:0.6729833433422266\n",
      "train loss:0.7456045162884993\n",
      "train loss:0.6933425556362903\n",
      "train loss:0.4935168320047306\n",
      "train loss:0.6201332408811961\n",
      "train loss:0.5548354712606511\n",
      "train loss:0.47971873341828164\n",
      "train loss:0.4699425650453987\n",
      "train loss:0.4249422209860306\n",
      "train loss:0.5455607016075379\n",
      "train loss:0.4948792326501816\n",
      "train loss:0.592624576246324\n",
      "train loss:0.9506297729472909\n",
      "train loss:0.4904348351505621\n",
      "train loss:0.39808474454374\n",
      "train loss:0.42101069239751887\n",
      "train loss:0.6140467833636496\n",
      "train loss:0.5052988026851325\n",
      "train loss:0.6035558570569395\n",
      "train loss:0.4229656428762543\n",
      "train loss:0.44545853265598545\n",
      "train loss:0.570769559051209\n",
      "train loss:0.3662538944392603\n",
      "train loss:0.5180061218455883\n",
      "train loss:0.5007320496563759\n",
      "train loss:0.2017295655263708\n",
      "train loss:0.43985672147192884\n",
      "train loss:0.48111839355635266\n",
      "train loss:1.0177777206128686\n",
      "train loss:1.1607592874062642\n",
      "train loss:0.8858370576262778\n",
      "train loss:0.4957164801377405\n",
      "train loss:0.7033308166054891\n",
      "train loss:0.5650740336771235\n",
      "train loss:0.5071683755925929\n",
      "train loss:0.6318748558701077\n",
      "train loss:0.5832682918148305\n",
      "train loss:0.5888171114921723\n",
      "train loss:0.5877090923399046\n",
      "train loss:0.583499273244375\n",
      "train loss:0.7232701607077551\n",
      "train loss:0.5884635830020118\n",
      "train loss:0.5958137398395913\n",
      "train loss:0.6633022499175897\n",
      "train loss:0.6276377976593095\n",
      "train loss:0.5477039021284609\n",
      "train loss:0.49596525236007355\n",
      "train loss:0.5959444234217357\n",
      "train loss:0.5744676190981752\n",
      "train loss:0.6335871644571751\n",
      "train loss:0.753403635667723\n",
      "train loss:0.5841356557802114\n",
      "train loss:0.5058117182843317\n",
      "train loss:0.4776954421350335\n",
      "train loss:0.47284176337568934\n",
      "train loss:0.5786522529613489\n",
      "train loss:0.5369460221371685\n",
      "train loss:0.611414480831147\n",
      "train loss:0.557969903256117\n",
      "train loss:0.5242269294271428\n",
      "train loss:0.325350531818433\n",
      "train loss:0.29159415955112267\n",
      "train loss:0.8061157696632542\n",
      "train loss:0.7396194553823864\n",
      "train loss:0.4965889279277994\n",
      "train loss:0.8064562693939031\n",
      "train loss:0.35474865900263486\n",
      "train loss:0.34511286250107653\n",
      "train loss:0.6519479783856886\n",
      "train loss:0.6441511341378339\n",
      "train loss:0.45298010361429036\n",
      "train loss:0.6932506068517936\n",
      "train loss:0.5546598699004174\n",
      "train loss:0.6349323070667985\n",
      "train loss:0.6426731953824347\n",
      "train loss:0.4069030202989423\n",
      "train loss:0.6319147404600995\n",
      "train loss:0.7284979473496108\n",
      "train loss:0.415730473651181\n",
      "train loss:0.6965863064630935\n",
      "train loss:0.5186585443872007\n",
      "train loss:0.7207367410302956\n",
      "train loss:0.5819499183498633\n",
      "train loss:0.6785409310004498\n",
      "train loss:0.41218044378217167\n",
      "train loss:0.5616278830307898\n",
      "train loss:0.5349985803211524\n",
      "train loss:0.6242509724020231\n",
      "train loss:0.5832126275521151\n",
      "train loss:0.6575464489026408\n",
      "train loss:0.37861532734748565\n",
      "train loss:0.41398143045311375\n",
      "train loss:0.5212607581235177\n",
      "train loss:0.4484791807084222\n",
      "train loss:0.5084606745958479\n",
      "train loss:0.6467354806013026\n",
      "train loss:0.3958327800850598\n",
      "train loss:0.3923185681111435\n",
      "train loss:0.49565282634656116\n",
      "train loss:0.5723684218516493\n",
      "train loss:0.6495258560819519\n",
      "train loss:0.5063391822992509\n",
      "train loss:0.4802873932528987\n",
      "train loss:0.6596658202070028\n",
      "train loss:0.7807674955696049\n",
      "train loss:0.5271273379370607\n",
      "train loss:0.6930395489008367\n",
      "train loss:0.5046179505069162\n",
      "train loss:0.5238302317992468\n",
      "train loss:0.7186496613680249\n",
      "train loss:0.586856641782212\n",
      "train loss:0.6581309101876027\n",
      "train loss:0.6435537698159066\n",
      "train loss:0.6570172193596516\n",
      "train loss:0.6218832748099422\n",
      "train loss:0.6303578259524557\n",
      "train loss:0.6464970637982537\n",
      "train loss:0.6161451158126015\n",
      "train loss:0.5915061378371573\n",
      "train loss:0.5249273195998271\n",
      "train loss:0.6095675254553521\n",
      "train loss:0.5680702149720172\n",
      "train loss:0.6741784715279174\n",
      "train loss:0.7469321378514024\n",
      "train loss:0.46229385877965284\n",
      "train loss:0.5321703370563653\n",
      "train loss:0.5840014086780572\n",
      "train loss:0.7405025837752277\n",
      "train loss:0.5698184497664223\n",
      "train loss:0.43409399257963\n",
      "train loss:0.7229067540710727\n",
      "train loss:0.7783206240676678\n",
      "train loss:0.5329504956933231\n",
      "=== epoch:5, train acc:0.73, test acc:0.69 ===\n",
      "train loss:0.7827703898912782\n",
      "train loss:0.48731165170520113\n",
      "train loss:0.6527499403678514\n",
      "train loss:0.5351131254861073\n",
      "train loss:0.48627838656702016\n",
      "train loss:0.6977825761845737\n",
      "train loss:0.5808034572123814\n",
      "train loss:0.6285440119888438\n",
      "train loss:0.5805452280918002\n",
      "train loss:0.7911757128530084\n",
      "train loss:0.5128256502069315\n",
      "train loss:0.6525969596001147\n",
      "train loss:0.5288938686076687\n",
      "train loss:0.4914267906257879\n",
      "train loss:0.6273643845872532\n",
      "train loss:0.5694537352790956\n",
      "train loss:0.5520552517626444\n",
      "train loss:0.5307836131933208\n",
      "train loss:0.649881479723774\n",
      "train loss:0.6375215001587913\n",
      "train loss:0.5749255079055303\n",
      "train loss:0.42343543019272367\n",
      "train loss:0.4855154468431125\n",
      "train loss:0.6295214538715995\n",
      "train loss:0.41548177338701614\n",
      "train loss:0.527659853632977\n",
      "train loss:0.37943327661051296\n",
      "train loss:0.8058727640809487\n",
      "train loss:1.156801470149174\n",
      "train loss:0.5336514902892414\n",
      "train loss:0.6307688655419996\n",
      "train loss:0.5049571344371894\n",
      "train loss:0.43807766117728175\n",
      "train loss:0.6048997284330543\n",
      "train loss:0.5720612222755486\n",
      "train loss:0.5048522209834789\n",
      "train loss:0.454786193793786\n",
      "train loss:0.44209733970004644\n",
      "train loss:0.36333242586132075\n",
      "train loss:0.7038062497414282\n",
      "train loss:0.798072423685236\n",
      "train loss:0.43498842145113004\n",
      "train loss:0.6319647176457517\n",
      "train loss:0.42412999275367536\n",
      "train loss:0.3876349566356916\n",
      "train loss:0.38208517741151765\n",
      "train loss:0.7032836363860607\n",
      "train loss:0.7858450239664831\n",
      "train loss:0.6124413595367996\n",
      "train loss:0.8609804773936401\n",
      "train loss:0.38479073717117285\n",
      "train loss:0.5383032590482595\n",
      "train loss:0.5142548628369255\n",
      "train loss:0.3837429868987063\n",
      "train loss:0.400336504530562\n",
      "train loss:0.5601041702563869\n",
      "train loss:0.686520541821291\n",
      "train loss:0.6473485333062493\n",
      "train loss:0.5323235813523864\n",
      "train loss:0.9201122566314691\n",
      "train loss:0.6207256262278109\n",
      "train loss:0.4427271103072882\n",
      "train loss:0.548104025474264\n",
      "train loss:0.6099971036073012\n",
      "train loss:0.4762550601964022\n",
      "train loss:0.5675321840310191\n",
      "train loss:0.46663248768243815\n",
      "train loss:0.7109429552766012\n",
      "train loss:0.43638687240847285\n",
      "train loss:0.6208927504351837\n",
      "train loss:0.523383860182715\n",
      "train loss:1.004028422689817\n",
      "train loss:0.6246630746830006\n",
      "train loss:0.5052369216356143\n",
      "train loss:0.5859099413402238\n",
      "train loss:0.3273176128476917\n",
      "train loss:0.568838507504289\n",
      "train loss:0.6401528966345051\n",
      "train loss:0.5188289507760645\n",
      "train loss:0.5482312288757729\n",
      "train loss:0.5597509893439281\n",
      "train loss:0.5288318907131457\n",
      "train loss:0.654773028478241\n",
      "train loss:0.370661816830485\n",
      "train loss:0.6366325199169219\n",
      "train loss:0.542940693408771\n",
      "train loss:0.5064325288146292\n",
      "train loss:0.5301747681861155\n",
      "train loss:0.4008188100610413\n",
      "train loss:0.35149774148426643\n",
      "train loss:0.9910432674238668\n",
      "train loss:0.49207717976937626\n",
      "train loss:0.7279640613094488\n",
      "train loss:0.6622717901313415\n",
      "train loss:0.5882078260514778\n",
      "train loss:0.44733667278418254\n",
      "train loss:0.6144409682514068\n",
      "train loss:0.5340178003045317\n",
      "train loss:0.44463921817406576\n",
      "train loss:0.5981033699965121\n",
      "train loss:0.7344110890877672\n",
      "train loss:0.6371780047659722\n",
      "train loss:0.6140631875645092\n",
      "train loss:0.49723204359245765\n",
      "train loss:0.47356111390107214\n",
      "train loss:0.8314711266983336\n",
      "train loss:0.5480326034465595\n",
      "train loss:0.6631575887874004\n",
      "train loss:0.6065956475823239\n",
      "train loss:0.46480497100538615\n",
      "train loss:0.7931159264187628\n",
      "train loss:0.6034412920575681\n",
      "train loss:0.5237918192184285\n",
      "train loss:0.5668285901794108\n",
      "train loss:0.6474136913523882\n",
      "train loss:0.5899018747610333\n",
      "train loss:0.5293525094302994\n",
      "train loss:0.670609739128474\n",
      "train loss:0.5597266072385944\n",
      "train loss:0.4958171669040599\n",
      "train loss:0.6855935845597554\n",
      "train loss:0.5763146900801086\n",
      "train loss:0.6830511757966466\n",
      "train loss:0.5657775644859909\n",
      "train loss:0.5896506132978586\n",
      "train loss:0.49667695550671126\n",
      "train loss:0.6062057122136025\n",
      "train loss:0.6385112144802231\n",
      "train loss:0.5857233031200522\n",
      "train loss:0.4670705285190676\n",
      "train loss:0.40609771649050186\n",
      "train loss:0.4844861653063937\n",
      "train loss:0.5378257802307282\n",
      "train loss:0.5428662470652718\n",
      "train loss:0.524540132872797\n",
      "train loss:0.47073815492213134\n",
      "train loss:0.4707829736572964\n",
      "train loss:0.4786203934887845\n",
      "train loss:0.645495245852719\n",
      "train loss:0.6961693459060323\n",
      "train loss:0.5093698274549864\n",
      "train loss:0.5289831544230488\n",
      "train loss:0.5483500020183579\n",
      "train loss:0.3019170682384811\n",
      "train loss:0.6404735982997248\n",
      "train loss:0.5489957391896064\n",
      "train loss:0.3152207803956467\n",
      "train loss:0.709098743913215\n",
      "train loss:0.7457448203879844\n",
      "train loss:0.5347396634355845\n",
      "train loss:0.4954445144737753\n",
      "train loss:0.7873677126861232\n",
      "train loss:0.44478208575480194\n",
      "=== epoch:6, train acc:0.74, test acc:0.7 ===\n",
      "train loss:0.790465162819949\n",
      "train loss:0.45126079949710374\n",
      "train loss:0.7267866140416757\n",
      "train loss:0.7575865938015247\n",
      "train loss:0.6056095928542528\n",
      "train loss:0.5073098331653285\n",
      "train loss:0.626107566451396\n",
      "train loss:0.5424315422048158\n",
      "train loss:0.5237697130319441\n",
      "train loss:0.7134507544372485\n",
      "train loss:0.45932966866507474\n",
      "train loss:0.5096809705240014\n",
      "train loss:0.5523236224548643\n",
      "train loss:0.5317439316309456\n",
      "train loss:0.5695840049385854\n",
      "train loss:0.6045504102672468\n",
      "train loss:0.6057067829711625\n",
      "train loss:0.42323704321911626\n",
      "train loss:0.7000502607540323\n",
      "train loss:0.37074864397213636\n",
      "train loss:0.7247974810421692\n",
      "train loss:0.5545128539499696\n",
      "train loss:0.5290144101002302\n",
      "train loss:0.6933339525800857\n",
      "train loss:0.26055744709308865\n",
      "train loss:0.4665074474562395\n",
      "train loss:0.6413180361013084\n",
      "train loss:0.46802776750733466\n",
      "train loss:0.4111481074197639\n",
      "train loss:0.6017118792561997\n",
      "train loss:0.3089624206973808\n",
      "train loss:0.4313100705035855\n",
      "train loss:0.668891400422384\n",
      "train loss:0.8587087619742368\n",
      "train loss:0.7354118593209109\n",
      "train loss:0.7171520936285433\n",
      "train loss:0.6355752252878856\n",
      "train loss:0.6252856521621519\n",
      "train loss:0.7212942979240242\n",
      "train loss:0.5379992557647355\n",
      "train loss:0.6563750292081543\n",
      "train loss:0.5035078935260557\n",
      "train loss:0.35015653071418906\n",
      "train loss:0.5163674201784174\n",
      "train loss:0.5800923396915879\n",
      "train loss:0.5057978301843215\n",
      "train loss:0.5629666559975262\n",
      "train loss:0.5664216586511907\n",
      "train loss:0.6089728503811666\n",
      "train loss:0.6012740408067306\n",
      "train loss:0.5402048445005505\n",
      "train loss:0.7254651745065794\n",
      "train loss:0.5932401553854041\n",
      "train loss:0.5276247956085648\n",
      "train loss:0.49529805757440115\n",
      "train loss:0.6285662574015258\n",
      "train loss:0.6941598623699254\n",
      "train loss:0.5087363477539644\n",
      "train loss:0.5039033737843757\n",
      "train loss:0.6142224705463281\n",
      "train loss:0.562306163435211\n",
      "train loss:0.4401182941260598\n",
      "train loss:0.5754229842452777\n",
      "train loss:0.5457119794355874\n",
      "train loss:0.4504074286160808\n",
      "train loss:0.6764977107866528\n",
      "train loss:0.51716895657109\n",
      "train loss:0.6707686265108106\n",
      "train loss:0.393643409978335\n",
      "train loss:0.8770065380653799\n",
      "train loss:0.7687763941390352\n",
      "train loss:0.6214617433500326\n",
      "train loss:0.4805118580397883\n",
      "train loss:0.6435744450554466\n",
      "train loss:0.4829081067960622\n",
      "train loss:0.5411912502303914\n",
      "train loss:0.7752927233725405\n",
      "train loss:0.5959864363634215\n",
      "train loss:0.5891141213891249\n",
      "train loss:0.44201216716655695\n",
      "train loss:0.6752841989179055\n",
      "train loss:0.8895721704991295\n",
      "train loss:0.6816889496160726\n",
      "train loss:0.40795722282156033\n",
      "train loss:0.6009277368758713\n",
      "train loss:0.5726130333979113\n",
      "train loss:0.789568489200968\n",
      "train loss:0.6168865647137092\n",
      "train loss:0.5531186660986046\n",
      "train loss:0.5557884196050545\n",
      "train loss:0.3748645335839945\n",
      "train loss:0.4886666714046731\n",
      "train loss:0.5611364437395994\n",
      "train loss:0.6869182034636625\n",
      "train loss:0.49055454776314456\n",
      "train loss:0.701578829532645\n",
      "train loss:0.2866400620315635\n",
      "train loss:0.5387986363326835\n",
      "train loss:0.5645495188266597\n",
      "train loss:0.6699576362161349\n",
      "train loss:0.5178060668411107\n",
      "train loss:0.2431549413410336\n",
      "train loss:0.48412297182645486\n",
      "train loss:0.6553568924508424\n",
      "train loss:0.595173041326488\n",
      "train loss:0.49086499727065913\n",
      "train loss:0.5439028133695658\n",
      "train loss:0.37214473383845603\n",
      "train loss:0.7464315424051396\n",
      "train loss:0.6938922659985276\n",
      "train loss:0.6030256321138506\n",
      "train loss:0.4881962403631362\n",
      "train loss:0.5789359446767903\n",
      "train loss:0.45350845432471304\n",
      "train loss:0.44096172555327995\n",
      "train loss:0.3613563702539766\n",
      "train loss:0.4467881370846591\n",
      "train loss:0.4588787797163252\n",
      "train loss:0.4074247338847246\n",
      "train loss:0.6409856551413301\n",
      "train loss:0.7757876031675435\n",
      "train loss:0.23555314847018013\n",
      "train loss:0.5886849014102866\n",
      "train loss:0.5142531592740911\n",
      "train loss:0.5798307235387219\n",
      "train loss:0.62262137803466\n",
      "train loss:0.33858229467059753\n",
      "train loss:0.3896182369812029\n",
      "train loss:0.47455161168016263\n",
      "train loss:0.5332074588779873\n",
      "train loss:0.7843407874782478\n",
      "train loss:0.6039214113428449\n",
      "train loss:0.5057683078708297\n",
      "train loss:0.6931260696225016\n",
      "train loss:0.6599948250403574\n",
      "train loss:0.8047631696525146\n",
      "train loss:0.7756047017233608\n",
      "train loss:0.6203986957506451\n",
      "train loss:0.5599576959753951\n",
      "train loss:0.64972117312587\n",
      "train loss:0.4892464030339904\n",
      "train loss:0.6712362541945648\n",
      "train loss:0.6613402539357276\n",
      "train loss:0.5100275618544914\n",
      "train loss:0.4748480028104104\n",
      "train loss:0.508607822958659\n",
      "train loss:0.3213483323442508\n",
      "train loss:0.855419617069944\n",
      "train loss:0.5415411399811543\n",
      "train loss:0.8096057076200218\n",
      "train loss:0.5668719038819495\n",
      "train loss:0.4256688004479585\n",
      "train loss:0.4309208463163613\n",
      "=== epoch:7, train acc:0.75, test acc:0.7 ===\n",
      "train loss:0.5218326878428189\n",
      "train loss:0.3641134579465095\n",
      "train loss:0.6017780189211244\n",
      "train loss:0.20149168572815798\n",
      "train loss:0.20612874718142069\n",
      "train loss:0.3941250097566869\n",
      "train loss:0.6654071026598248\n",
      "train loss:0.40847289947361015\n",
      "train loss:0.3027188173944976\n",
      "train loss:0.7243173582797962\n",
      "train loss:0.35270609889898974\n",
      "train loss:0.44579734019577016\n",
      "train loss:0.4432906889150868\n",
      "train loss:0.21529851840618383\n",
      "train loss:0.3493293219502229\n",
      "train loss:0.3499010393622798\n",
      "train loss:0.675321280574939\n",
      "train loss:0.8135281858985897\n",
      "train loss:0.25115511498544285\n",
      "train loss:0.6598165000189467\n",
      "train loss:0.39935687176972773\n",
      "train loss:0.18761888843415467\n",
      "train loss:0.655603661191359\n",
      "train loss:0.4291579637429579\n",
      "train loss:0.5072169513576748\n",
      "train loss:0.3620745650060422\n",
      "train loss:0.678291274513933\n",
      "train loss:0.21154229837431943\n",
      "train loss:0.3854533055351605\n",
      "train loss:0.8159374834151377\n",
      "train loss:0.6209100696019504\n",
      "train loss:0.6116527489148869\n",
      "train loss:0.5428528428938435\n",
      "train loss:0.19956356593543767\n",
      "train loss:0.6691557330905316\n",
      "train loss:0.44480908072675335\n",
      "train loss:0.5368039753031639\n",
      "train loss:0.38329278608271045\n",
      "train loss:0.5224952864904823\n",
      "train loss:0.7019189968772825\n",
      "train loss:0.4926124737967232\n",
      "train loss:0.4471867384352398\n",
      "train loss:0.5252502127264859\n",
      "train loss:0.7507763874024149\n",
      "train loss:0.7654019105435866\n",
      "train loss:0.3819737082501983\n",
      "train loss:0.4937696314469525\n",
      "train loss:0.4595684454591441\n",
      "train loss:0.6014600969987307\n",
      "train loss:0.38693489110392876\n",
      "train loss:0.5576544455757547\n",
      "train loss:0.38220813676854803\n",
      "train loss:0.37492237100168474\n",
      "train loss:0.7377736659501195\n",
      "train loss:0.5632606603672904\n",
      "train loss:0.4210439044293525\n",
      "train loss:0.7626345970150246\n",
      "train loss:0.2372573091946833\n",
      "train loss:0.3501687761902145\n",
      "train loss:0.6521499833421378\n",
      "train loss:0.5178522776086908\n",
      "train loss:0.38110388468608597\n",
      "train loss:0.7926525490018745\n",
      "train loss:0.22863268720098134\n",
      "train loss:0.6525825120748938\n",
      "train loss:0.22512210072186326\n",
      "train loss:0.3731465814897346\n",
      "train loss:0.6005489357137405\n",
      "train loss:0.5988141070922179\n",
      "train loss:0.6071971748748383\n",
      "train loss:0.5135432515585097\n",
      "train loss:0.6204319541269147\n",
      "train loss:0.3991524824089253\n",
      "train loss:0.5124218844545789\n",
      "train loss:0.33805598723749486\n",
      "train loss:0.6322816689888188\n",
      "train loss:0.5002864079877567\n",
      "train loss:0.4077822533030934\n",
      "train loss:0.6296106664294233\n",
      "train loss:0.6101232104242318\n",
      "train loss:0.3792854020944958\n",
      "train loss:0.5135934610684975\n",
      "train loss:0.35043182479348706\n",
      "train loss:0.5891419851262526\n",
      "train loss:0.37075552116530774\n",
      "train loss:0.8435816892056127\n",
      "train loss:0.5281993778884813\n",
      "train loss:0.2894050496177099\n",
      "train loss:0.48288923489145413\n",
      "train loss:0.4941655339869091\n",
      "train loss:0.4935982826551582\n",
      "train loss:0.7315317886677217\n",
      "train loss:0.48349210400621356\n",
      "train loss:0.5852491786159071\n",
      "train loss:0.2727880425479098\n",
      "train loss:0.5963647588259977\n",
      "train loss:0.4361398490946404\n",
      "train loss:0.666032483474031\n",
      "train loss:0.626991069316648\n",
      "train loss:0.3116141183183027\n",
      "train loss:0.4765604173759006\n",
      "train loss:0.4153433410467498\n",
      "train loss:0.4670651507191622\n",
      "train loss:0.4842556210739806\n",
      "train loss:0.4724087226791287\n",
      "train loss:0.7290432824505564\n",
      "train loss:0.6058624253542906\n",
      "train loss:0.38165841320416305\n",
      "train loss:0.8414389203892204\n",
      "train loss:0.2871577722771609\n",
      "train loss:0.18281255953854786\n",
      "train loss:0.7880445647349793\n",
      "train loss:0.48955803975839596\n",
      "train loss:0.5961949596191001\n",
      "train loss:0.5918105260471012\n",
      "train loss:0.4886472517534598\n",
      "train loss:0.39330429970868724\n",
      "train loss:0.3059637852097656\n",
      "train loss:0.6099165618593119\n",
      "train loss:0.34689317492957555\n",
      "train loss:0.33830706481774875\n",
      "train loss:0.37894205812269177\n",
      "train loss:0.7310391101284622\n",
      "train loss:0.5598925390432082\n",
      "train loss:0.3492361666438705\n",
      "train loss:0.40879568226670565\n",
      "train loss:0.5232625715152336\n",
      "train loss:0.8241681996494179\n",
      "train loss:0.430368723310352\n",
      "train loss:0.2219552860588619\n",
      "train loss:0.3542215978982484\n",
      "train loss:0.4754770905697857\n",
      "train loss:0.5308880011219739\n",
      "train loss:0.6271685319816261\n",
      "train loss:0.5033000279722881\n",
      "train loss:0.4633232872158109\n",
      "train loss:0.26645791288613246\n",
      "train loss:0.5376473213807192\n",
      "train loss:0.734429290968512\n",
      "train loss:0.43394855704452623\n",
      "train loss:0.45410157912670107\n",
      "train loss:0.5343105299360947\n",
      "train loss:0.7278729777512668\n",
      "train loss:0.5663744281804555\n",
      "train loss:0.38651831816191895\n",
      "train loss:0.49439711556555305\n",
      "train loss:0.4688546033204727\n",
      "train loss:0.6564372497765225\n",
      "train loss:0.6921287331118868\n",
      "train loss:0.37456254432904823\n",
      "train loss:0.6288381934431293\n",
      "train loss:0.5740985129759508\n",
      "train loss:0.6325871141374939\n",
      "=== epoch:8, train acc:0.79, test acc:0.68 ===\n",
      "train loss:0.62601875463761\n",
      "train loss:0.5225882752906587\n",
      "train loss:0.5424982520241867\n",
      "train loss:0.5204794367257074\n",
      "train loss:0.41909405695040614\n",
      "train loss:0.6338908694410007\n",
      "train loss:0.3711904898992921\n",
      "train loss:0.36142289503712827\n",
      "train loss:0.7766739227571828\n",
      "train loss:0.3558834316960694\n",
      "train loss:0.5442211563697283\n",
      "train loss:0.614908523873611\n",
      "train loss:0.9698984378923866\n",
      "train loss:0.7169846742871504\n",
      "train loss:0.48768791585579974\n",
      "train loss:0.43716460067464774\n",
      "train loss:0.5299562737404323\n",
      "train loss:0.4134473700790869\n",
      "train loss:0.2834214882359278\n",
      "train loss:0.38443127470587324\n",
      "train loss:0.43253965048327075\n",
      "train loss:0.7326613379800299\n",
      "train loss:0.7697333124905676\n",
      "train loss:0.6094297404491686\n",
      "train loss:0.9287982830361207\n",
      "train loss:0.5261025212761328\n",
      "train loss:0.6988938561439736\n",
      "train loss:0.6148322444753381\n",
      "train loss:0.5802964591990294\n",
      "train loss:0.3527960648278159\n",
      "train loss:0.4274156268331647\n",
      "train loss:0.5940097188332182\n",
      "train loss:0.45191990498354373\n",
      "train loss:0.3194280313223466\n",
      "train loss:0.6960291289321968\n",
      "train loss:0.8009827283551145\n",
      "train loss:0.4098667174079436\n",
      "train loss:0.766066794901738\n",
      "train loss:0.4753241257008162\n",
      "train loss:0.5041435033509594\n",
      "train loss:0.6217586808340952\n",
      "train loss:0.5143222700391016\n",
      "train loss:0.6364662695479069\n",
      "train loss:0.5356132117698432\n",
      "train loss:0.5235483947704006\n",
      "train loss:0.7108499895132253\n",
      "train loss:0.3468849489823009\n",
      "train loss:0.5190263255503101\n",
      "train loss:0.8220936864773375\n",
      "train loss:0.5377764790311007\n",
      "train loss:0.38795433584534394\n",
      "train loss:0.6826463576328223\n",
      "train loss:0.48774081220391813\n",
      "train loss:0.31516443779998154\n",
      "train loss:0.6460292035138351\n",
      "train loss:0.6101983576429626\n",
      "train loss:0.4862493883150204\n",
      "train loss:0.5933913582435191\n",
      "train loss:0.6751679990131308\n",
      "train loss:0.6102233911486998\n",
      "train loss:0.5016951635602709\n",
      "train loss:0.6146692001933469\n",
      "train loss:0.44560130952798127\n",
      "train loss:0.38735158306880973\n",
      "train loss:0.6162229632365639\n",
      "train loss:0.29979104160498704\n",
      "train loss:0.6655427400786562\n",
      "train loss:0.633864410470855\n",
      "train loss:0.7744793081921248\n",
      "train loss:0.39797531447665324\n",
      "train loss:0.7069184079075407\n",
      "train loss:0.5362334169259223\n",
      "train loss:0.41484919168923085\n",
      "train loss:0.5332633232811369\n",
      "train loss:0.36725018423374567\n",
      "train loss:0.6530761437971309\n",
      "train loss:0.49604084127634474\n",
      "train loss:0.5001877222667066\n",
      "train loss:0.5420137277936925\n",
      "train loss:0.6419692869668949\n",
      "train loss:0.7878053048909035\n",
      "train loss:0.40362344427003183\n",
      "train loss:0.2015551972075386\n",
      "train loss:0.32443446118567926\n",
      "train loss:0.3590952707399869\n",
      "train loss:0.4766881911741631\n",
      "train loss:0.8287290573011792\n",
      "train loss:0.7663269321424686\n",
      "train loss:0.49705464034344393\n",
      "train loss:1.0868274238875277\n",
      "train loss:0.6016131950763182\n",
      "train loss:0.4166633833049322\n",
      "train loss:0.8037881062991425\n",
      "train loss:0.6092249216746828\n",
      "train loss:0.6557290105319473\n",
      "train loss:0.6086251216260969\n",
      "train loss:0.5357762743824349\n",
      "train loss:0.5536061741049145\n",
      "train loss:0.5347404319619983\n",
      "train loss:0.5211568309689791\n",
      "train loss:0.51029834044983\n",
      "train loss:0.47378018350081286\n",
      "train loss:0.3756792816415479\n",
      "train loss:0.5838693436454043\n",
      "train loss:0.4564958421310711\n",
      "train loss:0.44679690261868066\n",
      "train loss:0.5090024197313285\n",
      "train loss:0.6304729692461813\n",
      "train loss:0.4263276656753863\n",
      "train loss:0.4489945694581687\n",
      "train loss:0.3298098759558245\n",
      "train loss:0.42901246136255206\n",
      "train loss:0.5516503413584196\n",
      "train loss:0.5415156058060966\n",
      "train loss:0.4910820068259306\n",
      "train loss:0.4753128592675708\n",
      "train loss:0.6461078115357196\n",
      "train loss:0.6665298671851108\n",
      "train loss:0.654088081382921\n",
      "train loss:0.31525358493405714\n",
      "train loss:0.823944813259228\n",
      "train loss:0.601993284522573\n",
      "train loss:0.45150701313680075\n",
      "train loss:0.5895250907589513\n",
      "train loss:0.6299460939074233\n",
      "train loss:0.34485311693200915\n",
      "train loss:0.5165936818859932\n",
      "train loss:0.41939601965217965\n",
      "train loss:0.4711192906948535\n",
      "train loss:0.2857720837191122\n",
      "train loss:0.5528002939680565\n",
      "train loss:0.6709890219455488\n",
      "train loss:0.5473751913331821\n",
      "train loss:0.6742736923213821\n",
      "train loss:0.736883862132222\n",
      "train loss:0.5414949958331781\n",
      "train loss:0.6468167379242278\n",
      "train loss:0.37761575786203955\n",
      "train loss:0.5271196903438659\n",
      "train loss:0.6108609903438487\n",
      "train loss:0.432327437234534\n",
      "train loss:0.4858998114467232\n",
      "train loss:0.43190679556871225\n",
      "train loss:0.5032163862873696\n",
      "train loss:0.7108088714019674\n",
      "train loss:0.40537998941152037\n",
      "train loss:0.6885377227022897\n",
      "train loss:0.3608732240289748\n",
      "train loss:0.6303289275001587\n",
      "train loss:0.5793416752213888\n",
      "train loss:0.3484227854364252\n",
      "train loss:0.5626145809330778\n",
      "train loss:0.39952432735590243\n",
      "=== epoch:9, train acc:0.76, test acc:0.7 ===\n",
      "train loss:0.6677431465355943\n",
      "train loss:0.8170713132671035\n",
      "train loss:0.5044835450623817\n",
      "train loss:0.4667331262772635\n",
      "train loss:0.5727441814290326\n",
      "train loss:0.45077055765672613\n",
      "train loss:0.333512822772258\n",
      "train loss:0.42407703656496293\n",
      "train loss:0.6314974669716742\n",
      "train loss:0.38860158446894\n",
      "train loss:0.5926165168568112\n",
      "train loss:0.5672258922574339\n",
      "train loss:0.36653973132136636\n",
      "train loss:0.3349777857155184\n",
      "train loss:0.5070713131903208\n",
      "train loss:0.5770159029769009\n",
      "train loss:0.4760583211570285\n",
      "train loss:0.531702657571768\n",
      "train loss:0.674814990390692\n",
      "train loss:0.41567101553207814\n",
      "train loss:0.3515191472852725\n",
      "train loss:0.5227364475334968\n",
      "train loss:0.3619735935460641\n",
      "train loss:0.6023526772639256\n",
      "train loss:0.7841795911830136\n",
      "train loss:0.5902913456974933\n",
      "train loss:0.8759058790848874\n",
      "train loss:0.5677920830837182\n",
      "train loss:0.7074976525370522\n",
      "train loss:0.8374522765386505\n",
      "train loss:0.4278533810735084\n",
      "train loss:0.43275473624935723\n",
      "train loss:0.790914881668001\n",
      "train loss:0.6668145634949558\n",
      "train loss:0.47803821127138024\n",
      "train loss:0.6078240561798433\n",
      "train loss:0.6056798002918343\n",
      "train loss:0.421745901354668\n",
      "train loss:0.5805100390979175\n",
      "train loss:0.523910915670853\n",
      "train loss:0.42499269377391913\n",
      "train loss:0.7152525641815205\n",
      "train loss:0.6204241322216915\n",
      "train loss:0.3567686446055172\n",
      "train loss:0.33763188145579914\n",
      "train loss:0.5187093383456459\n",
      "train loss:0.3986536350875872\n",
      "train loss:0.35667450810741663\n",
      "train loss:0.5495617205525111\n",
      "train loss:0.6728305293986313\n",
      "train loss:0.7057889819486484\n",
      "train loss:0.7950218368043378\n",
      "train loss:0.3583090011576927\n",
      "train loss:0.46043348262940736\n",
      "train loss:0.6569469323268637\n",
      "train loss:0.40444288867894046\n",
      "train loss:0.568685925161896\n",
      "train loss:0.35765196497422386\n",
      "train loss:0.37860679797298374\n",
      "train loss:0.590369919802315\n",
      "train loss:0.5117281257898082\n",
      "train loss:0.37541983437630944\n",
      "train loss:0.3450985919904702\n",
      "train loss:0.4868175461939625\n",
      "train loss:0.8847302424369999\n",
      "train loss:0.6081759939325102\n",
      "train loss:0.31472092203040936\n",
      "train loss:0.5557005772306093\n",
      "train loss:0.6296255168298028\n",
      "train loss:0.37782630361907843\n",
      "train loss:0.605995367071012\n",
      "train loss:0.43693340636550404\n",
      "train loss:0.5373481004027358\n",
      "train loss:0.42426152576778187\n",
      "train loss:0.3612962812731049\n",
      "train loss:0.7228030428179189\n",
      "train loss:0.6990315794751496\n",
      "train loss:0.36166639495566777\n",
      "train loss:0.7747639923574174\n",
      "train loss:0.8726426733502288\n",
      "train loss:0.4807017777926414\n",
      "train loss:0.3731618419889236\n",
      "train loss:0.46804793696522184\n",
      "train loss:0.3874263296235635\n",
      "train loss:0.7503642920421999\n",
      "train loss:0.5316076065658206\n",
      "train loss:0.38971008173286054\n",
      "train loss:0.46430292653519256\n",
      "train loss:0.44515480447739364\n",
      "train loss:0.4299832781470586\n",
      "train loss:0.3209950465445795\n",
      "train loss:0.37770773271722563\n",
      "train loss:0.32701710502343884\n",
      "train loss:0.6833013976758835\n",
      "train loss:0.6028041845729236\n",
      "train loss:0.4790463129407989\n",
      "train loss:0.33212157077438487\n",
      "train loss:0.41683301275567175\n",
      "train loss:0.6811567082946989\n",
      "train loss:0.5265005159833722\n",
      "train loss:0.346436943159227\n",
      "train loss:0.8292907436582599\n",
      "train loss:0.5037987411647696\n",
      "train loss:0.40153628570778765\n",
      "train loss:0.5441903393921013\n",
      "train loss:0.7603637132566454\n",
      "train loss:0.3775383996273966\n",
      "train loss:0.5949585408124\n",
      "train loss:0.5730762968062333\n",
      "train loss:0.3901829706072197\n",
      "train loss:0.5297907725084368\n",
      "train loss:0.6401076156957057\n",
      "train loss:0.28759816956982165\n",
      "train loss:0.6047510355375451\n",
      "train loss:0.5175363295929496\n",
      "train loss:0.4554113749670063\n",
      "train loss:0.3676363517715089\n",
      "train loss:0.5047971499326952\n",
      "train loss:0.4468231946747915\n",
      "train loss:0.2825672937355725\n",
      "train loss:0.47643721477752454\n",
      "train loss:0.8869265368047621\n",
      "train loss:0.45029056317859084\n",
      "train loss:0.6152073771733647\n",
      "train loss:0.6573747134715201\n",
      "train loss:0.34748301897254585\n",
      "train loss:0.5767141255227959\n",
      "train loss:0.4795154954516902\n",
      "train loss:0.954548200297428\n",
      "train loss:0.44262193231377606\n",
      "train loss:0.6328041309962185\n",
      "train loss:0.6245612085188468\n",
      "train loss:0.41070766965882155\n",
      "train loss:0.507281557507218\n",
      "train loss:0.4883703787967881\n",
      "train loss:0.4713759681192933\n",
      "train loss:0.4401321470568463\n",
      "train loss:0.5975223866903986\n",
      "train loss:0.6588828288893752\n",
      "train loss:0.6757761550147877\n",
      "train loss:0.5825869415417738\n",
      "train loss:0.45137837130237723\n",
      "train loss:0.3226618505541303\n",
      "train loss:0.24343677569057146\n",
      "train loss:0.7249196761459598\n",
      "train loss:0.8035672728671839\n",
      "train loss:0.46472979493527183\n",
      "train loss:0.6321646317526831\n",
      "train loss:0.575210495054632\n",
      "train loss:0.40160331202334876\n",
      "train loss:0.6193286787939203\n",
      "train loss:0.6007681167704445\n",
      "train loss:0.6167411240753525\n",
      "=== epoch:10, train acc:0.76, test acc:0.69 ===\n",
      "train loss:0.8775236342835244\n",
      "train loss:0.8882713266061597\n",
      "train loss:0.5203717750027879\n",
      "train loss:0.6087895033567707\n",
      "train loss:0.6758181331692914\n",
      "train loss:0.6347624710012398\n",
      "train loss:0.37618233018072844\n",
      "train loss:0.5826452502591322\n",
      "train loss:0.45351481392733495\n",
      "train loss:0.44067493167933536\n",
      "train loss:0.49848393543624603\n",
      "train loss:0.5833523339653363\n",
      "train loss:0.6063140239729783\n",
      "train loss:0.5755413878050979\n",
      "train loss:0.6177214728783047\n",
      "train loss:0.6463565075144414\n",
      "train loss:0.5897329154121901\n",
      "train loss:0.5105553640959071\n",
      "train loss:0.5689330963571236\n",
      "train loss:0.6440795673556631\n",
      "train loss:0.5116430937883589\n",
      "train loss:0.5822842073901241\n",
      "train loss:0.7308886847819154\n",
      "train loss:0.44848246489453986\n",
      "train loss:0.4986954424494\n",
      "train loss:0.3961452130053404\n",
      "train loss:0.6561171928762937\n",
      "train loss:0.42649592324818775\n",
      "train loss:0.37108176617135874\n",
      "train loss:0.5545744730050826\n",
      "train loss:0.7418396914234513\n",
      "train loss:0.3490187880489312\n",
      "train loss:0.5207663741459425\n",
      "train loss:0.7188528124677114\n",
      "train loss:0.5225160534194403\n",
      "train loss:0.37817240954607023\n",
      "train loss:0.33883499709339737\n",
      "train loss:0.3532140636276387\n",
      "train loss:0.6138775578602444\n",
      "train loss:0.6903509915448302\n",
      "train loss:0.587349254899714\n",
      "train loss:0.5885402263873228\n",
      "train loss:0.5725693398188849\n",
      "train loss:0.35991415468503335\n",
      "train loss:0.6042079071253635\n",
      "train loss:0.5981508055606384\n",
      "train loss:0.6979483511447203\n",
      "train loss:0.6166321749196275\n",
      "train loss:0.513578013718939\n",
      "train loss:0.5265698372027516\n",
      "train loss:0.4235247314656629\n",
      "train loss:0.4839245546444005\n",
      "train loss:0.5794947148052548\n",
      "train loss:0.39365344988337847\n",
      "train loss:0.3928829249413084\n",
      "train loss:0.45346171537482194\n",
      "train loss:0.6125009943348549\n",
      "train loss:0.5925828095528793\n",
      "train loss:0.7858684557276139\n",
      "train loss:0.6129936972005982\n",
      "train loss:0.35023115979472086\n",
      "train loss:0.6767794938360819\n",
      "train loss:0.41137787770178225\n",
      "train loss:0.5647025675191064\n",
      "train loss:0.3727615806777892\n",
      "train loss:0.37307290979361313\n",
      "train loss:0.19044025293293543\n",
      "train loss:0.47651041326538157\n",
      "train loss:0.475759177541469\n",
      "train loss:0.49793826629518706\n",
      "train loss:0.3391105027326093\n",
      "train loss:0.40242282174406163\n",
      "train loss:0.44847656309905154\n",
      "train loss:0.6046932187790689\n",
      "train loss:0.49940844739857315\n",
      "train loss:0.45290775762183666\n",
      "train loss:0.5389456824312295\n",
      "train loss:0.4394894507229516\n",
      "train loss:0.23824958208695368\n",
      "train loss:0.5515930765785292\n",
      "train loss:0.7266664630331132\n",
      "train loss:0.3079056394916451\n",
      "train loss:0.3129286097817011\n",
      "train loss:0.41883728882237004\n",
      "train loss:0.6686775847971278\n",
      "train loss:0.740674065698226\n",
      "train loss:0.5260540567349178\n",
      "train loss:0.45593166780160443\n",
      "train loss:0.6693012656835899\n",
      "train loss:0.745461822126227\n",
      "train loss:0.5391465010527453\n",
      "train loss:0.5439273611137001\n",
      "train loss:0.5305006278600174\n",
      "train loss:0.4045510643605801\n",
      "train loss:0.5151219552046157\n",
      "train loss:0.6667019134238409\n",
      "train loss:0.7238899949247315\n",
      "train loss:0.417340957653116\n",
      "train loss:0.4937802080745285\n",
      "train loss:0.7232771602602595\n",
      "train loss:0.5111610257268376\n",
      "train loss:0.5364253533820837\n",
      "train loss:0.5161785151881965\n",
      "train loss:0.39475695845700265\n",
      "train loss:0.3722811240996773\n",
      "train loss:0.5864019362416424\n",
      "train loss:0.5728053140897096\n",
      "train loss:0.34238935462605885\n",
      "train loss:0.6531425815865486\n",
      "train loss:0.5874422000402348\n",
      "train loss:0.5769775007885266\n",
      "train loss:0.6876029196927034\n",
      "train loss:0.5230260518313353\n",
      "train loss:0.39951840146204265\n",
      "train loss:0.3149694006330216\n",
      "train loss:0.6677797353996926\n",
      "train loss:0.5366458792551146\n",
      "train loss:0.5837433976918717\n",
      "train loss:0.5531450917045848\n",
      "train loss:0.5631819385780902\n",
      "train loss:0.4258892091691326\n",
      "train loss:0.3036884228101383\n",
      "train loss:0.36094207176534737\n",
      "train loss:0.7567597732314535\n",
      "train loss:0.5439143518215926\n",
      "train loss:0.5301837402060443\n",
      "train loss:0.338785539841641\n",
      "train loss:0.7236198316358535\n",
      "train loss:0.37629534182913005\n",
      "train loss:0.4651416109748788\n",
      "train loss:0.4374560323415927\n",
      "train loss:0.3892149562298973\n",
      "train loss:0.6013979331980985\n",
      "train loss:0.800549172517746\n",
      "train loss:0.5648121979188419\n",
      "train loss:0.3930496154756364\n",
      "train loss:0.46701145606787087\n",
      "train loss:0.8761734134613448\n",
      "train loss:0.7375465884370517\n",
      "train loss:0.6473250909583318\n",
      "train loss:0.6236715637586123\n",
      "train loss:0.6606462069909751\n",
      "train loss:0.5254793191254011\n",
      "train loss:0.4036168326285708\n",
      "train loss:0.6309974731912613\n",
      "train loss:0.529984533173167\n",
      "train loss:0.6036065302503778\n",
      "train loss:0.6772364639180893\n",
      "train loss:0.6174843938611478\n",
      "train loss:0.6050595208977241\n",
      "train loss:0.6627771913451244\n",
      "train loss:0.436910235462274\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5450980392156862\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=800, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec0b0b30-43eb-4287-92a9-fb6214708f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6909824160788863\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6814744653227081\n",
      "train loss:0.6943661072527665\n",
      "train loss:0.6545389237549544\n",
      "train loss:0.6849004006511732\n",
      "train loss:0.6789284124042931\n",
      "train loss:0.6752038990788205\n",
      "train loss:0.7134651111585124\n",
      "train loss:0.5809431896150474\n",
      "train loss:0.6653015955241987\n",
      "train loss:0.4654802273841849\n",
      "train loss:0.6067103935879833\n",
      "train loss:0.7365855299405001\n",
      "train loss:0.729275843462019\n",
      "train loss:0.6099115760500659\n",
      "train loss:0.5088340200037056\n",
      "train loss:0.6862744062039976\n",
      "train loss:0.6889831886610962\n",
      "train loss:0.6044972757527932\n",
      "train loss:0.6248856693274496\n",
      "train loss:0.5470179511813014\n",
      "train loss:0.522876561244608\n",
      "train loss:0.6834554635549649\n",
      "train loss:0.8205610308238478\n",
      "train loss:0.4604783158472613\n",
      "train loss:0.7472575799093989\n",
      "train loss:0.6319409263968259\n",
      "train loss:0.5623161842496363\n",
      "train loss:0.6164063825068101\n",
      "train loss:0.6779633956714761\n",
      "train loss:0.48039636835310817\n",
      "train loss:0.5259978643158308\n",
      "train loss:0.624832194313813\n",
      "train loss:0.7821444085038392\n",
      "train loss:0.6438540626203348\n",
      "train loss:0.7718612580601085\n",
      "train loss:0.4613757655395726\n",
      "train loss:0.6970577352034899\n",
      "train loss:0.6120786229286649\n",
      "train loss:0.6838095831977615\n",
      "train loss:0.540706014505408\n",
      "train loss:0.6926402613964563\n",
      "train loss:0.6823685177789681\n",
      "train loss:0.6149820790343415\n",
      "train loss:0.6173974859761782\n",
      "train loss:0.5557546478248937\n",
      "train loss:0.7351611545849005\n",
      "train loss:0.48655959876746585\n",
      "train loss:0.5221998059691\n",
      "train loss:0.6022541582449126\n",
      "train loss:0.6688429892747373\n",
      "train loss:0.6951820458222068\n",
      "train loss:0.5897571051543093\n",
      "train loss:0.5040753964132069\n",
      "train loss:0.4063478202285687\n",
      "train loss:0.5123452255735708\n",
      "train loss:0.6430908720120467\n",
      "train loss:0.5851955250493655\n",
      "train loss:0.6340615615708886\n",
      "train loss:0.6164919990687661\n",
      "train loss:0.6386767580923365\n",
      "train loss:0.5202739913203855\n",
      "train loss:0.7981312043205148\n",
      "train loss:0.44485068276712303\n",
      "train loss:0.5298085996927545\n",
      "train loss:0.6940197081829385\n",
      "train loss:0.3822424566596582\n",
      "train loss:0.5219494754181027\n",
      "train loss:0.6918845975527974\n",
      "train loss:0.526132420058943\n",
      "train loss:0.7082278480200788\n",
      "train loss:0.3876793123261623\n",
      "train loss:0.6175865258692289\n",
      "train loss:0.6788047733515091\n",
      "train loss:0.3972029264433697\n",
      "train loss:0.8907478011730054\n",
      "train loss:0.6122352762420886\n",
      "train loss:0.6860941116886219\n",
      "train loss:0.7584419513251203\n",
      "train loss:0.748280145167459\n",
      "train loss:0.5687929378787167\n",
      "train loss:0.5147309393852487\n",
      "train loss:0.6639357053643151\n",
      "train loss:0.6265092774089301\n",
      "train loss:0.620431382853831\n",
      "train loss:0.7562685452982716\n",
      "train loss:0.6801610281894159\n",
      "train loss:0.58430695669045\n",
      "train loss:0.7127791725002565\n",
      "train loss:0.6005876325346053\n",
      "train loss:0.5932312403236731\n",
      "train loss:0.533286073931033\n",
      "train loss:0.5688487064131369\n",
      "train loss:0.683132109029025\n",
      "train loss:0.6141469323403028\n",
      "train loss:0.6775615073468857\n",
      "train loss:0.7073408631410986\n",
      "train loss:0.6130659965502543\n",
      "train loss:0.7528828176047733\n",
      "train loss:0.7600087340013719\n",
      "train loss:0.6861095862660707\n",
      "train loss:0.6137892152290293\n",
      "train loss:0.5865425028132161\n",
      "train loss:0.550156669619242\n",
      "train loss:0.40713269807868857\n",
      "train loss:0.6198948375999307\n",
      "train loss:0.7125409373881524\n",
      "train loss:0.6797303788428051\n",
      "train loss:0.441072331141103\n",
      "train loss:0.7904858566376446\n",
      "train loss:0.5258243422610122\n",
      "train loss:0.6061750211756937\n",
      "train loss:0.7108644274785241\n",
      "train loss:0.5138290250621965\n",
      "train loss:0.5223250559700576\n",
      "train loss:0.8100588083702085\n",
      "train loss:0.5978289830647019\n",
      "train loss:0.7793728031462441\n",
      "train loss:0.7616224758704039\n",
      "train loss:0.6710764391633139\n",
      "train loss:0.6167211693491755\n",
      "train loss:0.6711254438503516\n",
      "train loss:0.6188032565973851\n",
      "train loss:0.629067418606587\n",
      "train loss:0.713905724329176\n",
      "train loss:0.7380061730884176\n",
      "train loss:0.5761172476262792\n",
      "train loss:0.6790055342901737\n",
      "train loss:0.6033075813604661\n",
      "train loss:0.5268980330811599\n",
      "train loss:0.6688208012961515\n",
      "train loss:0.5920852375667092\n",
      "train loss:0.5827029002137997\n",
      "train loss:0.6699888295243113\n",
      "train loss:0.6185101337667307\n",
      "train loss:0.6786855083079013\n",
      "train loss:0.4662382765107257\n",
      "train loss:0.8255717986957016\n",
      "train loss:0.6794767180266538\n",
      "train loss:0.4406482177684695\n",
      "train loss:0.7627196031386059\n",
      "train loss:0.6092423425941617\n",
      "train loss:0.5153988086510876\n",
      "train loss:0.6178463216616045\n",
      "train loss:0.6235113357723139\n",
      "train loss:0.4258167886759974\n",
      "train loss:0.5113286771930211\n",
      "train loss:0.9356521849937721\n",
      "train loss:0.624357344932849\n",
      "train loss:0.6135929849194904\n",
      "train loss:0.6083163443117277\n",
      "train loss:0.6774201659653457\n",
      "train loss:0.820428185122255\n",
      "train loss:0.5569354214064052\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5483219162840571\n",
      "train loss:0.6097535024187016\n",
      "train loss:0.6193131517554644\n",
      "train loss:0.6771193186815216\n",
      "train loss:0.7410502161310915\n",
      "train loss:0.6238824306819313\n",
      "train loss:0.5632337200312594\n",
      "train loss:0.4683134914915703\n",
      "train loss:0.783009473667635\n",
      "train loss:0.6243192779510378\n",
      "train loss:0.5672993454887219\n",
      "train loss:0.7332140174466143\n",
      "train loss:0.7267902110974812\n",
      "train loss:0.620063171665288\n",
      "train loss:0.7469824736351718\n",
      "train loss:0.5097561415719287\n",
      "train loss:0.6220033779630844\n",
      "train loss:0.5540672935065868\n",
      "train loss:0.5356572044357399\n",
      "train loss:0.5548201778229609\n",
      "train loss:0.5295352424275174\n",
      "train loss:0.5241847343883593\n",
      "train loss:0.9589513652110131\n",
      "train loss:0.7958650192043335\n",
      "train loss:0.7689057331927206\n",
      "train loss:0.4488452927820326\n",
      "train loss:0.6138761385589588\n",
      "train loss:0.5225768534827472\n",
      "train loss:0.5995997116754641\n",
      "train loss:0.8429523330567307\n",
      "train loss:0.6655980458293744\n",
      "train loss:0.7351514235699124\n",
      "train loss:0.6153002095807264\n",
      "train loss:0.4306084248821924\n",
      "train loss:0.4853400612200821\n",
      "train loss:0.6186304173082005\n",
      "train loss:0.7334892311028054\n",
      "train loss:0.5992380836688633\n",
      "train loss:0.534626310807435\n",
      "train loss:0.6323338665699432\n",
      "train loss:0.7594088820313522\n",
      "train loss:0.5964938819296208\n",
      "train loss:0.4589748454175188\n",
      "train loss:0.6093494696052707\n",
      "train loss:0.6172742053518752\n",
      "train loss:0.6793818729976654\n",
      "train loss:0.3185805816679487\n",
      "train loss:0.7988098944866774\n",
      "train loss:0.7028246488113554\n",
      "train loss:0.4145194844191531\n",
      "train loss:0.7769345844133035\n",
      "train loss:0.5825816698925628\n",
      "train loss:0.5311561742456241\n",
      "train loss:0.6080118994156494\n",
      "train loss:0.7200630229576507\n",
      "train loss:0.6935860294875069\n",
      "train loss:0.5474141308368481\n",
      "train loss:0.5175484347324496\n",
      "train loss:0.681984275368527\n",
      "train loss:0.5190126793118266\n",
      "train loss:0.5349979362053544\n",
      "train loss:0.5149266774334204\n",
      "train loss:0.4333133582947812\n",
      "train loss:0.6834106352162763\n",
      "train loss:0.29456625658455343\n",
      "train loss:0.3723497377880742\n",
      "train loss:0.3540421087229608\n",
      "train loss:0.5216870149412514\n",
      "train loss:0.8186325439656821\n",
      "train loss:0.7381280770601797\n",
      "train loss:0.8289689342666013\n",
      "train loss:0.5927930958830376\n",
      "train loss:0.5104554196522838\n",
      "train loss:0.7234759916707325\n",
      "train loss:0.6060381066681945\n",
      "train loss:0.587179963568409\n",
      "train loss:0.6014915122059149\n",
      "train loss:0.7195302548312087\n",
      "train loss:0.5837057550319047\n",
      "train loss:0.5771001175261788\n",
      "train loss:0.6553481002331583\n",
      "train loss:0.5660147355513063\n",
      "train loss:0.5114910662602469\n",
      "train loss:0.6037324054326108\n",
      "train loss:0.6104205269061567\n",
      "train loss:0.4753964285951825\n",
      "train loss:0.599245423992232\n",
      "train loss:0.6847101459137045\n",
      "train loss:0.3178284140350499\n",
      "train loss:0.7161758458197556\n",
      "train loss:0.7198115332548014\n",
      "train loss:0.3746404879163161\n",
      "train loss:0.4799256325621755\n",
      "train loss:0.6379177456733692\n",
      "train loss:0.21468095064216403\n",
      "train loss:0.17361268111310327\n",
      "train loss:0.7010532641051385\n",
      "train loss:0.6843739205568469\n",
      "train loss:1.209797282831299\n",
      "train loss:0.48721830660647897\n",
      "train loss:0.9501337980364968\n",
      "train loss:0.5124060005515041\n",
      "train loss:0.5907518200945098\n",
      "train loss:0.6725648845738695\n",
      "train loss:0.5590878400131976\n",
      "train loss:0.6360758197286263\n",
      "train loss:0.6748586757360023\n",
      "train loss:0.7798645250131987\n",
      "train loss:0.6190697957046009\n",
      "train loss:0.6424710903230644\n",
      "train loss:0.6301591078531741\n",
      "train loss:0.5488276886291611\n",
      "train loss:0.6293593149686536\n",
      "train loss:0.6595341191383898\n",
      "train loss:0.6038075247666181\n",
      "train loss:0.6221332644052403\n",
      "train loss:0.6574072125519701\n",
      "train loss:0.5867029126979447\n",
      "train loss:0.7003769881783636\n",
      "train loss:0.5809188139923388\n",
      "train loss:0.6409812046215142\n",
      "train loss:0.7171904664593212\n",
      "train loss:0.5984828948917694\n",
      "train loss:0.8268128323261126\n",
      "train loss:0.6762089498857043\n",
      "train loss:0.6769018212265641\n",
      "train loss:0.5063384771689755\n",
      "train loss:0.6725903096955241\n",
      "train loss:0.6112431317799614\n",
      "train loss:0.5486658806027901\n",
      "train loss:0.740515932772595\n",
      "train loss:0.40721500280047007\n",
      "train loss:0.7042042120453884\n",
      "train loss:0.6671773127854308\n",
      "train loss:0.6499431850981222\n",
      "train loss:0.5339353286840754\n",
      "train loss:0.620240218953111\n",
      "train loss:0.36196108191367954\n",
      "train loss:0.3332091681579111\n",
      "train loss:0.7189720257952339\n",
      "train loss:0.49005995122299406\n",
      "train loss:0.7015508093473173\n",
      "train loss:0.7211055453426896\n",
      "train loss:0.6033031871581102\n",
      "train loss:0.7277120390991633\n",
      "train loss:0.5716503344486179\n",
      "train loss:0.5003860190386965\n",
      "train loss:0.7828952967193722\n",
      "train loss:0.7983221761683245\n",
      "train loss:0.6181674605309669\n",
      "train loss:0.4128119883669753\n",
      "train loss:0.7858365574618502\n",
      "train loss:0.4803586785629722\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5138637301191554\n",
      "train loss:0.5709633734217457\n",
      "train loss:0.5295980517649156\n",
      "train loss:0.7501304408493819\n",
      "train loss:0.5780302468730184\n",
      "train loss:0.5367410878424851\n",
      "train loss:0.5331214530589135\n",
      "train loss:0.6931791061969225\n",
      "train loss:0.7781321318322955\n",
      "train loss:0.6478600961485453\n",
      "train loss:0.5213352579456818\n",
      "train loss:0.538832299082969\n",
      "train loss:0.6271184756986766\n",
      "train loss:0.5798410547300913\n",
      "train loss:0.5423932403801236\n",
      "train loss:0.6112850387209785\n",
      "train loss:0.6872623389449888\n",
      "train loss:0.46850739182552303\n",
      "train loss:0.7062014324498362\n",
      "train loss:0.8970180754276076\n",
      "train loss:0.5609301281870993\n",
      "train loss:0.4835960871544097\n",
      "train loss:0.5318889460145031\n",
      "train loss:0.6051160765727743\n",
      "train loss:0.6817637579959926\n",
      "train loss:0.7978897115617161\n",
      "train loss:0.5631085661113455\n",
      "train loss:0.6859964617535323\n",
      "train loss:0.6669174186664126\n",
      "train loss:0.6287283449410175\n",
      "train loss:0.6995565623217389\n",
      "train loss:0.6159141358116001\n",
      "train loss:0.5160350920870063\n",
      "train loss:0.5271556350480728\n",
      "train loss:0.5715187308503726\n",
      "train loss:0.5237111032201563\n",
      "train loss:0.6308467443075877\n",
      "train loss:0.5421271174353041\n",
      "train loss:0.44594869650681146\n",
      "train loss:0.6166471601333877\n",
      "train loss:0.5063298633371447\n",
      "train loss:0.4481795007847508\n",
      "train loss:0.8775896627047096\n",
      "train loss:0.5226832988109394\n",
      "train loss:0.47020989371600674\n",
      "train loss:0.4653800219629319\n",
      "train loss:0.7440885800133066\n",
      "train loss:0.7877596851919357\n",
      "train loss:0.35938894839527835\n",
      "train loss:0.8216270871217629\n",
      "train loss:0.3970591314629167\n",
      "train loss:0.8074954020947667\n",
      "train loss:0.39040960860896207\n",
      "train loss:0.6629768627749903\n",
      "train loss:0.5631407781780732\n",
      "train loss:0.7056528081902635\n",
      "train loss:0.7453427807526695\n",
      "train loss:0.6338261191278128\n",
      "train loss:0.5521406242899839\n",
      "train loss:0.47529118829874345\n",
      "train loss:0.6195620267800187\n",
      "train loss:0.4431656192889271\n",
      "train loss:0.4722122196752757\n",
      "train loss:0.5771351371086797\n",
      "train loss:0.43379925126542307\n",
      "train loss:0.8631350035127084\n",
      "train loss:0.9180107457162412\n",
      "train loss:0.7205457539457862\n",
      "train loss:0.7158851617306925\n",
      "train loss:0.5406903611588971\n",
      "train loss:0.5491443792759344\n",
      "train loss:0.8263630167053378\n",
      "train loss:0.5142932535176541\n",
      "train loss:0.7151858241999591\n",
      "train loss:0.5936686780058064\n",
      "train loss:0.7294791914830917\n",
      "train loss:0.5470025811885528\n",
      "train loss:0.5647266749072098\n",
      "train loss:0.5651224591349557\n",
      "train loss:0.7307574658053833\n",
      "train loss:0.5367220637710057\n",
      "train loss:0.6137893741058986\n",
      "train loss:0.7145278552215683\n",
      "train loss:0.7565921717796243\n",
      "train loss:0.4795896873172637\n",
      "train loss:0.655952806158133\n",
      "train loss:0.7089972052305555\n",
      "train loss:0.5626744811925162\n",
      "train loss:0.7445156885802884\n",
      "train loss:0.6753574880770677\n",
      "train loss:0.6879360330242164\n",
      "train loss:0.46461334874493276\n",
      "train loss:0.46444778767473\n",
      "train loss:0.39549671779922607\n",
      "train loss:0.7566463566089288\n",
      "train loss:0.7640208910175188\n",
      "train loss:0.526989708258268\n",
      "train loss:0.5184002183876927\n",
      "train loss:0.328883913906667\n",
      "train loss:0.6917646941824606\n",
      "train loss:0.7078602849678749\n",
      "train loss:0.29909015873438677\n",
      "train loss:0.4681731749739021\n",
      "train loss:0.6193332493983021\n",
      "train loss:0.3539203004787025\n",
      "train loss:0.520979701101045\n",
      "train loss:0.806864174903026\n",
      "train loss:0.7719197204684772\n",
      "train loss:0.508446468659306\n",
      "train loss:0.3568618893769132\n",
      "train loss:0.512686104583652\n",
      "train loss:0.6998514027946018\n",
      "train loss:0.3736265880550226\n",
      "train loss:0.31555841977363885\n",
      "train loss:0.4744276349191899\n",
      "train loss:0.6609469053256587\n",
      "train loss:0.6342449907728913\n",
      "train loss:0.5184614483690496\n",
      "train loss:0.6175538006936355\n",
      "train loss:0.6353991147982894\n",
      "train loss:0.6483441154956833\n",
      "train loss:0.5207516265604593\n",
      "train loss:0.7528525553879213\n",
      "train loss:0.5935117189648534\n",
      "train loss:0.6775064942287634\n",
      "train loss:0.767027016691676\n",
      "train loss:0.4848602418252663\n",
      "train loss:0.5816016955833414\n",
      "train loss:0.5199452645353481\n",
      "train loss:0.6305754820611897\n",
      "train loss:0.5108559866555458\n",
      "train loss:0.6854072328893597\n",
      "train loss:0.6537937257389815\n",
      "train loss:0.5882974355268152\n",
      "train loss:0.7916772658611066\n",
      "train loss:0.7123056258576506\n",
      "train loss:0.41851398069877266\n",
      "train loss:0.5709506532818581\n",
      "train loss:0.4906467118965054\n",
      "train loss:0.5303223249869912\n",
      "train loss:0.6666029946268421\n",
      "train loss:0.5523902427552523\n",
      "train loss:0.556668278510839\n",
      "train loss:0.706584037841811\n",
      "train loss:0.6051940931232681\n",
      "train loss:0.5913818278201916\n",
      "train loss:0.49312361081660566\n",
      "train loss:0.8137371227381125\n",
      "train loss:0.5023278872920622\n",
      "train loss:0.4984553922241758\n",
      "train loss:0.5257309463590233\n",
      "train loss:0.6369872717637716\n",
      "train loss:0.2866473731752205\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7439412033504513\n",
      "train loss:0.3978218351190526\n",
      "train loss:0.3766726677678609\n",
      "train loss:0.6348787545862515\n",
      "train loss:0.4916662073908757\n",
      "train loss:0.19665459559429213\n",
      "train loss:0.6779594713527379\n",
      "train loss:0.532937495654278\n",
      "train loss:0.6719380776847287\n",
      "train loss:0.507553313844329\n",
      "train loss:0.36074340627070545\n",
      "train loss:0.35237888230710046\n",
      "train loss:0.32125936866163723\n",
      "train loss:0.1439831601205728\n",
      "train loss:0.9098575993003719\n",
      "train loss:0.4686224694988\n",
      "train loss:0.5750906691252529\n",
      "train loss:0.6303402511203717\n",
      "train loss:0.615232494898894\n",
      "train loss:0.9494670858273793\n",
      "train loss:0.35224653555707497\n",
      "train loss:0.7262082513921345\n",
      "train loss:0.5809044834071267\n",
      "train loss:0.6254429366572577\n",
      "train loss:0.4747928240361145\n",
      "train loss:0.6190035838681855\n",
      "train loss:0.6260312365808212\n",
      "train loss:0.5119709709061722\n",
      "train loss:0.6084560135429459\n",
      "train loss:0.5450631683027896\n",
      "train loss:0.5402273761774996\n",
      "train loss:0.4882928802961361\n",
      "train loss:0.4593205858016728\n",
      "train loss:0.5653782647975274\n",
      "train loss:0.5152443760089225\n",
      "train loss:0.5987257185365051\n",
      "train loss:0.44721178934390593\n",
      "train loss:0.6496575496457714\n",
      "train loss:0.5037656694897126\n",
      "train loss:0.3608328910140778\n",
      "train loss:0.6043756918311001\n",
      "train loss:0.7977408521773777\n",
      "train loss:0.5422965719180988\n",
      "train loss:0.3622864867551157\n",
      "train loss:0.4191591504440585\n",
      "train loss:0.4813351674515845\n",
      "train loss:0.4873051512450008\n",
      "train loss:0.3695771968803879\n",
      "train loss:0.34257067510919115\n",
      "train loss:0.7543154169261526\n",
      "train loss:0.6072972187639725\n",
      "train loss:0.3848184653516039\n",
      "train loss:0.4720709454347798\n",
      "train loss:0.4134157886603463\n",
      "train loss:0.674307096027096\n",
      "train loss:0.5559071780163076\n",
      "train loss:0.5369865833490106\n",
      "train loss:0.5264272834671762\n",
      "train loss:0.7251155991415068\n",
      "train loss:0.7186172749474755\n",
      "train loss:0.6498645742541199\n",
      "train loss:0.4979888427892563\n",
      "train loss:0.5065712565124287\n",
      "train loss:0.6701510826007444\n",
      "train loss:0.5952965613498291\n",
      "train loss:0.6250156697763452\n",
      "train loss:0.6478693506334081\n",
      "train loss:0.47450444404073194\n",
      "train loss:0.6981255880443215\n",
      "train loss:0.4656706387129276\n",
      "train loss:0.7924215243248256\n",
      "train loss:0.7612616008126559\n",
      "train loss:0.6678209067540574\n",
      "train loss:0.5788862324485375\n",
      "train loss:0.6416647004200356\n",
      "train loss:0.5062931960171005\n",
      "train loss:0.5296138769833328\n",
      "train loss:0.623461321429161\n",
      "train loss:0.6375217298200471\n",
      "train loss:0.6046432317830118\n",
      "train loss:0.6443674996001205\n",
      "train loss:0.6318293394923004\n",
      "train loss:0.5613238069404163\n",
      "train loss:0.49190824805269007\n",
      "train loss:0.5064588469032606\n",
      "train loss:0.6387440329812369\n",
      "train loss:0.3744467920955993\n",
      "train loss:0.6978681015359885\n",
      "train loss:0.37953696102339124\n",
      "train loss:0.8535833684768688\n",
      "train loss:0.779510495894632\n",
      "train loss:0.6235397580705209\n",
      "train loss:0.39305897150744984\n",
      "train loss:0.49550503764021564\n",
      "train loss:0.5575340050092553\n",
      "train loss:0.7045585615033854\n",
      "train loss:0.493667620459762\n",
      "train loss:0.708008961802373\n",
      "train loss:0.5196820255010315\n",
      "train loss:0.5919513733224255\n",
      "train loss:0.6228358990406555\n",
      "train loss:0.536599842037931\n",
      "train loss:0.5008091704835265\n",
      "train loss:0.5550858087492052\n",
      "train loss:0.4621803701697817\n",
      "train loss:0.3382596963799497\n",
      "train loss:0.5805211090125625\n",
      "train loss:0.5058137771405804\n",
      "train loss:0.5351370049078981\n",
      "train loss:0.395344807735739\n",
      "train loss:0.6392595406286181\n",
      "train loss:0.18773312576324824\n",
      "train loss:0.8313542136523802\n",
      "train loss:0.6377078288554849\n",
      "train loss:0.5382818436019907\n",
      "train loss:0.6379899029454158\n",
      "train loss:0.39927998273585424\n",
      "train loss:0.4872456927112759\n",
      "train loss:0.7011730927153093\n",
      "train loss:0.33140327289493826\n",
      "train loss:0.32458142183413563\n",
      "train loss:0.495267898634998\n",
      "train loss:0.48654721343488533\n",
      "train loss:0.5386600864010236\n",
      "train loss:0.4302197950140239\n",
      "train loss:0.6292873877612253\n",
      "train loss:0.647890428960876\n",
      "train loss:0.6336661307402183\n",
      "train loss:0.6339538146067348\n",
      "train loss:0.5067681340193145\n",
      "train loss:0.7493609834202457\n",
      "train loss:0.7759880819203472\n",
      "train loss:0.47688791904613587\n",
      "train loss:0.5597513496307962\n",
      "train loss:0.45059306881499506\n",
      "train loss:0.4998249759606795\n",
      "train loss:0.7858489201932511\n",
      "train loss:0.5551382634041155\n",
      "train loss:0.45343013363941775\n",
      "train loss:0.6274935473533578\n",
      "train loss:0.7447435485086384\n",
      "train loss:0.509826445303933\n",
      "train loss:0.5659772915816537\n",
      "train loss:0.5592786052097546\n",
      "train loss:0.5971677719483923\n",
      "train loss:0.4660506601252238\n",
      "train loss:0.5771585439512943\n",
      "train loss:0.49382905696436186\n",
      "train loss:0.57892530310898\n",
      "train loss:0.5882427834371418\n",
      "train loss:0.3601758884203545\n",
      "train loss:0.6750018248761287\n",
      "train loss:0.8829863204193991\n",
      "=== epoch:5, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5720555221268293\n",
      "train loss:0.4952749125071194\n",
      "train loss:0.5445572675736485\n",
      "train loss:0.7102713602853911\n",
      "train loss:0.7056435645521291\n",
      "train loss:0.5572362902510993\n",
      "train loss:0.5751170186441925\n",
      "train loss:0.532279464285083\n",
      "train loss:0.6216255760502609\n",
      "train loss:0.5769879890969445\n",
      "train loss:0.5304766841903727\n",
      "train loss:0.5542603500674073\n",
      "train loss:0.6698454352295858\n",
      "train loss:0.45190401391717894\n",
      "train loss:0.635965647645465\n",
      "train loss:0.5497571281212703\n",
      "train loss:0.40938945093303303\n",
      "train loss:0.69181814980252\n",
      "train loss:0.5346880805407344\n",
      "train loss:0.557686723033822\n",
      "train loss:0.47297713542159403\n",
      "train loss:0.5641186651961511\n",
      "train loss:0.36270482140174337\n",
      "train loss:0.7302010000311014\n",
      "train loss:0.9101199887433514\n",
      "train loss:0.6983911007287036\n",
      "train loss:0.5443656319820505\n",
      "train loss:0.3457799674825094\n",
      "train loss:0.36588715356051854\n",
      "train loss:0.3228045226575482\n",
      "train loss:0.5849352325333113\n",
      "train loss:0.5918209662438321\n",
      "train loss:0.5284500999623862\n",
      "train loss:0.7638080337384867\n",
      "train loss:0.5789464240359458\n",
      "train loss:0.7363738560750603\n",
      "train loss:0.45695203625675235\n",
      "train loss:0.43936707212232456\n",
      "train loss:0.40317602654654755\n",
      "train loss:0.48226835627298703\n",
      "train loss:0.5816307906713786\n",
      "train loss:0.5436168388476641\n",
      "train loss:0.6474403073610266\n",
      "train loss:0.5097249797729047\n",
      "train loss:0.3804845632858052\n",
      "train loss:0.5036335328610788\n",
      "train loss:0.40895056455266854\n",
      "train loss:0.9934851392156652\n",
      "train loss:0.7771897483007265\n",
      "train loss:0.37514537995705766\n",
      "train loss:0.7453397289006756\n",
      "train loss:0.8079162751483565\n",
      "train loss:0.43905674685980367\n",
      "train loss:0.459199297008403\n",
      "train loss:0.40603605158966694\n",
      "train loss:0.5828694451435712\n",
      "train loss:0.5270560569197725\n",
      "train loss:0.5037517175331605\n",
      "train loss:0.5646755602902908\n",
      "train loss:0.5078784353334628\n",
      "train loss:0.33998089967110273\n",
      "train loss:0.49312365765494404\n",
      "train loss:0.535955492571631\n",
      "train loss:0.500001558535053\n",
      "train loss:0.4431080904567243\n",
      "train loss:0.3243463555866183\n",
      "train loss:0.8041951492889797\n",
      "train loss:0.8190869492044841\n",
      "train loss:0.7931419660621051\n",
      "train loss:0.5776121124630759\n",
      "train loss:0.3679779678392139\n",
      "train loss:0.4065467396830603\n",
      "train loss:0.4932324960769578\n",
      "train loss:0.5397846473976734\n",
      "train loss:0.6724713096329207\n",
      "train loss:0.7315068146217703\n",
      "train loss:0.6986088562410808\n",
      "train loss:0.5614157573797065\n",
      "train loss:0.8567580537572346\n",
      "train loss:0.4625519278321241\n",
      "train loss:0.5355283744025708\n",
      "train loss:0.43746845224580755\n",
      "train loss:0.5539370291828025\n",
      "train loss:0.7706967232705224\n",
      "train loss:0.5976206913582929\n",
      "train loss:0.7280823859784578\n",
      "train loss:0.49697004981550164\n",
      "train loss:0.49734257910356766\n",
      "train loss:0.7393793046695641\n",
      "train loss:0.6058288125108516\n",
      "train loss:0.5108647012962833\n",
      "train loss:0.5143963326093792\n",
      "train loss:0.5966004534803584\n",
      "train loss:0.5856537384430456\n",
      "train loss:0.7042200419203974\n",
      "train loss:0.41260390105526457\n",
      "train loss:0.5985844848394847\n",
      "train loss:0.39780183915754697\n",
      "train loss:0.4800254368864046\n",
      "train loss:0.5996181453257378\n",
      "train loss:0.6875353242879683\n",
      "train loss:0.531358005139839\n",
      "train loss:0.6009322236905731\n",
      "train loss:0.7256619375457901\n",
      "train loss:0.6310572910175676\n",
      "train loss:0.5261633593018293\n",
      "train loss:0.5259840286543719\n",
      "train loss:0.5621514585992888\n",
      "train loss:0.49144180504391644\n",
      "train loss:0.46650870155912794\n",
      "train loss:0.431376753058624\n",
      "train loss:0.7173702118933667\n",
      "train loss:0.6657426087581096\n",
      "train loss:0.36551932705062073\n",
      "train loss:0.742380226004598\n",
      "train loss:0.7492478180164549\n",
      "train loss:0.387127220749648\n",
      "train loss:0.911614408533357\n",
      "train loss:0.41462546159907526\n",
      "train loss:0.5778237587999441\n",
      "train loss:0.40737447814814526\n",
      "train loss:0.7873025927834046\n",
      "train loss:0.6855204454675861\n",
      "train loss:0.5242265318093111\n",
      "train loss:0.6441542605996661\n",
      "train loss:0.7382014871704589\n",
      "train loss:0.5639436386370045\n",
      "train loss:0.5386498020358979\n",
      "train loss:0.3967608767516926\n",
      "train loss:0.4261050035807784\n",
      "train loss:0.5556950595560741\n",
      "train loss:0.6191544633919246\n",
      "train loss:0.5142959331363092\n",
      "train loss:0.5543500620120478\n",
      "train loss:0.831213959648332\n",
      "train loss:0.6709413366541264\n",
      "train loss:0.5106726790916984\n",
      "train loss:0.6949287907075055\n",
      "train loss:0.6251500646737318\n",
      "train loss:0.5150826751008982\n",
      "train loss:0.5067446810308864\n",
      "train loss:0.45960777208113346\n",
      "train loss:0.39202584143674646\n",
      "train loss:0.5004940316202932\n",
      "train loss:0.6413559561310189\n",
      "train loss:0.46264125115956223\n",
      "train loss:0.5790713689301028\n",
      "train loss:0.553730739697678\n",
      "train loss:0.38989893854670654\n",
      "train loss:0.49597531364154923\n",
      "train loss:1.046089146512964\n",
      "train loss:0.5455596209490611\n",
      "train loss:0.6352774004654076\n",
      "=== epoch:6, train acc:0.72, test acc:0.7 ===\n",
      "train loss:0.5051190384790984\n",
      "train loss:0.4226415176568751\n",
      "train loss:0.41852669800604286\n",
      "train loss:0.6726760490708098\n",
      "train loss:0.40587240073246866\n",
      "train loss:0.7038053735261597\n",
      "train loss:0.450993423042407\n",
      "train loss:0.6805402029537879\n",
      "train loss:0.5951415574000332\n",
      "train loss:0.8967045581041994\n",
      "train loss:0.5690920868844767\n",
      "train loss:0.6459353407340638\n",
      "train loss:0.5988523233132239\n",
      "train loss:0.4553929121097979\n",
      "train loss:0.3897672758217282\n",
      "train loss:0.36951992062877803\n",
      "train loss:0.3521991069984974\n",
      "train loss:0.39349400988842687\n",
      "train loss:0.7748546568893875\n",
      "train loss:0.3819594869238333\n",
      "train loss:0.5196003796243948\n",
      "train loss:0.36165360589459516\n",
      "train loss:0.4778958683318099\n",
      "train loss:0.6829787382552035\n",
      "train loss:0.27900678882971736\n",
      "train loss:0.3449957442394402\n",
      "train loss:0.4967925159136025\n",
      "train loss:0.14072003489959872\n",
      "train loss:0.4028463573452744\n",
      "train loss:0.6336763101630558\n",
      "train loss:0.8551203679735494\n",
      "train loss:0.522875062174339\n",
      "train loss:0.31336134784123815\n",
      "train loss:0.40749647074593226\n",
      "train loss:0.20512388418895208\n",
      "train loss:0.6551054616044515\n",
      "train loss:0.7417860861330798\n",
      "train loss:0.5877458492489858\n",
      "train loss:0.4235586975546555\n",
      "train loss:0.2753258919250184\n",
      "train loss:0.4077559601204679\n",
      "train loss:0.2916043508786884\n",
      "train loss:0.4483964277855505\n",
      "train loss:0.42930825701941056\n",
      "train loss:0.6604262791709882\n",
      "train loss:0.5054000236070195\n",
      "train loss:0.6702174039719597\n",
      "train loss:0.6213208273597711\n",
      "train loss:0.5082792478368694\n",
      "train loss:0.6057676711408234\n",
      "train loss:0.5409091653096011\n",
      "train loss:0.6313419525854638\n",
      "train loss:0.6441722580761862\n",
      "train loss:0.5256532842093591\n",
      "train loss:0.6119836456887151\n",
      "train loss:0.5366817288692446\n",
      "train loss:0.6243083939970238\n",
      "train loss:0.6728404735535978\n",
      "train loss:0.6284433797195796\n",
      "train loss:0.4454896226240376\n",
      "train loss:0.529600795235255\n",
      "train loss:0.5255129356862226\n",
      "train loss:0.4973004349468761\n",
      "train loss:0.5577082021844701\n",
      "train loss:0.9390547027352266\n",
      "train loss:0.5539647518021859\n",
      "train loss:0.4113238894886937\n",
      "train loss:0.5042994378037255\n",
      "train loss:0.540953447027035\n",
      "train loss:0.5020141115496378\n",
      "train loss:0.5290493226306492\n",
      "train loss:0.5534468024808306\n",
      "train loss:0.7743232904406226\n",
      "train loss:0.659885341046112\n",
      "train loss:0.5144961507436128\n",
      "train loss:0.38568072343822113\n",
      "train loss:0.5531263537726103\n",
      "train loss:0.6171647705262726\n",
      "train loss:0.6121358301830168\n",
      "train loss:0.5235615254322045\n",
      "train loss:0.5527276645900604\n",
      "train loss:0.5317798374715997\n",
      "train loss:0.6477876582592558\n",
      "train loss:0.7402771252804461\n",
      "train loss:0.657611319555689\n",
      "train loss:0.8677712392827261\n",
      "train loss:0.5049395883809692\n",
      "train loss:0.5388344962777997\n",
      "train loss:0.5608910850514779\n",
      "train loss:0.622957414991148\n",
      "train loss:0.5211276180066502\n",
      "train loss:0.4321117630686994\n",
      "train loss:0.43784249542563625\n",
      "train loss:0.6991336226003761\n",
      "train loss:0.6174228084391273\n",
      "train loss:0.6325803161559594\n",
      "train loss:0.6026748062041858\n",
      "train loss:0.6749641893596664\n",
      "train loss:0.5738498570716966\n",
      "train loss:0.6881485447923794\n",
      "train loss:0.49636894999392434\n",
      "train loss:0.4658201012606068\n",
      "train loss:0.6768595834731694\n",
      "train loss:0.4735790060259233\n",
      "train loss:0.5144837294479279\n",
      "train loss:0.5135718000935476\n",
      "train loss:0.47255131828177976\n",
      "train loss:0.5804386006290362\n",
      "train loss:0.7662448868310234\n",
      "train loss:0.6852227672462148\n",
      "train loss:0.7365070613534508\n",
      "train loss:0.5661512294608019\n",
      "train loss:0.47965478275994206\n",
      "train loss:0.7457220157493467\n",
      "train loss:0.4816405520850912\n",
      "train loss:0.42180553472263965\n",
      "train loss:0.5706433025957064\n",
      "train loss:0.6337843853487665\n",
      "train loss:0.5418117534874881\n",
      "train loss:0.5441035124998888\n",
      "train loss:0.5855988300972335\n",
      "train loss:0.274968961910408\n",
      "train loss:0.4186928541003215\n",
      "train loss:0.46753956881278225\n",
      "train loss:0.4868526509118697\n",
      "train loss:0.9768587916027125\n",
      "train loss:0.774109511926219\n",
      "train loss:0.7375178884806799\n",
      "train loss:0.8704532654351004\n",
      "train loss:0.6693155130325401\n",
      "train loss:0.4272621707907274\n",
      "train loss:0.7268780233268861\n",
      "train loss:0.7093117456414227\n",
      "train loss:0.6414668486005467\n",
      "train loss:0.6109318428833858\n",
      "train loss:0.7279085950676251\n",
      "train loss:0.6935225268294156\n",
      "train loss:0.4364767554048246\n",
      "train loss:0.6095144223462808\n",
      "train loss:0.5807549812349819\n",
      "train loss:0.6903355440234379\n",
      "train loss:0.5408128229480631\n",
      "train loss:0.5855412706385489\n",
      "train loss:0.602229907418639\n",
      "train loss:0.5436558210330317\n",
      "train loss:0.6023449458270128\n",
      "train loss:0.5362202821564157\n",
      "train loss:0.48153398105134065\n",
      "train loss:0.560318209007056\n",
      "train loss:0.5857092506326161\n",
      "train loss:0.6031471098742245\n",
      "train loss:0.44113640518598335\n",
      "train loss:0.39639492905819046\n",
      "=== epoch:7, train acc:0.73, test acc:0.69 ===\n",
      "train loss:0.7293975573172751\n",
      "train loss:0.5521978491776508\n",
      "train loss:0.6606785310496168\n",
      "train loss:0.2699245120107335\n",
      "train loss:0.3188185169709811\n",
      "train loss:0.7086406125746737\n",
      "train loss:0.5628603825216785\n",
      "train loss:0.673225536013516\n",
      "train loss:0.9058892089306145\n",
      "train loss:0.869755382823729\n",
      "train loss:0.7193300307648853\n",
      "train loss:0.6082290472721873\n",
      "train loss:0.45603877051905756\n",
      "train loss:0.6257750014448878\n",
      "train loss:0.5524406068247727\n",
      "train loss:0.6606313842341665\n",
      "train loss:0.7571011258617216\n",
      "train loss:0.4454953211313805\n",
      "train loss:0.5461916924695556\n",
      "train loss:0.6526225712375335\n",
      "train loss:0.7431650998415334\n",
      "train loss:0.6609283251596606\n",
      "train loss:0.5421192647609813\n",
      "train loss:0.6318757816122126\n",
      "train loss:0.5788665670514407\n",
      "train loss:0.60295284430484\n",
      "train loss:0.5239299916416462\n",
      "train loss:0.6062943154505762\n",
      "train loss:0.5546120919599383\n",
      "train loss:0.5770635003595583\n",
      "train loss:0.5505158919684332\n",
      "train loss:0.5088272554913567\n",
      "train loss:0.6640803167461212\n",
      "train loss:0.4244461825282794\n",
      "train loss:0.5985536064794682\n",
      "train loss:0.542296594959331\n",
      "train loss:0.7224051116911667\n",
      "train loss:0.75675034344081\n",
      "train loss:0.6065494803224369\n",
      "train loss:0.731952559542179\n",
      "train loss:0.4403319603077606\n",
      "train loss:0.6291951509422218\n",
      "train loss:0.5895519070043285\n",
      "train loss:0.40458535830596387\n",
      "train loss:0.47549999456305414\n",
      "train loss:0.5770377290371738\n",
      "train loss:0.5725304258302549\n",
      "train loss:0.6530059404673207\n",
      "train loss:0.593921554272726\n",
      "train loss:0.37821708589906\n",
      "train loss:0.3059149760809352\n",
      "train loss:0.48453034051715926\n",
      "train loss:0.622214103737692\n",
      "train loss:0.4114998323166071\n",
      "train loss:0.5763772890447509\n",
      "train loss:0.7265848638847646\n",
      "train loss:0.5566044567910596\n",
      "train loss:0.3616610493953155\n",
      "train loss:0.42690191519065196\n",
      "train loss:0.5260931979804279\n",
      "train loss:0.49101986353947025\n",
      "train loss:0.6114793404230774\n",
      "train loss:0.4915940172341962\n",
      "train loss:0.4545912562199229\n",
      "train loss:0.5891680692132886\n",
      "train loss:0.2711428766199808\n",
      "train loss:0.6846238698686657\n",
      "train loss:0.606852810591125\n",
      "train loss:0.7131007955853128\n",
      "train loss:0.49381622374518486\n",
      "train loss:0.66812378401649\n",
      "train loss:0.40874770020726636\n",
      "train loss:0.5889748435454882\n",
      "train loss:0.6206077088839028\n",
      "train loss:0.6339700761971285\n",
      "train loss:0.5949369620131699\n",
      "train loss:0.6981242912811618\n",
      "train loss:0.5301600410329863\n",
      "train loss:0.7705751529701242\n",
      "train loss:0.6607219218003026\n",
      "train loss:0.6640132638980034\n",
      "train loss:0.6504752455134959\n",
      "train loss:0.5641116901807928\n",
      "train loss:0.5223980358688862\n",
      "train loss:0.6422014679525865\n",
      "train loss:0.5868751692372338\n",
      "train loss:0.7705478345193367\n",
      "train loss:0.4411242510879104\n",
      "train loss:0.6530105415411397\n",
      "train loss:0.5383996717341075\n",
      "train loss:0.4999447975453878\n",
      "train loss:0.585118818119716\n",
      "train loss:0.7880767969747737\n",
      "train loss:0.5538106440288197\n",
      "train loss:0.6753671849736892\n",
      "train loss:0.6539706958187631\n",
      "train loss:0.41558249344409887\n",
      "train loss:0.4870749786441979\n",
      "train loss:0.4506675979264053\n",
      "train loss:0.5921936915327761\n",
      "train loss:0.23129451783743252\n",
      "train loss:0.605878609697922\n",
      "train loss:0.3533484785825628\n",
      "train loss:0.7479200993399728\n",
      "train loss:0.5090936572190649\n",
      "train loss:0.5194134245246836\n",
      "train loss:0.48191760117206056\n",
      "train loss:0.45841592951357557\n",
      "train loss:0.8866579166645394\n",
      "train loss:0.49979791374958465\n",
      "train loss:0.3702590422091837\n",
      "train loss:0.4510603475975866\n",
      "train loss:0.5379509487267373\n",
      "train loss:0.41378685049776776\n",
      "train loss:0.7297894181092378\n",
      "train loss:0.44702914038698227\n",
      "train loss:0.5784864468602329\n",
      "train loss:0.520495381378795\n",
      "train loss:0.4661669258739236\n",
      "train loss:0.5106630044006273\n",
      "train loss:0.7944328276368241\n",
      "train loss:0.3890476361834615\n",
      "train loss:0.23329706816614712\n",
      "train loss:0.441242991308951\n",
      "train loss:0.42761206018144976\n",
      "train loss:0.20669970478792207\n",
      "train loss:0.41321561732881074\n",
      "train loss:0.08864433766437964\n",
      "train loss:0.9710565549600879\n",
      "train loss:0.7120262054644269\n",
      "train loss:0.6075817454360829\n",
      "train loss:0.47784225228465466\n",
      "train loss:0.3482763577105345\n",
      "train loss:0.8301379035459753\n",
      "train loss:0.6056155474433152\n",
      "train loss:0.39166654262833495\n",
      "train loss:0.6154470843795842\n",
      "train loss:0.7413501860336342\n",
      "train loss:0.705360814684943\n",
      "train loss:0.6717862525900442\n",
      "train loss:0.501244416354027\n",
      "train loss:0.5050101360310116\n",
      "train loss:0.5667825002808151\n",
      "train loss:0.4543850032317799\n",
      "train loss:0.5682029400261723\n",
      "train loss:0.5091471428712112\n",
      "train loss:0.43934201789290916\n",
      "train loss:0.3865710960336713\n",
      "train loss:0.6498551788336477\n",
      "train loss:0.6262862698333871\n",
      "train loss:0.36859450511361735\n",
      "train loss:0.5466995957228021\n",
      "train loss:0.351840173374468\n",
      "=== epoch:8, train acc:0.73, test acc:0.69 ===\n",
      "train loss:0.5259127327416231\n",
      "train loss:0.40885173567846655\n",
      "train loss:0.45116891922206304\n",
      "train loss:0.5629485669359214\n",
      "train loss:0.6431157749859381\n",
      "train loss:0.5187668365867006\n",
      "train loss:0.4527101530216309\n",
      "train loss:0.6872403235190354\n",
      "train loss:0.5491420946692119\n",
      "train loss:0.5522140187596477\n",
      "train loss:0.5546450808434498\n",
      "train loss:0.4934050801191961\n",
      "train loss:0.9335096973661751\n",
      "train loss:0.2981921779421497\n",
      "train loss:0.35094785579873766\n",
      "train loss:0.20026796444563022\n",
      "train loss:0.5133584816115287\n",
      "train loss:0.40169975577813954\n",
      "train loss:0.3759169599468397\n",
      "train loss:0.3887057962064318\n",
      "train loss:0.7523245396538843\n",
      "train loss:0.4320890168083292\n",
      "train loss:0.6631305108460286\n",
      "train loss:0.6315981026787141\n",
      "train loss:0.6402608879999265\n",
      "train loss:0.5586077024154366\n",
      "train loss:0.5720110120963484\n",
      "train loss:0.400567884778254\n",
      "train loss:0.36148480914892683\n",
      "train loss:0.6932036306759318\n",
      "train loss:0.4222916393302437\n",
      "train loss:0.5333721211112723\n",
      "train loss:0.3705372771883824\n",
      "train loss:0.6921069265363995\n",
      "train loss:0.6133906536344288\n",
      "train loss:0.708364614903207\n",
      "train loss:0.578102243706802\n",
      "train loss:0.4840725284912054\n",
      "train loss:0.5683173946109911\n",
      "train loss:0.6124577758802152\n",
      "train loss:0.6347261719151922\n",
      "train loss:0.7236027012048244\n",
      "train loss:0.6271049766716151\n",
      "train loss:0.5878669311215423\n",
      "train loss:0.40933683533381726\n",
      "train loss:0.4763347259134701\n",
      "train loss:0.5700013623528288\n",
      "train loss:0.7739725474837578\n",
      "train loss:0.7675733386036823\n",
      "train loss:0.4737382365417388\n",
      "train loss:0.484825592479614\n",
      "train loss:0.6243312959902019\n",
      "train loss:0.6003144262545492\n",
      "train loss:0.5955439760481993\n",
      "train loss:0.5070099347548367\n",
      "train loss:0.6458985295861994\n",
      "train loss:0.5040554645316846\n",
      "train loss:0.5323133044377087\n",
      "train loss:0.4382071241560265\n",
      "train loss:0.47982896071217873\n",
      "train loss:0.7215212365631751\n",
      "train loss:0.3711836672193256\n",
      "train loss:0.7734044591904297\n",
      "train loss:0.7531617612835676\n",
      "train loss:0.5346142808590331\n",
      "train loss:0.7409467585542452\n",
      "train loss:0.41998094677889836\n",
      "train loss:0.6972588531610052\n",
      "train loss:0.6126839994140346\n",
      "train loss:0.3473010854514486\n",
      "train loss:0.7723228101861135\n",
      "train loss:0.5121738122317138\n",
      "train loss:0.6672723917021202\n",
      "train loss:0.579252653289948\n",
      "train loss:0.6994051223205863\n",
      "train loss:0.3396004600030101\n",
      "train loss:0.6460519805923404\n",
      "train loss:0.31791564137060424\n",
      "train loss:0.6277806468511143\n",
      "train loss:0.37448279772749626\n",
      "train loss:0.5689265384447795\n",
      "train loss:0.5661143342652692\n",
      "train loss:0.632505005482411\n",
      "train loss:0.3905098330652167\n",
      "train loss:0.6802943852786163\n",
      "train loss:0.3986716320318484\n",
      "train loss:0.6300614283678716\n",
      "train loss:0.4848526490243358\n",
      "train loss:0.5687065360810829\n",
      "train loss:0.39957472096849367\n",
      "train loss:0.6571655505377991\n",
      "train loss:0.6377043074717395\n",
      "train loss:0.6348317025188455\n",
      "train loss:0.6281512804605\n",
      "train loss:0.5161206928168409\n",
      "train loss:0.719544230073983\n",
      "train loss:0.2870296454771847\n",
      "train loss:0.4235845642502121\n",
      "train loss:0.42340024036807267\n",
      "train loss:0.3198466597725381\n",
      "train loss:0.5707765947671359\n",
      "train loss:0.4275194444296237\n",
      "train loss:0.5533807696890571\n",
      "train loss:0.6485031767329917\n",
      "train loss:0.46127618432218276\n",
      "train loss:0.4380476261036363\n",
      "train loss:0.359803026065447\n",
      "train loss:0.5790478150952177\n",
      "train loss:0.5290745623564048\n",
      "train loss:0.22020055618514767\n",
      "train loss:0.5889850523803635\n",
      "train loss:0.6444817816937619\n",
      "train loss:0.3660104427498861\n",
      "train loss:0.18689765658830118\n",
      "train loss:0.20432163986043336\n",
      "train loss:0.6727861073566741\n",
      "train loss:0.6350242272250322\n",
      "train loss:0.5273905215357837\n",
      "train loss:0.8403376046335349\n",
      "train loss:0.1813523031431273\n",
      "train loss:0.9097958804260429\n",
      "train loss:0.38053968455041826\n",
      "train loss:0.7510066995570945\n",
      "train loss:0.7328973094197038\n",
      "train loss:0.6092090389718783\n",
      "train loss:0.3775589357036647\n",
      "train loss:0.3810940331129202\n",
      "train loss:0.6579913320523145\n",
      "train loss:0.7541580497997841\n",
      "train loss:0.6168895119486291\n",
      "train loss:0.38893630558632425\n",
      "train loss:0.4986427243991381\n",
      "train loss:0.41118974133839553\n",
      "train loss:0.5676425274595938\n",
      "train loss:0.7622895030328523\n",
      "train loss:0.6226998512765047\n",
      "train loss:0.5536850336711051\n",
      "train loss:0.5007031382152587\n",
      "train loss:0.5537041427445109\n",
      "train loss:0.4833245227957982\n",
      "train loss:0.5660942740244228\n",
      "train loss:0.3508295396705188\n",
      "train loss:0.5047713865276215\n",
      "train loss:0.5702191602481923\n",
      "train loss:0.5058896500627366\n",
      "train loss:0.7484907820755705\n",
      "train loss:0.6104093807511326\n",
      "train loss:0.7232262203203961\n",
      "train loss:0.5226964651985287\n",
      "train loss:0.541402518739717\n",
      "train loss:0.5954991067278719\n",
      "train loss:0.5014628472077917\n",
      "train loss:0.30464863719299323\n",
      "=== epoch:9, train acc:0.74, test acc:0.69 ===\n",
      "train loss:0.518622427222752\n",
      "train loss:0.37099693917525295\n",
      "train loss:0.5135896402533919\n",
      "train loss:0.490352253472668\n",
      "train loss:0.7249052986397737\n",
      "train loss:0.563862581442548\n",
      "train loss:0.626120915309557\n",
      "train loss:0.5818819817210403\n",
      "train loss:0.5132948316543735\n",
      "train loss:0.39800137688434295\n",
      "train loss:0.4896868442055157\n",
      "train loss:0.5370125745854015\n",
      "train loss:0.4953142793835939\n",
      "train loss:0.47417682239565656\n",
      "train loss:0.3117720081878534\n",
      "train loss:0.38104963254345503\n",
      "train loss:0.45593454820207\n",
      "train loss:0.5423149025669488\n",
      "train loss:0.5978802093850742\n",
      "train loss:0.9699857654940824\n",
      "train loss:0.5331838357206544\n",
      "train loss:0.42687594541553997\n",
      "train loss:0.39543304718818817\n",
      "train loss:0.7257107456849249\n",
      "train loss:0.539424709212138\n",
      "train loss:0.7286521436154647\n",
      "train loss:0.7464375061921353\n",
      "train loss:0.2781653255386664\n",
      "train loss:0.5576535483092094\n",
      "train loss:0.3177044086580373\n",
      "train loss:0.5061387410093617\n",
      "train loss:0.6197917098900712\n",
      "train loss:0.6793015719658919\n",
      "train loss:0.6663451466232088\n",
      "train loss:0.47135587635447607\n",
      "train loss:0.6985055359946631\n",
      "train loss:0.47716155703235463\n",
      "train loss:0.5069006979034825\n",
      "train loss:0.2950581235082057\n",
      "train loss:0.3854243695364744\n",
      "train loss:0.5935718398950633\n",
      "train loss:0.6892838668638259\n",
      "train loss:0.8983688557431794\n",
      "train loss:0.6075783477283532\n",
      "train loss:0.6746659953341391\n",
      "train loss:0.5185673473602548\n",
      "train loss:0.45771214389330606\n",
      "train loss:0.4526378648755153\n",
      "train loss:0.6447962891280576\n",
      "train loss:0.6433570710475862\n",
      "train loss:0.4748591006476605\n",
      "train loss:0.7353424904020786\n",
      "train loss:0.5218834670017228\n",
      "train loss:0.3339426206273759\n",
      "train loss:0.4656485155034032\n",
      "train loss:0.5612485415042234\n",
      "train loss:0.4677949420972164\n",
      "train loss:0.5634343209072888\n",
      "train loss:0.422212194751983\n",
      "train loss:0.3633632787512321\n",
      "train loss:0.2892328837063919\n",
      "train loss:0.32955137632372916\n",
      "train loss:0.7719741734153194\n",
      "train loss:0.6341923232209683\n",
      "train loss:0.4769635674036796\n",
      "train loss:0.743456584462543\n",
      "train loss:0.8353221963742243\n",
      "train loss:0.426119616193157\n",
      "train loss:0.5400713651740199\n",
      "train loss:0.6967601043347555\n",
      "train loss:0.29517384395953483\n",
      "train loss:0.3933073537726389\n",
      "train loss:0.5371525029282032\n",
      "train loss:0.6007041699985852\n",
      "train loss:0.7419702101041127\n",
      "train loss:0.609926549768165\n",
      "train loss:0.5872435000543962\n",
      "train loss:0.4574284827042385\n",
      "train loss:0.4597461430249049\n",
      "train loss:0.5646261279322178\n",
      "train loss:0.599725923980805\n",
      "train loss:0.5784259193252982\n",
      "train loss:0.5232817392404637\n",
      "train loss:0.636221376864742\n",
      "train loss:0.5139070873853917\n",
      "train loss:0.44815487379689556\n",
      "train loss:0.7679318142171205\n",
      "train loss:0.40247837423595956\n",
      "train loss:0.7574810699210263\n",
      "train loss:0.7482545139113146\n",
      "train loss:0.4035171454799153\n",
      "train loss:0.3702477150755626\n",
      "train loss:0.554079371827877\n",
      "train loss:0.5318064012560211\n",
      "train loss:0.27901764776068594\n",
      "train loss:0.577129531196955\n",
      "train loss:0.42988018901597586\n",
      "train loss:0.685324862922117\n",
      "train loss:0.5641866822345691\n",
      "train loss:0.4888432192592565\n",
      "train loss:0.652163891264623\n",
      "train loss:0.3983978888312062\n",
      "train loss:0.589989503672806\n",
      "train loss:0.592099510787681\n",
      "train loss:0.555488349508209\n",
      "train loss:0.4222165330926109\n",
      "train loss:0.4189299312374229\n",
      "train loss:0.998635096521636\n",
      "train loss:0.40467823264137265\n",
      "train loss:0.5512801147623556\n",
      "train loss:0.48390565844568334\n",
      "train loss:0.34996394211193604\n",
      "train loss:0.578987916005319\n",
      "train loss:0.40288030039482087\n",
      "train loss:0.7485372154852807\n",
      "train loss:0.4680388509342211\n",
      "train loss:0.46369799309961623\n",
      "train loss:0.5936918678112879\n",
      "train loss:0.518526915460514\n",
      "train loss:0.5279043137817359\n",
      "train loss:0.40671539923133304\n",
      "train loss:0.3143058581410106\n",
      "train loss:0.8547939940759743\n",
      "train loss:0.6707819424054556\n",
      "train loss:0.3181295745470667\n",
      "train loss:0.34808598973445903\n",
      "train loss:0.5605447313204625\n",
      "train loss:0.4367307400610657\n",
      "train loss:0.45516091141651016\n",
      "train loss:0.36858571149142744\n",
      "train loss:0.5346829464696554\n",
      "train loss:0.3544842022788161\n",
      "train loss:0.9042598580785823\n",
      "train loss:0.5844549499612994\n",
      "train loss:0.6396664808520826\n",
      "train loss:0.5804191573474953\n",
      "train loss:0.6048807324174964\n",
      "train loss:0.4841775826112098\n",
      "train loss:0.6514685969856343\n",
      "train loss:0.61188108050225\n",
      "train loss:0.4809248632955704\n",
      "train loss:0.4555349309895401\n",
      "train loss:0.6538897027573735\n",
      "train loss:0.5918822349807252\n",
      "train loss:0.6218987253312291\n",
      "train loss:0.4081152844687212\n",
      "train loss:0.4287164416891569\n",
      "train loss:0.6089993149010552\n",
      "train loss:0.3608621457503496\n",
      "train loss:0.5238452864562839\n",
      "train loss:0.4124611911407701\n",
      "train loss:0.4460851067400203\n",
      "train loss:0.4829150882746919\n",
      "=== epoch:10, train acc:0.74, test acc:0.71 ===\n",
      "train loss:0.7018138306395468\n",
      "train loss:0.4268460408198481\n",
      "train loss:0.501276609486631\n",
      "train loss:0.3369956604320582\n",
      "train loss:0.6549116039571301\n",
      "train loss:0.5696306526735082\n",
      "train loss:0.6094370091598494\n",
      "train loss:0.5122960030896792\n",
      "train loss:0.4729476891537677\n",
      "train loss:0.5189565551048285\n",
      "train loss:0.6567376011052559\n",
      "train loss:0.22917748706080548\n",
      "train loss:0.43139218471790863\n",
      "train loss:0.3435458945794389\n",
      "train loss:0.478965796188525\n",
      "train loss:0.5374091526628509\n",
      "train loss:0.567942560752804\n",
      "train loss:0.6091768439768396\n",
      "train loss:0.32907228310517567\n",
      "train loss:0.43148364451571436\n",
      "train loss:0.7215835982905318\n",
      "train loss:0.33473615708472915\n",
      "train loss:0.33464026731829954\n",
      "train loss:0.31511852486749553\n",
      "train loss:0.557569942116749\n",
      "train loss:0.44153749360365924\n",
      "train loss:0.388363275458692\n",
      "train loss:0.5764341132322196\n",
      "train loss:0.4592819537799707\n",
      "train loss:0.6214169654599657\n",
      "train loss:0.8116481889328965\n",
      "train loss:0.32033762081729894\n",
      "train loss:0.4502827579487825\n",
      "train loss:0.43905638742379277\n",
      "train loss:0.6389812012069936\n",
      "train loss:0.350702248267204\n",
      "train loss:0.6114641337614023\n",
      "train loss:0.34619314846572075\n",
      "train loss:0.32152629135281835\n",
      "train loss:0.5281885704469416\n",
      "train loss:0.5217199042447902\n",
      "train loss:0.3552303149804748\n",
      "train loss:0.4418401996910909\n",
      "train loss:0.6543948276413298\n",
      "train loss:0.5112719756435505\n",
      "train loss:0.18771414549422213\n",
      "train loss:0.4048978090459891\n",
      "train loss:0.3770559180392697\n",
      "train loss:0.21000691195715754\n",
      "train loss:0.3668220184070708\n",
      "train loss:0.24746110453118936\n",
      "train loss:0.4731504205738937\n",
      "train loss:0.7471301987404799\n",
      "train loss:0.8379201781611577\n",
      "train loss:0.6313117374664761\n",
      "train loss:0.3417987975559427\n",
      "train loss:0.8376032053532929\n",
      "train loss:0.5718824034631416\n",
      "train loss:0.5141974506218225\n",
      "train loss:0.6378377174453467\n",
      "train loss:0.29450094761189005\n",
      "train loss:0.47121438267257376\n",
      "train loss:0.41134940310191687\n",
      "train loss:0.7350134950882999\n",
      "train loss:0.6061448827861907\n",
      "train loss:0.23693915820509334\n",
      "train loss:0.6679415044661516\n",
      "train loss:0.44087627592037143\n",
      "train loss:0.39917290351212864\n",
      "train loss:0.4191606589818419\n",
      "train loss:0.5073007016283444\n",
      "train loss:0.49142919609075075\n",
      "train loss:0.5282521648144453\n",
      "train loss:0.5754665040816278\n",
      "train loss:0.3478221451255386\n",
      "train loss:0.6015638969796424\n",
      "train loss:0.7671890191188507\n",
      "train loss:0.6058611211118384\n",
      "train loss:0.47615222193160545\n",
      "train loss:0.4730821849365453\n",
      "train loss:0.5056361137968348\n",
      "train loss:0.4207062516165833\n",
      "train loss:0.39938629517752783\n",
      "train loss:0.5792279080301246\n",
      "train loss:0.4674160548839968\n",
      "train loss:0.6476957838138213\n",
      "train loss:0.33702038764453013\n",
      "train loss:0.5586297219822333\n",
      "train loss:0.4323571710203292\n",
      "train loss:0.3556665344660038\n",
      "train loss:0.1885021880457423\n",
      "train loss:0.16727577476864994\n",
      "train loss:0.35906854076609784\n",
      "train loss:0.5065183822915496\n",
      "train loss:0.2603645144232893\n",
      "train loss:0.5690329877619699\n",
      "train loss:0.7102692146256657\n",
      "train loss:0.4618624437371251\n",
      "train loss:0.7622311417071587\n",
      "train loss:0.6648062440701891\n",
      "train loss:0.48133395488482356\n",
      "train loss:0.7203070544451953\n",
      "train loss:0.24106313466763485\n",
      "train loss:0.4661352460241147\n",
      "train loss:0.5990045402459819\n",
      "train loss:0.4481478052858354\n",
      "train loss:0.37940856222497765\n",
      "train loss:0.502621544490947\n",
      "train loss:0.5824580524649067\n",
      "train loss:0.32126634432538165\n",
      "train loss:0.503362165465496\n",
      "train loss:0.4438745861897627\n",
      "train loss:0.626560694214195\n",
      "train loss:0.37759332698093867\n",
      "train loss:0.8511544971599493\n",
      "train loss:0.3984030851558185\n",
      "train loss:0.3656791148356411\n",
      "train loss:0.35815103013081073\n",
      "train loss:0.1951279953935688\n",
      "train loss:0.6666561224009735\n",
      "train loss:0.4468370127916355\n",
      "train loss:0.7638102448527049\n",
      "train loss:0.46541533003971824\n",
      "train loss:0.5484611600024503\n",
      "train loss:0.3600890139995894\n",
      "train loss:0.5105128689764468\n",
      "train loss:0.5416694004661663\n",
      "train loss:0.5516668808407619\n",
      "train loss:0.5172184973714261\n",
      "train loss:0.4096192446653049\n",
      "train loss:0.6149446172034996\n",
      "train loss:0.4864369175079295\n",
      "train loss:0.4386499514052459\n",
      "train loss:0.5916260810955833\n",
      "train loss:0.4067464362519176\n",
      "train loss:0.7289804618088935\n",
      "train loss:0.581247992404617\n",
      "train loss:0.5067709578884119\n",
      "train loss:0.5360997957577739\n",
      "train loss:0.5186666785379945\n",
      "train loss:0.37838433958594575\n",
      "train loss:0.47750662386458254\n",
      "train loss:0.6081769114811599\n",
      "train loss:0.5767253594117335\n",
      "train loss:0.3870009392617996\n",
      "train loss:0.5311830795553297\n",
      "train loss:0.6452967761159257\n",
      "train loss:0.5721293551604846\n",
      "train loss:0.5333216407405501\n",
      "train loss:0.723210352288545\n",
      "train loss:0.45974517429681205\n",
      "train loss:0.6093847228259418\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5490196078431373\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=400, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e67d6ad-228d-4336-a821-4a1c72b87b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6923065324438722\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6900539516762788\n",
      "train loss:0.6785795539552646\n",
      "train loss:0.6926293538203626\n",
      "train loss:0.6519385977281065\n",
      "train loss:0.7110663507178523\n",
      "train loss:0.6805593160904274\n",
      "train loss:0.6310258241797849\n",
      "train loss:0.5798678480290612\n",
      "train loss:0.7548611800807905\n",
      "train loss:0.6313526356054178\n",
      "train loss:0.6620689252720314\n",
      "train loss:0.5046001002308925\n",
      "train loss:0.6748377156805425\n",
      "train loss:0.433976593128408\n",
      "train loss:0.5052319219820154\n",
      "train loss:0.6282170490540857\n",
      "train loss:0.7798666693645958\n",
      "train loss:0.23647990442297345\n",
      "train loss:0.9266857054516138\n",
      "train loss:0.9696962389923849\n",
      "train loss:0.5263165277857417\n",
      "train loss:0.5043298190062286\n",
      "train loss:0.5259392589529408\n",
      "train loss:0.9584236162602074\n",
      "train loss:0.7668224947740918\n",
      "train loss:0.6146647313605811\n",
      "train loss:0.7088964626818457\n",
      "train loss:0.6283279879232773\n",
      "train loss:0.5926747373608432\n",
      "train loss:0.6712719964915854\n",
      "train loss:0.5707726652822073\n",
      "train loss:0.5685178480608173\n",
      "train loss:0.6076401320860094\n",
      "train loss:0.6002520879230187\n",
      "train loss:0.6612918045516966\n",
      "train loss:0.5412532164253501\n",
      "train loss:0.5756334286373646\n",
      "train loss:0.617872215833905\n",
      "train loss:0.44119987882557365\n",
      "train loss:0.7486683863137867\n",
      "train loss:0.6817295941915524\n",
      "train loss:0.5457692304987645\n",
      "train loss:0.43375297944904007\n",
      "train loss:0.6119612085686559\n",
      "train loss:0.5017553820223328\n",
      "train loss:0.37835853976898093\n",
      "train loss:0.49840401017699626\n",
      "train loss:0.6438818969702158\n",
      "train loss:0.9388309361207406\n",
      "train loss:0.8717656339371427\n",
      "train loss:0.7689264504806317\n",
      "train loss:0.5141533083265113\n",
      "train loss:0.7358962391263996\n",
      "train loss:0.624569464293062\n",
      "train loss:0.6953575203022471\n",
      "train loss:0.628818840215201\n",
      "train loss:0.4877169384490555\n",
      "train loss:0.6215677037610472\n",
      "train loss:0.5764674221238211\n",
      "train loss:0.6670060745995572\n",
      "train loss:0.5057686159285903\n",
      "train loss:0.6595793472907259\n",
      "train loss:0.5688578080058319\n",
      "train loss:0.5455636707335748\n",
      "train loss:0.4956800052944842\n",
      "train loss:0.5337829091541926\n",
      "train loss:0.534378968976001\n",
      "train loss:0.5343101826426169\n",
      "train loss:0.6188693661047793\n",
      "train loss:0.5109672125904312\n",
      "train loss:0.3977728481490367\n",
      "train loss:0.7610771745901718\n",
      "train loss:0.5073448294808273\n",
      "train loss:0.9776596663276396\n",
      "train loss:0.7646946571819647\n",
      "train loss:0.49735424801805966\n",
      "train loss:0.6308969081651312\n",
      "train loss:0.3914649836458843\n",
      "train loss:0.7226850993189654\n",
      "train loss:0.6925520522748211\n",
      "train loss:0.6126755284259955\n",
      "train loss:0.70248149080958\n",
      "train loss:0.691226505591955\n",
      "train loss:0.6791695120410429\n",
      "train loss:0.6019689647424493\n",
      "train loss:0.6114937051050787\n",
      "train loss:0.7291690985855204\n",
      "train loss:0.6678376703678646\n",
      "train loss:0.5717415467217312\n",
      "train loss:0.6214441649122558\n",
      "train loss:0.5318282951899922\n",
      "train loss:0.5683533568079925\n",
      "train loss:0.567576690341433\n",
      "train loss:0.5801566274247418\n",
      "train loss:0.5457482154890959\n",
      "train loss:0.6131128385874709\n",
      "train loss:0.6890898271386956\n",
      "train loss:0.44498038369209647\n",
      "train loss:0.5086157046806923\n",
      "train loss:0.5920016927432042\n",
      "train loss:0.6225481140412926\n",
      "train loss:0.6297938871107369\n",
      "train loss:0.3945609654947263\n",
      "train loss:0.7456877072414205\n",
      "train loss:0.5188677480478895\n",
      "train loss:0.7775284992720587\n",
      "train loss:0.4877604385163313\n",
      "train loss:0.39584676781148537\n",
      "train loss:0.6051986968188643\n",
      "train loss:0.4964442068512581\n",
      "train loss:0.7979540214447317\n",
      "train loss:0.38138751338673893\n",
      "train loss:0.5216944824870589\n",
      "train loss:0.6133534431725013\n",
      "train loss:0.7273518596379004\n",
      "train loss:0.8357769461322297\n",
      "train loss:0.5149839202908937\n",
      "train loss:0.7057801122735046\n",
      "train loss:0.6920624885850392\n",
      "train loss:0.6900522014062334\n",
      "train loss:0.793758858472221\n",
      "train loss:0.6283586657312541\n",
      "train loss:0.6244432980061079\n",
      "train loss:0.6284439540771214\n",
      "train loss:0.5510225905807932\n",
      "train loss:0.5550936634885344\n",
      "train loss:0.7147019445379899\n",
      "train loss:0.5539841749212762\n",
      "train loss:0.5046831388761774\n",
      "train loss:0.6315725810916134\n",
      "train loss:0.5322289774281744\n",
      "train loss:0.5590062136756322\n",
      "train loss:0.7393336974609559\n",
      "train loss:0.5579781903670403\n",
      "train loss:0.6063596933425317\n",
      "train loss:0.7602361524076745\n",
      "train loss:0.46885513439035026\n",
      "train loss:0.5267738839371615\n",
      "train loss:0.706444843368282\n",
      "train loss:0.6041418381175949\n",
      "train loss:0.6034435448821441\n",
      "train loss:0.5947652480423515\n",
      "train loss:0.28996202180898917\n",
      "train loss:0.8470376881448717\n",
      "train loss:0.610227520924808\n",
      "train loss:0.40240845735992065\n",
      "train loss:0.5007477343847953\n",
      "train loss:0.4909379205454976\n",
      "train loss:0.7519170920469732\n",
      "train loss:0.8705296792349493\n",
      "train loss:0.7340328003304938\n",
      "train loss:0.49524562393024973\n",
      "train loss:0.32627060219730725\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6992178962827211\n",
      "train loss:0.6052704421497039\n",
      "train loss:0.6950187192802735\n",
      "train loss:0.6941817975894622\n",
      "train loss:0.615106020996489\n",
      "train loss:0.7555254781858366\n",
      "train loss:0.6776254628973823\n",
      "train loss:0.5567600171154525\n",
      "train loss:0.6186591924143807\n",
      "train loss:0.6151321135855613\n",
      "train loss:0.6692499543231795\n",
      "train loss:0.61995286734018\n",
      "train loss:0.6212117558829184\n",
      "train loss:0.5149788822218393\n",
      "train loss:0.6611465944829188\n",
      "train loss:0.7201879301979685\n",
      "train loss:0.503322506157623\n",
      "train loss:0.5486672804403749\n",
      "train loss:0.5268866019024134\n",
      "train loss:0.4485410601794125\n",
      "train loss:0.5252630817000589\n",
      "train loss:0.6295292214786243\n",
      "train loss:0.8288731236441678\n",
      "train loss:0.6167583015637192\n",
      "train loss:0.8998957209965995\n",
      "train loss:0.7840159973749009\n",
      "train loss:0.43989754887001836\n",
      "train loss:0.6807914932261452\n",
      "train loss:0.709624595116812\n",
      "train loss:0.5213477175623369\n",
      "train loss:0.6115155687648015\n",
      "train loss:0.5338880268855217\n",
      "train loss:0.7442381937969009\n",
      "train loss:0.5414945464640984\n",
      "train loss:0.7984700347546588\n",
      "train loss:0.48431484127513647\n",
      "train loss:0.4064536033215547\n",
      "train loss:0.5351763128866316\n",
      "train loss:0.6704528095476512\n",
      "train loss:0.6070333341087146\n",
      "train loss:0.68853630295615\n",
      "train loss:0.43695025142459204\n",
      "train loss:0.6009472407186299\n",
      "train loss:0.5200061252249639\n",
      "train loss:0.5937421364335883\n",
      "train loss:0.5054266022249686\n",
      "train loss:0.5173199386168651\n",
      "train loss:0.40020224481030214\n",
      "train loss:0.6376420608299932\n",
      "train loss:0.6315922770599338\n",
      "train loss:0.6117327963714557\n",
      "train loss:0.7886451132108401\n",
      "train loss:0.627600844746618\n",
      "train loss:0.7230809369120459\n",
      "train loss:0.7213287296441877\n",
      "train loss:0.6028704827299962\n",
      "train loss:0.4506836740989074\n",
      "train loss:0.6934508837635331\n",
      "train loss:0.5267127711273929\n",
      "train loss:0.6166041236406038\n",
      "train loss:0.7288565513955393\n",
      "train loss:0.4887188861842787\n",
      "train loss:0.5963802611101072\n",
      "train loss:0.6612766352566382\n",
      "train loss:0.4834425012945866\n",
      "train loss:0.6751503319048922\n",
      "train loss:0.6321266728272275\n",
      "train loss:0.5464276419882468\n",
      "train loss:0.7481725229524654\n",
      "train loss:0.7550134482498637\n",
      "train loss:0.5472147060691448\n",
      "train loss:0.6899318387004613\n",
      "train loss:0.6947759278213116\n",
      "train loss:0.6075721226492214\n",
      "train loss:0.6078598916090605\n",
      "train loss:0.5573797677420224\n",
      "train loss:0.5390233254243352\n",
      "train loss:0.6170274788910551\n",
      "train loss:0.5459502182537253\n",
      "train loss:0.6016588846575069\n",
      "train loss:0.5052242741442977\n",
      "train loss:0.5227631509523339\n",
      "train loss:0.5266303714607855\n",
      "train loss:0.5010564465920124\n",
      "train loss:0.73692280987004\n",
      "train loss:0.8658441493785711\n",
      "train loss:0.5039531232011727\n",
      "train loss:0.7046662283612035\n",
      "train loss:0.6216523007567372\n",
      "train loss:0.40350484589277597\n",
      "train loss:0.629300079424716\n",
      "train loss:0.6399976233930456\n",
      "train loss:0.6141158060908134\n",
      "train loss:0.6018785705439442\n",
      "train loss:0.663367095383406\n",
      "train loss:0.5343871632797621\n",
      "train loss:0.5255670257813032\n",
      "train loss:0.6061661653411993\n",
      "train loss:0.5304148281540281\n",
      "train loss:0.6079464072254503\n",
      "train loss:0.5197641890703262\n",
      "train loss:0.6733466998935134\n",
      "train loss:0.7710693484736316\n",
      "train loss:0.6049527896722052\n",
      "train loss:0.5375954156623907\n",
      "train loss:0.5407194603146328\n",
      "train loss:0.5185357268270864\n",
      "train loss:0.4805826322374244\n",
      "train loss:0.4108941659295736\n",
      "train loss:0.5186488162330714\n",
      "train loss:0.519313748301397\n",
      "train loss:0.7801067286554899\n",
      "train loss:0.7231109288999321\n",
      "train loss:0.8484865459863169\n",
      "train loss:0.7027663224461718\n",
      "train loss:0.6165252070277696\n",
      "train loss:0.32798017632824406\n",
      "train loss:0.7900516132769034\n",
      "train loss:0.531171022634448\n",
      "train loss:0.6885134192277832\n",
      "train loss:0.4607337182023311\n",
      "train loss:0.7494406129284035\n",
      "train loss:0.6128691079453163\n",
      "train loss:0.7542654295925695\n",
      "train loss:0.6137464004404524\n",
      "train loss:0.5422689913616476\n",
      "train loss:0.6113108516657915\n",
      "train loss:0.4857200675172749\n",
      "train loss:0.6767913760860248\n",
      "train loss:0.5488823019346072\n",
      "train loss:0.7406551776344006\n",
      "train loss:0.5384231312218786\n",
      "train loss:0.600675962067839\n",
      "train loss:0.5377296602859231\n",
      "train loss:0.34908441007410695\n",
      "train loss:0.5910726032853332\n",
      "train loss:0.7122070729210688\n",
      "train loss:0.740358036552671\n",
      "train loss:0.715473431900088\n",
      "train loss:0.6627180962888521\n",
      "train loss:0.6856401344758412\n",
      "train loss:0.5176754766400766\n",
      "train loss:0.44530208789025194\n",
      "train loss:0.5232223236895072\n",
      "train loss:0.5012373520009974\n",
      "train loss:0.4963552983350713\n",
      "train loss:0.5081436457248549\n",
      "train loss:0.8238941955732221\n",
      "train loss:0.8267558592868267\n",
      "train loss:0.7374996433621781\n",
      "train loss:0.42499605293369846\n",
      "train loss:0.7752821657035656\n",
      "train loss:0.769656701647374\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7392757960112243\n",
      "train loss:0.4871923506292676\n",
      "train loss:0.6550098454031787\n",
      "train loss:0.6164493071218253\n",
      "train loss:0.7176180734062463\n",
      "train loss:0.573936443400899\n",
      "train loss:0.5242419713109615\n",
      "train loss:0.6028738305205402\n",
      "train loss:0.5049897638647833\n",
      "train loss:0.6246248047351133\n",
      "train loss:0.5915865238049592\n",
      "train loss:0.5504586018991074\n",
      "train loss:0.5309811455089085\n",
      "train loss:0.4317386516074417\n",
      "train loss:0.5999088161261612\n",
      "train loss:0.4999663022341516\n",
      "train loss:0.6088264637322793\n",
      "train loss:0.461287948873935\n",
      "train loss:0.48500545865402883\n",
      "train loss:0.604985585935281\n",
      "train loss:0.3450658193239201\n",
      "train loss:0.9566163947683053\n",
      "train loss:0.6206978571703297\n",
      "train loss:0.47023550978575157\n",
      "train loss:0.8806988638660462\n",
      "train loss:0.9087556417402297\n",
      "train loss:0.7296840600608199\n",
      "train loss:0.5165571370882869\n",
      "train loss:0.6247562946609849\n",
      "train loss:0.5604651149403544\n",
      "train loss:0.5725511607407512\n",
      "train loss:0.6111337169906345\n",
      "train loss:0.646115725818128\n",
      "train loss:0.7620614943458127\n",
      "train loss:0.5982077539357465\n",
      "train loss:0.7151054921212741\n",
      "train loss:0.62429821036121\n",
      "train loss:0.6931743392000606\n",
      "train loss:0.6398791936539588\n",
      "train loss:0.6107099096563884\n",
      "train loss:0.572729944293668\n",
      "train loss:0.6531221182479925\n",
      "train loss:0.7489827076501501\n",
      "train loss:0.633251942097565\n",
      "train loss:0.7745718395043579\n",
      "train loss:0.5873944108311608\n",
      "train loss:0.6403778891045115\n",
      "train loss:0.6902666502813659\n",
      "train loss:0.6957889019620693\n",
      "train loss:0.5466057885639768\n",
      "train loss:0.5754096754033202\n",
      "train loss:0.6063480567465772\n",
      "train loss:0.6719462003709042\n",
      "train loss:0.5262886828957809\n",
      "train loss:0.6230215886449801\n",
      "train loss:0.8124867161505952\n",
      "train loss:0.6677086562247341\n",
      "train loss:0.5988782821861193\n",
      "train loss:0.6606098642168285\n",
      "train loss:0.6899410679855102\n",
      "train loss:0.7947759929256915\n",
      "train loss:0.5934813331909685\n",
      "train loss:0.6149903650242874\n",
      "train loss:0.6851869107141427\n",
      "train loss:0.4252194801044541\n",
      "train loss:0.6202121591405952\n",
      "train loss:0.4822480398086145\n",
      "train loss:0.6575307335349413\n",
      "train loss:0.6693675102231725\n",
      "train loss:0.39353184220111487\n",
      "train loss:0.5535442364741731\n",
      "train loss:0.5127536397127914\n",
      "train loss:0.46817654702453293\n",
      "train loss:0.39338549080260166\n",
      "train loss:0.6224582087578496\n",
      "train loss:0.7608447876953496\n",
      "train loss:0.5137946910271397\n",
      "train loss:0.7131359201304732\n",
      "train loss:0.8970732218157996\n",
      "train loss:0.6249840907136835\n",
      "train loss:0.5881929111018345\n",
      "train loss:0.6263515764363021\n",
      "train loss:0.8166116715284794\n",
      "train loss:0.777777460940708\n",
      "train loss:0.6910634779111793\n",
      "train loss:0.5662517488054987\n",
      "train loss:0.6826462659267815\n",
      "train loss:0.5191305067515575\n",
      "train loss:0.6088801691256394\n",
      "train loss:0.5355296047306347\n",
      "train loss:0.46738607468526416\n",
      "train loss:0.6852618536870189\n",
      "train loss:0.6239848179249705\n",
      "train loss:0.7007042920596042\n",
      "train loss:0.5574172812224664\n",
      "train loss:0.6854507655727315\n",
      "train loss:0.591035100413359\n",
      "train loss:0.8076268447937041\n",
      "train loss:0.5233467039176737\n",
      "train loss:0.5961170024233695\n",
      "train loss:0.5359425747977383\n",
      "train loss:0.5297763017777937\n",
      "train loss:0.545358288543598\n",
      "train loss:0.3542376444151988\n",
      "train loss:0.7416396861702896\n",
      "train loss:0.4928228035623639\n",
      "train loss:0.5087911325864527\n",
      "train loss:0.6076258231957447\n",
      "train loss:0.448475195707597\n",
      "train loss:0.6166521050889748\n",
      "train loss:0.5139382617088166\n",
      "train loss:0.6540469757603526\n",
      "train loss:0.6077238075715712\n",
      "train loss:0.6296527501671891\n",
      "train loss:0.7152097525948575\n",
      "train loss:0.5375024881906016\n",
      "train loss:0.41027767307436375\n",
      "train loss:0.4151455994746489\n",
      "train loss:0.685648266415374\n",
      "train loss:0.5972374379862008\n",
      "train loss:0.7684137654331795\n",
      "train loss:0.6275744760074372\n",
      "train loss:0.9061205714609434\n",
      "train loss:0.5554458925917505\n",
      "train loss:0.5885030432863204\n",
      "train loss:0.5483650076875797\n",
      "train loss:0.6114681217867466\n",
      "train loss:0.5056926262755784\n",
      "train loss:0.6375165335461521\n",
      "train loss:0.5010861423644822\n",
      "train loss:0.48014012620576374\n",
      "train loss:0.5401391589062357\n",
      "train loss:0.5452674927369601\n",
      "train loss:0.6115349464037882\n",
      "train loss:0.5884200179077533\n",
      "train loss:0.6166727902967989\n",
      "train loss:0.6347381759993326\n",
      "train loss:0.3810112377865108\n",
      "train loss:0.49128088385494867\n",
      "train loss:0.6383736676475819\n",
      "train loss:0.4898647861868134\n",
      "train loss:0.510820898123504\n",
      "train loss:0.3309753351132995\n",
      "train loss:0.5184639443500675\n",
      "train loss:0.8250272136344391\n",
      "train loss:0.7408739078677852\n",
      "train loss:0.7328641606757672\n",
      "train loss:0.5637847440059857\n",
      "train loss:0.6458576241349285\n",
      "train loss:0.5315954285347007\n",
      "train loss:0.510334971648492\n",
      "train loss:0.5101016700201637\n",
      "train loss:0.5285094532797381\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5158252661162798\n",
      "train loss:0.5525378454050502\n",
      "train loss:0.44391588543262983\n",
      "train loss:0.6593366171047474\n",
      "train loss:0.5687563354698957\n",
      "train loss:0.5799546959779368\n",
      "train loss:0.3890267129107242\n",
      "train loss:0.8557092886409707\n",
      "train loss:0.6723548058168574\n",
      "train loss:0.5366499841355058\n",
      "train loss:0.5896326584079492\n",
      "train loss:0.6663921523730825\n",
      "train loss:0.6532750601207056\n",
      "train loss:0.5988384397267538\n",
      "train loss:0.447535765091439\n",
      "train loss:0.517094928683542\n",
      "train loss:0.6874155255695582\n",
      "train loss:0.41951076944036086\n",
      "train loss:0.6707331830742792\n",
      "train loss:0.53478438485744\n",
      "train loss:0.7230511946011952\n",
      "train loss:0.7717746331495502\n",
      "train loss:0.5557100178591352\n",
      "train loss:0.5462250947077697\n",
      "train loss:0.3179217118898959\n",
      "train loss:0.5857188295042882\n",
      "train loss:0.793964383587219\n",
      "train loss:0.5712230334540893\n",
      "train loss:0.5769633792194035\n",
      "train loss:0.40860706572964195\n",
      "train loss:0.5072362220128853\n",
      "train loss:0.790787659448593\n",
      "train loss:0.6186663366940526\n",
      "train loss:0.4440790479564362\n",
      "train loss:0.7054822052563126\n",
      "train loss:0.5071980143602683\n",
      "train loss:0.3916192827219045\n",
      "train loss:0.644432185920933\n",
      "train loss:0.8487785600748572\n",
      "train loss:0.7959173653432734\n",
      "train loss:0.9141101045704525\n",
      "train loss:0.4705119841270652\n",
      "train loss:0.6759621659115621\n",
      "train loss:0.5472121003084798\n",
      "train loss:0.4733090751493747\n",
      "train loss:0.5704064736011283\n",
      "train loss:0.6176377266760213\n",
      "train loss:0.7378551366069077\n",
      "train loss:0.46721823433835186\n",
      "train loss:0.6508759019210362\n",
      "train loss:0.4571178743510873\n",
      "train loss:0.45269178477184796\n",
      "train loss:0.5609556984144418\n",
      "train loss:0.7037876436405088\n",
      "train loss:0.530271317160526\n",
      "train loss:0.6108303696829438\n",
      "train loss:0.5683042977032389\n",
      "train loss:0.6130846463193522\n",
      "train loss:0.7034658473508582\n",
      "train loss:0.6229390335749634\n",
      "train loss:0.3003356556660607\n",
      "train loss:0.26109976385592965\n",
      "train loss:0.6681348881919735\n",
      "train loss:0.36371041485789407\n",
      "train loss:0.6561973090137391\n",
      "train loss:0.48410790360110234\n",
      "train loss:0.5027558594878585\n",
      "train loss:0.9366576402151185\n",
      "train loss:0.3139373099542888\n",
      "train loss:0.638742560113083\n",
      "train loss:0.3159723118654384\n",
      "train loss:0.6598446451887484\n",
      "train loss:0.5141045436073722\n",
      "train loss:0.9318072166031364\n",
      "train loss:0.7899891512404545\n",
      "train loss:0.6397786037024247\n",
      "train loss:0.6935677848585324\n",
      "train loss:0.36826334536414207\n",
      "train loss:0.5020360234901599\n",
      "train loss:0.6975998104872894\n",
      "train loss:0.6162888118341228\n",
      "train loss:0.6854507725527677\n",
      "train loss:0.5402328316689811\n",
      "train loss:0.4900171953074233\n",
      "train loss:0.5724870203516707\n",
      "train loss:0.5324178689703121\n",
      "train loss:0.5423347007350363\n",
      "train loss:0.7424839932582241\n",
      "train loss:0.5194711690450922\n",
      "train loss:0.44641708939129937\n",
      "train loss:0.6076017060654934\n",
      "train loss:0.42429361466712756\n",
      "train loss:0.5245514092674701\n",
      "train loss:0.7358494359322296\n",
      "train loss:0.5053378617963034\n",
      "train loss:0.4181966823631399\n",
      "train loss:0.6826533884596913\n",
      "train loss:0.8490263196724092\n",
      "train loss:0.4266939657316099\n",
      "train loss:0.6297180182115798\n",
      "train loss:0.7559291593213839\n",
      "train loss:0.5526141880134837\n",
      "train loss:0.4970648662332075\n",
      "train loss:0.27602284376696845\n",
      "train loss:0.7075544970814514\n",
      "train loss:0.651680803909813\n",
      "train loss:0.7115470333017113\n",
      "train loss:0.6057407558606849\n",
      "train loss:0.7971570767695779\n",
      "train loss:0.5196208741247312\n",
      "train loss:0.4897682995401663\n",
      "train loss:0.7012206870372885\n",
      "train loss:0.706269717298422\n",
      "train loss:0.5802322910524407\n",
      "train loss:0.6084135299215971\n",
      "train loss:0.7916716684388059\n",
      "train loss:0.4296849019752103\n",
      "train loss:0.5233569659280274\n",
      "train loss:0.5407161506740394\n",
      "train loss:0.6620732984660316\n",
      "train loss:0.5477241138809023\n",
      "train loss:0.5506227119690247\n",
      "train loss:0.7462538483288912\n",
      "train loss:0.6824554548883784\n",
      "train loss:0.6465563028251397\n",
      "train loss:0.5270853255510527\n",
      "train loss:0.608381346575937\n",
      "train loss:0.6128555906207984\n",
      "train loss:0.5304987519761728\n",
      "train loss:0.6082501480743685\n",
      "train loss:0.5114200649496071\n",
      "train loss:0.4778349074731499\n",
      "train loss:0.6914278559247001\n",
      "train loss:0.9094441712304949\n",
      "train loss:0.6149657432302085\n",
      "train loss:0.8160425226531313\n",
      "train loss:0.6284129565258498\n",
      "train loss:0.45710658800038895\n",
      "train loss:0.5437136396518144\n",
      "train loss:0.6753157441124984\n",
      "train loss:0.6448447872842327\n",
      "train loss:0.4819851852206506\n",
      "train loss:0.46743951724650146\n",
      "train loss:0.642548019165232\n",
      "train loss:0.48343392784430916\n",
      "train loss:0.6341421137887583\n",
      "train loss:0.6172618135871646\n",
      "train loss:0.5027727902350156\n",
      "train loss:0.5688885419051793\n",
      "train loss:0.7112966956195181\n",
      "train loss:0.39466275841754195\n",
      "train loss:0.6462611685052566\n",
      "train loss:0.4496478546750037\n",
      "=== epoch:5, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6156855794701537\n",
      "train loss:0.9066848723283956\n",
      "train loss:0.48271766433018853\n",
      "train loss:0.5814916678460029\n",
      "train loss:0.42878169596794374\n",
      "train loss:0.5431784133144192\n",
      "train loss:0.2865029820475084\n",
      "train loss:0.52322120619147\n",
      "train loss:0.6846610218853731\n",
      "train loss:0.5241542742533762\n",
      "train loss:0.3480179525527055\n",
      "train loss:0.7387010222063732\n",
      "train loss:0.6381294192780007\n",
      "train loss:0.3450335033763411\n",
      "train loss:0.8880835214268051\n",
      "train loss:0.3664763785316859\n",
      "train loss:0.7469579702072056\n",
      "train loss:0.7521614453108303\n",
      "train loss:0.6680680746213857\n",
      "train loss:0.6667172541528527\n",
      "train loss:0.45256420084993765\n",
      "train loss:0.3307262249778817\n",
      "train loss:0.48848365089247564\n",
      "train loss:0.8158092950604265\n",
      "train loss:0.7039465722550233\n",
      "train loss:0.6423270505989533\n",
      "train loss:0.5022838485831139\n",
      "train loss:0.6510758176446302\n",
      "train loss:0.6227611592801736\n",
      "train loss:0.557170951909067\n",
      "train loss:0.5040204768683461\n",
      "train loss:0.6388384214845659\n",
      "train loss:0.5211909438326229\n",
      "train loss:0.40257760609941695\n",
      "train loss:0.5029990752573127\n",
      "train loss:0.5709587503359332\n",
      "train loss:0.7022779579242654\n",
      "train loss:0.7464354940180832\n",
      "train loss:0.5407226517082628\n",
      "train loss:0.5862779181679952\n",
      "train loss:0.802467779292782\n",
      "train loss:0.6104545981019454\n",
      "train loss:0.6918394903643901\n",
      "train loss:0.657402309841518\n",
      "train loss:0.5968976729385526\n",
      "train loss:0.36532999856372556\n",
      "train loss:0.5397018915468565\n",
      "train loss:1.0083815588178846\n",
      "train loss:0.7513506178353182\n",
      "train loss:0.5529137217177442\n",
      "train loss:0.4442447518508608\n",
      "train loss:0.6216980907818697\n",
      "train loss:0.5387823196389957\n",
      "train loss:0.5656165839571641\n",
      "train loss:0.5438466629832861\n",
      "train loss:0.5698686737963727\n",
      "train loss:0.6834551277360202\n",
      "train loss:0.5207898211309236\n",
      "train loss:0.6234092015363779\n",
      "train loss:0.4409542534789601\n",
      "train loss:0.6874008162365752\n",
      "train loss:0.5738525125272969\n",
      "train loss:0.49896276517090943\n",
      "train loss:0.6154488499198568\n",
      "train loss:0.7972881173060797\n",
      "train loss:0.4544976654054575\n",
      "train loss:0.488066961577062\n",
      "train loss:0.3643803143483736\n",
      "train loss:0.6293090209022636\n",
      "train loss:0.5656228403154616\n",
      "train loss:0.4242294338674911\n",
      "train loss:0.8267739919912497\n",
      "train loss:0.5854097875417515\n",
      "train loss:0.7641526162230674\n",
      "train loss:0.6330871084555053\n",
      "train loss:0.534611512149459\n",
      "train loss:0.7200780668201576\n",
      "train loss:0.7385046766853329\n",
      "train loss:0.5123606625624859\n",
      "train loss:0.4787307793093902\n",
      "train loss:0.4418309710971373\n",
      "train loss:0.7209958028174883\n",
      "train loss:0.48108244365332337\n",
      "train loss:0.5274127431352513\n",
      "train loss:0.5242684017199737\n",
      "train loss:0.5450617306962389\n",
      "train loss:0.6663299821505352\n",
      "train loss:0.5325168825262461\n",
      "train loss:0.4805137933077647\n",
      "train loss:0.4185014785788207\n",
      "train loss:0.6842543517974694\n",
      "train loss:0.7208081805434429\n",
      "train loss:0.7500083106061111\n",
      "train loss:0.48890096663065485\n",
      "train loss:0.7125909769727422\n",
      "train loss:0.3800939528608259\n",
      "train loss:0.40695956314690374\n",
      "train loss:0.6597516749419243\n",
      "train loss:0.6264997406193111\n",
      "train loss:0.7513939868444801\n",
      "train loss:0.8748914547497991\n",
      "train loss:0.5297355257702414\n",
      "train loss:0.6908808157297005\n",
      "train loss:0.6896476101304512\n",
      "train loss:0.5204373683391296\n",
      "train loss:0.508389166332735\n",
      "train loss:0.48052177979259464\n",
      "train loss:0.596416139522215\n",
      "train loss:0.641329372102948\n",
      "train loss:0.5653615661996674\n",
      "train loss:0.6145300970106218\n",
      "train loss:0.5316684592562446\n",
      "train loss:0.40274788728159583\n",
      "train loss:0.5252827753817955\n",
      "train loss:0.5039334639963131\n",
      "train loss:0.8043353297713528\n",
      "train loss:0.6791412904220308\n",
      "train loss:0.6071370687299503\n",
      "train loss:0.6257447933169004\n",
      "train loss:0.5540493375468535\n",
      "train loss:0.5116338150232521\n",
      "train loss:0.686038376707603\n",
      "train loss:0.5943351970995461\n",
      "train loss:0.3471441317743337\n",
      "train loss:0.3396046127479315\n",
      "train loss:0.6675309506794517\n",
      "train loss:0.8839696829775064\n",
      "train loss:0.4998741939821101\n",
      "train loss:0.5448775685696973\n",
      "train loss:0.4504077030732822\n",
      "train loss:0.5198150906050103\n",
      "train loss:0.5107132378816693\n",
      "train loss:0.5450111030063965\n",
      "train loss:0.6296132186529991\n",
      "train loss:0.628719469012918\n",
      "train loss:0.8449162008843555\n",
      "train loss:0.41773087932362457\n",
      "train loss:0.44948228412928015\n",
      "train loss:0.7291827748367508\n",
      "train loss:0.5480633904758838\n",
      "train loss:0.9670110926760624\n",
      "train loss:0.6073819843721113\n",
      "train loss:0.6034473616469042\n",
      "train loss:0.5302463841790226\n",
      "train loss:0.7609383549425993\n",
      "train loss:0.5564560686077437\n",
      "train loss:0.5499306732759637\n",
      "train loss:0.468606646468878\n",
      "train loss:0.6363862285409241\n",
      "train loss:0.7331581845544204\n",
      "train loss:0.5659394206548553\n",
      "train loss:0.5318556502996008\n",
      "train loss:0.6878541211627868\n",
      "=== epoch:6, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.4976908367250549\n",
      "train loss:0.5409284862583381\n",
      "train loss:0.8080186890331994\n",
      "train loss:0.4819270258978702\n",
      "train loss:0.43286527387722096\n",
      "train loss:0.6939250333699909\n",
      "train loss:0.7169086081890501\n",
      "train loss:0.6287259315195138\n",
      "train loss:0.6464984329961684\n",
      "train loss:0.42070074594657025\n",
      "train loss:0.6126279562428306\n",
      "train loss:0.8129529265352662\n",
      "train loss:0.6367514162607507\n",
      "train loss:0.5238532813813787\n",
      "train loss:0.7334031482164581\n",
      "train loss:0.831082010893652\n",
      "train loss:0.6351265856662237\n",
      "train loss:0.5316155743585693\n",
      "train loss:0.6153925379425607\n",
      "train loss:0.5531520219180047\n",
      "train loss:0.7678624050530104\n",
      "train loss:0.6017566362284569\n",
      "train loss:0.6412640195045618\n",
      "train loss:0.41557591832432667\n",
      "train loss:0.7392034417877067\n",
      "train loss:0.6472285996530083\n",
      "train loss:0.5959251137009565\n",
      "train loss:0.5940765282852237\n",
      "train loss:0.5202659643546557\n",
      "train loss:0.4943025649786116\n",
      "train loss:0.5291604631775468\n",
      "train loss:0.7177965326190641\n",
      "train loss:0.43239859788811874\n",
      "train loss:0.3986531406408084\n",
      "train loss:0.559387267150847\n",
      "train loss:0.5217726003575711\n",
      "train loss:0.5365287505773112\n",
      "train loss:0.584567253390501\n",
      "train loss:0.4566389773475213\n",
      "train loss:0.8489868864389564\n",
      "train loss:0.5210089105540614\n",
      "train loss:0.36919518510590427\n",
      "train loss:0.6162152978510422\n",
      "train loss:0.24862880378557423\n",
      "train loss:0.4022539194786976\n",
      "train loss:0.7824188572094426\n",
      "train loss:0.8552153545114585\n",
      "train loss:0.6528285527599503\n",
      "train loss:0.5007500495113573\n",
      "train loss:0.5958133127998183\n",
      "train loss:0.501453554955944\n",
      "train loss:0.39384845950963204\n",
      "train loss:0.5446111808072236\n",
      "train loss:0.5475827876746994\n",
      "train loss:0.5290024902848083\n",
      "train loss:0.473143813008137\n",
      "train loss:0.5374882207563572\n",
      "train loss:0.2782604919600754\n",
      "train loss:0.574170277212357\n",
      "train loss:0.7746319240751933\n",
      "train loss:0.42553624606860607\n",
      "train loss:0.7781004321559528\n",
      "train loss:0.47494806359623387\n",
      "train loss:0.4996334934093188\n",
      "train loss:0.38222277403863936\n",
      "train loss:0.703846071386796\n",
      "train loss:0.6884802977337022\n",
      "train loss:0.41007656365014944\n",
      "train loss:0.3696856093036789\n",
      "train loss:0.8718346176631455\n",
      "train loss:0.5227879934584636\n",
      "train loss:0.3541203645618398\n",
      "train loss:0.5484439989580007\n",
      "train loss:0.5959156182822396\n",
      "train loss:0.7485662384331545\n",
      "train loss:0.4013052720625419\n",
      "train loss:0.5890664823823143\n",
      "train loss:0.5308070134242658\n",
      "train loss:0.8398261253948098\n",
      "train loss:0.3458898808364841\n",
      "train loss:0.48465305201567527\n",
      "train loss:0.5355125815896725\n",
      "train loss:0.7564374063058886\n",
      "train loss:0.5694033897323467\n",
      "train loss:0.686692517963336\n",
      "train loss:0.6159286482609102\n",
      "train loss:0.6999310728402433\n",
      "train loss:0.6143451452046265\n",
      "train loss:0.3504072659915854\n",
      "train loss:0.550128580975653\n",
      "train loss:0.5111287026021607\n",
      "train loss:0.7226176948039784\n",
      "train loss:0.5674820667848646\n",
      "train loss:0.568987311872424\n",
      "train loss:0.6120079707567763\n",
      "train loss:0.61443079293826\n",
      "train loss:0.3459160072518501\n",
      "train loss:0.6059331936082303\n",
      "train loss:0.8805401279386983\n",
      "train loss:0.43059061395522563\n",
      "train loss:0.4960770370920392\n",
      "train loss:0.6074355354443528\n",
      "train loss:0.5772812273856804\n",
      "train loss:0.7225790575931281\n",
      "train loss:0.5523880730167788\n",
      "train loss:0.2964704764382894\n",
      "train loss:0.44864542991969475\n",
      "train loss:0.6194822843006728\n",
      "train loss:0.8182535278696402\n",
      "train loss:0.4432802169573189\n",
      "train loss:0.710244783562121\n",
      "train loss:0.592734105684954\n",
      "train loss:0.6915739096721796\n",
      "train loss:0.802940926159853\n",
      "train loss:0.49764104577244417\n",
      "train loss:0.6752979347298889\n",
      "train loss:0.35653484937973645\n",
      "train loss:0.7143270266027949\n",
      "train loss:0.6253266849753648\n",
      "train loss:0.61338127361443\n",
      "train loss:0.5812329485585263\n",
      "train loss:0.5933212250828902\n",
      "train loss:0.44996132121797594\n",
      "train loss:0.5717069091971576\n",
      "train loss:0.49102442386205436\n",
      "train loss:0.5470164037281494\n",
      "train loss:0.662502940214455\n",
      "train loss:0.5499021211749944\n",
      "train loss:0.5568878154324863\n",
      "train loss:0.5980823380425051\n",
      "train loss:0.49079990858791256\n",
      "train loss:0.6499203447313369\n",
      "train loss:0.7113135554269494\n",
      "train loss:0.6018894755276616\n",
      "train loss:0.6245325410490671\n",
      "train loss:0.6181256005612041\n",
      "train loss:0.7033100749745145\n",
      "train loss:0.5052004567478653\n",
      "train loss:0.7294906474007398\n",
      "train loss:0.5834700088310989\n",
      "train loss:0.584259667883614\n",
      "train loss:0.6958368199412409\n",
      "train loss:0.5371892548984795\n",
      "train loss:0.6110910518480334\n",
      "train loss:0.6985372302637952\n",
      "train loss:0.582899583655518\n",
      "train loss:0.7064793764732028\n",
      "train loss:0.6136972132936349\n",
      "train loss:0.630396904945804\n",
      "train loss:0.6397077555760307\n",
      "train loss:0.549555586299186\n",
      "train loss:0.6045283488001109\n",
      "train loss:0.3789651815442529\n",
      "=== epoch:7, train acc:0.74, test acc:0.7 ===\n",
      "train loss:0.5133021593441163\n",
      "train loss:0.7316087593282999\n",
      "train loss:0.4928877555371326\n",
      "train loss:0.7291173480305643\n",
      "train loss:0.7553403153496102\n",
      "train loss:0.47114064065179423\n",
      "train loss:0.7198258141787772\n",
      "train loss:0.6069393054918595\n",
      "train loss:0.5872807732040456\n",
      "train loss:0.7346582604470699\n",
      "train loss:0.9129800773256385\n",
      "train loss:0.4008012481708244\n",
      "train loss:0.668544963452063\n",
      "train loss:0.49361945480855135\n",
      "train loss:0.5427034352226017\n",
      "train loss:0.794580767476458\n",
      "train loss:0.5331123406452435\n",
      "train loss:0.5094583923452775\n",
      "train loss:0.5731825975847682\n",
      "train loss:0.5604534485286908\n",
      "train loss:0.5372977466843893\n",
      "train loss:0.5863611202661879\n",
      "train loss:0.5106041718130783\n",
      "train loss:0.5272632021256687\n",
      "train loss:0.3888865690147418\n",
      "train loss:0.34070529575680847\n",
      "train loss:0.549969121716207\n",
      "train loss:0.4955617342153499\n",
      "train loss:0.3288032906560154\n",
      "train loss:0.6799064046057554\n",
      "train loss:0.6665386092936588\n",
      "train loss:0.7977360476557388\n",
      "train loss:0.5557987397017965\n",
      "train loss:0.34664275642884573\n",
      "train loss:0.44314205579438887\n",
      "train loss:0.6634669348181135\n",
      "train loss:0.3213766899086958\n",
      "train loss:0.41934402528574816\n",
      "train loss:0.4460273423270388\n",
      "train loss:0.3772237468977776\n",
      "train loss:0.37872685270845163\n",
      "train loss:0.33389269604150457\n",
      "train loss:0.7809053047341895\n",
      "train loss:0.47969780680338037\n",
      "train loss:0.4638796771848435\n",
      "train loss:0.6394040524058597\n",
      "train loss:0.5568245251566148\n",
      "train loss:0.6937729216876684\n",
      "train loss:0.42448938084279214\n",
      "train loss:0.48379740313272485\n",
      "train loss:0.966755873827337\n",
      "train loss:0.28772958786978864\n",
      "train loss:0.49613487599029915\n",
      "train loss:0.3450563825478402\n",
      "train loss:0.7137147874712901\n",
      "train loss:0.5969610124852621\n",
      "train loss:0.8073912177194587\n",
      "train loss:0.519093494475204\n",
      "train loss:0.5771602761786327\n",
      "train loss:0.3314706126198544\n",
      "train loss:0.5099296256373265\n",
      "train loss:0.6019114663570961\n",
      "train loss:0.6760759170641111\n",
      "train loss:0.47467085622961597\n",
      "train loss:0.5068796528728883\n",
      "train loss:0.4130605928310036\n",
      "train loss:0.36650293214619184\n",
      "train loss:0.5137343993914619\n",
      "train loss:0.6741486330669659\n",
      "train loss:0.5240952341111431\n",
      "train loss:0.3737888767497709\n",
      "train loss:0.47362835657452385\n",
      "train loss:0.641423996514165\n",
      "train loss:0.505682143234278\n",
      "train loss:0.45933782510918386\n",
      "train loss:0.5788055353599755\n",
      "train loss:0.6329131881752581\n",
      "train loss:0.5288548820714463\n",
      "train loss:0.31905343956049215\n",
      "train loss:0.6252227569983901\n",
      "train loss:1.0454590488918794\n",
      "train loss:0.3145406496237452\n",
      "train loss:0.46604146986968475\n",
      "train loss:0.7420835374197491\n",
      "train loss:0.350437258108839\n",
      "train loss:0.7251579856263857\n",
      "train loss:0.6178006068212818\n",
      "train loss:0.481540868098676\n",
      "train loss:0.44608789956639283\n",
      "train loss:0.5270475680345913\n",
      "train loss:0.7108360367341323\n",
      "train loss:0.3870403729613774\n",
      "train loss:0.5694476957207774\n",
      "train loss:0.4812932098091072\n",
      "train loss:0.5039264422027314\n",
      "train loss:0.48589379996989823\n",
      "train loss:0.6279997523366647\n",
      "train loss:0.7159354952647442\n",
      "train loss:0.5923164047022784\n",
      "train loss:0.5776191906451875\n",
      "train loss:0.6019301998094214\n",
      "train loss:0.7946982821070915\n",
      "train loss:0.5225642956492043\n",
      "train loss:0.6414757753181137\n",
      "train loss:0.5699449066032497\n",
      "train loss:0.5663731680930315\n",
      "train loss:0.526127594577128\n",
      "train loss:0.505765724866381\n",
      "train loss:0.6047766408337021\n",
      "train loss:0.5062624846718096\n",
      "train loss:0.47292903312327467\n",
      "train loss:0.7595333374681082\n",
      "train loss:0.6233536769423413\n",
      "train loss:0.7079888729479876\n",
      "train loss:0.581579841547814\n",
      "train loss:0.511592075626975\n",
      "train loss:0.4254617840669601\n",
      "train loss:0.43433475657288695\n",
      "train loss:0.5816437871202659\n",
      "train loss:0.26188751137948574\n",
      "train loss:0.6493857791960631\n",
      "train loss:0.5066349177896237\n",
      "train loss:0.6880727766785819\n",
      "train loss:0.734526168876265\n",
      "train loss:0.7052689183244113\n",
      "train loss:0.49954283094101404\n",
      "train loss:0.6992637037930098\n",
      "train loss:0.7347674838255475\n",
      "train loss:0.45717292393382297\n",
      "train loss:0.5715235547004344\n",
      "train loss:0.6532497981190089\n",
      "train loss:0.5453890949307263\n",
      "train loss:0.6751183468776916\n",
      "train loss:0.5032379881255613\n",
      "train loss:0.4595595914610864\n",
      "train loss:0.37643106103250806\n",
      "train loss:0.49664366375135194\n",
      "train loss:0.4738182910842639\n",
      "train loss:0.4122781982728359\n",
      "train loss:0.458155064332226\n",
      "train loss:0.3708424485803282\n",
      "train loss:0.6765446796872339\n",
      "train loss:0.6711387332297087\n",
      "train loss:0.5148275371017885\n",
      "train loss:0.7724766539657147\n",
      "train loss:0.2076804510252721\n",
      "train loss:0.7303838123666068\n",
      "train loss:1.1623737561612022\n",
      "train loss:0.5006105421537997\n",
      "train loss:0.5085623228287779\n",
      "train loss:0.7916347507167503\n",
      "train loss:0.5763712090717628\n",
      "train loss:0.5493315802499911\n",
      "=== epoch:8, train acc:0.73, test acc:0.69 ===\n",
      "train loss:0.5542015987552888\n",
      "train loss:0.5778887536560852\n",
      "train loss:0.5713770540385981\n",
      "train loss:0.5641942692418833\n",
      "train loss:0.5350418927894021\n",
      "train loss:0.6162640268124193\n",
      "train loss:0.586860869631782\n",
      "train loss:0.5471744316537196\n",
      "train loss:0.6714363628377001\n",
      "train loss:0.640482599252737\n",
      "train loss:0.536060454895661\n",
      "train loss:0.5681956794792814\n",
      "train loss:0.5286514939728789\n",
      "train loss:0.38581259357864506\n",
      "train loss:0.6521017624056548\n",
      "train loss:0.6647972209415031\n",
      "train loss:0.597397853065534\n",
      "train loss:0.48984963223247613\n",
      "train loss:0.5853252560338053\n",
      "train loss:0.490462804415681\n",
      "train loss:0.6471372660491337\n",
      "train loss:0.3599092959385815\n",
      "train loss:0.43799452166462266\n",
      "train loss:0.5048658800826529\n",
      "train loss:0.7830908028151178\n",
      "train loss:0.8541333337988879\n",
      "train loss:0.6789993716449644\n",
      "train loss:0.7066964115687605\n",
      "train loss:0.5226391925590466\n",
      "train loss:0.591636217903353\n",
      "train loss:0.6685103775489446\n",
      "train loss:0.47553621474894836\n",
      "train loss:0.5332298095272844\n",
      "train loss:0.6488359717373706\n",
      "train loss:0.5650078326782804\n",
      "train loss:0.599041151360018\n",
      "train loss:0.4472196847238001\n",
      "train loss:0.4296468286173164\n",
      "train loss:0.5192069397578167\n",
      "train loss:0.4222621607711634\n",
      "train loss:0.7064632176757023\n",
      "train loss:0.570587425044364\n",
      "train loss:0.4421188450217004\n",
      "train loss:0.6944488941940753\n",
      "train loss:0.5031144458602357\n",
      "train loss:0.4545922949143745\n",
      "train loss:0.3079641901764992\n",
      "train loss:0.7089050064464643\n",
      "train loss:0.7164110110931191\n",
      "train loss:0.7505022776635594\n",
      "train loss:0.5071058276444455\n",
      "train loss:0.697084854226796\n",
      "train loss:0.3963359293927865\n",
      "train loss:0.6139985862419765\n",
      "train loss:0.5517518087976166\n",
      "train loss:0.5112127377463471\n",
      "train loss:0.34669449153043475\n",
      "train loss:0.7692802430821453\n",
      "train loss:0.5996829298068154\n",
      "train loss:0.6912785040178269\n",
      "train loss:0.4060468657332542\n",
      "train loss:0.6794117568390184\n",
      "train loss:0.49648275291630783\n",
      "train loss:0.6676249048593654\n",
      "train loss:0.7909226831396772\n",
      "train loss:0.6112194881712034\n",
      "train loss:0.5378900968761245\n",
      "train loss:0.49484884101630733\n",
      "train loss:0.5130828715943303\n",
      "train loss:0.47038693953615596\n",
      "train loss:0.5025315133708312\n",
      "train loss:0.5870707469361527\n",
      "train loss:0.599219842155523\n",
      "train loss:0.6143330136276011\n",
      "train loss:0.3160343666094545\n",
      "train loss:0.4889200878632566\n",
      "train loss:0.7159148695978206\n",
      "train loss:0.6064348230676105\n",
      "train loss:0.6610550999232856\n",
      "train loss:0.569782519386363\n",
      "train loss:0.5574495933376096\n",
      "train loss:0.5465279421877878\n",
      "train loss:0.44415896135043653\n",
      "train loss:0.6042781479901905\n",
      "train loss:0.6056662112852758\n",
      "train loss:0.6410413103688195\n",
      "train loss:0.6763584906958483\n",
      "train loss:0.6842124903713199\n",
      "train loss:0.4604139472526489\n",
      "train loss:0.7959094242486889\n",
      "train loss:0.44303787373110454\n",
      "train loss:0.5811316312244119\n",
      "train loss:0.4838063639021481\n",
      "train loss:0.6029536893407582\n",
      "train loss:0.43901270068529197\n",
      "train loss:0.5490685983138046\n",
      "train loss:0.5023974373996066\n",
      "train loss:0.5673943229631175\n",
      "train loss:0.5802388083551617\n",
      "train loss:0.4918486878326833\n",
      "train loss:0.43546015847695674\n",
      "train loss:0.45791808009840695\n",
      "train loss:0.5721284743530175\n",
      "train loss:0.46502753411482\n",
      "train loss:0.4639400545691147\n",
      "train loss:0.5075939707895426\n",
      "train loss:0.373142331331492\n",
      "train loss:0.7690119390711011\n",
      "train loss:0.4211755831831745\n",
      "train loss:0.5903678439730266\n",
      "train loss:0.6024456279910236\n",
      "train loss:0.4130125511580373\n",
      "train loss:0.5823543525631355\n",
      "train loss:0.39510014454529135\n",
      "train loss:0.3947020493076759\n",
      "train loss:0.48089028049199944\n",
      "train loss:0.32307478166054515\n",
      "train loss:0.5465738346044039\n",
      "train loss:0.628868730508667\n",
      "train loss:0.2877636697226159\n",
      "train loss:0.6223448255680417\n",
      "train loss:0.8088972734685832\n",
      "train loss:0.5444324730577287\n",
      "train loss:0.709815659723872\n",
      "train loss:0.46023554352055135\n",
      "train loss:0.4006620978597315\n",
      "train loss:0.5896570406897792\n",
      "train loss:0.5952269540592185\n",
      "train loss:0.6238982038027643\n",
      "train loss:0.5319406662330949\n",
      "train loss:0.6136701898557697\n",
      "train loss:0.5593685505831558\n",
      "train loss:0.5957028263707362\n",
      "train loss:0.5228332621773578\n",
      "train loss:0.5570256836273062\n",
      "train loss:0.5399565341295549\n",
      "train loss:0.529427497493809\n",
      "train loss:0.6207365116290247\n",
      "train loss:0.6206707440113863\n",
      "train loss:0.44833275047683374\n",
      "train loss:0.5032716311672565\n",
      "train loss:0.617462714522109\n",
      "train loss:0.7756454876764989\n",
      "train loss:0.4122565299110927\n",
      "train loss:0.6191744339948213\n",
      "train loss:0.5638360542805618\n",
      "train loss:0.6594376775798464\n",
      "train loss:0.5892158687288429\n",
      "train loss:0.5990713627513211\n",
      "train loss:0.5965476904491207\n",
      "train loss:0.4214712645854878\n",
      "train loss:0.7637044196732603\n",
      "train loss:0.5334112059152233\n",
      "=== epoch:9, train acc:0.74, test acc:0.69 ===\n",
      "train loss:0.6104937734571738\n",
      "train loss:0.48646788008286723\n",
      "train loss:0.7786672038909532\n",
      "train loss:0.6366973354348231\n",
      "train loss:0.5497354814079338\n",
      "train loss:0.5130534592657611\n",
      "train loss:0.6786297522884194\n",
      "train loss:0.4850314997936597\n",
      "train loss:0.6555154364473204\n",
      "train loss:0.7074976733505309\n",
      "train loss:0.625542563861465\n",
      "train loss:0.5196559600055652\n",
      "train loss:0.4375290852591315\n",
      "train loss:0.5152019943838186\n",
      "train loss:0.6275113031893381\n",
      "train loss:0.4710723384666581\n",
      "train loss:0.47385493226224734\n",
      "train loss:0.5803416576029429\n",
      "train loss:0.5349816246711858\n",
      "train loss:0.7294092508597039\n",
      "train loss:0.5566670251468563\n",
      "train loss:0.48795195133730196\n",
      "train loss:0.6270990767145307\n",
      "train loss:0.3339315067485685\n",
      "train loss:0.23083148060731498\n",
      "train loss:0.4421007647206444\n",
      "train loss:0.756730376110436\n",
      "train loss:0.5120653839521934\n",
      "train loss:0.5592326776015044\n",
      "train loss:0.7293086556109252\n",
      "train loss:0.42769145032659706\n",
      "train loss:0.5548739166440113\n",
      "train loss:0.26042966239409476\n",
      "train loss:0.7288840455250621\n",
      "train loss:0.6547690434836205\n",
      "train loss:0.5284079445119815\n",
      "train loss:0.43420851081620376\n",
      "train loss:0.34840155832319697\n",
      "train loss:0.6106450820397791\n",
      "train loss:0.6055439053261689\n",
      "train loss:0.5848216294369288\n",
      "train loss:0.7020717019297774\n",
      "train loss:0.6559122579012034\n",
      "train loss:0.591685626111301\n",
      "train loss:0.425072673200152\n",
      "train loss:0.6507699812444544\n",
      "train loss:0.5289498471936696\n",
      "train loss:0.4419283769929046\n",
      "train loss:0.4961873067108703\n",
      "train loss:0.6597296626266658\n",
      "train loss:0.6290143455441137\n",
      "train loss:0.4011245963584783\n",
      "train loss:0.42476799686942224\n",
      "train loss:0.5041621240439911\n",
      "train loss:0.4049208883624331\n",
      "train loss:0.49562430764087495\n",
      "train loss:0.5890854151179943\n",
      "train loss:0.43309938236351664\n",
      "train loss:0.37688335173396653\n",
      "train loss:0.9969036649856505\n",
      "train loss:0.5503944441677808\n",
      "train loss:0.6528915216478832\n",
      "train loss:0.19946894266400328\n",
      "train loss:0.5944764157109621\n",
      "train loss:0.3574991812205265\n",
      "train loss:0.2296865072449164\n",
      "train loss:0.4722925853520302\n",
      "train loss:0.4825551051007732\n",
      "train loss:0.4932314836579053\n",
      "train loss:0.34263242228394103\n",
      "train loss:0.3632306990473662\n",
      "train loss:0.45216091367614997\n",
      "train loss:0.5749621007066136\n",
      "train loss:0.5142115987766581\n",
      "train loss:0.6314347886359436\n",
      "train loss:0.4165324450231392\n",
      "train loss:0.30289813540355404\n",
      "train loss:0.4650371967833973\n",
      "train loss:0.7835915997496865\n",
      "train loss:0.37024854563872245\n",
      "train loss:0.6147951438993255\n",
      "train loss:0.6514506850566728\n",
      "train loss:0.5266047706416634\n",
      "train loss:0.5379083277971037\n",
      "train loss:0.5779466972019787\n",
      "train loss:0.4775333514593839\n",
      "train loss:0.4995773609940599\n",
      "train loss:0.5158982793568112\n",
      "train loss:0.48225526863468654\n",
      "train loss:0.8356182269853537\n",
      "train loss:0.5675609132041133\n",
      "train loss:0.556622732783316\n",
      "train loss:0.592916892699806\n",
      "train loss:0.50664927793174\n",
      "train loss:0.5346860358903912\n",
      "train loss:0.4786903297014636\n",
      "train loss:0.60412540548212\n",
      "train loss:0.5754680185102468\n",
      "train loss:0.40195179406180975\n",
      "train loss:0.4429686412305413\n",
      "train loss:0.4771575341568496\n",
      "train loss:0.580808562289754\n",
      "train loss:0.5285211529531701\n",
      "train loss:0.4754987789843136\n",
      "train loss:0.8293025957461634\n",
      "train loss:0.7933796195071701\n",
      "train loss:0.8930525566115358\n",
      "train loss:0.5971523666318544\n",
      "train loss:0.5509713727465781\n",
      "train loss:0.4018098406002008\n",
      "train loss:0.6706099422195246\n",
      "train loss:0.4651903941214166\n",
      "train loss:0.5300450072204546\n",
      "train loss:0.598022610167732\n",
      "train loss:0.3959624200327082\n",
      "train loss:0.510954316590851\n",
      "train loss:0.4339835454643402\n",
      "train loss:0.6848360198263816\n",
      "train loss:0.7238239921397854\n",
      "train loss:0.6333427547604813\n",
      "train loss:0.4184089961541767\n",
      "train loss:0.7102826929538609\n",
      "train loss:0.6576509871262759\n",
      "train loss:0.45624516973067725\n",
      "train loss:0.7151406874530479\n",
      "train loss:0.8163180896021587\n",
      "train loss:0.43790028291193\n",
      "train loss:0.37049218784107923\n",
      "train loss:0.4998968198906808\n",
      "train loss:0.46206026529015887\n",
      "train loss:0.5922926742113084\n",
      "train loss:0.6777389797770677\n",
      "train loss:0.5056425624189173\n",
      "train loss:0.4366986615641294\n",
      "train loss:0.42134443375313213\n",
      "train loss:0.9157661364664957\n",
      "train loss:0.6266054825684431\n",
      "train loss:0.37103674867008596\n",
      "train loss:0.3684827605956665\n",
      "train loss:0.3718646510464455\n",
      "train loss:0.3942039536696654\n",
      "train loss:0.48537728675632213\n",
      "train loss:0.1934177742929296\n",
      "train loss:0.8608332469106802\n",
      "train loss:0.5326244669630079\n",
      "train loss:0.48091783280392997\n",
      "train loss:0.5500213603751984\n",
      "train loss:0.4917525402679518\n",
      "train loss:0.5939855829567338\n",
      "train loss:0.43891161986941424\n",
      "train loss:0.4845128415923483\n",
      "train loss:0.6684403994278478\n",
      "train loss:0.7598529550184321\n",
      "=== epoch:10, train acc:0.75, test acc:0.7 ===\n",
      "train loss:0.5848809367633332\n",
      "train loss:0.5277932095360397\n",
      "train loss:0.5037320061271615\n",
      "train loss:0.4650684451638115\n",
      "train loss:0.740226075874\n",
      "train loss:0.39632827184290603\n",
      "train loss:0.453449591369852\n",
      "train loss:0.6132048710944836\n",
      "train loss:0.7269955003784935\n",
      "train loss:0.7426359197353227\n",
      "train loss:0.4449243963640428\n",
      "train loss:0.6789473970150858\n",
      "train loss:0.48059947266494757\n",
      "train loss:0.44300577383007544\n",
      "train loss:0.3122381689308876\n",
      "train loss:0.6486098805635027\n",
      "train loss:0.4269045579078208\n",
      "train loss:0.6068834403764769\n",
      "train loss:0.48048255336261675\n",
      "train loss:0.9378076392896585\n",
      "train loss:0.41587995216189233\n",
      "train loss:0.29710548951822663\n",
      "train loss:0.6827086470254563\n",
      "train loss:0.4185621240095734\n",
      "train loss:0.6145759649957314\n",
      "train loss:0.5746723548221041\n",
      "train loss:0.35880049112731444\n",
      "train loss:0.5182049423549293\n",
      "train loss:0.5343990045602569\n",
      "train loss:0.5414227550944226\n",
      "train loss:0.756228764347662\n",
      "train loss:0.7187776264874743\n",
      "train loss:0.7588211962893491\n",
      "train loss:0.5576890915457032\n",
      "train loss:0.464558733687315\n",
      "train loss:0.3328587965569422\n",
      "train loss:0.7029236658722045\n",
      "train loss:0.721026724439074\n",
      "train loss:0.582643165834662\n",
      "train loss:0.5379838568485376\n",
      "train loss:0.6835609736715609\n",
      "train loss:0.38794459219040156\n",
      "train loss:0.629797187132098\n",
      "train loss:0.5456895194235682\n",
      "train loss:0.5250223138244656\n",
      "train loss:0.6770774827711081\n",
      "train loss:0.46655059552125183\n",
      "train loss:0.685237398926915\n",
      "train loss:0.6451382208485349\n",
      "train loss:0.6955408685240568\n",
      "train loss:0.7496218820905434\n",
      "train loss:0.46432207504678524\n",
      "train loss:0.7048080700949939\n",
      "train loss:0.6289520639387873\n",
      "train loss:0.6018639541979063\n",
      "train loss:0.6719315519699937\n",
      "train loss:0.46180553397465385\n",
      "train loss:0.44475791379781704\n",
      "train loss:0.3930359557019764\n",
      "train loss:0.30962963634311225\n",
      "train loss:0.7023842267679781\n",
      "train loss:0.5525237800648928\n",
      "train loss:0.659123942943\n",
      "train loss:0.48122988039930553\n",
      "train loss:0.7546581396859571\n",
      "train loss:0.5362458392202694\n",
      "train loss:0.5118628252985253\n",
      "train loss:0.5122709383600296\n",
      "train loss:0.5753232202900331\n",
      "train loss:0.5421416601943617\n",
      "train loss:0.574140252701979\n",
      "train loss:0.4338763901839483\n",
      "train loss:0.5072734471812387\n",
      "train loss:0.6480549999282521\n",
      "train loss:0.3192637291321251\n",
      "train loss:0.4174991933364615\n",
      "train loss:0.7681384549018497\n",
      "train loss:0.4093049355786997\n",
      "train loss:0.39439358066959496\n",
      "train loss:0.4975371591887218\n",
      "train loss:0.47836820228592625\n",
      "train loss:0.5647108606191288\n",
      "train loss:0.6807654291218748\n",
      "train loss:0.7831068496026677\n",
      "train loss:0.2883335104757986\n",
      "train loss:0.49514511763596136\n",
      "train loss:0.46883122057629667\n",
      "train loss:0.4183041544605759\n",
      "train loss:0.6261190337427586\n",
      "train loss:0.44566033289749535\n",
      "train loss:0.5736853331221565\n",
      "train loss:0.6450390926918077\n",
      "train loss:0.5022814536889775\n",
      "train loss:0.5693324031266362\n",
      "train loss:0.4679642284693874\n",
      "train loss:0.6302702519270651\n",
      "train loss:0.5472403807788437\n",
      "train loss:0.5925574740015388\n",
      "train loss:0.6292832345848065\n",
      "train loss:0.6954418992248049\n",
      "train loss:0.38662108656304356\n",
      "train loss:0.5763836425378512\n",
      "train loss:0.6936420065760237\n",
      "train loss:0.5289175047349545\n",
      "train loss:0.5535812588372704\n",
      "train loss:0.6991911923685545\n",
      "train loss:0.5078270513940917\n",
      "train loss:0.4756638063709281\n",
      "train loss:0.42355798939829514\n",
      "train loss:0.6606332506045606\n",
      "train loss:0.5675178440440651\n",
      "train loss:0.8176931937362927\n",
      "train loss:0.6788480743337764\n",
      "train loss:0.538247564678888\n",
      "train loss:0.49141437567423846\n",
      "train loss:0.4475249216840755\n",
      "train loss:0.5049246566024294\n",
      "train loss:0.6508026120516173\n",
      "train loss:0.4554365031205531\n",
      "train loss:0.5132710839490582\n",
      "train loss:0.5094325512670398\n",
      "train loss:0.4738298510981017\n",
      "train loss:0.6561367435730406\n",
      "train loss:0.678182827923146\n",
      "train loss:0.5537966888824519\n",
      "train loss:0.5164241235089785\n",
      "train loss:0.46583379897366334\n",
      "train loss:0.7958119312618079\n",
      "train loss:0.6047050395890882\n",
      "train loss:0.8624103712356674\n",
      "train loss:0.5385618922963963\n",
      "train loss:0.36079955974847283\n",
      "train loss:0.48654614135328317\n",
      "train loss:0.48202977464346\n",
      "train loss:0.7247775895644393\n",
      "train loss:0.5683796489054485\n",
      "train loss:0.7323515914220416\n",
      "train loss:0.5640198461740394\n",
      "train loss:0.6198425371404054\n",
      "train loss:0.4276526190388926\n",
      "train loss:0.6205933651767015\n",
      "train loss:0.8215330397117206\n",
      "train loss:0.7813128476528787\n",
      "train loss:0.4763520488048239\n",
      "train loss:0.5476748284278894\n",
      "train loss:0.7066707720070022\n",
      "train loss:0.3794283832368837\n",
      "train loss:0.4220829097819772\n",
      "train loss:0.42226015350189094\n",
      "train loss:0.5714197997106061\n",
      "train loss:0.5518438039298753\n",
      "train loss:0.7149863995732567\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5411764705882353\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=200, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01cb1e0d-8879-4699-92c1-a926434b68de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6921748338013021\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.693109063585999\n",
      "train loss:0.6920560667850038\n",
      "train loss:0.6894129112988512\n",
      "train loss:0.6810069370303801\n",
      "train loss:0.6779076744468158\n",
      "train loss:0.6864852345226911\n",
      "train loss:0.6748340971213171\n",
      "train loss:0.6480278622268467\n",
      "train loss:0.6569427926364431\n",
      "train loss:0.677917185628655\n",
      "train loss:0.7716812994359413\n",
      "train loss:0.6512146603201818\n",
      "train loss:0.6143314566197547\n",
      "train loss:0.5898373644420102\n",
      "train loss:0.6843218834598428\n",
      "train loss:0.5633830904606827\n",
      "train loss:0.4055983290797739\n",
      "train loss:0.6778291779270587\n",
      "train loss:0.584002321449316\n",
      "train loss:0.5874270549761963\n",
      "train loss:0.49704735018831847\n",
      "train loss:0.658078155064939\n",
      "train loss:0.913076294776254\n",
      "train loss:0.5158956863801503\n",
      "train loss:0.5283407732177358\n",
      "train loss:0.3838111457350678\n",
      "train loss:0.4985338601298116\n",
      "train loss:0.7666264903690381\n",
      "train loss:0.66442003699832\n",
      "train loss:0.51731422177642\n",
      "train loss:0.3998135937118093\n",
      "train loss:0.6296457971008335\n",
      "train loss:0.8630688614866218\n",
      "train loss:0.4120628318705063\n",
      "train loss:0.6024467641341493\n",
      "train loss:0.6929572807140973\n",
      "train loss:0.7688338341509033\n",
      "train loss:0.4498684032149072\n",
      "train loss:0.5279121354422339\n",
      "train loss:0.613701080672642\n",
      "train loss:0.6798015197304851\n",
      "train loss:0.6084033636108512\n",
      "train loss:0.6003488621664222\n",
      "train loss:0.5502944352949382\n",
      "train loss:0.37788796167449706\n",
      "train loss:0.595492761286757\n",
      "train loss:0.4329687201154037\n",
      "train loss:0.7947976896876218\n",
      "train loss:0.5217587686238755\n",
      "train loss:0.5105587408061523\n",
      "train loss:0.39235599199996074\n",
      "train loss:0.39344867566510355\n",
      "train loss:0.6363792396516612\n",
      "train loss:0.7844804782657805\n",
      "train loss:0.7713933941937559\n",
      "train loss:0.6481598448691305\n",
      "train loss:0.7389209141206042\n",
      "train loss:0.4932795852751043\n",
      "train loss:0.5035085206450398\n",
      "train loss:0.6182033550917049\n",
      "train loss:0.7067332131064313\n",
      "train loss:0.6944861173420995\n",
      "train loss:0.6762762649623426\n",
      "train loss:0.5986455951013956\n",
      "train loss:0.6764633252867369\n",
      "train loss:0.6077153895596697\n",
      "train loss:0.665700666027217\n",
      "train loss:0.7180557155775051\n",
      "train loss:0.4957153118995664\n",
      "train loss:0.5795315015207757\n",
      "train loss:0.5644640246845947\n",
      "train loss:0.5641950054436232\n",
      "train loss:0.4853777749175835\n",
      "train loss:0.60703958142835\n",
      "train loss:0.6754789481051982\n",
      "train loss:0.5433909059777566\n",
      "train loss:0.5971717673497303\n",
      "train loss:0.6980218028675951\n",
      "train loss:0.6885059498249823\n",
      "train loss:0.7183094269696668\n",
      "train loss:0.5269628591092824\n",
      "train loss:0.7850754909697085\n",
      "train loss:0.6137262428587436\n",
      "train loss:0.6077770431892906\n",
      "train loss:0.46135495249829905\n",
      "train loss:0.461706545810961\n",
      "train loss:0.5208697216901114\n",
      "train loss:0.7221373375064745\n",
      "train loss:0.49364183082090474\n",
      "train loss:0.4100168541327225\n",
      "train loss:0.842618206737406\n",
      "train loss:0.6971315860331633\n",
      "train loss:0.48771146254138403\n",
      "train loss:0.38867825987471205\n",
      "train loss:0.830949628721649\n",
      "train loss:0.48950874600174227\n",
      "train loss:0.6113945109237602\n",
      "train loss:0.6400952996214301\n",
      "train loss:0.5237189714534901\n",
      "train loss:0.39665239591564955\n",
      "train loss:0.3965904660938706\n",
      "train loss:0.37776787377519894\n",
      "train loss:0.5966628018986473\n",
      "train loss:0.6377899977506014\n",
      "train loss:0.7613510328039557\n",
      "train loss:0.4998630857513445\n",
      "train loss:0.642928265151498\n",
      "train loss:0.6417694548545165\n",
      "train loss:0.3938910402175689\n",
      "train loss:0.7187382197234632\n",
      "train loss:0.7980248820932965\n",
      "train loss:0.8113445690154544\n",
      "train loss:0.7021568124478341\n",
      "train loss:0.6807518834067885\n",
      "train loss:0.6144547698445557\n",
      "train loss:0.5771818699611827\n",
      "train loss:0.6177540960865524\n",
      "train loss:0.6609620250620365\n",
      "train loss:0.7721737328625184\n",
      "train loss:0.5781124703355712\n",
      "train loss:0.66832951118509\n",
      "train loss:0.6374498708988579\n",
      "train loss:0.5920944431117597\n",
      "train loss:0.5542053764731024\n",
      "train loss:0.5505556558301896\n",
      "train loss:0.5938263485511521\n",
      "train loss:0.5689581564264926\n",
      "train loss:0.613113780778163\n",
      "train loss:0.6036923834837216\n",
      "train loss:0.5444513217743386\n",
      "train loss:0.3797434215806136\n",
      "train loss:0.8658461069096491\n",
      "train loss:0.6923332780913132\n",
      "train loss:0.5087879438371257\n",
      "train loss:0.5198518384523928\n",
      "train loss:0.511889232576109\n",
      "train loss:0.7078129984431826\n",
      "train loss:0.8383743038795604\n",
      "train loss:0.74010064671075\n",
      "train loss:0.5351904396153341\n",
      "train loss:0.5294895298046015\n",
      "train loss:0.712735425826526\n",
      "train loss:0.4302485302429921\n",
      "train loss:0.7676880041562112\n",
      "train loss:0.5482341909340132\n",
      "train loss:0.6100107368146219\n",
      "train loss:0.6030743272446318\n",
      "train loss:0.6015372730391526\n",
      "train loss:0.7095071454886261\n",
      "train loss:0.5394122200894605\n",
      "train loss:0.6366186366418658\n",
      "train loss:0.8415495688249713\n",
      "train loss:0.47322789605381965\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.464472608800867\n",
      "train loss:0.6750039424490728\n",
      "train loss:0.6267969066999746\n",
      "train loss:0.5342414499038143\n",
      "train loss:0.44530056621807895\n",
      "train loss:0.8408803540245355\n",
      "train loss:0.6809041466149723\n",
      "train loss:0.6103189514578391\n",
      "train loss:0.5176173459055515\n",
      "train loss:0.6977978862729082\n",
      "train loss:0.6971295381520202\n",
      "train loss:0.6050414893668958\n",
      "train loss:0.5378740367352235\n",
      "train loss:0.5489600654139368\n",
      "train loss:0.622979720064303\n",
      "train loss:0.5530918668600311\n",
      "train loss:0.7033936052615246\n",
      "train loss:0.6966494111183994\n",
      "train loss:0.7885573312842323\n",
      "train loss:0.6214605988278988\n",
      "train loss:0.607350793982951\n",
      "train loss:0.4655464461169413\n",
      "train loss:0.6128271999135164\n",
      "train loss:0.4582004717435896\n",
      "train loss:0.4382420447717954\n",
      "train loss:0.5186611738646614\n",
      "train loss:0.5165085144675434\n",
      "train loss:0.2879925580315471\n",
      "train loss:0.3839634287343697\n",
      "train loss:0.36105272551282075\n",
      "train loss:0.9576004534739336\n",
      "train loss:0.7927049369736975\n",
      "train loss:0.18820070889478477\n",
      "train loss:0.5441903167646523\n",
      "train loss:0.6517608056780656\n",
      "train loss:0.6335947308513278\n",
      "train loss:0.5050065368214057\n",
      "train loss:0.7849350396849599\n",
      "train loss:0.36850170785463593\n",
      "train loss:0.39410444887740026\n",
      "train loss:0.8750499946605087\n",
      "train loss:0.6233126436863191\n",
      "train loss:0.7150364117171616\n",
      "train loss:0.41869685195291967\n",
      "train loss:0.4169560313691411\n",
      "train loss:0.5169363445230418\n",
      "train loss:0.7679282786233903\n",
      "train loss:0.600372867803073\n",
      "train loss:0.7679127082688486\n",
      "train loss:0.6912485996363795\n",
      "train loss:0.5967390233648507\n",
      "train loss:0.6894467633194555\n",
      "train loss:0.7349566685451928\n",
      "train loss:0.5172578814128654\n",
      "train loss:0.6685402685301838\n",
      "train loss:0.6730438964704575\n",
      "train loss:0.7828467356550816\n",
      "train loss:0.6233762678027301\n",
      "train loss:0.5226986689978795\n",
      "train loss:0.6314963697903136\n",
      "train loss:0.6294468313739381\n",
      "train loss:0.6757413316492322\n",
      "train loss:0.7561278273862804\n",
      "train loss:0.5824287604605413\n",
      "train loss:0.6460402702461442\n",
      "train loss:0.6240983736339388\n",
      "train loss:0.6293189267731242\n",
      "train loss:0.6616909361720984\n",
      "train loss:0.5259217678079615\n",
      "train loss:0.6723535556696363\n",
      "train loss:0.6850008374186427\n",
      "train loss:0.6707880288046751\n",
      "train loss:0.6097936604051496\n",
      "train loss:0.49053186774033186\n",
      "train loss:0.6710790048002936\n",
      "train loss:0.6926486313650565\n",
      "train loss:0.49311788050494976\n",
      "train loss:0.5360386464958945\n",
      "train loss:0.7636303951532029\n",
      "train loss:0.5205215092892701\n",
      "train loss:0.600220242833482\n",
      "train loss:0.7901356384708034\n",
      "train loss:0.5214271558872021\n",
      "train loss:0.7845849996486113\n",
      "train loss:0.7640559007964727\n",
      "train loss:0.7658035082611104\n",
      "train loss:0.5978349576765849\n",
      "train loss:0.8248811009742167\n",
      "train loss:0.6762639583943895\n",
      "train loss:0.6330877444874192\n",
      "train loss:0.7142800605626151\n",
      "train loss:0.617075392961884\n",
      "train loss:0.5665345349010809\n",
      "train loss:0.7520102433026947\n",
      "train loss:0.6272097243579755\n",
      "train loss:0.6288939753727063\n",
      "train loss:0.6338098248276275\n",
      "train loss:0.6839752304108944\n",
      "train loss:0.5933053751168438\n",
      "train loss:0.6265228062532542\n",
      "train loss:0.6752135372364723\n",
      "train loss:0.5865051288202008\n",
      "train loss:0.5373517908236953\n",
      "train loss:0.5147597254074561\n",
      "train loss:0.566905661285914\n",
      "train loss:0.6763015503875259\n",
      "train loss:0.6062285375875177\n",
      "train loss:0.6077451781424583\n",
      "train loss:0.7695016734462414\n",
      "train loss:0.6079848492858809\n",
      "train loss:0.5977793029667546\n",
      "train loss:0.44413818438516683\n",
      "train loss:0.871569553959862\n",
      "train loss:0.502186813399603\n",
      "train loss:0.5284366260339592\n",
      "train loss:0.5979030871020331\n",
      "train loss:0.5175563294873948\n",
      "train loss:0.4906835480660837\n",
      "train loss:0.6131178003972589\n",
      "train loss:0.5077204406163803\n",
      "train loss:0.7336326119688198\n",
      "train loss:0.4879478787182758\n",
      "train loss:0.7334261167637468\n",
      "train loss:0.3843341147296591\n",
      "train loss:0.5156507391059787\n",
      "train loss:0.7516518459601118\n",
      "train loss:0.7132183880310379\n",
      "train loss:0.7991111265893456\n",
      "train loss:0.8143863632293732\n",
      "train loss:0.6468435104783332\n",
      "train loss:0.6796485039439325\n",
      "train loss:0.5561132617707061\n",
      "train loss:0.7237665564328171\n",
      "train loss:0.674222929996505\n",
      "train loss:0.5154105389408705\n",
      "train loss:0.6274602222843539\n",
      "train loss:0.5790650918126221\n",
      "train loss:0.6240716610780246\n",
      "train loss:0.6731580575569943\n",
      "train loss:0.5150922855365307\n",
      "train loss:0.6775530654550704\n",
      "train loss:0.47305932553310254\n",
      "train loss:0.6259603014919964\n",
      "train loss:0.5081803197355154\n",
      "train loss:0.4308147480730121\n",
      "train loss:0.6164763751802664\n",
      "train loss:0.5928429197965768\n",
      "train loss:0.6846619981438197\n",
      "train loss:0.41550030271297234\n",
      "train loss:0.29752708078002815\n",
      "train loss:0.5003764203457745\n",
      "train loss:0.4914842588377237\n",
      "train loss:0.36999694985427645\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6553672019783221\n",
      "train loss:1.1294409317931673\n",
      "train loss:0.6721089601587129\n",
      "train loss:0.9052046955043622\n",
      "train loss:0.7097210110151952\n",
      "train loss:0.7154840094271865\n",
      "train loss:0.6140715328292784\n",
      "train loss:0.43116807590737044\n",
      "train loss:0.4523356848642452\n",
      "train loss:0.4569561424488732\n",
      "train loss:0.4451856913361407\n",
      "train loss:0.5203933872688606\n",
      "train loss:0.7440696321886768\n",
      "train loss:0.4544444300485374\n",
      "train loss:0.7727684565984829\n",
      "train loss:0.44388367462140826\n",
      "train loss:0.8033678890713848\n",
      "train loss:0.520389087989985\n",
      "train loss:0.6949432432916489\n",
      "train loss:0.616563897581476\n",
      "train loss:0.5246573364823977\n",
      "train loss:0.5367984204735137\n",
      "train loss:0.6793052056768787\n",
      "train loss:0.42339754448889544\n",
      "train loss:0.3451520128289681\n",
      "train loss:0.5187535833753762\n",
      "train loss:0.8082907615782003\n",
      "train loss:0.4072259502785475\n",
      "train loss:0.7144791605944066\n",
      "train loss:0.5117643659973183\n",
      "train loss:0.7337344524212073\n",
      "train loss:0.6123630252989354\n",
      "train loss:0.7318586532747343\n",
      "train loss:0.47866053262554464\n",
      "train loss:0.5111346709289687\n",
      "train loss:0.48070870771103485\n",
      "train loss:1.0093225110206814\n",
      "train loss:0.5064507939441043\n",
      "train loss:0.4219976828505418\n",
      "train loss:0.702727082722088\n",
      "train loss:0.7746393509473213\n",
      "train loss:0.7013364958343404\n",
      "train loss:0.8241131615966927\n",
      "train loss:0.6749401296782546\n",
      "train loss:0.5576450393260305\n",
      "train loss:0.4191129538810661\n",
      "train loss:0.6687192587120974\n",
      "train loss:0.4843173111979195\n",
      "train loss:0.7389244147639242\n",
      "train loss:0.485536492103127\n",
      "train loss:0.5435217490351192\n",
      "train loss:0.6080100914614557\n",
      "train loss:0.6705025057006129\n",
      "train loss:0.6318176525297285\n",
      "train loss:0.620990165302276\n",
      "train loss:0.6915977143630911\n",
      "train loss:0.7693100449754958\n",
      "train loss:0.6133797014329211\n",
      "train loss:0.6808176566678503\n",
      "train loss:0.6176751788265007\n",
      "train loss:0.8039311082990028\n",
      "train loss:0.6048534027440959\n",
      "train loss:0.590267109541407\n",
      "train loss:0.6681546463505306\n",
      "train loss:0.6730092451320047\n",
      "train loss:0.6619814097003167\n",
      "train loss:0.49381329913855615\n",
      "train loss:0.6319672752978425\n",
      "train loss:0.8079446905635393\n",
      "train loss:0.48124178599644285\n",
      "train loss:0.6126803155248163\n",
      "train loss:0.6043609546194759\n",
      "train loss:0.5985818592700458\n",
      "train loss:0.6261001348360091\n",
      "train loss:0.7595037283551273\n",
      "train loss:0.5481832877887465\n",
      "train loss:0.6193505255109912\n",
      "train loss:0.644329854438322\n",
      "train loss:0.7039554290991935\n",
      "train loss:0.6879848607369883\n",
      "train loss:0.6205801028171916\n",
      "train loss:0.5906627814630363\n",
      "train loss:0.5377158080403605\n",
      "train loss:0.8063557262900579\n",
      "train loss:0.631345905959293\n",
      "train loss:0.6845806578361426\n",
      "train loss:0.5564495010808026\n",
      "train loss:0.536597803423055\n",
      "train loss:0.38311307931918226\n",
      "train loss:0.45498215686619153\n",
      "train loss:0.8592108805232378\n",
      "train loss:0.615222777225225\n",
      "train loss:0.7782779421752876\n",
      "train loss:0.44409724080561375\n",
      "train loss:0.6743759256447246\n",
      "train loss:0.5121669413610557\n",
      "train loss:0.9273995312067449\n",
      "train loss:0.7667620210493844\n",
      "train loss:0.7002643185190791\n",
      "train loss:0.6171876975879519\n",
      "train loss:0.5996464649723923\n",
      "train loss:0.5660068354241024\n",
      "train loss:0.4616825636306382\n",
      "train loss:0.6091350035373295\n",
      "train loss:0.5955306374631522\n",
      "train loss:0.6734295432681734\n",
      "train loss:0.8225680443277843\n",
      "train loss:0.685216258273867\n",
      "train loss:0.4845749205807409\n",
      "train loss:0.6122315957021207\n",
      "train loss:0.6182418470348832\n",
      "train loss:0.5512066859054807\n",
      "train loss:0.5464257871834273\n",
      "train loss:0.6084470067387062\n",
      "train loss:0.7427761519628288\n",
      "train loss:0.598995351305015\n",
      "train loss:0.5407944594005533\n",
      "train loss:0.5813360876898044\n",
      "train loss:0.6620684580315522\n",
      "train loss:0.5368081233887723\n",
      "train loss:0.6428911069939768\n",
      "train loss:0.6018290152583736\n",
      "train loss:0.5158408122674062\n",
      "train loss:0.563409912436858\n",
      "train loss:0.527384218969735\n",
      "train loss:0.5913943890143485\n",
      "train loss:0.5221228089603579\n",
      "train loss:0.390445603634481\n",
      "train loss:0.4937623479049975\n",
      "train loss:0.593651070791862\n",
      "train loss:0.37548410160222717\n",
      "train loss:0.3396035721404428\n",
      "train loss:0.6532600494946545\n",
      "train loss:0.8122504059002742\n",
      "train loss:0.6051093273962997\n",
      "train loss:0.7423097544509688\n",
      "train loss:0.7219603333608531\n",
      "train loss:0.7401259314718917\n",
      "train loss:0.4934191634278978\n",
      "train loss:0.6389777641300873\n",
      "train loss:0.5947719547777819\n",
      "train loss:0.5127265996995372\n",
      "train loss:0.7706021350916163\n",
      "train loss:0.5491365298539843\n",
      "train loss:0.6057205244935395\n",
      "train loss:0.6118020397611603\n",
      "train loss:0.598449544361348\n",
      "train loss:0.7131962645406186\n",
      "train loss:0.4368381555206803\n",
      "train loss:0.6556886891338531\n",
      "train loss:0.5565998043838711\n",
      "train loss:0.41781448397057286\n",
      "train loss:0.6287492989060298\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5270727308308458\n",
      "train loss:0.4239109259546886\n",
      "train loss:0.508626497840441\n",
      "train loss:0.5081004510654419\n",
      "train loss:0.7409657516785763\n",
      "train loss:0.41176037933825915\n",
      "train loss:0.745945754113765\n",
      "train loss:0.47292943368639817\n",
      "train loss:0.601422500319006\n",
      "train loss:0.7648381424782986\n",
      "train loss:0.3294735161587962\n",
      "train loss:0.34716084455229723\n",
      "train loss:0.6107028908228062\n",
      "train loss:0.7098151945967186\n",
      "train loss:0.49665255084007925\n",
      "train loss:0.48490020435377745\n",
      "train loss:0.9695710187237747\n",
      "train loss:0.7453076527396705\n",
      "train loss:0.6289719184659538\n",
      "train loss:0.31574567636952083\n",
      "train loss:0.41549744425052715\n",
      "train loss:0.5089470538100008\n",
      "train loss:0.6168024227426688\n",
      "train loss:0.514514181668634\n",
      "train loss:0.5119273335791795\n",
      "train loss:0.6605011892961097\n",
      "train loss:0.5052259648270759\n",
      "train loss:0.7287688973963254\n",
      "train loss:0.9808385480784484\n",
      "train loss:0.8652062764455366\n",
      "train loss:0.5383302663333069\n",
      "train loss:0.7130944713504662\n",
      "train loss:0.6094620614207773\n",
      "train loss:0.5921695217273198\n",
      "train loss:0.706688959061626\n",
      "train loss:0.6587696096665947\n",
      "train loss:0.683118478810995\n",
      "train loss:0.5724576225232607\n",
      "train loss:0.6053902231967928\n",
      "train loss:0.6200267867316618\n",
      "train loss:0.5843348409200813\n",
      "train loss:0.6604362065653856\n",
      "train loss:0.6544352482242565\n",
      "train loss:0.6236387996648993\n",
      "train loss:0.5532538410021675\n",
      "train loss:0.6332699599132413\n",
      "train loss:0.6180757016827501\n",
      "train loss:0.653532350443804\n",
      "train loss:0.5580123439021865\n",
      "train loss:0.6380990214812227\n",
      "train loss:0.6846234685733315\n",
      "train loss:0.5542204249900984\n",
      "train loss:0.6636975769344009\n",
      "train loss:0.7314159969923032\n",
      "train loss:0.6090238559381735\n",
      "train loss:0.783019252553067\n",
      "train loss:0.5554679340471407\n",
      "train loss:0.6213448532461125\n",
      "train loss:0.6408302730010066\n",
      "train loss:0.5320776448451823\n",
      "train loss:0.6756848386492903\n",
      "train loss:0.6021919317228234\n",
      "train loss:0.675374449716563\n",
      "train loss:0.6852664864819261\n",
      "train loss:0.6603102316936776\n",
      "train loss:0.6221826027331174\n",
      "train loss:0.7079665061247316\n",
      "train loss:0.567231829701256\n",
      "train loss:0.5838333867144492\n",
      "train loss:0.6395496164731638\n",
      "train loss:0.6189911974926778\n",
      "train loss:0.6184326261636486\n",
      "train loss:0.7016972131385797\n",
      "train loss:0.6252261001419939\n",
      "train loss:0.5454964759472254\n",
      "train loss:0.7814333244899908\n",
      "train loss:0.6071783340416789\n",
      "train loss:0.7745319155630535\n",
      "train loss:0.7477522082890632\n",
      "train loss:0.6389642926107608\n",
      "train loss:0.6842391952597635\n",
      "train loss:0.6533661684622121\n",
      "train loss:0.6938446380173765\n",
      "train loss:0.6230420761274219\n",
      "train loss:0.5821171309337981\n",
      "train loss:0.6769321907678861\n",
      "train loss:0.7119364342731822\n",
      "train loss:0.6662070009392942\n",
      "train loss:0.6270071639019827\n",
      "train loss:0.5467593941460622\n",
      "train loss:0.7166888321243169\n",
      "train loss:0.5809304131669144\n",
      "train loss:0.5091156969031962\n",
      "train loss:0.6059465074417866\n",
      "train loss:0.6598181715494292\n",
      "train loss:0.7000831529898047\n",
      "train loss:0.697055665576314\n",
      "train loss:0.6224420369946955\n",
      "train loss:0.520958109781778\n",
      "train loss:0.7138224417697856\n",
      "train loss:0.5660952059322548\n",
      "train loss:0.5023918462876564\n",
      "train loss:0.36522421596149357\n",
      "train loss:0.626101012730962\n",
      "train loss:0.7211979075322265\n",
      "train loss:0.5118448946698224\n",
      "train loss:0.5735994290340891\n",
      "train loss:0.2732341037702809\n",
      "train loss:0.23880140749893278\n",
      "train loss:0.49354661570269204\n",
      "train loss:0.6680944544541688\n",
      "train loss:0.8158595595972683\n",
      "train loss:0.5100760787397024\n",
      "train loss:0.5091678679316191\n",
      "train loss:0.9395984184037184\n",
      "train loss:0.4928203325182425\n",
      "train loss:0.38287133376035626\n",
      "train loss:0.47973085616865463\n",
      "train loss:0.4978279960181675\n",
      "train loss:0.5833629926788151\n",
      "train loss:0.5082847343963219\n",
      "train loss:0.7369176128752948\n",
      "train loss:0.4166036462375639\n",
      "train loss:0.6072126770409035\n",
      "train loss:0.6084688558736708\n",
      "train loss:0.8071065804053392\n",
      "train loss:0.418531550186089\n",
      "train loss:0.603782550463124\n",
      "train loss:0.5285857942975611\n",
      "train loss:0.40848878678164596\n",
      "train loss:0.49087266040639854\n",
      "train loss:0.798594456686727\n",
      "train loss:0.9012912076876571\n",
      "train loss:0.6719942551979204\n",
      "train loss:0.7831757703799941\n",
      "train loss:0.6752435119131945\n",
      "train loss:0.5941113920350809\n",
      "train loss:0.6468842185953528\n",
      "train loss:0.6005602895429372\n",
      "train loss:0.6633806629046235\n",
      "train loss:0.5873913522952415\n",
      "train loss:0.7258457243868286\n",
      "train loss:0.6212787027737771\n",
      "train loss:0.5882587235511656\n",
      "train loss:0.5507010160823347\n",
      "train loss:0.5774151177028561\n",
      "train loss:0.6739922168425068\n",
      "train loss:0.6308303131148676\n",
      "train loss:0.5146852265979026\n",
      "train loss:0.5761607076032386\n",
      "train loss:0.5990620806251188\n",
      "train loss:0.6533435808578298\n",
      "train loss:0.6732491669778262\n",
      "=== epoch:5, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.655185919969394\n",
      "train loss:0.5972397831286707\n",
      "train loss:0.6519302156785245\n",
      "train loss:0.6783105447337974\n",
      "train loss:0.41577948416570615\n",
      "train loss:0.48212312237234195\n",
      "train loss:0.6926655787708945\n",
      "train loss:0.5965579802021029\n",
      "train loss:0.6969958551298963\n",
      "train loss:0.7511305792457853\n",
      "train loss:0.7738782581631996\n",
      "train loss:0.5257372596549392\n",
      "train loss:0.43599915114351245\n",
      "train loss:0.4939487066568896\n",
      "train loss:0.6111481092308251\n",
      "train loss:0.7943751881532414\n",
      "train loss:0.7141039525582784\n",
      "train loss:0.3839700443414006\n",
      "train loss:0.5912551946517902\n",
      "train loss:0.5037477991385856\n",
      "train loss:0.41638398523460857\n",
      "train loss:0.40918523676047186\n",
      "train loss:0.5940820176825843\n",
      "train loss:0.5202476588897919\n",
      "train loss:0.8622358489017221\n",
      "train loss:0.7237605936434586\n",
      "train loss:0.836251441722568\n",
      "train loss:0.5160084961150163\n",
      "train loss:0.3765391381388816\n",
      "train loss:0.2992482702336454\n",
      "train loss:0.5445169273663736\n",
      "train loss:0.7446219736239608\n",
      "train loss:0.47039638057378746\n",
      "train loss:0.5389600948485402\n",
      "train loss:0.9845679356125118\n",
      "train loss:0.42533897754602396\n",
      "train loss:0.6150031670402172\n",
      "train loss:0.6097927142553029\n",
      "train loss:0.6065563090827165\n",
      "train loss:0.6035274383240391\n",
      "train loss:0.7972810505980265\n",
      "train loss:0.6166191744591301\n",
      "train loss:0.3902400220014062\n",
      "train loss:0.5429940568598798\n",
      "train loss:0.5974403362866618\n",
      "train loss:0.4462994044503792\n",
      "train loss:0.5125442479115563\n",
      "train loss:0.676814364527781\n",
      "train loss:0.8208516275106412\n",
      "train loss:0.6166974847005721\n",
      "train loss:0.6852907961136703\n",
      "train loss:0.8435967886973378\n",
      "train loss:0.5695017789769355\n",
      "train loss:0.6634002199505529\n",
      "train loss:0.49002015787959874\n",
      "train loss:0.5241592285997456\n",
      "train loss:0.5566471035743884\n",
      "train loss:0.6106946186905616\n",
      "train loss:0.628123393689528\n",
      "train loss:0.4162158443561547\n",
      "train loss:0.4594579894852492\n",
      "train loss:0.44509383385927015\n",
      "train loss:0.5709812558778422\n",
      "train loss:0.7325878695421795\n",
      "train loss:0.4839238392746343\n",
      "train loss:0.706898976358603\n",
      "train loss:0.4960013455900067\n",
      "train loss:0.6369372886389072\n",
      "train loss:0.5267040532348684\n",
      "train loss:0.4989601984408839\n",
      "train loss:0.44864349791934044\n",
      "train loss:1.0176368869875176\n",
      "train loss:0.6020670396891735\n",
      "train loss:0.6863394045214527\n",
      "train loss:0.604552996323606\n",
      "train loss:0.5536818124244094\n",
      "train loss:0.7011725126740194\n",
      "train loss:0.4100616102400399\n",
      "train loss:0.6008701495213793\n",
      "train loss:0.5360973303385234\n",
      "train loss:0.47457903151038794\n",
      "train loss:0.5678188797117711\n",
      "train loss:0.5247238564248209\n",
      "train loss:0.8173548194567687\n",
      "train loss:0.5448478602900851\n",
      "train loss:0.7539648640764468\n",
      "train loss:0.6198719019707315\n",
      "train loss:0.7073897653406975\n",
      "train loss:0.5100330137060876\n",
      "train loss:0.514434439564532\n",
      "train loss:0.3391216178881394\n",
      "train loss:0.45832626148070893\n",
      "train loss:0.6839261882337716\n",
      "train loss:0.5025233674590538\n",
      "train loss:0.4294704400217009\n",
      "train loss:0.7544630907691767\n",
      "train loss:0.9312434516498069\n",
      "train loss:0.6990657631943265\n",
      "train loss:0.6090777833822283\n",
      "train loss:0.7553937652426939\n",
      "train loss:0.5498504690194064\n",
      "train loss:0.6582431704089705\n",
      "train loss:0.6078037328374725\n",
      "train loss:0.5386363207459401\n",
      "train loss:0.6837424720345119\n",
      "train loss:0.6043693009121902\n",
      "train loss:0.6085908279863351\n",
      "train loss:0.6453735984078047\n",
      "train loss:0.5158770998245084\n",
      "train loss:0.5491351525796404\n",
      "train loss:0.5730684198665859\n",
      "train loss:0.5484793786219868\n",
      "train loss:0.4239196184182087\n",
      "train loss:0.6921555970481219\n",
      "train loss:0.7599033761123387\n",
      "train loss:0.5152206611604726\n",
      "train loss:0.6906334293066714\n",
      "train loss:0.8724279987598962\n",
      "train loss:0.6152492414067043\n",
      "train loss:0.6081431743027371\n",
      "train loss:0.5893284258779776\n",
      "train loss:0.6979510279509107\n",
      "train loss:0.46428582367155447\n",
      "train loss:0.6923645244073618\n",
      "train loss:0.625433661268276\n",
      "train loss:0.621126069588625\n",
      "train loss:0.6951629482788639\n",
      "train loss:0.6648187712512744\n",
      "train loss:0.39406780106301104\n",
      "train loss:0.7434833193095136\n",
      "train loss:0.5423442860899728\n",
      "train loss:0.5487167329140707\n",
      "train loss:0.6985885568182298\n",
      "train loss:0.6146389835955957\n",
      "train loss:0.707507879893534\n",
      "train loss:0.5692637692953502\n",
      "train loss:0.5404806011432791\n",
      "train loss:0.5962087912234157\n",
      "train loss:0.5099295129514477\n",
      "train loss:0.5366685348531475\n",
      "train loss:0.5987364619522026\n",
      "train loss:0.7683684544799483\n",
      "train loss:0.6539159153716627\n",
      "train loss:0.5438490118748293\n",
      "train loss:0.5591473568316399\n",
      "train loss:0.6756792635052378\n",
      "train loss:0.5879029437404379\n",
      "train loss:0.5232569655598034\n",
      "train loss:0.6836305958364854\n",
      "train loss:0.6049743791131325\n",
      "train loss:0.6851145751022523\n",
      "train loss:0.5340016493514611\n",
      "train loss:0.7342703659518427\n",
      "=== epoch:6, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.44953610908431285\n",
      "train loss:0.7121326827580117\n",
      "train loss:0.6533292379873732\n",
      "train loss:0.6453867895019662\n",
      "train loss:0.4757388370756999\n",
      "train loss:0.7264608054192899\n",
      "train loss:0.6047873985029847\n",
      "train loss:0.5939681295774583\n",
      "train loss:0.7067265580021167\n",
      "train loss:0.6181808215641486\n",
      "train loss:0.526312664371472\n",
      "train loss:0.7251482658267481\n",
      "train loss:0.5312344507852262\n",
      "train loss:0.4771955276178829\n",
      "train loss:0.46070940207919076\n",
      "train loss:0.5226443746758023\n",
      "train loss:0.6415297391973637\n",
      "train loss:0.6726408914733285\n",
      "train loss:0.6613089365246876\n",
      "train loss:0.49100770102483954\n",
      "train loss:0.608079969478348\n",
      "train loss:0.2871891075717168\n",
      "train loss:0.7265330883803537\n",
      "train loss:0.601597471412606\n",
      "train loss:0.5872550115157735\n",
      "train loss:0.705486725029614\n",
      "train loss:0.7103390649662475\n",
      "train loss:0.45132515398268636\n",
      "train loss:0.7100931002128087\n",
      "train loss:0.5623826617431747\n",
      "train loss:0.5558026582056323\n",
      "train loss:0.39338407516934004\n",
      "train loss:0.5595553842498159\n",
      "train loss:0.7759851853583077\n",
      "train loss:0.5199686682447368\n",
      "train loss:0.5356207466921232\n",
      "train loss:0.5114297563499863\n",
      "train loss:0.40150411236917327\n",
      "train loss:0.44322344494129123\n",
      "train loss:0.4944386953408072\n",
      "train loss:0.7476610621059554\n",
      "train loss:0.47991027562064853\n",
      "train loss:0.4838329033034042\n",
      "train loss:0.36250314659066707\n",
      "train loss:0.7321461924444923\n",
      "train loss:0.6866879491422256\n",
      "train loss:0.8089757474986013\n",
      "train loss:0.560825504459031\n",
      "train loss:0.37732004585710477\n",
      "train loss:0.6441138504492965\n",
      "train loss:0.6670049569312111\n",
      "train loss:0.455653721315152\n",
      "train loss:0.46588316670287977\n",
      "train loss:0.546877951648411\n",
      "train loss:0.7408347373803403\n",
      "train loss:0.3634723650169721\n",
      "train loss:0.8690419352668208\n",
      "train loss:0.5982047143308726\n",
      "train loss:0.4763889067891961\n",
      "train loss:0.6713010653055737\n",
      "train loss:0.6797965956582921\n",
      "train loss:0.44676592254292685\n",
      "train loss:0.47071102757663275\n",
      "train loss:0.6103145769941639\n",
      "train loss:0.7835523776668925\n",
      "train loss:0.5948133979821881\n",
      "train loss:0.6445676986457318\n",
      "train loss:0.45332425156975076\n",
      "train loss:0.8024716058248\n",
      "train loss:0.6678518062238088\n",
      "train loss:0.5237226769910304\n",
      "train loss:0.5561309454401022\n",
      "train loss:0.5789521987665824\n",
      "train loss:0.5714230567085985\n",
      "train loss:0.8384248354023207\n",
      "train loss:0.6754906956168534\n",
      "train loss:0.7254824052268779\n",
      "train loss:0.5384303264447803\n",
      "train loss:0.6251737426981652\n",
      "train loss:0.7059095768600584\n",
      "train loss:0.5347524595399693\n",
      "train loss:0.6630855044767651\n",
      "train loss:0.7122179179584045\n",
      "train loss:0.6276675755184344\n",
      "train loss:0.5239846059551325\n",
      "train loss:0.5279214868711052\n",
      "train loss:0.5377506829713673\n",
      "train loss:0.5366600073183199\n",
      "train loss:0.4371313743093232\n",
      "train loss:0.6880750104674188\n",
      "train loss:0.7545602701940657\n",
      "train loss:0.4963416789488314\n",
      "train loss:0.44984210231732435\n",
      "train loss:0.618217038924086\n",
      "train loss:0.42331699666335176\n",
      "train loss:0.6523080883057156\n",
      "train loss:0.6499796256168501\n",
      "train loss:0.5050001863613935\n",
      "train loss:0.3388979987943311\n",
      "train loss:0.524946162369807\n",
      "train loss:0.6747455761656386\n",
      "train loss:0.5261785875023974\n",
      "train loss:0.5175889708443256\n",
      "train loss:0.6766506931527433\n",
      "train loss:0.5523583788142553\n",
      "train loss:0.49428704875783414\n",
      "train loss:0.6051120628913809\n",
      "train loss:0.5757553754553567\n",
      "train loss:0.6253580714845677\n",
      "train loss:0.8210802713275592\n",
      "train loss:0.9215228014753034\n",
      "train loss:0.6124688543484733\n",
      "train loss:0.73082656937796\n",
      "train loss:0.575482076722771\n",
      "train loss:0.38749873109820127\n",
      "train loss:0.6956558072206438\n",
      "train loss:0.5839824725686917\n",
      "train loss:0.6808280999781118\n",
      "train loss:0.626950781427168\n",
      "train loss:0.6784906910715438\n",
      "train loss:0.5962085037868177\n",
      "train loss:0.5562397658220848\n",
      "train loss:0.5397233607204768\n",
      "train loss:0.5178242890300063\n",
      "train loss:0.5928259324590072\n",
      "train loss:0.5362040260272953\n",
      "train loss:0.513390539304552\n",
      "train loss:0.6049918079925141\n",
      "train loss:0.5395490282950919\n",
      "train loss:0.5305182810599474\n",
      "train loss:0.5472687776388898\n",
      "train loss:0.5276509032637924\n",
      "train loss:0.3634218479376188\n",
      "train loss:0.5025733354364313\n",
      "train loss:0.6359892295901088\n",
      "train loss:0.48865619600359855\n",
      "train loss:0.6118815148542871\n",
      "train loss:0.6241573398020883\n",
      "train loss:0.7201127381596978\n",
      "train loss:0.25407538155294723\n",
      "train loss:0.966127256805969\n",
      "train loss:0.38506553753693556\n",
      "train loss:0.68163327834984\n",
      "train loss:0.6049972761809617\n",
      "train loss:0.8800308536239619\n",
      "train loss:0.3973136656288653\n",
      "train loss:0.3893000590023198\n",
      "train loss:0.8999529484764637\n",
      "train loss:0.33575704714446963\n",
      "train loss:0.6494968891351147\n",
      "train loss:0.5920451394163578\n",
      "train loss:0.7149713987784196\n",
      "train loss:0.6144994614342119\n",
      "=== epoch:7, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5380643602156112\n",
      "train loss:0.4802717055816362\n",
      "train loss:0.6978025646475254\n",
      "train loss:0.6519265345835701\n",
      "train loss:0.650521796574109\n",
      "train loss:0.5920045724330361\n",
      "train loss:0.6267076636981406\n",
      "train loss:0.7874285849657208\n",
      "train loss:0.5873082394624012\n",
      "train loss:0.5849945902273311\n",
      "train loss:0.5359369451949546\n",
      "train loss:0.5060689967723941\n",
      "train loss:0.645753392375294\n",
      "train loss:0.4871347834337206\n",
      "train loss:0.5657971250562721\n",
      "train loss:0.5438668338234042\n",
      "train loss:0.458614007094137\n",
      "train loss:0.5324471815254717\n",
      "train loss:0.4321035886772198\n",
      "train loss:0.5366956283612637\n",
      "train loss:0.5764316041788937\n",
      "train loss:0.8020621113922385\n",
      "train loss:0.6164722440587257\n",
      "train loss:0.5966651148666169\n",
      "train loss:0.7104822164465323\n",
      "train loss:0.6686248865585104\n",
      "train loss:0.376400025812052\n",
      "train loss:0.4026865955289967\n",
      "train loss:0.49899308282841603\n",
      "train loss:0.6710996421887485\n",
      "train loss:0.7025271055247684\n",
      "train loss:0.5854413393343931\n",
      "train loss:0.3864614757549413\n",
      "train loss:0.8702899257626886\n",
      "train loss:0.49987361411606096\n",
      "train loss:0.6399361646195921\n",
      "train loss:0.272643983694308\n",
      "train loss:0.44859680997566553\n",
      "train loss:0.638699245775362\n",
      "train loss:0.3812534240383899\n",
      "train loss:0.556722397945684\n",
      "train loss:0.6760917843878865\n",
      "train loss:0.5312424252619466\n",
      "train loss:0.5374989717839688\n",
      "train loss:0.7611325274903095\n",
      "train loss:0.7399121946139281\n",
      "train loss:0.6713559476711282\n",
      "train loss:0.8184403675700983\n",
      "train loss:0.5030306398971198\n",
      "train loss:0.5180640832183709\n",
      "train loss:0.5321163063748823\n",
      "train loss:0.6321322959645441\n",
      "train loss:0.48615748269599657\n",
      "train loss:0.6712599639245354\n",
      "train loss:0.5232926547694051\n",
      "train loss:0.5348747512030468\n",
      "train loss:0.6659447989480416\n",
      "train loss:0.5598499549004877\n",
      "train loss:0.6361438944778239\n",
      "train loss:0.6082405926923184\n",
      "train loss:0.4181979206161265\n",
      "train loss:0.4694002612811893\n",
      "train loss:0.34553208715451034\n",
      "train loss:1.0265410339342396\n",
      "train loss:0.5043960648117445\n",
      "train loss:0.39102349407759557\n",
      "train loss:0.3983193227511136\n",
      "train loss:0.5419655797822721\n",
      "train loss:0.35992048319336395\n",
      "train loss:0.888393461479224\n",
      "train loss:0.5277468632162855\n",
      "train loss:0.4219496214341619\n",
      "train loss:0.495660478574066\n",
      "train loss:0.9128349914395344\n",
      "train loss:0.5178224285255006\n",
      "train loss:0.6275785075779239\n",
      "train loss:0.4362976248730958\n",
      "train loss:0.6204818106478115\n",
      "train loss:0.3647666466936219\n",
      "train loss:0.62650893482027\n",
      "train loss:0.49292514472559645\n",
      "train loss:0.7224474078437007\n",
      "train loss:0.4639044376080161\n",
      "train loss:0.39429340370953436\n",
      "train loss:0.8633716903605301\n",
      "train loss:0.4123648314496985\n",
      "train loss:0.6177406237279355\n",
      "train loss:0.4973855201046257\n",
      "train loss:0.42384592174363755\n",
      "train loss:0.35678310548863884\n",
      "train loss:0.6967456008362782\n",
      "train loss:0.5058953180086296\n",
      "train loss:0.7459739086130457\n",
      "train loss:0.5631810701522272\n",
      "train loss:0.2896953672728716\n",
      "train loss:0.33664133484997455\n",
      "train loss:0.49465359472132436\n",
      "train loss:0.6585942933659293\n",
      "train loss:0.38302678547433977\n",
      "train loss:0.5225234010688353\n",
      "train loss:0.7155367379069494\n",
      "train loss:0.4338534943300674\n",
      "train loss:0.2673676448817133\n",
      "train loss:0.5311930203186314\n",
      "train loss:0.7017424741915627\n",
      "train loss:0.5351969872114728\n",
      "train loss:0.433651873507596\n",
      "train loss:0.472847016729854\n",
      "train loss:0.6925632367367809\n",
      "train loss:0.5511296335379436\n",
      "train loss:0.5859928194379954\n",
      "train loss:0.5774085568944614\n",
      "train loss:0.5703882798909807\n",
      "train loss:0.5853169024509043\n",
      "train loss:0.8021823477084048\n",
      "train loss:0.7540600578083129\n",
      "train loss:0.5280618935093732\n",
      "train loss:0.6387297098193732\n",
      "train loss:0.6189881737630666\n",
      "train loss:0.4659237908458086\n",
      "train loss:0.6744336669682631\n",
      "train loss:0.4937516978415905\n",
      "train loss:0.4991115191140446\n",
      "train loss:0.5085897396599154\n",
      "train loss:0.665482729149266\n",
      "train loss:0.47494471671511757\n",
      "train loss:0.7739337459447687\n",
      "train loss:0.6577939650820189\n",
      "train loss:0.5269629014327057\n",
      "train loss:0.5902742425210242\n",
      "train loss:0.6068513067416338\n",
      "train loss:0.6877535568987762\n",
      "train loss:0.6611035120390192\n",
      "train loss:0.7520486223769065\n",
      "train loss:0.5214987105526158\n",
      "train loss:0.6938819605338852\n",
      "train loss:0.5892294013471732\n",
      "train loss:0.626154980668351\n",
      "train loss:0.6012799864024654\n",
      "train loss:0.4389696217532763\n",
      "train loss:0.5480172959104911\n",
      "train loss:0.7314744127442557\n",
      "train loss:0.46798056654605763\n",
      "train loss:0.4803856539130593\n",
      "train loss:0.7631045822279973\n",
      "train loss:0.4741874791885888\n",
      "train loss:0.4418669113759902\n",
      "train loss:0.5431138967474602\n",
      "train loss:0.5920880353227664\n",
      "train loss:0.40459131362335654\n",
      "train loss:0.6032297669734622\n",
      "train loss:0.44699036150935234\n",
      "train loss:0.4038404710348621\n",
      "=== epoch:8, train acc:0.74, test acc:0.69 ===\n",
      "train loss:0.6698234306473468\n",
      "train loss:0.5796543229689743\n",
      "train loss:0.4893097935034871\n",
      "train loss:0.6592905974582449\n",
      "train loss:0.3526428490878748\n",
      "train loss:0.5248987935938703\n",
      "train loss:0.6515679321889126\n",
      "train loss:0.7264850075668146\n",
      "train loss:0.5037613777725494\n",
      "train loss:0.6538701202875781\n",
      "train loss:0.46621884250632534\n",
      "train loss:0.7329517847659657\n",
      "train loss:0.427912097980125\n",
      "train loss:0.5295247809790515\n",
      "train loss:0.5731651529126258\n",
      "train loss:0.56320879820962\n",
      "train loss:0.7866311930242811\n",
      "train loss:0.49445706603992284\n",
      "train loss:0.5888628981758204\n",
      "train loss:0.6818512330038995\n",
      "train loss:0.5293541105280066\n",
      "train loss:0.5111461407957825\n",
      "train loss:0.6811067263413018\n",
      "train loss:0.6004389066930955\n",
      "train loss:0.42583630563748615\n",
      "train loss:0.7140716774145823\n",
      "train loss:0.4159811518476812\n",
      "train loss:0.6531428080939983\n",
      "train loss:0.5063528021537991\n",
      "train loss:0.659635790552541\n",
      "train loss:0.5549586322898533\n",
      "train loss:0.42472496740684856\n",
      "train loss:0.2784530163608272\n",
      "train loss:0.19428715338811106\n",
      "train loss:0.2635077708382637\n",
      "train loss:0.48396798424183113\n",
      "train loss:1.2240527131733159\n",
      "train loss:0.5165691429675361\n",
      "train loss:0.7711321953638095\n",
      "train loss:0.6139859075341343\n",
      "train loss:0.5418325597352628\n",
      "train loss:0.3738823160104107\n",
      "train loss:0.48550341205094727\n",
      "train loss:0.45878626408818624\n",
      "train loss:0.49271109628722226\n",
      "train loss:0.40118669712449284\n",
      "train loss:0.8395782289775588\n",
      "train loss:0.4760181832101088\n",
      "train loss:0.5343911847279255\n",
      "train loss:0.5987270045521467\n",
      "train loss:0.2926311761139477\n",
      "train loss:0.4253745214900668\n",
      "train loss:0.5458160390805282\n",
      "train loss:0.6076362476679855\n",
      "train loss:0.7108411505843899\n",
      "train loss:0.5061792257958646\n",
      "train loss:0.6140096701257836\n",
      "train loss:0.30939043369043323\n",
      "train loss:0.5398160885458468\n",
      "train loss:0.8040875511543089\n",
      "train loss:0.6502907677931129\n",
      "train loss:0.4666260963867364\n",
      "train loss:0.5476200258814558\n",
      "train loss:0.7736231484854506\n",
      "train loss:0.538852725032578\n",
      "train loss:0.38198259351743286\n",
      "train loss:0.48796326300505566\n",
      "train loss:0.6502393062049187\n",
      "train loss:0.3621760592859779\n",
      "train loss:0.5621105893574896\n",
      "train loss:0.4515605589156674\n",
      "train loss:0.4986710882707313\n",
      "train loss:0.5953460664467946\n",
      "train loss:0.4506875830408153\n",
      "train loss:0.46586989674278206\n",
      "train loss:0.34675493004063995\n",
      "train loss:0.45410335014738523\n",
      "train loss:0.5247555061730033\n",
      "train loss:0.632491593044617\n",
      "train loss:0.3339682609971385\n",
      "train loss:0.5445808326887162\n",
      "train loss:0.34791499158900707\n",
      "train loss:0.8260390993086336\n",
      "train loss:0.44219514015263545\n",
      "train loss:0.7998803722240064\n",
      "train loss:0.5365015610902042\n",
      "train loss:0.5316843807205034\n",
      "train loss:0.3724768845522616\n",
      "train loss:0.6463644955990813\n",
      "train loss:0.5696389864130245\n",
      "train loss:0.34399879719616416\n",
      "train loss:0.5751576901847325\n",
      "train loss:0.6527967940897589\n",
      "train loss:0.8180637818298658\n",
      "train loss:0.529166900138188\n",
      "train loss:0.48555523394483446\n",
      "train loss:0.6115262995031038\n",
      "train loss:0.6286549312661928\n",
      "train loss:0.570484561406996\n",
      "train loss:0.484315609978102\n",
      "train loss:0.5835524059755991\n",
      "train loss:0.5282430763416766\n",
      "train loss:0.5135430165453159\n",
      "train loss:0.4117285685731084\n",
      "train loss:0.6978956642565712\n",
      "train loss:0.544794758425427\n",
      "train loss:0.7059718793010958\n",
      "train loss:0.5159472151733316\n",
      "train loss:0.4122022183789159\n",
      "train loss:1.091508849551866\n",
      "train loss:0.36769635795005773\n",
      "train loss:0.9513912266172195\n",
      "train loss:0.8498699447575163\n",
      "train loss:0.36254329937409985\n",
      "train loss:0.49137630108732144\n",
      "train loss:0.4104661351797586\n",
      "train loss:0.4558087385855825\n",
      "train loss:0.6091837124473545\n",
      "train loss:0.6106017374156362\n",
      "train loss:0.5025025159825989\n",
      "train loss:0.5652781926646484\n",
      "train loss:0.4494797905456226\n",
      "train loss:0.5764342618772972\n",
      "train loss:0.623947767758287\n",
      "train loss:0.42620355332751814\n",
      "train loss:0.4934152004426651\n",
      "train loss:0.5732198720856564\n",
      "train loss:0.5930838737585706\n",
      "train loss:0.3236274054202716\n",
      "train loss:0.48332497643761485\n",
      "train loss:0.49532734383398314\n",
      "train loss:0.8195363589941321\n",
      "train loss:0.22410823099573376\n",
      "train loss:0.6477643747496422\n",
      "train loss:0.7124702733685083\n",
      "train loss:0.44286055415695424\n",
      "train loss:0.707405061156191\n",
      "train loss:0.6013678891515022\n",
      "train loss:0.557692137591405\n",
      "train loss:0.6078760001386454\n",
      "train loss:0.6344290249912101\n",
      "train loss:0.562767813071391\n",
      "train loss:0.4976424267536583\n",
      "train loss:0.5225666670155669\n",
      "train loss:0.35109143268567894\n",
      "train loss:0.5646816253414715\n",
      "train loss:0.4287492218757669\n",
      "train loss:0.6057690521807855\n",
      "train loss:0.6071995768074548\n",
      "train loss:0.5437729825228443\n",
      "train loss:0.6900517213058001\n",
      "train loss:0.5407453437667871\n",
      "train loss:0.6170924924523252\n",
      "=== epoch:9, train acc:0.74, test acc:0.69 ===\n",
      "train loss:0.49380566939910747\n",
      "train loss:0.5052053771662224\n",
      "train loss:0.3785405080546978\n",
      "train loss:0.6512594944347312\n",
      "train loss:0.6727850019884823\n",
      "train loss:0.6125225683985598\n",
      "train loss:0.4465864644141484\n",
      "train loss:0.5943974820406044\n",
      "train loss:0.5573083662926622\n",
      "train loss:0.6147663433795736\n",
      "train loss:0.389789557865366\n",
      "train loss:0.49115919337781\n",
      "train loss:0.5115708838373995\n",
      "train loss:0.39737781799667315\n",
      "train loss:0.622557109079905\n",
      "train loss:0.7231716755953587\n",
      "train loss:0.577492583091398\n",
      "train loss:0.6764271212608548\n",
      "train loss:0.6700890414212533\n",
      "train loss:0.4346065735496734\n",
      "train loss:0.30379699628782025\n",
      "train loss:0.286019816196967\n",
      "train loss:0.8510367290577839\n",
      "train loss:0.7019769963574826\n",
      "train loss:0.48138353653857663\n",
      "train loss:0.6817976989267255\n",
      "train loss:0.22443078979923134\n",
      "train loss:0.6856073178688847\n",
      "train loss:0.6272146430112631\n",
      "train loss:0.5891245027543899\n",
      "train loss:0.5758647692235004\n",
      "train loss:0.5328810912650702\n",
      "train loss:0.6462893354958628\n",
      "train loss:0.5557150183557129\n",
      "train loss:0.6243669952553008\n",
      "train loss:0.6694625740695477\n",
      "train loss:0.6896761313890553\n",
      "train loss:0.6235865967846934\n",
      "train loss:0.351235365386912\n",
      "train loss:0.45790621015496996\n",
      "train loss:0.6185156034710653\n",
      "train loss:0.32881391975175134\n",
      "train loss:0.6802378620017687\n",
      "train loss:0.4154731946729309\n",
      "train loss:0.7977454201352203\n",
      "train loss:0.4120245596623467\n",
      "train loss:0.5497316019371752\n",
      "train loss:0.4596782201658264\n",
      "train loss:0.7770977268916746\n",
      "train loss:0.3393022489107511\n",
      "train loss:0.666315372581929\n",
      "train loss:0.5996797134814685\n",
      "train loss:0.5245922426125719\n",
      "train loss:0.5092532330890573\n",
      "train loss:0.4788417905479987\n",
      "train loss:0.7005433368647884\n",
      "train loss:0.5103943767733142\n",
      "train loss:0.6185555208063378\n",
      "train loss:0.2783108122109576\n",
      "train loss:0.3969399633781192\n",
      "train loss:0.7131560571514551\n",
      "train loss:0.6515030993556146\n",
      "train loss:0.6218757001296448\n",
      "train loss:0.67928645362427\n",
      "train loss:0.6653445340376554\n",
      "train loss:0.38996666330318774\n",
      "train loss:0.6799487659284925\n",
      "train loss:0.5677649737337885\n",
      "train loss:0.4525374115762303\n",
      "train loss:0.690939651932376\n",
      "train loss:0.5229850508408981\n",
      "train loss:0.5425308098336018\n",
      "train loss:0.5496156062474304\n",
      "train loss:0.6106634086549738\n",
      "train loss:0.45912250276189104\n",
      "train loss:0.6104385320500381\n",
      "train loss:0.34343301492923634\n",
      "train loss:0.4649032261637275\n",
      "train loss:0.5147625529549533\n",
      "train loss:0.4960270012489339\n",
      "train loss:0.5124794838366151\n",
      "train loss:0.28731446839773045\n",
      "train loss:0.3502544851489042\n",
      "train loss:0.1636595574309226\n",
      "train loss:0.4931937731572316\n",
      "train loss:0.42968343337691917\n",
      "train loss:0.7142087155774197\n",
      "train loss:0.39403118372214074\n",
      "train loss:0.5561679125211196\n",
      "train loss:0.6515263011810475\n",
      "train loss:0.8420001703283428\n",
      "train loss:0.5489412172590072\n",
      "train loss:0.798171017762854\n",
      "train loss:0.5349560437381227\n",
      "train loss:0.43412139240546554\n",
      "train loss:0.40222204753779983\n",
      "train loss:0.8070262813577982\n",
      "train loss:0.6818785181800286\n",
      "train loss:0.6291000671868037\n",
      "train loss:0.378553651966315\n",
      "train loss:0.6914553483114395\n",
      "train loss:0.6533739090367237\n",
      "train loss:0.5104852057859368\n",
      "train loss:0.42164151798923155\n",
      "train loss:0.7061794356395051\n",
      "train loss:0.48377860479422735\n",
      "train loss:0.6590932605270023\n",
      "train loss:0.47457716190371313\n",
      "train loss:0.556009876260433\n",
      "train loss:0.5840711476023068\n",
      "train loss:0.5064279706171333\n",
      "train loss:0.7008117566897867\n",
      "train loss:0.6431542357873419\n",
      "train loss:0.5096676937190351\n",
      "train loss:0.5607309959669186\n",
      "train loss:0.47599799657095404\n",
      "train loss:0.41694762125782603\n",
      "train loss:0.5878476471371277\n",
      "train loss:0.8111966436660853\n",
      "train loss:0.46970860007673637\n",
      "train loss:0.7785534516097262\n",
      "train loss:0.5605942587367061\n",
      "train loss:0.5906938056429362\n",
      "train loss:0.5508627666231495\n",
      "train loss:0.3537223965437746\n",
      "train loss:0.4851846008217682\n",
      "train loss:0.4927316016345893\n",
      "train loss:0.7781197510440772\n",
      "train loss:0.580123974877591\n",
      "train loss:0.7988391384332163\n",
      "train loss:0.5004781220998608\n",
      "train loss:0.4463116956927561\n",
      "train loss:0.5853058249707881\n",
      "train loss:0.6817454136356582\n",
      "train loss:0.49950464155663676\n",
      "train loss:0.27237243348741197\n",
      "train loss:0.49175288720816956\n",
      "train loss:0.48882994084302356\n",
      "train loss:0.42949447300560395\n",
      "train loss:0.5577663805163546\n",
      "train loss:0.4111914632097071\n",
      "train loss:0.4699108429496587\n",
      "train loss:0.779624571411175\n",
      "train loss:0.4086444719693837\n",
      "train loss:0.4674608773876193\n",
      "train loss:0.6768450219257127\n",
      "train loss:0.4782794729400126\n",
      "train loss:0.6141434035732225\n",
      "train loss:0.5957222418704615\n",
      "train loss:0.6012322030569478\n",
      "train loss:0.49722341721703345\n",
      "train loss:0.585377255638925\n",
      "train loss:0.3737928082469728\n",
      "=== epoch:10, train acc:0.75, test acc:0.69 ===\n",
      "train loss:0.631034137476392\n",
      "train loss:0.6661708548636496\n",
      "train loss:0.4190886880932421\n",
      "train loss:0.4363816851975638\n",
      "train loss:0.6504617055017318\n",
      "train loss:0.5408669419354852\n",
      "train loss:0.48971493442804553\n",
      "train loss:0.2732788018274298\n",
      "train loss:0.3153197457520807\n",
      "train loss:0.8871207401425323\n",
      "train loss:0.6260813577553246\n",
      "train loss:0.27038914715696316\n",
      "train loss:0.7728963027291937\n",
      "train loss:0.6372271935999152\n",
      "train loss:0.5351038946968765\n",
      "train loss:0.7626459014648889\n",
      "train loss:0.6077138821746344\n",
      "train loss:0.32305516040369664\n",
      "train loss:0.7773683873414081\n",
      "train loss:0.45911770247410955\n",
      "train loss:0.47196704875720696\n",
      "train loss:0.6103740202425298\n",
      "train loss:0.7656389766353169\n",
      "train loss:0.47095516197566917\n",
      "train loss:0.5397772138118861\n",
      "train loss:0.7338845762815597\n",
      "train loss:0.30987867090285737\n",
      "train loss:0.38703639775300835\n",
      "train loss:0.5811505428974696\n",
      "train loss:0.6900249789626318\n",
      "train loss:0.46783739748599784\n",
      "train loss:0.5501483492866387\n",
      "train loss:0.39184352017469526\n",
      "train loss:0.5204727141828498\n",
      "train loss:0.32392745654610033\n",
      "train loss:0.25913471485870065\n",
      "train loss:0.6333072222502624\n",
      "train loss:0.38588046466114473\n",
      "train loss:0.4368605376298079\n",
      "train loss:0.4010005574875393\n",
      "train loss:0.49527407430030895\n",
      "train loss:0.515293456003445\n",
      "train loss:0.7152627246162395\n",
      "train loss:0.19033618616684606\n",
      "train loss:0.919694856393399\n",
      "train loss:0.5966036813920816\n",
      "train loss:0.4837361010997331\n",
      "train loss:0.3683754801597263\n",
      "train loss:0.704955720931894\n",
      "train loss:0.518991267909888\n",
      "train loss:0.5479233474405105\n",
      "train loss:0.616497434766148\n",
      "train loss:0.4953348259682177\n",
      "train loss:0.4922487253403565\n",
      "train loss:0.4450344522561542\n",
      "train loss:0.5119444257643719\n",
      "train loss:0.6990307488299499\n",
      "train loss:0.5799840428327423\n",
      "train loss:0.65771421641559\n",
      "train loss:0.45313571661846985\n",
      "train loss:0.5136276831665276\n",
      "train loss:0.5797229142947676\n",
      "train loss:0.5505048473175378\n",
      "train loss:0.4078854132936856\n",
      "train loss:0.4349271921033452\n",
      "train loss:0.4948096721908047\n",
      "train loss:0.7328146097376844\n",
      "train loss:0.39481923871789854\n",
      "train loss:0.3971679308538184\n",
      "train loss:0.49565120776726224\n",
      "train loss:0.8095009218331878\n",
      "train loss:0.5301474913235092\n",
      "train loss:0.6396378800470597\n",
      "train loss:0.5440791341763749\n",
      "train loss:0.47140354295893105\n",
      "train loss:0.48941559293142795\n",
      "train loss:0.5441833532479616\n",
      "train loss:0.6399689834201101\n",
      "train loss:0.48429224220512923\n",
      "train loss:0.8347647909083701\n",
      "train loss:0.5662049937756578\n",
      "train loss:0.40519085666727894\n",
      "train loss:0.5200485512645008\n",
      "train loss:0.4299202901781355\n",
      "train loss:0.5671686613383606\n",
      "train loss:0.5936445831316824\n",
      "train loss:0.913323205857105\n",
      "train loss:0.4483791294496566\n",
      "train loss:0.4573924910537304\n",
      "train loss:0.46406127730922764\n",
      "train loss:0.5136533347995428\n",
      "train loss:0.5322843375845804\n",
      "train loss:0.5939496727018561\n",
      "train loss:0.5966994445354384\n",
      "train loss:0.6749271953399834\n",
      "train loss:0.6173895888726773\n",
      "train loss:0.5137312706987273\n",
      "train loss:0.39455870750271854\n",
      "train loss:0.5486333489351851\n",
      "train loss:0.6877650297132318\n",
      "train loss:0.5294176483346289\n",
      "train loss:0.4921414034423954\n",
      "train loss:0.5013294007092883\n",
      "train loss:0.7346033476722417\n",
      "train loss:0.6621526917712772\n",
      "train loss:0.6377327923389247\n",
      "train loss:0.5491427161118244\n",
      "train loss:0.5297172420097107\n",
      "train loss:0.5249160039350454\n",
      "train loss:0.5106098074275767\n",
      "train loss:0.5656572605286264\n",
      "train loss:0.6338232574590796\n",
      "train loss:0.5359217571303672\n",
      "train loss:0.5730975854465108\n",
      "train loss:0.49659281780359965\n",
      "train loss:0.49130236350578105\n",
      "train loss:0.6465779339408917\n",
      "train loss:0.5971441271379101\n",
      "train loss:0.4051414873175728\n",
      "train loss:0.5400304852194177\n",
      "train loss:0.5653526618414323\n",
      "train loss:0.3247620365159527\n",
      "train loss:0.4021518958056737\n",
      "train loss:0.259619833148791\n",
      "train loss:0.706500081549774\n",
      "train loss:0.8422441325681996\n",
      "train loss:0.42966097474035114\n",
      "train loss:0.5681351564453119\n",
      "train loss:0.5835004519390372\n",
      "train loss:0.644004502055856\n",
      "train loss:0.6273602196448561\n",
      "train loss:0.5361136763571548\n",
      "train loss:0.7630491598200224\n",
      "train loss:0.35603847212323314\n",
      "train loss:0.6358136479723958\n",
      "train loss:0.4567331042626188\n",
      "train loss:0.543937199397576\n",
      "train loss:0.5149892226542409\n",
      "train loss:0.5731961565151058\n",
      "train loss:0.8373924824638946\n",
      "train loss:0.6367875073242312\n",
      "train loss:0.5580672483864004\n",
      "train loss:0.5518919174934063\n",
      "train loss:0.5244157264338802\n",
      "train loss:0.40564858533889403\n",
      "train loss:0.77354486816487\n",
      "train loss:0.5338416324403596\n",
      "train loss:0.5032475943061498\n",
      "train loss:0.42630267163043783\n",
      "train loss:0.7611199961433253\n",
      "train loss:0.45675377601443434\n",
      "train loss:0.3680996654030182\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5490196078431373\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40eba388-e580-43a9-b3e4-4dc9fbd64f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6930378438128482\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6918337634970844\n",
      "train loss:0.690765156017754\n",
      "train loss:0.6869969365131162\n",
      "train loss:0.6927076284710189\n",
      "train loss:0.6992316447729181\n",
      "train loss:0.6807970093359582\n",
      "train loss:0.6830573590247719\n",
      "train loss:0.6854768430300144\n",
      "train loss:0.6834496291446024\n",
      "train loss:0.6396679924498543\n",
      "train loss:0.6458097208705056\n",
      "train loss:0.5950228235092085\n",
      "train loss:0.6233892406494023\n",
      "train loss:0.576557222270312\n",
      "train loss:0.6415549405363413\n",
      "train loss:0.527669538557233\n",
      "train loss:0.9912410535656729\n",
      "train loss:0.5789541494877574\n",
      "train loss:0.6120192510065499\n",
      "train loss:0.5530885522050062\n",
      "train loss:0.7097391293698327\n",
      "train loss:0.5107824168023287\n",
      "train loss:0.45958110499664667\n",
      "train loss:0.5485884268129012\n",
      "train loss:0.7148671575031547\n",
      "train loss:0.8768498150540361\n",
      "train loss:0.5591737669494017\n",
      "train loss:0.5433385272623324\n",
      "train loss:0.6980352195315926\n",
      "train loss:0.6028730036358568\n",
      "train loss:0.67222077243356\n",
      "train loss:0.6013518227889035\n",
      "train loss:0.7226886561766166\n",
      "train loss:0.7391932144630206\n",
      "train loss:0.5763151991220701\n",
      "train loss:0.5572558811929187\n",
      "train loss:0.6837673118314689\n",
      "train loss:0.5817218655000306\n",
      "train loss:0.618241421172019\n",
      "train loss:0.6239419865610444\n",
      "train loss:0.6163138239952474\n",
      "train loss:0.5868065341129998\n",
      "train loss:0.4736423138620326\n",
      "train loss:0.700745359660397\n",
      "train loss:0.6136799916940698\n",
      "train loss:0.6214869628140779\n",
      "train loss:0.5980958901875404\n",
      "train loss:0.7995780311012646\n",
      "train loss:0.5146410006648912\n",
      "train loss:0.42043568516562446\n",
      "train loss:0.5950587830030971\n",
      "train loss:0.5324928671256317\n",
      "train loss:0.7286490647300785\n",
      "train loss:0.27318604041108285\n",
      "train loss:0.6529787336737453\n",
      "train loss:0.36326344477793326\n",
      "train loss:0.7190992104321563\n",
      "train loss:0.3661187137933597\n",
      "train loss:0.6763727573421896\n",
      "train loss:0.35075677440606495\n",
      "train loss:0.5019839736584506\n",
      "train loss:0.6466139391948135\n",
      "train loss:0.33613104100313396\n",
      "train loss:0.35110919031858273\n",
      "train loss:0.4722960594652263\n",
      "train loss:0.7889645974538443\n",
      "train loss:0.621404920595025\n",
      "train loss:0.6662289874426539\n",
      "train loss:0.5455345356817838\n",
      "train loss:0.7437857639319988\n",
      "train loss:0.6364068679015823\n",
      "train loss:0.7798312666530035\n",
      "train loss:0.46828734965443264\n",
      "train loss:0.6137591859026921\n",
      "train loss:0.6366696538904703\n",
      "train loss:0.5498056537224916\n",
      "train loss:0.6256382005158179\n",
      "train loss:0.7743585494170853\n",
      "train loss:0.46280781910038726\n",
      "train loss:0.7122619922063077\n",
      "train loss:0.67469143474956\n",
      "train loss:0.5925124082095952\n",
      "train loss:0.5250721445357686\n",
      "train loss:0.5737956264733016\n",
      "train loss:0.5668281058231683\n",
      "train loss:0.6292441541040608\n",
      "train loss:0.4081959816963761\n",
      "train loss:0.530029898841902\n",
      "train loss:0.759672360437303\n",
      "train loss:0.43639083687757846\n",
      "train loss:0.5171423992326225\n",
      "train loss:0.3604568365316515\n",
      "train loss:0.6518208049740059\n",
      "train loss:0.9100219344550895\n",
      "train loss:0.5023866936835325\n",
      "train loss:0.7763972448050722\n",
      "train loss:0.47965385869188226\n",
      "train loss:0.37358416356048374\n",
      "train loss:0.3750257619796426\n",
      "train loss:0.22660686655795464\n",
      "train loss:0.7774946265032566\n",
      "train loss:1.0580487108132506\n",
      "train loss:0.49879811807564856\n",
      "train loss:0.48251287022203754\n",
      "train loss:0.6341042360690665\n",
      "train loss:0.5029722151435354\n",
      "train loss:0.7229125005104693\n",
      "train loss:0.5230805009060239\n",
      "train loss:0.6971616503162587\n",
      "train loss:0.6909861503478155\n",
      "train loss:0.6096512962513374\n",
      "train loss:0.46519264374946373\n",
      "train loss:0.6747317967008444\n",
      "train loss:0.6805507442182155\n",
      "train loss:0.6135174432713402\n",
      "train loss:0.5529668294052743\n",
      "train loss:0.9024654880850733\n",
      "train loss:0.6209824656384553\n",
      "train loss:0.6619543068302269\n",
      "train loss:0.7066753849140569\n",
      "train loss:0.732683161702558\n",
      "train loss:0.6719954636068057\n",
      "train loss:0.6206452890166876\n",
      "train loss:0.7174106423601132\n",
      "train loss:0.6679748925642702\n",
      "train loss:0.6724975885003278\n",
      "train loss:0.6666063992355475\n",
      "train loss:0.6117428718923098\n",
      "train loss:0.5750986341198934\n",
      "train loss:0.6354750600346434\n",
      "train loss:0.6754279125497787\n",
      "train loss:0.6393648936807365\n",
      "train loss:0.667373274891222\n",
      "train loss:0.6394797884908666\n",
      "train loss:0.6284436957010998\n",
      "train loss:0.5892579691353237\n",
      "train loss:0.6776644776902752\n",
      "train loss:0.6590802141096915\n",
      "train loss:0.6156798468342524\n",
      "train loss:0.7371080140377465\n",
      "train loss:0.6823859379907948\n",
      "train loss:0.5080803173720319\n",
      "train loss:0.6801474570598685\n",
      "train loss:0.5591182401252108\n",
      "train loss:0.7403779519659132\n",
      "train loss:0.6092990334366132\n",
      "train loss:0.6834470563230608\n",
      "train loss:0.5564908437241681\n",
      "train loss:0.5405182438370832\n",
      "train loss:0.6180453330313653\n",
      "train loss:0.4229894191231951\n",
      "train loss:0.4318641574919736\n",
      "train loss:0.6454619603041876\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.3875313608282411\n",
      "train loss:0.40736228387478635\n",
      "train loss:0.3740753421015463\n",
      "train loss:0.6645159944195284\n",
      "train loss:0.5205412721805953\n",
      "train loss:0.7983920954643026\n",
      "train loss:0.7867532291286427\n",
      "train loss:0.7669816490689297\n",
      "train loss:0.49039479446106393\n",
      "train loss:0.37399915503790104\n",
      "train loss:0.5007017694397804\n",
      "train loss:0.6425391879524568\n",
      "train loss:0.5092103775606003\n",
      "train loss:0.49833393901420864\n",
      "train loss:0.5260282269100909\n",
      "train loss:0.8390773394329407\n",
      "train loss:0.51287420233309\n",
      "train loss:0.6820770890622299\n",
      "train loss:0.42459963832991576\n",
      "train loss:0.6821261494396692\n",
      "train loss:0.44790565293492673\n",
      "train loss:0.4356994151941769\n",
      "train loss:0.8522565715033196\n",
      "train loss:0.792547421570793\n",
      "train loss:0.5174638714358231\n",
      "train loss:0.6178425212314558\n",
      "train loss:0.5433215767389038\n",
      "train loss:0.6254967676002107\n",
      "train loss:0.7724811440903677\n",
      "train loss:0.6273174968798317\n",
      "train loss:0.4700154936902332\n",
      "train loss:0.5384769458033201\n",
      "train loss:0.4662002431331945\n",
      "train loss:0.685507853016546\n",
      "train loss:0.8399455675489345\n",
      "train loss:0.5084333469275626\n",
      "train loss:0.6117941762110326\n",
      "train loss:0.5268556565442767\n",
      "train loss:0.531302910168651\n",
      "train loss:0.5801279447725768\n",
      "train loss:0.6239209333728905\n",
      "train loss:0.6091557616985142\n",
      "train loss:0.5283585481805175\n",
      "train loss:0.632575182402794\n",
      "train loss:0.3148246011399458\n",
      "train loss:0.5184046499667836\n",
      "train loss:0.6478587249451604\n",
      "train loss:0.7462642776162296\n",
      "train loss:0.7217308648728162\n",
      "train loss:0.39910988151573495\n",
      "train loss:1.1320429043917184\n",
      "train loss:0.5185767189736422\n",
      "train loss:0.614958999152857\n",
      "train loss:0.8664418153956893\n",
      "train loss:0.6941062333734455\n",
      "train loss:0.6704187829453099\n",
      "train loss:0.5580733814602563\n",
      "train loss:0.674013413522608\n",
      "train loss:0.6250382314443704\n",
      "train loss:0.5834403061594877\n",
      "train loss:0.710258487746257\n",
      "train loss:0.6714105590988404\n",
      "train loss:0.5513233921626959\n",
      "train loss:0.5881632956191402\n",
      "train loss:0.5949003721805117\n",
      "train loss:0.6283778968447796\n",
      "train loss:0.6749209892184782\n",
      "train loss:0.5774531979985454\n",
      "train loss:0.71973108382641\n",
      "train loss:0.6704174463717197\n",
      "train loss:0.6021319226447903\n",
      "train loss:0.6874887132205831\n",
      "train loss:0.4594912394649727\n",
      "train loss:0.4949898744210329\n",
      "train loss:0.4098188247054675\n",
      "train loss:0.696142240628179\n",
      "train loss:0.5308617608244872\n",
      "train loss:0.6199648620073097\n",
      "train loss:0.7036277524703578\n",
      "train loss:0.7138636492205225\n",
      "train loss:0.4031585296832489\n",
      "train loss:0.7063293784243624\n",
      "train loss:0.6286707004992943\n",
      "train loss:0.5211088664858619\n",
      "train loss:0.3910046929278076\n",
      "train loss:0.2480604532607261\n",
      "train loss:0.8984324103136204\n",
      "train loss:0.7401041772389045\n",
      "train loss:0.7914111594678683\n",
      "train loss:0.7168444500386701\n",
      "train loss:0.7253587113423174\n",
      "train loss:0.6729295955040618\n",
      "train loss:0.6169646399905507\n",
      "train loss:0.7064250238540546\n",
      "train loss:0.6144472764892093\n",
      "train loss:0.6857832007177753\n",
      "train loss:0.437220520225972\n",
      "train loss:0.6762966133541672\n",
      "train loss:0.5078642278245866\n",
      "train loss:0.7282134668791176\n",
      "train loss:0.5087260713900089\n",
      "train loss:0.6730009516071294\n",
      "train loss:0.6686435897059182\n",
      "train loss:0.5007971889094576\n",
      "train loss:0.6088836477051048\n",
      "train loss:0.681225340353962\n",
      "train loss:0.7329504679323999\n",
      "train loss:0.7225685560074879\n",
      "train loss:0.6763724663450775\n",
      "train loss:0.5481414156135598\n",
      "train loss:0.6757372742507183\n",
      "train loss:0.7546687215666582\n",
      "train loss:0.6109010072172321\n",
      "train loss:0.557909066294634\n",
      "train loss:0.6761934830016717\n",
      "train loss:0.49183593010583343\n",
      "train loss:0.5674478954574045\n",
      "train loss:0.666138000935533\n",
      "train loss:0.5394217577422937\n",
      "train loss:0.5263868400474909\n",
      "train loss:0.674764532303565\n",
      "train loss:0.5312308019113788\n",
      "train loss:0.5068429684388388\n",
      "train loss:0.5870757130971497\n",
      "train loss:0.40148155796486673\n",
      "train loss:0.735664710791723\n",
      "train loss:0.5826825522415442\n",
      "train loss:0.6101768525925035\n",
      "train loss:0.5933291473685035\n",
      "train loss:0.6259139936411934\n",
      "train loss:0.5744397946931354\n",
      "train loss:0.7464476910193684\n",
      "train loss:0.6314377730651467\n",
      "train loss:0.6067384497672876\n",
      "train loss:0.48384077771839235\n",
      "train loss:0.5394930015956106\n",
      "train loss:0.7117313697943776\n",
      "train loss:0.5073411680400021\n",
      "train loss:0.6213875301439448\n",
      "train loss:0.5437377785006758\n",
      "train loss:0.788205135991886\n",
      "train loss:0.5320307652144958\n",
      "train loss:0.8255205347280812\n",
      "train loss:0.4638280573026637\n",
      "train loss:0.6711360937678374\n",
      "train loss:0.6869128132642259\n",
      "train loss:0.4643744126799724\n",
      "train loss:0.6807253462154612\n",
      "train loss:0.5444837103286659\n",
      "train loss:0.4764734531776641\n",
      "train loss:0.5196922607771799\n",
      "train loss:0.5305066524716284\n",
      "train loss:0.5202257989812118\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.697931315321395\n",
      "train loss:0.9363785823100097\n",
      "train loss:0.4314748126786374\n",
      "train loss:0.7779630982380875\n",
      "train loss:0.5178483203003516\n",
      "train loss:0.6758229447101541\n",
      "train loss:0.5260329364097156\n",
      "train loss:0.43328298666815124\n",
      "train loss:0.5156383071167887\n",
      "train loss:0.40517751774666094\n",
      "train loss:0.41574946605368623\n",
      "train loss:0.7371202655267363\n",
      "train loss:0.253633096718713\n",
      "train loss:0.6549492667094411\n",
      "train loss:0.5878581153321354\n",
      "train loss:0.640218796329006\n",
      "train loss:0.47129106655059694\n",
      "train loss:0.7822335336674062\n",
      "train loss:0.7049407785975463\n",
      "train loss:0.7680490375237696\n",
      "train loss:0.5134388175659785\n",
      "train loss:0.604597757095534\n",
      "train loss:0.5812462831155945\n",
      "train loss:0.6003319481527237\n",
      "train loss:0.7684840445552951\n",
      "train loss:0.4678878192846823\n",
      "train loss:0.5872893029168319\n",
      "train loss:0.7549463411576817\n",
      "train loss:0.6270680056437526\n",
      "train loss:0.4838729234069709\n",
      "train loss:0.6357793214401164\n",
      "train loss:0.6090008400096093\n",
      "train loss:0.5398024265881504\n",
      "train loss:0.47683483979730956\n",
      "train loss:0.5490065911576526\n",
      "train loss:0.547572611449882\n",
      "train loss:0.5268478959860892\n",
      "train loss:0.6066453969447154\n",
      "train loss:0.5121037869738645\n",
      "train loss:0.3129565010780344\n",
      "train loss:0.6228235914487952\n",
      "train loss:0.5995940495910415\n",
      "train loss:0.35033963679721436\n",
      "train loss:0.8636952360439836\n",
      "train loss:0.7270100343276489\n",
      "train loss:0.775341521375571\n",
      "train loss:0.7434865352642738\n",
      "train loss:0.5074687909081617\n",
      "train loss:0.5066940062760336\n",
      "train loss:0.6230105633472223\n",
      "train loss:0.7007851403568995\n",
      "train loss:0.8240513430172701\n",
      "train loss:0.4391813258653011\n",
      "train loss:0.44949661774614047\n",
      "train loss:0.3503225397615236\n",
      "train loss:0.6073646691256669\n",
      "train loss:0.5959579737818708\n",
      "train loss:0.4203441255678329\n",
      "train loss:0.7181405156165607\n",
      "train loss:0.5024851133561001\n",
      "train loss:0.4974705327447051\n",
      "train loss:0.8151829422009322\n",
      "train loss:0.6308099211275053\n",
      "train loss:0.6373971607026705\n",
      "train loss:0.6052404439092653\n",
      "train loss:0.6100679810732392\n",
      "train loss:0.6889686487900096\n",
      "train loss:0.44972051291015447\n",
      "train loss:0.5314697105756426\n",
      "train loss:0.5620358435293858\n",
      "train loss:0.6924327535400991\n",
      "train loss:0.6896042559746703\n",
      "train loss:0.61109421889399\n",
      "train loss:0.5358124239924861\n",
      "train loss:0.7538491897247742\n",
      "train loss:0.5238753650043375\n",
      "train loss:0.6991539537800808\n",
      "train loss:0.901398873469111\n",
      "train loss:0.6805456363867759\n",
      "train loss:0.5574117623208921\n",
      "train loss:0.5948757465182019\n",
      "train loss:0.5521117753325703\n",
      "train loss:0.6757678295944104\n",
      "train loss:0.6342124707812768\n",
      "train loss:0.4984847266528057\n",
      "train loss:0.7005133121094772\n",
      "train loss:0.4809800897840625\n",
      "train loss:0.557089718865717\n",
      "train loss:0.5462688528788794\n",
      "train loss:0.5304771457619649\n",
      "train loss:0.5335873469716405\n",
      "train loss:0.6946559233586632\n",
      "train loss:0.6059079372717995\n",
      "train loss:0.5307821607508234\n",
      "train loss:0.602508199181481\n",
      "train loss:0.41509566651261914\n",
      "train loss:0.9274751949647607\n",
      "train loss:0.5185637970172278\n",
      "train loss:0.7155649144871628\n",
      "train loss:0.6159448477696545\n",
      "train loss:0.5049012144611242\n",
      "train loss:0.5820603381758104\n",
      "train loss:0.7230681699260932\n",
      "train loss:0.6109923133316199\n",
      "train loss:0.5249878344480594\n",
      "train loss:0.40837189147967673\n",
      "train loss:0.8949105256749231\n",
      "train loss:0.515398132256091\n",
      "train loss:0.515938400847145\n",
      "train loss:0.7843100199252389\n",
      "train loss:0.5115961855131852\n",
      "train loss:0.694961791395141\n",
      "train loss:0.4311796729031281\n",
      "train loss:0.6112634371171255\n",
      "train loss:0.36057893458481594\n",
      "train loss:0.6226619306686976\n",
      "train loss:0.8106191734283359\n",
      "train loss:0.5049423337455193\n",
      "train loss:0.39969460573175175\n",
      "train loss:0.7204633344132457\n",
      "train loss:0.5950870748427327\n",
      "train loss:0.2996659401346623\n",
      "train loss:0.7144067893944731\n",
      "train loss:0.5134875513665106\n",
      "train loss:0.5283474073376653\n",
      "train loss:0.7085867452220235\n",
      "train loss:0.745941397600427\n",
      "train loss:0.7123944166796675\n",
      "train loss:0.5990723113968246\n",
      "train loss:0.3008935551071741\n",
      "train loss:0.5129264602529349\n",
      "train loss:0.6108062151269011\n",
      "train loss:0.5172932140026045\n",
      "train loss:0.5107812232225977\n",
      "train loss:0.8892375979445595\n",
      "train loss:0.7938405802596495\n",
      "train loss:0.504186116567934\n",
      "train loss:0.44057749920982314\n",
      "train loss:0.8841134821878651\n",
      "train loss:0.7614434860710233\n",
      "train loss:0.6026762401723135\n",
      "train loss:0.6184662387205317\n",
      "train loss:0.5485965628453164\n",
      "train loss:0.4839962137559418\n",
      "train loss:0.554074577132304\n",
      "train loss:0.6243042041394115\n",
      "train loss:0.7450478598179239\n",
      "train loss:0.5370686992612647\n",
      "train loss:0.5458693090250545\n",
      "train loss:0.8605535072618598\n",
      "train loss:0.6787702086669037\n",
      "train loss:0.48093956968650176\n",
      "train loss:0.5397451600917864\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6569115754578367\n",
      "train loss:0.671721062120283\n",
      "train loss:0.6774981746990073\n",
      "train loss:0.4816402822601294\n",
      "train loss:0.5882343513870042\n",
      "train loss:0.541840497049323\n",
      "train loss:0.449114307478252\n",
      "train loss:0.5076023907619852\n",
      "train loss:0.502065426501429\n",
      "train loss:0.6928557830620345\n",
      "train loss:0.590057734668091\n",
      "train loss:0.5461392745973407\n",
      "train loss:0.4036369534471804\n",
      "train loss:0.6233879720706237\n",
      "train loss:0.7299472247970031\n",
      "train loss:0.620919964860593\n",
      "train loss:0.672595370281108\n",
      "train loss:0.3885846559751131\n",
      "train loss:0.49520168990606966\n",
      "train loss:0.4012425072535466\n",
      "train loss:0.8510147563969763\n",
      "train loss:0.7340197981611821\n",
      "train loss:0.5358290652642144\n",
      "train loss:0.4934060282711908\n",
      "train loss:0.41242101260973946\n",
      "train loss:0.7486468255841808\n",
      "train loss:0.7969644165216363\n",
      "train loss:0.7045598773937052\n",
      "train loss:0.6232633613384376\n",
      "train loss:0.6036012877038348\n",
      "train loss:0.5485338667840579\n",
      "train loss:0.7987508737269168\n",
      "train loss:0.42456638868665253\n",
      "train loss:0.5627478752209853\n",
      "train loss:0.6136539764830874\n",
      "train loss:0.607022820133305\n",
      "train loss:0.6715217104411884\n",
      "train loss:0.5644338736529999\n",
      "train loss:0.5391555352966007\n",
      "train loss:0.6623707749406843\n",
      "train loss:0.541837811196473\n",
      "train loss:0.6648878419053422\n",
      "train loss:0.5321024094861146\n",
      "train loss:0.6748459772910814\n",
      "train loss:0.5323370122287802\n",
      "train loss:0.6676990198723722\n",
      "train loss:0.4146640619972014\n",
      "train loss:0.6013852252596581\n",
      "train loss:0.39001814824102254\n",
      "train loss:0.5852935119649257\n",
      "train loss:0.36015268788592697\n",
      "train loss:0.39088817385518393\n",
      "train loss:0.794515743307683\n",
      "train loss:0.3236714295727292\n",
      "train loss:0.35422280008358215\n",
      "train loss:0.7909648488839076\n",
      "train loss:0.7550460046755885\n",
      "train loss:0.6590042639226612\n",
      "train loss:0.21937102715619985\n",
      "train loss:0.634457713701757\n",
      "train loss:0.8908334588788284\n",
      "train loss:0.3692318289126228\n",
      "train loss:0.5303133084299969\n",
      "train loss:0.7130431873545002\n",
      "train loss:0.6230818169194917\n",
      "train loss:0.5121922256070724\n",
      "train loss:0.6928426692565448\n",
      "train loss:0.5222398040501159\n",
      "train loss:0.5271299186262899\n",
      "train loss:0.44941665482087634\n",
      "train loss:0.4469768138646737\n",
      "train loss:0.5823398422186791\n",
      "train loss:0.528272741844504\n",
      "train loss:0.689298676921683\n",
      "train loss:0.5310427818313694\n",
      "train loss:0.7160898138526561\n",
      "train loss:0.6093062352047804\n",
      "train loss:0.4382472088830858\n",
      "train loss:0.7155123295550712\n",
      "train loss:0.5975048879219107\n",
      "train loss:0.5157077071453762\n",
      "train loss:0.4277530730407853\n",
      "train loss:0.5242158225584268\n",
      "train loss:0.617282205348011\n",
      "train loss:0.6308076925714016\n",
      "train loss:0.8235397377286373\n",
      "train loss:0.5828622563043842\n",
      "train loss:0.80273582680831\n",
      "train loss:0.42600720730353\n",
      "train loss:0.6045358672398957\n",
      "train loss:0.680976904446877\n",
      "train loss:0.6115631967888098\n",
      "train loss:0.8361554047180709\n",
      "train loss:0.4763937908494924\n",
      "train loss:0.7031289940451764\n",
      "train loss:0.47615621942044184\n",
      "train loss:0.4699166972144253\n",
      "train loss:0.81479364683178\n",
      "train loss:0.733975092651865\n",
      "train loss:0.4855707563354808\n",
      "train loss:0.4021248279292979\n",
      "train loss:0.684487554783383\n",
      "train loss:0.5995562295712745\n",
      "train loss:0.8421992162429348\n",
      "train loss:0.5868950974889836\n",
      "train loss:0.5504724833603427\n",
      "train loss:0.7697001523157964\n",
      "train loss:0.6073091038731142\n",
      "train loss:0.6234194952157374\n",
      "train loss:0.40751010868190274\n",
      "train loss:0.6711947603920336\n",
      "train loss:0.3999311953181797\n",
      "train loss:0.5611772467054068\n",
      "train loss:0.5354707750261044\n",
      "train loss:0.8437938274316256\n",
      "train loss:0.5916248246062332\n",
      "train loss:0.8022836420278772\n",
      "train loss:0.4095573578405217\n",
      "train loss:0.5804796335191835\n",
      "train loss:0.5889588462905717\n",
      "train loss:0.5973808621467626\n",
      "train loss:0.4256173104721245\n",
      "train loss:0.6224835290252347\n",
      "train loss:0.4116494202014203\n",
      "train loss:0.6143914904532851\n",
      "train loss:0.35416444643500233\n",
      "train loss:0.46628296076959697\n",
      "train loss:0.9006065760278545\n",
      "train loss:0.372730758596046\n",
      "train loss:0.7560694678107583\n",
      "train loss:0.7642442928788313\n",
      "train loss:0.6244818854086804\n",
      "train loss:0.4921473880780202\n",
      "train loss:0.5613409761004694\n",
      "train loss:0.5058073274737863\n",
      "train loss:0.6157459581784414\n",
      "train loss:0.5146301078179085\n",
      "train loss:0.7062554045783632\n",
      "train loss:0.5877927048659262\n",
      "train loss:0.7914866015739296\n",
      "train loss:0.36779524972966765\n",
      "train loss:0.6938430068549966\n",
      "train loss:0.7441534458996653\n",
      "train loss:0.6790759123157867\n",
      "train loss:0.4720507791699656\n",
      "train loss:0.463439157775347\n",
      "train loss:0.620317097928502\n",
      "train loss:0.6979689065908464\n",
      "train loss:0.5375000571376197\n",
      "train loss:0.6868302877570207\n",
      "train loss:0.69426365179397\n",
      "train loss:0.4882885567317613\n",
      "train loss:0.4677640664789885\n",
      "=== epoch:5, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7222416705987971\n",
      "train loss:0.6100031821070628\n",
      "train loss:0.5413742932806075\n",
      "train loss:0.7403192154446987\n",
      "train loss:0.4592002118265858\n",
      "train loss:0.4797806831632143\n",
      "train loss:0.5293739339594656\n",
      "train loss:0.5004232867746995\n",
      "train loss:0.6315250688561072\n",
      "train loss:0.6063132623888331\n",
      "train loss:0.6177937382900482\n",
      "train loss:0.5995507849581512\n",
      "train loss:0.5878777325462602\n",
      "train loss:0.6038637939767251\n",
      "train loss:0.3836907398031329\n",
      "train loss:0.6961803332603922\n",
      "train loss:0.7536579982489697\n",
      "train loss:0.6813399609602613\n",
      "train loss:0.57321188548575\n",
      "train loss:0.6068987167430231\n",
      "train loss:0.8397216908925087\n",
      "train loss:0.7765318561394196\n",
      "train loss:0.5409757789147532\n",
      "train loss:0.5349494501196661\n",
      "train loss:0.6552734433029814\n",
      "train loss:0.5122423031271554\n",
      "train loss:0.7302657748612166\n",
      "train loss:0.6969460340506537\n",
      "train loss:0.6412886043784466\n",
      "train loss:0.7165918185422748\n",
      "train loss:0.596456674562769\n",
      "train loss:0.5868465591684842\n",
      "train loss:0.5763480153488808\n",
      "train loss:0.6251050816962903\n",
      "train loss:0.624374465805945\n",
      "train loss:0.5871512688755246\n",
      "train loss:0.596961652656558\n",
      "train loss:0.7567307360591319\n",
      "train loss:0.49847059872311456\n",
      "train loss:0.5653599243646283\n",
      "train loss:0.6904622448548222\n",
      "train loss:0.655418376671673\n",
      "train loss:0.5751493551733777\n",
      "train loss:0.6921991648308566\n",
      "train loss:0.619079553853014\n",
      "train loss:0.6044497439361105\n",
      "train loss:0.737392825863107\n",
      "train loss:0.5315212701695335\n",
      "train loss:0.8449186126388046\n",
      "train loss:0.5406431136034529\n",
      "train loss:0.685293329206387\n",
      "train loss:0.5256483780452299\n",
      "train loss:0.4341455989422357\n",
      "train loss:0.5808347111048572\n",
      "train loss:0.6940658987372863\n",
      "train loss:0.5127561089872126\n",
      "train loss:0.6113432496881495\n",
      "train loss:0.6412540932202957\n",
      "train loss:0.6739102489264905\n",
      "train loss:0.7692165587434571\n",
      "train loss:0.749652798544205\n",
      "train loss:0.5258420040044115\n",
      "train loss:0.5340868539262182\n",
      "train loss:0.6629004503970852\n",
      "train loss:0.6930624973655093\n",
      "train loss:0.532730230302981\n",
      "train loss:0.7592994574547289\n",
      "train loss:0.6679421819651282\n",
      "train loss:0.5562643725856996\n",
      "train loss:0.7355786454858992\n",
      "train loss:0.5382956161606487\n",
      "train loss:0.4721118929670019\n",
      "train loss:0.5926832597525329\n",
      "train loss:0.6555192309982919\n",
      "train loss:0.5515412662088955\n",
      "train loss:0.6350932008386352\n",
      "train loss:0.6939290906707504\n",
      "train loss:0.5630075406709837\n",
      "train loss:0.9410861699506128\n",
      "train loss:0.6692615643849822\n",
      "train loss:0.5253600552758353\n",
      "train loss:0.6844471399866879\n",
      "train loss:0.690084867547391\n",
      "train loss:0.5346326585364901\n",
      "train loss:0.6001633898462122\n",
      "train loss:0.5499347902099013\n",
      "train loss:0.6226648163954982\n",
      "train loss:0.5464911680658354\n",
      "train loss:0.7856544493301906\n",
      "train loss:0.6804503056266891\n",
      "train loss:0.6766699862889436\n",
      "train loss:0.7056921839885566\n",
      "train loss:0.5649126409734313\n",
      "train loss:0.6352084912155311\n",
      "train loss:0.7219584227911927\n",
      "train loss:0.5019261471880258\n",
      "train loss:0.6192762515132739\n",
      "train loss:0.6444314351112939\n",
      "train loss:0.4899963857370696\n",
      "train loss:0.748248360257888\n",
      "train loss:0.6055846186130778\n",
      "train loss:0.5483173828437508\n",
      "train loss:0.5198288805228256\n",
      "train loss:0.6862724738096121\n",
      "train loss:0.5389856344632677\n",
      "train loss:0.6543548516436668\n",
      "train loss:0.847914690177255\n",
      "train loss:0.6735521137447693\n",
      "train loss:0.7395262643950327\n",
      "train loss:0.6212678151763119\n",
      "train loss:0.41815656263855006\n",
      "train loss:0.6117552296103408\n",
      "train loss:0.4854946071146326\n",
      "train loss:0.4941374378481977\n",
      "train loss:0.6676560508994533\n",
      "train loss:0.7902903014861189\n",
      "train loss:0.6817114151346311\n",
      "train loss:0.688092217222615\n",
      "train loss:0.6081379831499625\n",
      "train loss:0.6098828174608907\n",
      "train loss:0.7443771692267604\n",
      "train loss:0.4674846534533182\n",
      "train loss:0.619182137069472\n",
      "train loss:0.6933914285084076\n",
      "train loss:0.6527245872160549\n",
      "train loss:0.6944648914340468\n",
      "train loss:0.6283078730616574\n",
      "train loss:0.5462569002726815\n",
      "train loss:0.7673687723232602\n",
      "train loss:0.5181540756372649\n",
      "train loss:0.6004829603192747\n",
      "train loss:0.5578072247392564\n",
      "train loss:0.4446781980204362\n",
      "train loss:0.6828444651533644\n",
      "train loss:0.4382775866313617\n",
      "train loss:0.5395296851059588\n",
      "train loss:0.5007406151991675\n",
      "train loss:0.6894007012057344\n",
      "train loss:0.4053052667602561\n",
      "train loss:0.6275656021608064\n",
      "train loss:0.8535663620412626\n",
      "train loss:0.9146685690077083\n",
      "train loss:0.6033567064085165\n",
      "train loss:0.4973861389591496\n",
      "train loss:0.6651456622518847\n",
      "train loss:0.7034420469536297\n",
      "train loss:0.45617057341388795\n",
      "train loss:0.5933956252180423\n",
      "train loss:0.6265079077150864\n",
      "train loss:0.6849924806167703\n",
      "train loss:0.814737821281674\n",
      "train loss:0.625476581075575\n",
      "train loss:0.694063350056473\n",
      "=== epoch:6, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5857419711018405\n",
      "train loss:0.5729275585425604\n",
      "train loss:0.6636873715997149\n",
      "train loss:0.645047537153605\n",
      "train loss:0.505063739837994\n",
      "train loss:0.5653242772750642\n",
      "train loss:0.5820330270647478\n",
      "train loss:0.56006064808305\n",
      "train loss:0.683901709403741\n",
      "train loss:0.666325629376953\n",
      "train loss:0.6140992460767152\n",
      "train loss:0.5458562848111759\n",
      "train loss:0.743045526816833\n",
      "train loss:0.5011413164076038\n",
      "train loss:0.7357384198612045\n",
      "train loss:0.5181353127669239\n",
      "train loss:0.5953138036003295\n",
      "train loss:0.4961236180565039\n",
      "train loss:0.7049800250049895\n",
      "train loss:0.5936534495178714\n",
      "train loss:0.554229590846555\n",
      "train loss:0.5820186827782543\n",
      "train loss:0.7035119830293368\n",
      "train loss:0.2997427926683346\n",
      "train loss:0.49716331778290435\n",
      "train loss:0.4916323977669067\n",
      "train loss:0.521528597029371\n",
      "train loss:0.8431288221759319\n",
      "train loss:0.51365615600559\n",
      "train loss:0.8492491198715507\n",
      "train loss:0.5140287769271776\n",
      "train loss:0.40732125317690804\n",
      "train loss:0.5741501951456964\n",
      "train loss:0.7088884856864748\n",
      "train loss:0.48333018469891886\n",
      "train loss:0.7315470930846042\n",
      "train loss:0.7232515989789968\n",
      "train loss:0.5952042831019984\n",
      "train loss:0.6785109189148775\n",
      "train loss:0.6410070136937326\n",
      "train loss:0.5941764482863415\n",
      "train loss:0.4830684846148106\n",
      "train loss:0.6552206213828167\n",
      "train loss:0.48122701749359054\n",
      "train loss:0.6420839208894035\n",
      "train loss:0.49789886521738136\n",
      "train loss:0.7182653926218097\n",
      "train loss:0.39982447425166223\n",
      "train loss:0.5220271000875117\n",
      "train loss:0.5508979403961342\n",
      "train loss:0.4281439782153852\n",
      "train loss:0.3405673930417665\n",
      "train loss:0.7903676058809022\n",
      "train loss:0.6326701729427799\n",
      "train loss:0.2390859015861448\n",
      "train loss:0.49786551436798704\n",
      "train loss:0.853680704247598\n",
      "train loss:0.6428362141337318\n",
      "train loss:0.4547551586142145\n",
      "train loss:0.586009675824464\n",
      "train loss:0.7978725236579458\n",
      "train loss:0.38405982872135047\n",
      "train loss:0.5175163846809427\n",
      "train loss:0.6718461382325976\n",
      "train loss:0.6435958525633428\n",
      "train loss:0.46440734162493563\n",
      "train loss:0.48543423007119013\n",
      "train loss:0.9815953233907587\n",
      "train loss:0.6888639338478276\n",
      "train loss:0.37432431267757404\n",
      "train loss:0.5568418994960461\n",
      "train loss:0.8832105563265358\n",
      "train loss:0.48948073999698466\n",
      "train loss:0.5507498698062515\n",
      "train loss:0.5426337825266955\n",
      "train loss:0.8670900672357144\n",
      "train loss:0.49812978600784436\n",
      "train loss:0.6340721497037413\n",
      "train loss:0.5901126967340959\n",
      "train loss:0.601819785025455\n",
      "train loss:0.7311186544401884\n",
      "train loss:0.669268279532403\n",
      "train loss:0.6395090819330506\n",
      "train loss:0.5434654853375238\n",
      "train loss:0.6709717273278804\n",
      "train loss:0.6570130502780677\n",
      "train loss:0.7230117540517526\n",
      "train loss:0.6499388773417879\n",
      "train loss:0.686676630464255\n",
      "train loss:0.49339039358129355\n",
      "train loss:0.6139687672415479\n",
      "train loss:0.671445133759687\n",
      "train loss:0.5044683940682488\n",
      "train loss:0.5764611628988546\n",
      "train loss:0.5374968054251676\n",
      "train loss:0.7946875544738734\n",
      "train loss:0.46808177652221267\n",
      "train loss:0.5345295680526403\n",
      "train loss:0.6208323856668712\n",
      "train loss:0.3225416297958353\n",
      "train loss:0.5039547913315363\n",
      "train loss:0.8874182981719076\n",
      "train loss:0.5601829852271002\n",
      "train loss:0.7380544140931912\n",
      "train loss:0.7214741327232848\n",
      "train loss:0.43866401809918776\n",
      "train loss:0.4441128250247865\n",
      "train loss:0.39379544234350944\n",
      "train loss:0.5929545831838265\n",
      "train loss:0.49626577961422813\n",
      "train loss:0.5443027327977928\n",
      "train loss:0.5147320290776103\n",
      "train loss:0.372167247384319\n",
      "train loss:0.7251661661632481\n",
      "train loss:0.7157155810124375\n",
      "train loss:0.3988017211328684\n",
      "train loss:0.795682625536901\n",
      "train loss:0.4950396710473023\n",
      "train loss:0.3856696594545715\n",
      "train loss:0.7748587156864326\n",
      "train loss:0.615179675402702\n",
      "train loss:0.5087381684191139\n",
      "train loss:0.7624134073340371\n",
      "train loss:0.5941009385584153\n",
      "train loss:0.3269748067753452\n",
      "train loss:0.5688741736631197\n",
      "train loss:0.6832169887468325\n",
      "train loss:0.5837890344639816\n",
      "train loss:0.714179756893146\n",
      "train loss:0.5820538897252159\n",
      "train loss:0.5520015807088812\n",
      "train loss:0.43531053693053023\n",
      "train loss:0.7218206129638618\n",
      "train loss:0.5522065223307336\n",
      "train loss:0.5971717258067565\n",
      "train loss:0.4353080247844138\n",
      "train loss:0.5875233306344259\n",
      "train loss:0.5770293197586625\n",
      "train loss:0.5495520391249995\n",
      "train loss:0.6368814988768978\n",
      "train loss:0.4083258929722782\n",
      "train loss:0.6658976354145569\n",
      "train loss:0.4647381675367095\n",
      "train loss:0.8574159337948547\n",
      "train loss:0.5065803882796728\n",
      "train loss:0.7434718774534586\n",
      "train loss:0.723364916178039\n",
      "train loss:0.7214617624394124\n",
      "train loss:0.6867561412357223\n",
      "train loss:0.5837095103309367\n",
      "train loss:0.45936037928312884\n",
      "train loss:0.6146809898695798\n",
      "train loss:0.6917232056889424\n",
      "=== epoch:7, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6495233979594378\n",
      "train loss:0.7483152254967411\n",
      "train loss:0.4038812251057392\n",
      "train loss:0.601695592696994\n",
      "train loss:0.40855770106222955\n",
      "train loss:0.5929590720703904\n",
      "train loss:0.7112232321493541\n",
      "train loss:0.494301105923721\n",
      "train loss:0.588163654197267\n",
      "train loss:0.5426807557408133\n",
      "train loss:0.5381326357735275\n",
      "train loss:0.5041765637573623\n",
      "train loss:0.44134326183170786\n",
      "train loss:0.33188715180457196\n",
      "train loss:0.6128449496383153\n",
      "train loss:0.7855242805982412\n",
      "train loss:0.5620934205111959\n",
      "train loss:0.3848523449138039\n",
      "train loss:0.47132186589885283\n",
      "train loss:0.609444795266456\n",
      "train loss:0.5165223897207911\n",
      "train loss:0.7573870734349443\n",
      "train loss:0.3269196740799568\n",
      "train loss:0.38389755646762735\n",
      "train loss:0.36380497897854575\n",
      "train loss:0.721883742743328\n",
      "train loss:0.7857007349004099\n",
      "train loss:0.4489360794412803\n",
      "train loss:0.794905090345251\n",
      "train loss:0.7055876429717183\n",
      "train loss:0.40313640720940613\n",
      "train loss:0.5668513415712634\n",
      "train loss:0.4971470982560618\n",
      "train loss:0.4921989459530109\n",
      "train loss:0.748523303194794\n",
      "train loss:0.44513435256543693\n",
      "train loss:0.6376298598897497\n",
      "train loss:0.5404350206612791\n",
      "train loss:0.5979575798240775\n",
      "train loss:0.5124280057356406\n",
      "train loss:0.5058158690592421\n",
      "train loss:0.5816710030610313\n",
      "train loss:0.6223460591792539\n",
      "train loss:0.6143595809290502\n",
      "train loss:0.7173682889533575\n",
      "train loss:0.4441726627178606\n",
      "train loss:0.5498650584181776\n",
      "train loss:0.5727073672041488\n",
      "train loss:0.49194264922227326\n",
      "train loss:0.861004883559757\n",
      "train loss:0.3903336159654068\n",
      "train loss:0.3932604733270506\n",
      "train loss:0.514640188617994\n",
      "train loss:0.3078800524475006\n",
      "train loss:0.7590241307458508\n",
      "train loss:0.6574953164380887\n",
      "train loss:0.8296001445583446\n",
      "train loss:0.7064352215441093\n",
      "train loss:0.7614891243130376\n",
      "train loss:0.57686922734919\n",
      "train loss:0.510016068937505\n",
      "train loss:0.38719838135983103\n",
      "train loss:0.5783078749995784\n",
      "train loss:0.7985189554836613\n",
      "train loss:0.6153663108099859\n",
      "train loss:0.41616699058237405\n",
      "train loss:0.4759383334523727\n",
      "train loss:0.4532072609278119\n",
      "train loss:1.0302649538106217\n",
      "train loss:0.4553580954594981\n",
      "train loss:0.5027021506833951\n",
      "train loss:0.5932930747230807\n",
      "train loss:0.3972517109369763\n",
      "train loss:0.8024799715579525\n",
      "train loss:0.6074407575480075\n",
      "train loss:0.6987681411498688\n",
      "train loss:0.5777622926909116\n",
      "train loss:0.4852859324751185\n",
      "train loss:0.8161496012509846\n",
      "train loss:0.5215150209006925\n",
      "train loss:0.6016616199882046\n",
      "train loss:0.6377270286830188\n",
      "train loss:0.6341565219469328\n",
      "train loss:0.5826262042186696\n",
      "train loss:0.5132527296066158\n",
      "train loss:0.3650482043285368\n",
      "train loss:0.5967704300296831\n",
      "train loss:0.5749959718798235\n",
      "train loss:0.5429828462600417\n",
      "train loss:0.5804531973887357\n",
      "train loss:0.44954128181361624\n",
      "train loss:0.5260702732947191\n",
      "train loss:0.6854444783696556\n",
      "train loss:0.6567318253793029\n",
      "train loss:0.6281593913107575\n",
      "train loss:0.49329352487679373\n",
      "train loss:0.37057208216390436\n",
      "train loss:0.7843061679491622\n",
      "train loss:0.5965553946929828\n",
      "train loss:0.7048335782164984\n",
      "train loss:0.443252178610485\n",
      "train loss:0.6217269318676812\n",
      "train loss:0.6850818198187437\n",
      "train loss:0.44302744232254954\n",
      "train loss:0.7380167260634272\n",
      "train loss:0.49017787173973576\n",
      "train loss:0.6748286990755765\n",
      "train loss:0.5808326934178564\n",
      "train loss:0.6682036507921338\n",
      "train loss:0.40537883371216654\n",
      "train loss:0.6672875584466637\n",
      "train loss:0.73497962704362\n",
      "train loss:0.5110983964075911\n",
      "train loss:0.4440004106840589\n",
      "train loss:0.5241604845634736\n",
      "train loss:0.7068490443383811\n",
      "train loss:0.7380594256710176\n",
      "train loss:0.4690931556275255\n",
      "train loss:0.3856980163045028\n",
      "train loss:0.6344694410745092\n",
      "train loss:0.6274293819010799\n",
      "train loss:0.3948624549638647\n",
      "train loss:0.506638725798609\n",
      "train loss:0.7760769471612268\n",
      "train loss:0.625085094391608\n",
      "train loss:0.5014734010562145\n",
      "train loss:0.39068071711039726\n",
      "train loss:0.6248134277638012\n",
      "train loss:0.6144952517685471\n",
      "train loss:0.44079059060125336\n",
      "train loss:0.8012717955228519\n",
      "train loss:0.6487713538077535\n",
      "train loss:0.4983528832639445\n",
      "train loss:0.6105030158590494\n",
      "train loss:0.5535255306699338\n",
      "train loss:0.5540127414859115\n",
      "train loss:0.44358715892411527\n",
      "train loss:0.46272187270377074\n",
      "train loss:0.41572580964267825\n",
      "train loss:0.4951361903057937\n",
      "train loss:0.5395676273406573\n",
      "train loss:0.5963058624858656\n",
      "train loss:0.585270698286755\n",
      "train loss:0.48701311015447646\n",
      "train loss:0.5627640527943016\n",
      "train loss:0.44942104062379623\n",
      "train loss:0.7996612404491749\n",
      "train loss:0.8419114484289028\n",
      "train loss:0.5327542834623988\n",
      "train loss:0.6220281666936447\n",
      "train loss:0.3910787195586917\n",
      "train loss:0.5787083369187689\n",
      "train loss:0.46267039177503066\n",
      "=== epoch:8, train acc:0.73, test acc:0.69 ===\n",
      "train loss:0.26544570614111607\n",
      "train loss:0.8782164100895766\n",
      "train loss:0.5921954351680778\n",
      "train loss:0.8610452447245533\n",
      "train loss:0.9082277190918477\n",
      "train loss:0.5922050192319275\n",
      "train loss:0.5782444502379688\n",
      "train loss:0.503414090130088\n",
      "train loss:0.5555447503011938\n",
      "train loss:0.5832259101625836\n",
      "train loss:0.6488175762958137\n",
      "train loss:0.5109342315065536\n",
      "train loss:0.6624150286543303\n",
      "train loss:0.4722053632737275\n",
      "train loss:0.5471266949865472\n",
      "train loss:0.4763413903018498\n",
      "train loss:0.6241264696471058\n",
      "train loss:0.5823567895476841\n",
      "train loss:0.592603550559198\n",
      "train loss:0.5151654576932586\n",
      "train loss:0.6069314027015009\n",
      "train loss:0.30460681154458513\n",
      "train loss:0.605906301463691\n",
      "train loss:0.7014390107753068\n",
      "train loss:0.6099114667962808\n",
      "train loss:0.6068692790644435\n",
      "train loss:0.6777690406415624\n",
      "train loss:0.5463063016683865\n",
      "train loss:0.5075083336290842\n",
      "train loss:0.4890409870649909\n",
      "train loss:0.3592624911050291\n",
      "train loss:0.5047418367391444\n",
      "train loss:1.039395203867\n",
      "train loss:0.3068491265308891\n",
      "train loss:0.5197989818439149\n",
      "train loss:0.6586460097682327\n",
      "train loss:0.4092081183920393\n",
      "train loss:0.41880333547370086\n",
      "train loss:0.631693581994188\n",
      "train loss:0.5625931784895202\n",
      "train loss:0.6249188083165742\n",
      "train loss:0.7212585735068369\n",
      "train loss:0.6830060011612529\n",
      "train loss:0.6280929626600412\n",
      "train loss:0.4641116837921492\n",
      "train loss:0.44956100879796745\n",
      "train loss:0.7990016357777339\n",
      "train loss:0.5742168457859762\n",
      "train loss:0.5754831570134484\n",
      "train loss:0.5725135201541203\n",
      "train loss:0.4664580329941204\n",
      "train loss:0.6446565392070213\n",
      "train loss:0.574344132493122\n",
      "train loss:0.6146385797932907\n",
      "train loss:0.5473997990478381\n",
      "train loss:0.6535418834604713\n",
      "train loss:0.5298404635057616\n",
      "train loss:0.36670606706602654\n",
      "train loss:0.29294274793288905\n",
      "train loss:0.47994174709798826\n",
      "train loss:0.5288466102711956\n",
      "train loss:0.7700237753743576\n",
      "train loss:0.6329571530983225\n",
      "train loss:0.4488106129597663\n",
      "train loss:0.16615988809279944\n",
      "train loss:0.9443253634179888\n",
      "train loss:0.5356806995699012\n",
      "train loss:0.5885932237669741\n",
      "train loss:0.6658518155121531\n",
      "train loss:0.6644493894654936\n",
      "train loss:0.4953282804690944\n",
      "train loss:0.5708520399927748\n",
      "train loss:0.6334525258848072\n",
      "train loss:0.7677905664402542\n",
      "train loss:0.5586946791376997\n",
      "train loss:0.5704951679543061\n",
      "train loss:0.6057669993759675\n",
      "train loss:0.6966621766553864\n",
      "train loss:0.6366078159437841\n",
      "train loss:0.6402099928076627\n",
      "train loss:0.6389450523688129\n",
      "train loss:0.5330396742739258\n",
      "train loss:0.5443845549418013\n",
      "train loss:0.565620365406874\n",
      "train loss:0.5085857906571933\n",
      "train loss:0.626616460884936\n",
      "train loss:0.5330601658501948\n",
      "train loss:0.4646147725988037\n",
      "train loss:0.5727636443271102\n",
      "train loss:0.49128397941908275\n",
      "train loss:0.588828885072337\n",
      "train loss:0.4466168926464742\n",
      "train loss:0.6366916090844682\n",
      "train loss:0.8264630088976406\n",
      "train loss:0.783241843583025\n",
      "train loss:0.497617570885291\n",
      "train loss:0.5057777216801377\n",
      "train loss:0.5547168568200153\n",
      "train loss:0.6409611286544972\n",
      "train loss:0.5289513988864459\n",
      "train loss:0.5530733412412492\n",
      "train loss:0.5146946482989694\n",
      "train loss:0.32959044495914847\n",
      "train loss:0.5631651002521432\n",
      "train loss:0.5224950642863337\n",
      "train loss:0.5495957283373689\n",
      "train loss:0.6075482286903628\n",
      "train loss:0.49644972753612987\n",
      "train loss:0.3226047380404138\n",
      "train loss:0.45955343082020883\n",
      "train loss:0.6666717075711073\n",
      "train loss:0.5486405061980836\n",
      "train loss:0.5705634703014044\n",
      "train loss:0.7474477531662019\n",
      "train loss:0.45638304261725704\n",
      "train loss:0.2518506454529402\n",
      "train loss:0.6054643896986134\n",
      "train loss:0.42645547918512666\n",
      "train loss:0.5502942340804955\n",
      "train loss:0.3485765645702106\n",
      "train loss:0.7124887425451086\n",
      "train loss:0.5279055610998162\n",
      "train loss:0.5177428110735404\n",
      "train loss:0.7316298688154376\n",
      "train loss:0.34204780941595264\n",
      "train loss:0.4243466840171914\n",
      "train loss:0.4431577837626698\n",
      "train loss:0.5722783659301547\n",
      "train loss:0.35755276235665434\n",
      "train loss:0.5181677378187012\n",
      "train loss:0.5626360780937164\n",
      "train loss:0.5382817213076841\n",
      "train loss:0.41904681923977527\n",
      "train loss:0.6432720296023209\n",
      "train loss:0.3730282844631196\n",
      "train loss:0.6416689247673962\n",
      "train loss:0.8720783841030466\n",
      "train loss:0.3494163376379281\n",
      "train loss:0.8565566285561339\n",
      "train loss:0.7837829443150407\n",
      "train loss:0.5936663747655369\n",
      "train loss:0.8173290549602962\n",
      "train loss:0.7351380297521726\n",
      "train loss:0.5628180746567807\n",
      "train loss:0.480678128773957\n",
      "train loss:0.40016925127780956\n",
      "train loss:0.544408979045832\n",
      "train loss:0.5178149457126582\n",
      "train loss:0.48195936978283926\n",
      "train loss:0.6026190085752183\n",
      "train loss:0.46975534626771065\n",
      "train loss:0.5860259206884961\n",
      "train loss:0.6531934683370773\n",
      "=== epoch:9, train acc:0.74, test acc:0.69 ===\n",
      "train loss:0.6345617563744566\n",
      "train loss:0.41662485542980254\n",
      "train loss:0.5336358345495522\n",
      "train loss:0.48103459171725793\n",
      "train loss:0.46544397488088984\n",
      "train loss:0.44825832607917915\n",
      "train loss:0.6210019290220756\n",
      "train loss:0.35092104502372334\n",
      "train loss:0.8243213784726772\n",
      "train loss:0.3264507326892502\n",
      "train loss:0.765400179936612\n",
      "train loss:0.48082935562789497\n",
      "train loss:0.3302892296576462\n",
      "train loss:0.653234918981233\n",
      "train loss:0.37510480700296267\n",
      "train loss:0.8665018986158513\n",
      "train loss:0.4454292650346724\n",
      "train loss:0.4603698308458018\n",
      "train loss:0.7921126187996659\n",
      "train loss:0.8509555314001622\n",
      "train loss:0.5356349515157173\n",
      "train loss:0.4168406938305472\n",
      "train loss:0.49253842851515534\n",
      "train loss:0.5440858668190669\n",
      "train loss:0.4058110251529353\n",
      "train loss:0.30274042159092335\n",
      "train loss:0.4129853531861844\n",
      "train loss:0.6312216096733608\n",
      "train loss:0.5175019872830586\n",
      "train loss:0.37661788691238585\n",
      "train loss:0.7705267886979361\n",
      "train loss:0.7883160903470132\n",
      "train loss:0.29097935339494707\n",
      "train loss:0.38682819505926297\n",
      "train loss:0.8702511420370502\n",
      "train loss:0.359653957126647\n",
      "train loss:0.812327240120333\n",
      "train loss:0.5942143426459121\n",
      "train loss:0.6374989006556916\n",
      "train loss:0.41203955130074793\n",
      "train loss:0.9311610316754372\n",
      "train loss:0.5608715761604338\n",
      "train loss:0.3137995926942407\n",
      "train loss:0.6633750477502021\n",
      "train loss:0.6777901980448485\n",
      "train loss:0.4127294581619738\n",
      "train loss:0.4539328842077902\n",
      "train loss:0.3643492794527241\n",
      "train loss:0.4567340998530719\n",
      "train loss:0.6165586887392318\n",
      "train loss:0.7824289428392983\n",
      "train loss:0.61068772876234\n",
      "train loss:0.7224051985585216\n",
      "train loss:0.7152749707198562\n",
      "train loss:0.4552253067916371\n",
      "train loss:0.4834265893066155\n",
      "train loss:0.6161636973099671\n",
      "train loss:0.6885333737178868\n",
      "train loss:0.5775807127918882\n",
      "train loss:0.6600090868377148\n",
      "train loss:0.8165721641187023\n",
      "train loss:0.8719901913323241\n",
      "train loss:0.4744329653071106\n",
      "train loss:0.6061857667455459\n",
      "train loss:0.5409731077211039\n",
      "train loss:0.5994303428757745\n",
      "train loss:0.5587216742936093\n",
      "train loss:0.5459645156225282\n",
      "train loss:0.4780543491242299\n",
      "train loss:0.5755654776623093\n",
      "train loss:0.6567158153198359\n",
      "train loss:0.3930119224882919\n",
      "train loss:0.6875541675076849\n",
      "train loss:0.4300234046567078\n",
      "train loss:0.6742598627218241\n",
      "train loss:0.44940093582883955\n",
      "train loss:0.38727547215500246\n",
      "train loss:0.665804553353644\n",
      "train loss:0.5358951578111364\n",
      "train loss:0.4565705210388775\n",
      "train loss:0.49294267502431366\n",
      "train loss:0.33743162896332946\n",
      "train loss:0.7730994309916326\n",
      "train loss:0.7367874745121208\n",
      "train loss:0.7154278908988939\n",
      "train loss:0.33587844976099157\n",
      "train loss:0.6666537693652577\n",
      "train loss:0.6869908499898991\n",
      "train loss:0.8766160159800979\n",
      "train loss:0.43806678151579026\n",
      "train loss:0.7720514166807707\n",
      "train loss:0.707955198176265\n",
      "train loss:0.6576613906657973\n",
      "train loss:0.44866242013768803\n",
      "train loss:0.44516386317616014\n",
      "train loss:0.5786849600029785\n",
      "train loss:0.569843877121792\n",
      "train loss:0.6112117786168149\n",
      "train loss:0.5923234103376656\n",
      "train loss:0.693043865634994\n",
      "train loss:0.6388857399234441\n",
      "train loss:0.5650013933601807\n",
      "train loss:0.5397845979770899\n",
      "train loss:0.5090672341776099\n",
      "train loss:0.6810239271337919\n",
      "train loss:0.5376883573286428\n",
      "train loss:0.5189860274657548\n",
      "train loss:0.5821903277556093\n",
      "train loss:0.504542422831069\n",
      "train loss:0.3896879240518182\n",
      "train loss:0.5845814376991453\n",
      "train loss:0.5491324917581963\n",
      "train loss:0.4454879308248131\n",
      "train loss:0.6487818357678762\n",
      "train loss:0.7846753650982009\n",
      "train loss:0.41077295575096817\n",
      "train loss:0.6405614763778038\n",
      "train loss:0.57709404626052\n",
      "train loss:0.3901712371454115\n",
      "train loss:0.7939912603101859\n",
      "train loss:0.5933173989387781\n",
      "train loss:0.5026664528680946\n",
      "train loss:0.6882422141022045\n",
      "train loss:0.6083848121135037\n",
      "train loss:0.6436415091475086\n",
      "train loss:0.4219447146908094\n",
      "train loss:0.5850492382633836\n",
      "train loss:0.6163217017889078\n",
      "train loss:0.616934034896968\n",
      "train loss:0.43779329582705007\n",
      "train loss:0.63979723846873\n",
      "train loss:0.6159638317142251\n",
      "train loss:0.46114439266115453\n",
      "train loss:0.4237438663943146\n",
      "train loss:0.44457397980355706\n",
      "train loss:0.25711529643433767\n",
      "train loss:0.6378944327053112\n",
      "train loss:0.6074167708670832\n",
      "train loss:0.4234023861828603\n",
      "train loss:0.36642120050163884\n",
      "train loss:0.6320239203025239\n",
      "train loss:0.447353223720426\n",
      "train loss:0.6811091399427746\n",
      "train loss:0.4613534086073369\n",
      "train loss:0.47768202710817337\n",
      "train loss:0.31734977036015577\n",
      "train loss:0.5483373157440569\n",
      "train loss:0.7836977558525342\n",
      "train loss:0.4965544912821313\n",
      "train loss:0.8058507069446372\n",
      "train loss:0.7889795792818657\n",
      "train loss:0.5193770741668138\n",
      "train loss:0.5600240473141953\n",
      "=== epoch:10, train acc:0.75, test acc:0.69 ===\n",
      "train loss:0.5803895338442716\n",
      "train loss:0.5270186764298717\n",
      "train loss:0.725356126432135\n",
      "train loss:0.44463974073013157\n",
      "train loss:0.4827689071064887\n",
      "train loss:0.5564542676822282\n",
      "train loss:0.40331724312288514\n",
      "train loss:0.4460514330827833\n",
      "train loss:0.6153443294570237\n",
      "train loss:0.6325692556582322\n",
      "train loss:0.47888618779054565\n",
      "train loss:0.5291474564696081\n",
      "train loss:0.45262865918742357\n",
      "train loss:0.5414564050827135\n",
      "train loss:0.536862762919352\n",
      "train loss:0.5294581175025539\n",
      "train loss:0.4206622307925228\n",
      "train loss:0.4961190988472354\n",
      "train loss:0.6138944958575172\n",
      "train loss:0.6709054666355814\n",
      "train loss:0.21043295134301426\n",
      "train loss:0.9695433430195998\n",
      "train loss:0.7051219491827034\n",
      "train loss:0.6323601786932811\n",
      "train loss:0.4024093954801378\n",
      "train loss:0.48910246829523246\n",
      "train loss:0.6465598920100668\n",
      "train loss:0.707927909270958\n",
      "train loss:0.45032650544299424\n",
      "train loss:0.6698078293187331\n",
      "train loss:0.4683844289966531\n",
      "train loss:0.4590277759619671\n",
      "train loss:0.47072656132038215\n",
      "train loss:0.3979694177755246\n",
      "train loss:0.4704621607634613\n",
      "train loss:0.262377500252012\n",
      "train loss:0.2527762486429057\n",
      "train loss:0.7594366474200303\n",
      "train loss:0.5094091833306071\n",
      "train loss:0.6146592532709796\n",
      "train loss:0.22538817546809237\n",
      "train loss:0.48941587785048746\n",
      "train loss:0.7416493662504455\n",
      "train loss:0.19015811653499096\n",
      "train loss:0.6235355330872412\n",
      "train loss:0.5683334070412206\n",
      "train loss:1.0033856114456108\n",
      "train loss:0.6237187042517511\n",
      "train loss:0.4277700751553383\n",
      "train loss:0.875173411348469\n",
      "train loss:0.5528089494621721\n",
      "train loss:0.5019315926652983\n",
      "train loss:0.6146449296517642\n",
      "train loss:0.6148496743113379\n",
      "train loss:0.5609191842936444\n",
      "train loss:0.7319314765330953\n",
      "train loss:0.5423093662376945\n",
      "train loss:0.4820258120210318\n",
      "train loss:0.5695666438949711\n",
      "train loss:0.44592999689690327\n",
      "train loss:0.4745376501516114\n",
      "train loss:0.37766437165134636\n",
      "train loss:0.5781632112934898\n",
      "train loss:0.5041370432871765\n",
      "train loss:0.5970974283242408\n",
      "train loss:0.2919542254683108\n",
      "train loss:0.6167279792751953\n",
      "train loss:0.39450456279067075\n",
      "train loss:0.4866600844915231\n",
      "train loss:0.5018235065083488\n",
      "train loss:0.7206162508505993\n",
      "train loss:0.4642641034734246\n",
      "train loss:0.8353114984768876\n",
      "train loss:0.6338133463633719\n",
      "train loss:0.7606112896921429\n",
      "train loss:0.4918866130372252\n",
      "train loss:0.7940974218518665\n",
      "train loss:0.40253631953670127\n",
      "train loss:0.5090522613391515\n",
      "train loss:0.5250341783953625\n",
      "train loss:0.5160913631245811\n",
      "train loss:0.5265670894995438\n",
      "train loss:0.5328115327273493\n",
      "train loss:0.32863287079852044\n",
      "train loss:0.6229542248129771\n",
      "train loss:0.5058411128370512\n",
      "train loss:0.5121553780788596\n",
      "train loss:0.5232400892148281\n",
      "train loss:0.4553149548223921\n",
      "train loss:0.7521138102280251\n",
      "train loss:0.6367596913012709\n",
      "train loss:0.5132935157602669\n",
      "train loss:0.8625979931481194\n",
      "train loss:0.3684365438539889\n",
      "train loss:0.7407312117127494\n",
      "train loss:0.45330603523736734\n",
      "train loss:0.5190516892488514\n",
      "train loss:0.6185486064070667\n",
      "train loss:0.6428121001082435\n",
      "train loss:0.4842245084315892\n",
      "train loss:0.7697555958663771\n",
      "train loss:0.6212984164162357\n",
      "train loss:0.3847560017185497\n",
      "train loss:0.600692626082116\n",
      "train loss:0.5208005039989481\n",
      "train loss:0.6797434079837985\n",
      "train loss:0.5579776742713122\n",
      "train loss:0.43770629933793365\n",
      "train loss:0.718384400161608\n",
      "train loss:0.49424894796117325\n",
      "train loss:0.5240594892874715\n",
      "train loss:0.37734361125185123\n",
      "train loss:0.5491336646598599\n",
      "train loss:0.6246112254601356\n",
      "train loss:0.5621679090130178\n",
      "train loss:0.464653219513054\n",
      "train loss:0.5810640277248538\n",
      "train loss:0.584918399096643\n",
      "train loss:0.5121072277646694\n",
      "train loss:0.41570650927436564\n",
      "train loss:0.4149373113401186\n",
      "train loss:0.3280604173300533\n",
      "train loss:0.36834990975047305\n",
      "train loss:0.6467375428937494\n",
      "train loss:0.09924855686148201\n",
      "train loss:1.0857087328804118\n",
      "train loss:0.493138119595244\n",
      "train loss:0.5887769074839803\n",
      "train loss:0.5581412611555178\n",
      "train loss:0.8619539229446793\n",
      "train loss:0.35336640497945065\n",
      "train loss:0.5391677508077708\n",
      "train loss:0.41369049243571343\n",
      "train loss:0.5727657291601365\n",
      "train loss:0.3515861763595266\n",
      "train loss:0.557266833948806\n",
      "train loss:0.6819169594053235\n",
      "train loss:0.6893738663842081\n",
      "train loss:0.6042558800061332\n",
      "train loss:0.6959162197316132\n",
      "train loss:0.5341558471437563\n",
      "train loss:0.5009207065280996\n",
      "train loss:0.5415129425724474\n",
      "train loss:0.4534988804872303\n",
      "train loss:0.5728185626754895\n",
      "train loss:0.31952615610835616\n",
      "train loss:0.7030184766586742\n",
      "train loss:0.695910421598042\n",
      "train loss:0.6094825038893049\n",
      "train loss:0.47395967621088025\n",
      "train loss:0.5187256699783208\n",
      "train loss:0.8928068935957864\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5568627450980392\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 7, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "494cb7b0-f361-4f12-826e-6ea253035e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6918380304498158\n",
      "=== epoch:1, train acc:0.28, test acc:0.31 ===\n",
      "train loss:0.6937317236707203\n",
      "train loss:0.6931829514006907\n",
      "train loss:0.6930594339075553\n",
      "train loss:0.6931397950555411\n",
      "train loss:0.6930268298695752\n",
      "train loss:0.6929864411400037\n",
      "train loss:0.6922641694841714\n",
      "train loss:0.6925514098439929\n",
      "train loss:0.6912922735209209\n",
      "train loss:0.6875516247828867\n",
      "train loss:0.6846957059843235\n",
      "train loss:0.6849112265770742\n",
      "train loss:0.6838648931797373\n",
      "train loss:0.6860769500608837\n",
      "train loss:0.6813964634068741\n",
      "train loss:0.6863235095627853\n",
      "train loss:0.6728095591120169\n",
      "train loss:0.6717742054864806\n",
      "train loss:0.6330306239444109\n",
      "train loss:0.6601433946925852\n",
      "train loss:0.6058585200468326\n",
      "train loss:0.6729817785093024\n",
      "train loss:0.6032526726548615\n",
      "train loss:0.7208666700261942\n",
      "train loss:0.6823594138771565\n",
      "train loss:0.7157479089555767\n",
      "train loss:0.7624254672750462\n",
      "train loss:0.6228062243487635\n",
      "train loss:0.67189754784276\n",
      "train loss:0.5175285118790867\n",
      "train loss:0.6313707018948655\n",
      "train loss:0.4936983695470768\n",
      "train loss:0.5385187023902189\n",
      "train loss:0.7596797659889919\n",
      "train loss:0.7424694955216642\n",
      "train loss:0.4708756618032046\n",
      "train loss:0.8794027984080628\n",
      "train loss:0.43219048618426764\n",
      "train loss:0.5986582583659554\n",
      "train loss:0.5601580189199138\n",
      "train loss:0.8548934984694867\n",
      "train loss:0.5183506448945405\n",
      "train loss:0.6168619658571148\n",
      "train loss:0.5340210036703534\n",
      "train loss:0.7287284714660136\n",
      "train loss:0.5378279115533606\n",
      "train loss:0.8021376205735272\n",
      "train loss:0.41365736582450496\n",
      "train loss:0.8729300864069129\n",
      "train loss:0.5283766862774042\n",
      "train loss:0.641252641077365\n",
      "train loss:0.6887003007727615\n",
      "train loss:0.7539133100032689\n",
      "train loss:0.7498098943219864\n",
      "train loss:0.6295468411169692\n",
      "train loss:0.6125988958442935\n",
      "train loss:0.6136739462994065\n",
      "train loss:0.732125459667413\n",
      "train loss:0.6706515027846982\n",
      "train loss:0.48991600132150087\n",
      "train loss:0.5064169344921324\n",
      "train loss:0.6640549534418092\n",
      "train loss:0.4208252869970752\n",
      "train loss:0.5412532591243766\n",
      "train loss:0.5834808713493158\n",
      "train loss:0.4867072159303921\n",
      "train loss:0.4745590695529446\n",
      "train loss:0.5046201007007348\n",
      "train loss:0.6892375371108537\n",
      "train loss:0.5832100875527233\n",
      "train loss:0.917404070323623\n",
      "train loss:0.628191091724071\n",
      "train loss:0.42580718890354186\n",
      "train loss:0.8642014724383943\n",
      "train loss:0.7776683329782854\n",
      "train loss:0.6042613707763546\n",
      "train loss:0.8608346871885496\n",
      "train loss:0.7150047851013122\n",
      "train loss:0.551767058935323\n",
      "train loss:0.4314958645036375\n",
      "train loss:0.8021191397734515\n",
      "train loss:0.5566372449444398\n",
      "train loss:0.6811427234263395\n",
      "train loss:0.7389736015961021\n",
      "train loss:0.5259254269341616\n",
      "train loss:0.6510664342318921\n",
      "train loss:0.7462509968794505\n",
      "train loss:0.6108626439940155\n",
      "train loss:0.6242912478417324\n",
      "train loss:0.691691505863065\n",
      "train loss:0.5904843755618361\n",
      "train loss:0.5694176316636674\n",
      "train loss:0.6892022611940384\n",
      "train loss:0.7157100249106059\n",
      "train loss:0.5943812473674951\n",
      "train loss:0.560539410456532\n",
      "train loss:0.7385679711240896\n",
      "train loss:0.6340826394139851\n",
      "train loss:0.5552783573096101\n",
      "train loss:0.6305889202715529\n",
      "train loss:0.6553979951953133\n",
      "train loss:0.5177324712533271\n",
      "train loss:0.8046584936963374\n",
      "train loss:0.5893609355206991\n",
      "train loss:0.5564175223898434\n",
      "train loss:0.42505155987502474\n",
      "train loss:0.8732336414553268\n",
      "train loss:0.5576457804672057\n",
      "train loss:0.7391138653641531\n",
      "train loss:0.4758243243257973\n",
      "train loss:0.5432826396569216\n",
      "train loss:0.5906988479131373\n",
      "train loss:0.5213886849032215\n",
      "train loss:0.5307009621560472\n",
      "train loss:0.7967621178986147\n",
      "train loss:0.5948085520681486\n",
      "train loss:0.43197104425284005\n",
      "train loss:0.6168800290235168\n",
      "train loss:0.7856559564878072\n",
      "train loss:0.5857657880424305\n",
      "train loss:0.6974431905800533\n",
      "train loss:0.43209013059930657\n",
      "train loss:0.4399601140505746\n",
      "train loss:0.5026880838622028\n",
      "train loss:0.6208738445207665\n",
      "train loss:0.6383949197316421\n",
      "train loss:0.4970546953719223\n",
      "train loss:0.7676647880832311\n",
      "train loss:0.8857297158634138\n",
      "train loss:0.4139471196512236\n",
      "train loss:0.5122381428796668\n",
      "train loss:0.5221079918633399\n",
      "train loss:0.6792366414189217\n",
      "train loss:0.5366524410081348\n",
      "train loss:0.5345765488225025\n",
      "train loss:0.7063182937107959\n",
      "train loss:0.6310979392037726\n",
      "train loss:0.5206003532915349\n",
      "train loss:0.6755526466647916\n",
      "train loss:0.7084593103202017\n",
      "train loss:0.6177076371086546\n",
      "train loss:0.544943984459601\n",
      "train loss:0.42870559585309564\n",
      "train loss:0.454066766678548\n",
      "train loss:0.5339440571263017\n",
      "train loss:0.6478169867887121\n",
      "train loss:0.805255692904249\n",
      "train loss:0.7010816130474057\n",
      "train loss:0.6955407940171761\n",
      "train loss:0.6779798215741539\n",
      "train loss:0.5217882899776365\n",
      "train loss:0.6906433182190537\n",
      "train loss:0.6175874075902646\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6176922170748178\n",
      "train loss:0.7110572716758116\n",
      "train loss:0.6915631691444971\n",
      "train loss:0.5539084234899642\n",
      "train loss:0.7970426594829638\n",
      "train loss:0.6908158496163351\n",
      "train loss:0.601334505238343\n",
      "train loss:0.6281395090494776\n",
      "train loss:0.6629305913416184\n",
      "train loss:0.5500549227957603\n",
      "train loss:0.6666601729662125\n",
      "train loss:0.6336436623430535\n",
      "train loss:0.618979729745733\n",
      "train loss:0.7431167444523613\n",
      "train loss:0.5636332314657289\n",
      "train loss:0.6164039880560732\n",
      "train loss:0.6849424022313371\n",
      "train loss:0.5897638942322126\n",
      "train loss:0.4786671204759231\n",
      "train loss:0.550359490803481\n",
      "train loss:0.7535446270447256\n",
      "train loss:0.74347848158034\n",
      "train loss:0.5223975675054182\n",
      "train loss:0.6185119925858221\n",
      "train loss:0.6252367579185324\n",
      "train loss:0.46341885417059114\n",
      "train loss:0.3818496631091481\n",
      "train loss:0.7079239623285274\n",
      "train loss:0.43527697678712113\n",
      "train loss:0.47814641460420093\n",
      "train loss:0.6343165198857549\n",
      "train loss:0.799862993088215\n",
      "train loss:0.5217489678069827\n",
      "train loss:0.8956778699735033\n",
      "train loss:0.7994682094396359\n",
      "train loss:0.5025792209030515\n",
      "train loss:0.5913973425836245\n",
      "train loss:0.5971572563894414\n",
      "train loss:0.6803894855186886\n",
      "train loss:0.623189503269504\n",
      "train loss:0.7660348643652096\n",
      "train loss:0.6381896367186455\n",
      "train loss:0.6229468418401021\n",
      "train loss:0.6781186682366327\n",
      "train loss:0.5969016198285821\n",
      "train loss:0.5361872909465306\n",
      "train loss:0.6738612356990159\n",
      "train loss:0.4081407629445263\n",
      "train loss:0.82786597766903\n",
      "train loss:0.6462258430660996\n",
      "train loss:0.5718902028786561\n",
      "train loss:0.417477549555188\n",
      "train loss:0.4625854713793823\n",
      "train loss:0.5593874526110051\n",
      "train loss:0.7046951845384326\n",
      "train loss:0.5377850467447516\n",
      "train loss:0.6035020548505371\n",
      "train loss:0.6779922908604683\n",
      "train loss:0.515345242523696\n",
      "train loss:0.5414979753213041\n",
      "train loss:0.9431950090024153\n",
      "train loss:0.436859167062145\n",
      "train loss:0.6061743602534024\n",
      "train loss:0.38547609435931735\n",
      "train loss:0.6037998336978575\n",
      "train loss:0.6182090039686924\n",
      "train loss:0.4192415300639779\n",
      "train loss:0.703791152691229\n",
      "train loss:0.6147488560413785\n",
      "train loss:0.6069770691250853\n",
      "train loss:0.5992869945745553\n",
      "train loss:0.5160947272530564\n",
      "train loss:0.7009843167246186\n",
      "train loss:0.6386787737117567\n",
      "train loss:0.43464752659421446\n",
      "train loss:0.6189393786122334\n",
      "train loss:0.613585494672856\n",
      "train loss:0.6757701203239289\n",
      "train loss:0.3987133504451674\n",
      "train loss:0.42798258377157505\n",
      "train loss:0.7313586541912596\n",
      "train loss:0.7898779634257423\n",
      "train loss:0.612059474852149\n",
      "train loss:0.6126202749430708\n",
      "train loss:0.6935742046942414\n",
      "train loss:0.792298344715647\n",
      "train loss:0.6157280354000434\n",
      "train loss:0.5299036555045351\n",
      "train loss:0.6035296833446177\n",
      "train loss:0.5203468450738221\n",
      "train loss:0.5517911029841458\n",
      "train loss:0.4466940647286042\n",
      "train loss:0.8248661986823065\n",
      "train loss:0.6040095046438416\n",
      "train loss:0.530638940673654\n",
      "train loss:0.6960968350312771\n",
      "train loss:0.827858664174744\n",
      "train loss:0.38650198953729714\n",
      "train loss:0.6809839314017267\n",
      "train loss:0.7339326672422694\n",
      "train loss:0.458475585419288\n",
      "train loss:0.5306649806056216\n",
      "train loss:0.7780774095462148\n",
      "train loss:0.5281048765059471\n",
      "train loss:0.5989366932969311\n",
      "train loss:0.5220750245859577\n",
      "train loss:0.6985598735239675\n",
      "train loss:0.536088376274927\n",
      "train loss:0.4509703116579401\n",
      "train loss:0.4111500960610449\n",
      "train loss:0.8861167754160213\n",
      "train loss:0.5201724917115258\n",
      "train loss:0.4192390506020257\n",
      "train loss:0.6853484965491825\n",
      "train loss:0.7713474971604237\n",
      "train loss:0.39396290122006083\n",
      "train loss:0.530628590048788\n",
      "train loss:0.4958514398039953\n",
      "train loss:0.6184096206642926\n",
      "train loss:0.5786162762419301\n",
      "train loss:0.2605432278688298\n",
      "train loss:0.5736298416105802\n",
      "train loss:0.48960662598174504\n",
      "train loss:0.6437111069238788\n",
      "train loss:0.48723389553827373\n",
      "train loss:0.3794941434325102\n",
      "train loss:0.7863777746508523\n",
      "train loss:0.644503652399013\n",
      "train loss:0.49890593844283426\n",
      "train loss:0.6181579092416432\n",
      "train loss:0.8553199190134748\n",
      "train loss:0.4038215350154283\n",
      "train loss:0.650623307447343\n",
      "train loss:0.5120945338573031\n",
      "train loss:0.7793970298887521\n",
      "train loss:0.5983843048382617\n",
      "train loss:0.6151670638162907\n",
      "train loss:0.6945448334851825\n",
      "train loss:0.5932394086900709\n",
      "train loss:0.6116244235967783\n",
      "train loss:0.47816286308341727\n",
      "train loss:0.5426394567189291\n",
      "train loss:0.5363938038825533\n",
      "train loss:0.6966922072352566\n",
      "train loss:0.6773216353174588\n",
      "train loss:0.615206309394907\n",
      "train loss:0.5179725490287084\n",
      "train loss:0.5393764425531615\n",
      "train loss:0.5235235989824002\n",
      "train loss:0.7665010157746974\n",
      "train loss:0.5367653088359468\n",
      "train loss:0.6065868728132873\n",
      "train loss:0.6686762922223449\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6928123740941821\n",
      "train loss:0.6054594220822044\n",
      "train loss:0.7175832429042062\n",
      "train loss:0.3755211126176725\n",
      "train loss:0.5579540464183481\n",
      "train loss:0.5140020288071487\n",
      "train loss:0.3501344473704969\n",
      "train loss:0.7729833313596417\n",
      "train loss:0.7011779823056783\n",
      "train loss:0.5936373247823756\n",
      "train loss:0.6110124669544448\n",
      "train loss:0.7307122230564831\n",
      "train loss:0.5014376767211315\n",
      "train loss:0.5384437794115323\n",
      "train loss:0.6875176588022496\n",
      "train loss:0.8956338976111132\n",
      "train loss:0.50327639879887\n",
      "train loss:0.6064186899142777\n",
      "train loss:0.42494594499942295\n",
      "train loss:0.5144968000996222\n",
      "train loss:0.44172196934368346\n",
      "train loss:0.6237021424089888\n",
      "train loss:0.41486109372844276\n",
      "train loss:0.48509469556090307\n",
      "train loss:0.5222036085883919\n",
      "train loss:0.6299373042894708\n",
      "train loss:0.9832148531224455\n",
      "train loss:0.8340012974697306\n",
      "train loss:0.8306019887327094\n",
      "train loss:0.7340029124202223\n",
      "train loss:0.6955583337484752\n",
      "train loss:0.5355820421079013\n",
      "train loss:0.522677074901393\n",
      "train loss:0.676786645582282\n",
      "train loss:0.6178960557655777\n",
      "train loss:0.6625318978312743\n",
      "train loss:0.6749721529606546\n",
      "train loss:0.5573654144903907\n",
      "train loss:0.6763934321070713\n",
      "train loss:0.534848959141421\n",
      "train loss:0.6803303284973808\n",
      "train loss:0.6039957141581445\n",
      "train loss:0.6259765321517049\n",
      "train loss:0.499591114617735\n",
      "train loss:0.5603834321790425\n",
      "train loss:0.483278316435449\n",
      "train loss:0.4992926144696588\n",
      "train loss:0.8258272048686364\n",
      "train loss:0.6916498452027826\n",
      "train loss:0.8634443266691815\n",
      "train loss:0.5262180149091162\n",
      "train loss:0.6898128084197849\n",
      "train loss:0.4481578471016573\n",
      "train loss:0.5170191136916988\n",
      "train loss:0.5887987063969562\n",
      "train loss:0.5865411414496176\n",
      "train loss:0.6278667662801144\n",
      "train loss:0.5094103200967895\n",
      "train loss:0.5961466489644456\n",
      "train loss:0.4012591775410782\n",
      "train loss:0.5171742801747705\n",
      "train loss:0.716840558592785\n",
      "train loss:0.7259796859786356\n",
      "train loss:0.5401055939537928\n",
      "train loss:0.7516639282269182\n",
      "train loss:0.4055615681316831\n",
      "train loss:0.49202941908955183\n",
      "train loss:0.6135124564438266\n",
      "train loss:0.4929444402758293\n",
      "train loss:0.7352285965415214\n",
      "train loss:0.3885440371886107\n",
      "train loss:0.38556947672952635\n",
      "train loss:0.7630221183845409\n",
      "train loss:0.6366373166664095\n",
      "train loss:0.4832896314912117\n",
      "train loss:0.6017077126488142\n",
      "train loss:0.4761588692944138\n",
      "train loss:0.6313785363048027\n",
      "train loss:0.5040847058573137\n",
      "train loss:0.6205317265370177\n",
      "train loss:0.9357817135020948\n",
      "train loss:0.6337904323516365\n",
      "train loss:0.7016112369390813\n",
      "train loss:0.416836794636541\n",
      "train loss:0.7772189041528468\n",
      "train loss:0.6137071207822291\n",
      "train loss:0.5331512852974336\n",
      "train loss:0.7988167574122438\n",
      "train loss:0.38887813835256446\n",
      "train loss:0.6791643521844459\n",
      "train loss:0.6085634796205456\n",
      "train loss:0.6686370465252933\n",
      "train loss:0.6697559387455889\n",
      "train loss:0.7849226657886462\n",
      "train loss:0.5605515754233356\n",
      "train loss:0.6002254551466617\n",
      "train loss:0.49328328406607014\n",
      "train loss:0.5624847052497265\n",
      "train loss:0.47215772323505895\n",
      "train loss:0.5345930036436338\n",
      "train loss:0.6058140660935789\n",
      "train loss:0.6089734630986159\n",
      "train loss:0.5728761808134241\n",
      "train loss:0.8976725557873284\n",
      "train loss:0.5387808626657371\n",
      "train loss:0.5491937938861454\n",
      "train loss:0.623775055864231\n",
      "train loss:0.44660604775955826\n",
      "train loss:0.69251918863165\n",
      "train loss:0.6097604375289467\n",
      "train loss:0.6137643531238149\n",
      "train loss:0.419198155964103\n",
      "train loss:0.7961378527911969\n",
      "train loss:0.319109401033545\n",
      "train loss:0.7255920678086208\n",
      "train loss:0.7142880986651968\n",
      "train loss:0.515438824239712\n",
      "train loss:0.5052766637190838\n",
      "train loss:0.6912749009231457\n",
      "train loss:0.9426704288155909\n",
      "train loss:0.7685490534450257\n",
      "train loss:0.7650176819376766\n",
      "train loss:0.7666724750872149\n",
      "train loss:0.5303501079681066\n",
      "train loss:0.73984733959137\n",
      "train loss:0.4733323525136746\n",
      "train loss:0.6180183563007817\n",
      "train loss:0.498724340733984\n",
      "train loss:0.7837322801875862\n",
      "train loss:0.5014574582135471\n",
      "train loss:0.502572046815107\n",
      "train loss:0.7418470535725326\n",
      "train loss:0.6129890287987674\n",
      "train loss:0.5496662495927233\n",
      "train loss:0.6790028555318618\n",
      "train loss:0.5563104098396093\n",
      "train loss:0.5091976096936031\n",
      "train loss:0.5475509566019624\n",
      "train loss:0.41020383168359703\n",
      "train loss:0.5454165603053257\n",
      "train loss:0.4434708952814808\n",
      "train loss:0.8496250755903217\n",
      "train loss:0.5238974184896773\n",
      "train loss:0.41420382369920644\n",
      "train loss:0.6342938980618916\n",
      "train loss:0.7967356116206372\n",
      "train loss:0.5915659584209865\n",
      "train loss:0.9296599080182171\n",
      "train loss:0.6099273071815932\n",
      "train loss:0.5905355496047269\n",
      "train loss:0.5955404944281535\n",
      "train loss:0.49522118009815275\n",
      "train loss:0.5219791744252755\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7175723984259016\n",
      "train loss:0.5212512672439658\n",
      "train loss:0.6836528079891153\n",
      "train loss:0.6824333563182481\n",
      "train loss:0.36786722840842223\n",
      "train loss:0.5082587590910335\n",
      "train loss:0.7145832066199224\n",
      "train loss:0.6091983340279604\n",
      "train loss:0.6137936181465973\n",
      "train loss:0.5884604095361484\n",
      "train loss:0.6071331490686\n",
      "train loss:0.6200879956426432\n",
      "train loss:0.6442860284843318\n",
      "train loss:0.5177479859882743\n",
      "train loss:0.5856881454862226\n",
      "train loss:0.5442221095611822\n",
      "train loss:0.7023319867531506\n",
      "train loss:0.6155816883208415\n",
      "train loss:0.4326690848929016\n",
      "train loss:0.7658114786935895\n",
      "train loss:0.39608566586089305\n",
      "train loss:0.7020410650390989\n",
      "train loss:0.6190506569580829\n",
      "train loss:0.7021843825486296\n",
      "train loss:0.7034604297438122\n",
      "train loss:0.5511811600622265\n",
      "train loss:0.6168100787559021\n",
      "train loss:0.6708740719402162\n",
      "train loss:0.588726889825194\n",
      "train loss:0.6114264333158803\n",
      "train loss:0.640173475322076\n",
      "train loss:0.6724525303601909\n",
      "train loss:0.5535233275961323\n",
      "train loss:0.5995250027521273\n",
      "train loss:0.5429204267802796\n",
      "train loss:0.4517141002374488\n",
      "train loss:0.6423495132146344\n",
      "train loss:0.5986399081135755\n",
      "train loss:0.6725795439678295\n",
      "train loss:0.5225233692531959\n",
      "train loss:0.5195323153985618\n",
      "train loss:0.7102576489659926\n",
      "train loss:0.6214890701921253\n",
      "train loss:0.7205277239175445\n",
      "train loss:0.6748696213213272\n",
      "train loss:0.691714381594619\n",
      "train loss:0.6763079621739613\n",
      "train loss:0.8241175611953503\n",
      "train loss:0.4556176841608447\n",
      "train loss:0.45026463036177355\n",
      "train loss:0.45164387426486235\n",
      "train loss:0.3809923598742658\n",
      "train loss:0.4982642031645138\n",
      "train loss:0.7242514022118071\n",
      "train loss:0.3388134504926326\n",
      "train loss:0.5109404393399981\n",
      "train loss:0.5918670983728304\n",
      "train loss:0.521550168410176\n",
      "train loss:0.4124725997152023\n",
      "train loss:0.4843210163415986\n",
      "train loss:0.6940133342004335\n",
      "train loss:0.47991795730066195\n",
      "train loss:0.475507602335966\n",
      "train loss:0.599924156114698\n",
      "train loss:0.574567331516257\n",
      "train loss:0.7700219507163357\n",
      "train loss:0.6296340536458052\n",
      "train loss:0.4889184624145077\n",
      "train loss:0.48982305534746756\n",
      "train loss:0.5215726564323316\n",
      "train loss:0.6317420354721597\n",
      "train loss:0.5684163143126366\n",
      "train loss:0.48720578272543164\n",
      "train loss:0.4890500821680385\n",
      "train loss:0.4071490676520967\n",
      "train loss:0.4862240692787977\n",
      "train loss:0.5036317709013193\n",
      "train loss:0.48166346451137965\n",
      "train loss:0.7700736128849067\n",
      "train loss:0.47681018295044264\n",
      "train loss:0.36437703571769964\n",
      "train loss:0.3850401309519233\n",
      "train loss:0.7518801045558465\n",
      "train loss:0.9175264920368111\n",
      "train loss:0.48448752318854627\n",
      "train loss:0.6878618821218626\n",
      "train loss:0.7759452366404338\n",
      "train loss:0.6032447545348285\n",
      "train loss:0.3357444970024853\n",
      "train loss:0.5136629181858015\n",
      "train loss:0.5882587405863517\n",
      "train loss:0.7757721954866781\n",
      "train loss:0.5517980274622204\n",
      "train loss:0.7427032232692871\n",
      "train loss:0.6083655397599355\n",
      "train loss:0.5127644283302355\n",
      "train loss:0.574580219575589\n",
      "train loss:0.8225002334090961\n",
      "train loss:0.604021480388005\n",
      "train loss:0.5161532410231249\n",
      "train loss:0.4612837542694028\n",
      "train loss:0.5388513543785225\n",
      "train loss:0.550559037966885\n",
      "train loss:0.6420517671202741\n",
      "train loss:0.6315242714222404\n",
      "train loss:0.5360163151289132\n",
      "train loss:0.6986067768389308\n",
      "train loss:0.4584783378650578\n",
      "train loss:0.5802939864831963\n",
      "train loss:0.7752417243360232\n",
      "train loss:0.4353713724811\n",
      "train loss:0.5086066458694088\n",
      "train loss:0.6211684369834397\n",
      "train loss:0.4869461097083002\n",
      "train loss:0.44699592921215087\n",
      "train loss:0.6289735792364687\n",
      "train loss:0.4087388665192517\n",
      "train loss:0.6130469047417781\n",
      "train loss:0.7392616772636355\n",
      "train loss:0.7521976347985497\n",
      "train loss:0.6347624737879954\n",
      "train loss:0.5197828637446584\n",
      "train loss:0.6150861565171397\n",
      "train loss:0.622608522799362\n",
      "train loss:0.6022932114513603\n",
      "train loss:0.560846000718349\n",
      "train loss:0.6685232071723777\n",
      "train loss:0.567108675516588\n",
      "train loss:0.502289368931582\n",
      "train loss:0.44197822867604036\n",
      "train loss:0.5898495251719433\n",
      "train loss:0.4650803510358939\n",
      "train loss:0.5047868155693485\n",
      "train loss:0.4652877232609455\n",
      "train loss:0.709310887850494\n",
      "train loss:0.47932532896933033\n",
      "train loss:0.5194104926922114\n",
      "train loss:0.44748170294324935\n",
      "train loss:0.468975241679371\n",
      "train loss:0.25412854553536157\n",
      "train loss:0.6127365810121408\n",
      "train loss:0.3283610926481929\n",
      "train loss:0.6112317375636789\n",
      "train loss:0.5117278952078108\n",
      "train loss:0.4740053522210214\n",
      "train loss:0.5143987969235566\n",
      "train loss:0.3629878590964652\n",
      "train loss:0.6214478890180322\n",
      "train loss:0.3294069757478381\n",
      "train loss:0.30487832119819214\n",
      "train loss:1.0565981917651521\n",
      "train loss:0.35718852302061743\n",
      "train loss:0.8758213888449013\n",
      "=== epoch:5, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6074676440554698\n",
      "train loss:0.55804455980186\n",
      "train loss:0.48379389417899976\n",
      "train loss:0.6278387229037222\n",
      "train loss:0.6606174183299082\n",
      "train loss:0.5474143018994684\n",
      "train loss:0.687818520158463\n",
      "train loss:0.4690905885282414\n",
      "train loss:0.6155077460681253\n",
      "train loss:0.5219763521697349\n",
      "train loss:0.5947217697808127\n",
      "train loss:0.41491845279375583\n",
      "train loss:0.6904261949342146\n",
      "train loss:0.6788495033067378\n",
      "train loss:0.6045724749121342\n",
      "train loss:0.5663859958655866\n",
      "train loss:0.5226160866719783\n",
      "train loss:0.6360273328193574\n",
      "train loss:0.6892416085454921\n",
      "train loss:0.7571269978784096\n",
      "train loss:0.5275034054866101\n",
      "train loss:0.39648513790840056\n",
      "train loss:0.6420251514530297\n",
      "train loss:0.6046008115333507\n",
      "train loss:0.6908974676022804\n",
      "train loss:0.5233198987585134\n",
      "train loss:0.661860820724027\n",
      "train loss:0.7237461333326831\n",
      "train loss:0.7333378781151912\n",
      "train loss:0.380103334194475\n",
      "train loss:0.6839958161570233\n",
      "train loss:0.628403576372907\n",
      "train loss:0.44355069847029666\n",
      "train loss:0.564474493240475\n",
      "train loss:0.48443864055579045\n",
      "train loss:0.4115427285050757\n",
      "train loss:0.4867131268397061\n",
      "train loss:0.7277282783841171\n",
      "train loss:0.4451743254829889\n",
      "train loss:0.8157400862364768\n",
      "train loss:0.4044937509673282\n",
      "train loss:0.6710427531112613\n",
      "train loss:0.5973922503852809\n",
      "train loss:0.5574827761229202\n",
      "train loss:0.5782146275489481\n",
      "train loss:0.5149401982309966\n",
      "train loss:0.48332598310902936\n",
      "train loss:0.5872433075826373\n",
      "train loss:0.7103459249432847\n",
      "train loss:0.9028535371932798\n",
      "train loss:0.7423376535005989\n",
      "train loss:0.4974523211972615\n",
      "train loss:0.6316131787872263\n",
      "train loss:0.600496274097911\n",
      "train loss:0.543778543345966\n",
      "train loss:0.5507224557247798\n",
      "train loss:0.4849980663985397\n",
      "train loss:0.6071931853469479\n",
      "train loss:0.5385017062182957\n",
      "train loss:0.7407466953689851\n",
      "train loss:0.5764532830588103\n",
      "train loss:0.4395861363826178\n",
      "train loss:0.4421065739495069\n",
      "train loss:0.7665683721980958\n",
      "train loss:0.3973434217748694\n",
      "train loss:0.8327344045867658\n",
      "train loss:0.6654843428718964\n",
      "train loss:0.789845393716998\n",
      "train loss:0.8528651783304328\n",
      "train loss:0.44439327994063776\n",
      "train loss:0.5355915541685095\n",
      "train loss:0.687296974144113\n",
      "train loss:0.548528869406266\n",
      "train loss:0.530834534116317\n",
      "train loss:0.5470107229646889\n",
      "train loss:0.42703704192631864\n",
      "train loss:0.5113765505719086\n",
      "train loss:0.5041104779509253\n",
      "train loss:0.409729540377756\n",
      "train loss:0.6259424002732963\n",
      "train loss:0.6800405310572465\n",
      "train loss:0.29852321150507793\n",
      "train loss:0.5322175898239516\n",
      "train loss:0.6121736107160949\n",
      "train loss:0.38104313740051987\n",
      "train loss:0.6494360578504124\n",
      "train loss:0.5059007734651177\n",
      "train loss:0.2172839598570488\n",
      "train loss:0.5820868010470103\n",
      "train loss:0.3526005187795818\n",
      "train loss:0.4998492327698673\n",
      "train loss:0.7425625576963742\n",
      "train loss:0.44143457490931465\n",
      "train loss:0.5592911167649419\n",
      "train loss:0.6371756590200468\n",
      "train loss:0.8004983405493634\n",
      "train loss:0.8951002503028136\n",
      "train loss:0.915825647878488\n",
      "train loss:0.5953923623309653\n",
      "train loss:0.7784708424641996\n",
      "train loss:0.7481372535647768\n",
      "train loss:0.5094294402151427\n",
      "train loss:0.621874861874142\n",
      "train loss:0.4500969269212011\n",
      "train loss:0.46548291716762213\n",
      "train loss:0.6614649533160035\n",
      "train loss:0.6486977680968632\n",
      "train loss:0.7031067556719636\n",
      "train loss:0.7164162897876467\n",
      "train loss:0.5168699100425849\n",
      "train loss:0.4790618719882124\n",
      "train loss:0.6017022928841811\n",
      "train loss:0.6792718473974465\n",
      "train loss:0.4546124987164715\n",
      "train loss:0.6255909888846501\n",
      "train loss:0.6173382584111564\n",
      "train loss:0.6753444613618756\n",
      "train loss:0.5311521142176818\n",
      "train loss:0.5782086387801776\n",
      "train loss:0.4977801325375844\n",
      "train loss:0.5819377980926252\n",
      "train loss:0.7869699714388215\n",
      "train loss:0.4591334120145126\n",
      "train loss:0.4737481506621043\n",
      "train loss:1.0569528462877247\n",
      "train loss:0.5139992144556146\n",
      "train loss:0.3106999002127341\n",
      "train loss:0.39090873641088\n",
      "train loss:0.5216989978833263\n",
      "train loss:0.9017324409989456\n",
      "train loss:0.5019621743494668\n",
      "train loss:0.5359444567809752\n",
      "train loss:0.5063103452328169\n",
      "train loss:0.37672349332321825\n",
      "train loss:0.696685033333645\n",
      "train loss:0.6414399180641238\n",
      "train loss:0.28367579779123986\n",
      "train loss:0.4903706842848627\n",
      "train loss:0.3913596035972885\n",
      "train loss:0.5091568221134353\n",
      "train loss:0.656337305965349\n",
      "train loss:0.6345024720908423\n",
      "train loss:0.6753931136031823\n",
      "train loss:0.6242452025499079\n",
      "train loss:0.6144007111289906\n",
      "train loss:0.4646443349560264\n",
      "train loss:0.6445013337262506\n",
      "train loss:0.7690232595673356\n",
      "train loss:0.5256653959414355\n",
      "train loss:0.6094950924728677\n",
      "train loss:0.6592226687519014\n",
      "train loss:0.7920995073837217\n",
      "train loss:0.48351683834980896\n",
      "=== epoch:6, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7526539470994837\n",
      "train loss:0.5399902501476012\n",
      "train loss:0.5123885509119994\n",
      "train loss:0.6953620822683766\n",
      "train loss:0.5016072084467399\n",
      "train loss:0.6544911410284665\n",
      "train loss:0.5955887801475583\n",
      "train loss:0.552481507538751\n",
      "train loss:0.5535500948808447\n",
      "train loss:0.4899956803891891\n",
      "train loss:0.5078191047854735\n",
      "train loss:0.5221199477036994\n",
      "train loss:0.6045317088741428\n",
      "train loss:0.46170427047728735\n",
      "train loss:0.7440668825827503\n",
      "train loss:0.7146498921213729\n",
      "train loss:0.6356018282306907\n",
      "train loss:0.402349504103535\n",
      "train loss:0.3477557469433353\n",
      "train loss:0.3814518204864289\n",
      "train loss:0.6958326027159123\n",
      "train loss:0.6775405258779238\n",
      "train loss:0.9238381867996871\n",
      "train loss:0.4918928110986111\n",
      "train loss:0.6413365482245889\n",
      "train loss:0.44913301094382\n",
      "train loss:0.46381979726782924\n",
      "train loss:0.8378901568720949\n",
      "train loss:0.48155306603868275\n",
      "train loss:0.5196799002067692\n",
      "train loss:0.7724734800702755\n",
      "train loss:0.703872356539944\n",
      "train loss:0.5055163424410123\n",
      "train loss:0.5210500803256355\n",
      "train loss:0.34516008357984135\n",
      "train loss:0.3435827517821168\n",
      "train loss:0.5584081790907813\n",
      "train loss:0.5658206061546883\n",
      "train loss:0.7361149097916397\n",
      "train loss:0.7098712586747501\n",
      "train loss:0.7569499596395641\n",
      "train loss:0.667920557322881\n",
      "train loss:0.9465015604288638\n",
      "train loss:0.6091831039408662\n",
      "train loss:0.696455830574511\n",
      "train loss:0.6193741998721528\n",
      "train loss:0.6226877626526145\n",
      "train loss:0.636425224979386\n",
      "train loss:0.6116794629521642\n",
      "train loss:0.5967098149653036\n",
      "train loss:0.6284834219925685\n",
      "train loss:0.7287257509377485\n",
      "train loss:0.6917550141305904\n",
      "train loss:0.5443275199073152\n",
      "train loss:0.6486766540621755\n",
      "train loss:0.6944030485644636\n",
      "train loss:0.5749900778933231\n",
      "train loss:0.5657858015649697\n",
      "train loss:0.6112924272053396\n",
      "train loss:0.5189356229733636\n",
      "train loss:0.44219187661025644\n",
      "train loss:0.6708101633268395\n",
      "train loss:0.5955214403546363\n",
      "train loss:0.6072337027995062\n",
      "train loss:0.7981702260685614\n",
      "train loss:0.6907309617108346\n",
      "train loss:0.7237141856903382\n",
      "train loss:0.5211983198087828\n",
      "train loss:0.47059432374466265\n",
      "train loss:0.5372078235504975\n",
      "train loss:0.500484825871292\n",
      "train loss:0.537650803007721\n",
      "train loss:0.693878191782875\n",
      "train loss:0.843044554611429\n",
      "train loss:0.632118700468628\n",
      "train loss:0.5908668936702943\n",
      "train loss:0.7350538726900735\n",
      "train loss:0.6734852024565023\n",
      "train loss:0.6421918166743235\n",
      "train loss:0.4360769257327203\n",
      "train loss:0.5371192280287393\n",
      "train loss:0.659244386055674\n",
      "train loss:0.5384550862109247\n",
      "train loss:0.5040293831315468\n",
      "train loss:0.6729463786675848\n",
      "train loss:0.6950254001563457\n",
      "train loss:0.7599966128511864\n",
      "train loss:0.37564230364997864\n",
      "train loss:0.5158354257308551\n",
      "train loss:0.7088550127460641\n",
      "train loss:0.6940465328681366\n",
      "train loss:0.8342189871768559\n",
      "train loss:0.5115628162884989\n",
      "train loss:0.6868505180832074\n",
      "train loss:0.7396796947693878\n",
      "train loss:0.4317032858990603\n",
      "train loss:0.49779947601921953\n",
      "train loss:0.7100517531079011\n",
      "train loss:0.5967627408329399\n",
      "train loss:0.5812264360851468\n",
      "train loss:0.494915080700219\n",
      "train loss:0.470705869958502\n",
      "train loss:0.5658885547722615\n",
      "train loss:0.4758463073517456\n",
      "train loss:0.7760372183623032\n",
      "train loss:0.520040998650457\n",
      "train loss:0.4234499558008199\n",
      "train loss:0.4266348022807399\n",
      "train loss:0.5201845212297479\n",
      "train loss:0.5216824991171534\n",
      "train loss:0.5489910326059675\n",
      "train loss:0.8562019307854415\n",
      "train loss:0.5935178321166854\n",
      "train loss:0.4920757585876941\n",
      "train loss:0.3882659273802468\n",
      "train loss:0.46162223090165433\n",
      "train loss:0.37209126067705284\n",
      "train loss:0.6781638748420354\n",
      "train loss:0.9763998156829746\n",
      "train loss:0.31114823137683745\n",
      "train loss:0.8807025629310556\n",
      "train loss:0.4074689461866578\n",
      "train loss:0.9795350818685463\n",
      "train loss:0.5647797555969838\n",
      "train loss:0.5050987240145829\n",
      "train loss:0.4952011943597932\n",
      "train loss:0.6417156958776905\n",
      "train loss:0.6353519355130832\n",
      "train loss:0.7689942377827103\n",
      "train loss:0.6680929876245043\n",
      "train loss:0.6021584322592568\n",
      "train loss:0.5305504186503668\n",
      "train loss:0.694084297043853\n",
      "train loss:0.6748014661392008\n",
      "train loss:0.515392697739173\n",
      "train loss:0.6044538237059678\n",
      "train loss:0.5992836404011046\n",
      "train loss:0.5567408985517863\n",
      "train loss:0.6137405424064938\n",
      "train loss:0.6016723217818148\n",
      "train loss:0.7059116553568072\n",
      "train loss:0.6941368591694842\n",
      "train loss:0.6475698279103208\n",
      "train loss:0.6418507102628015\n",
      "train loss:0.744357243115007\n",
      "train loss:0.6494664948598141\n",
      "train loss:0.6143356249708978\n",
      "train loss:0.5608496264372452\n",
      "train loss:0.7837494840907148\n",
      "train loss:0.6451252323262651\n",
      "train loss:0.49222979275263173\n",
      "train loss:0.6404298726132491\n",
      "train loss:0.5406438359317973\n",
      "=== epoch:7, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.4690756550999132\n",
      "train loss:0.5550289856381978\n",
      "train loss:0.48025534125129365\n",
      "train loss:0.7591899512627094\n",
      "train loss:0.49821816066527375\n",
      "train loss:0.5137994491888908\n",
      "train loss:0.6821437012863163\n",
      "train loss:0.6979466607951743\n",
      "train loss:0.618850954640049\n",
      "train loss:0.5695399182406533\n",
      "train loss:0.6818846850969756\n",
      "train loss:0.7457255202595167\n",
      "train loss:0.8219720819483204\n",
      "train loss:0.5076818355352609\n",
      "train loss:0.4276639666657158\n",
      "train loss:0.6622793391126021\n",
      "train loss:0.4653287530301487\n",
      "train loss:0.7013208351421677\n",
      "train loss:0.6358688648237576\n",
      "train loss:0.6248339205070245\n",
      "train loss:0.5842404069822471\n",
      "train loss:0.6580074848865842\n",
      "train loss:0.5319690613313116\n",
      "train loss:0.5978905839850186\n",
      "train loss:0.48920800181836716\n",
      "train loss:0.6071179747831645\n",
      "train loss:0.4067760380965216\n",
      "train loss:0.428111755462909\n",
      "train loss:0.5298274175052586\n",
      "train loss:0.5154454603502755\n",
      "train loss:0.7639695094129919\n",
      "train loss:0.5670947576899302\n",
      "train loss:0.4070626962954331\n",
      "train loss:0.44989000935866635\n",
      "train loss:0.4784491527168642\n",
      "train loss:0.6108046680473672\n",
      "train loss:0.3823104320752112\n",
      "train loss:0.38372654610610135\n",
      "train loss:0.6237012192070931\n",
      "train loss:0.676734664666472\n",
      "train loss:0.6508320470039697\n",
      "train loss:0.839619894868364\n",
      "train loss:0.6051922256032535\n",
      "train loss:0.613745992438712\n",
      "train loss:0.43739220558469327\n",
      "train loss:0.417633392855105\n",
      "train loss:0.6202036228759976\n",
      "train loss:0.6305760755451714\n",
      "train loss:0.5014586502140099\n",
      "train loss:0.503994443135747\n",
      "train loss:0.3730535105388986\n",
      "train loss:0.5184621076769568\n",
      "train loss:0.7161761097452871\n",
      "train loss:0.7103835213856543\n",
      "train loss:0.4105902204244199\n",
      "train loss:0.9062271290251822\n",
      "train loss:0.5297670475600603\n",
      "train loss:0.5913886927590576\n",
      "train loss:0.5160931668874718\n",
      "train loss:0.6102289842705211\n",
      "train loss:0.4009736525835346\n",
      "train loss:0.6059575771388965\n",
      "train loss:0.7898328891387394\n",
      "train loss:0.6727987819965102\n",
      "train loss:0.6791420813643586\n",
      "train loss:0.5783916272827225\n",
      "train loss:0.4998580825530564\n",
      "train loss:0.4446570996371652\n",
      "train loss:0.6184875507211813\n",
      "train loss:0.5678523998090526\n",
      "train loss:0.37672690998269537\n",
      "train loss:0.49516749135625265\n",
      "train loss:0.5501914738946916\n",
      "train loss:0.5087590856766621\n",
      "train loss:0.8083179755235209\n",
      "train loss:0.601163736670237\n",
      "train loss:0.5627523874501476\n",
      "train loss:0.5182263231661446\n",
      "train loss:0.6957353019585251\n",
      "train loss:0.43355127893894974\n",
      "train loss:0.5884666658046133\n",
      "train loss:0.6111226746481447\n",
      "train loss:0.5809470025259525\n",
      "train loss:0.7423635128080582\n",
      "train loss:0.5522453578957948\n",
      "train loss:0.42971308430880556\n",
      "train loss:0.8350544296202068\n",
      "train loss:0.5125429998508236\n",
      "train loss:0.624740715826354\n",
      "train loss:0.6050843381383404\n",
      "train loss:0.7788248602486778\n",
      "train loss:0.6106031106391899\n",
      "train loss:0.6182139461177438\n",
      "train loss:0.6407099050603187\n",
      "train loss:0.4791318714237457\n",
      "train loss:0.5171326310990716\n",
      "train loss:0.839241822672286\n",
      "train loss:0.6237370775485627\n",
      "train loss:0.6789217246454878\n",
      "train loss:0.55397465805467\n",
      "train loss:0.45858562575091144\n",
      "train loss:0.5428051698056278\n",
      "train loss:0.6985041467052036\n",
      "train loss:0.4418384402188149\n",
      "train loss:0.6583476987827404\n",
      "train loss:0.6065039174315466\n",
      "train loss:0.6993544989837854\n",
      "train loss:0.4948640687142383\n",
      "train loss:0.47153105657720384\n",
      "train loss:0.9896540827919823\n",
      "train loss:0.45305043407420414\n",
      "train loss:0.7795332289890982\n",
      "train loss:0.44420205333734375\n",
      "train loss:0.6867066108966242\n",
      "train loss:0.4948971160571302\n",
      "train loss:0.5746499232070427\n",
      "train loss:0.5391118862059514\n",
      "train loss:0.5257873912252728\n",
      "train loss:0.5855553462975746\n",
      "train loss:0.6919874787412693\n",
      "train loss:0.7081516051888644\n",
      "train loss:0.8435197102281089\n",
      "train loss:0.7729333719461915\n",
      "train loss:0.5291730454817603\n",
      "train loss:0.6089265463453394\n",
      "train loss:0.5213719397529881\n",
      "train loss:0.3809606258363777\n",
      "train loss:0.6084541252437592\n",
      "train loss:0.43834183159793894\n",
      "train loss:0.7882251083331127\n",
      "train loss:0.684262618951111\n",
      "train loss:0.52862305542755\n",
      "train loss:0.6699300476159258\n",
      "train loss:0.6413461305780273\n",
      "train loss:0.6620712642455854\n",
      "train loss:0.49884196155617155\n",
      "train loss:0.5562583698378634\n",
      "train loss:0.6311224546742479\n",
      "train loss:0.40125173164630273\n",
      "train loss:0.559500718689353\n",
      "train loss:0.5072641643863695\n",
      "train loss:0.6368974116467754\n",
      "train loss:0.5542097572283218\n",
      "train loss:0.4970047533376447\n",
      "train loss:0.5430234499780681\n",
      "train loss:0.2206160427360675\n",
      "train loss:0.4264659288785384\n",
      "train loss:0.6247242862727884\n",
      "train loss:0.7760653282844409\n",
      "train loss:0.6462741620655041\n",
      "train loss:0.5339930559787585\n",
      "train loss:0.6175142694814048\n",
      "train loss:0.5131173654975729\n",
      "=== epoch:8, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7635318236081048\n",
      "train loss:0.5961115130914217\n",
      "train loss:0.5570646749952226\n",
      "train loss:0.6259323309572439\n",
      "train loss:0.5017448556140607\n",
      "train loss:0.6244331671630302\n",
      "train loss:0.5129499056846643\n",
      "train loss:0.5565196409979962\n",
      "train loss:0.8053071993771557\n",
      "train loss:0.46788682884735167\n",
      "train loss:0.4686929478138252\n",
      "train loss:0.538949002925051\n",
      "train loss:0.5680786242057\n",
      "train loss:0.6099421979878363\n",
      "train loss:0.3124490882705809\n",
      "train loss:0.6599926672270452\n",
      "train loss:0.5397636274490777\n",
      "train loss:0.5295333402775247\n",
      "train loss:0.4803644962175288\n",
      "train loss:0.5851444315591332\n",
      "train loss:0.5833894016221335\n",
      "train loss:0.37857117784852357\n",
      "train loss:0.6259044591713092\n",
      "train loss:0.4975470482201542\n",
      "train loss:0.5047765479392922\n",
      "train loss:0.514672115975269\n",
      "train loss:0.6047811324161844\n",
      "train loss:0.545680088324709\n",
      "train loss:0.5563602627013003\n",
      "train loss:0.35808382419609563\n",
      "train loss:0.6300651345857831\n",
      "train loss:0.4753457429319514\n",
      "train loss:0.4757232626955668\n",
      "train loss:0.7165421887467646\n",
      "train loss:0.5070209129378421\n",
      "train loss:0.5973806686333465\n",
      "train loss:0.5338233596802173\n",
      "train loss:0.517116626333115\n",
      "train loss:0.7057606052167176\n",
      "train loss:0.6197262638446233\n",
      "train loss:0.44466042308963444\n",
      "train loss:0.6176538877328209\n",
      "train loss:0.6555499877775208\n",
      "train loss:0.3293902817502957\n",
      "train loss:0.5392644634969443\n",
      "train loss:0.6713333299122652\n",
      "train loss:0.7511037878748645\n",
      "train loss:0.8266096471437063\n",
      "train loss:0.7281753881741035\n",
      "train loss:0.6220133858489599\n",
      "train loss:0.5380600761141804\n",
      "train loss:0.599487549825273\n",
      "train loss:0.7499772835081422\n",
      "train loss:0.5978273604952383\n",
      "train loss:0.5736837158910226\n",
      "train loss:0.7636062857945703\n",
      "train loss:0.5751129800349628\n",
      "train loss:0.6858136801897641\n",
      "train loss:0.6562487128055811\n",
      "train loss:0.6871736299614168\n",
      "train loss:0.5402080627147249\n",
      "train loss:0.6010901745267118\n",
      "train loss:0.61441990950559\n",
      "train loss:0.6968308199256509\n",
      "train loss:0.7964564430325065\n",
      "train loss:0.6714171478192577\n",
      "train loss:0.6689319679887792\n",
      "train loss:0.6371409999682781\n",
      "train loss:0.71021477392149\n",
      "train loss:0.6446793208423053\n",
      "train loss:0.5270589639512071\n",
      "train loss:0.6084830291712138\n",
      "train loss:0.5328949253986849\n",
      "train loss:0.5830742915911071\n",
      "train loss:0.5998255307606483\n",
      "train loss:0.5776741909903598\n",
      "train loss:0.6122143432645226\n",
      "train loss:0.48208867410033623\n",
      "train loss:0.5696700799273355\n",
      "train loss:0.5980990592500883\n",
      "train loss:0.5976651142919928\n",
      "train loss:0.5786856875935363\n",
      "train loss:0.4140538313383978\n",
      "train loss:0.6340815267792517\n",
      "train loss:0.5942987262462092\n",
      "train loss:0.8074901701378234\n",
      "train loss:0.7441984221626473\n",
      "train loss:0.32772247933494114\n",
      "train loss:0.49408017810652616\n",
      "train loss:0.8666458074716905\n",
      "train loss:0.5853668221351249\n",
      "train loss:0.6482698513490843\n",
      "train loss:0.582980239780897\n",
      "train loss:0.6733209599728796\n",
      "train loss:0.4907395610446981\n",
      "train loss:0.5801669455568458\n",
      "train loss:0.6140792783720055\n",
      "train loss:0.6281044373037858\n",
      "train loss:0.3981292447958972\n",
      "train loss:0.4389887460224998\n",
      "train loss:0.6958423353202752\n",
      "train loss:0.6303319050360894\n",
      "train loss:0.5428866501543246\n",
      "train loss:0.716332383068133\n",
      "train loss:0.46218455918716794\n",
      "train loss:0.4085744264948462\n",
      "train loss:0.8053221181283498\n",
      "train loss:0.6579823515589271\n",
      "train loss:0.4516806055000564\n",
      "train loss:0.44374031713791207\n",
      "train loss:0.4448365074302109\n",
      "train loss:0.5812813865488156\n",
      "train loss:0.6755744385855508\n",
      "train loss:0.5131675021070714\n",
      "train loss:0.4789797945903862\n",
      "train loss:0.6073281767947848\n",
      "train loss:0.5057251823501145\n",
      "train loss:0.6958398732576958\n",
      "train loss:0.4947924357639832\n",
      "train loss:0.7561189435844045\n",
      "train loss:0.6830390147155629\n",
      "train loss:0.7501509221449321\n",
      "train loss:0.7860105832038629\n",
      "train loss:0.5157631009365056\n",
      "train loss:0.6769005291371102\n",
      "train loss:0.5717323643636143\n",
      "train loss:0.43488187810128914\n",
      "train loss:0.5206745483776231\n",
      "train loss:0.5349002634714823\n",
      "train loss:0.621471365324618\n",
      "train loss:0.4183478376412982\n",
      "train loss:0.6016314704251152\n",
      "train loss:0.6049703954115546\n",
      "train loss:0.5117841634284858\n",
      "train loss:0.6180722573357876\n",
      "train loss:0.4227286033050205\n",
      "train loss:0.5137165606428991\n",
      "train loss:0.640501356162605\n",
      "train loss:0.5052714602085234\n",
      "train loss:0.8530850669625739\n",
      "train loss:0.5347619271558388\n",
      "train loss:0.4795055652595163\n",
      "train loss:0.45635561122095786\n",
      "train loss:0.5848470596018768\n",
      "train loss:0.7286026465904781\n",
      "train loss:0.5136309919444462\n",
      "train loss:0.5741596364084233\n",
      "train loss:0.6673089103983375\n",
      "train loss:0.7252222670021039\n",
      "train loss:0.5536843101960526\n",
      "train loss:0.3414737995889642\n",
      "train loss:0.43814430215790556\n",
      "train loss:0.7177455994199727\n",
      "=== epoch:9, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5902660745279715\n",
      "train loss:0.3276176493518456\n",
      "train loss:0.7586227641904344\n",
      "train loss:0.35928165699227743\n",
      "train loss:0.5029678021633541\n",
      "train loss:0.9214677497683643\n",
      "train loss:0.4088441493069581\n",
      "train loss:0.5362695926893122\n",
      "train loss:0.7071596549615381\n",
      "train loss:0.49481244302307575\n",
      "train loss:1.0049168040524095\n",
      "train loss:0.732952530369624\n",
      "train loss:0.45737324894300063\n",
      "train loss:0.42302902797498276\n",
      "train loss:0.6943456657096551\n",
      "train loss:0.5951218408143\n",
      "train loss:0.5185002298291874\n",
      "train loss:0.45684980968988265\n",
      "train loss:0.5426861131005636\n",
      "train loss:0.41004803948559737\n",
      "train loss:0.5588172189698134\n",
      "train loss:0.6214901068459321\n",
      "train loss:0.7200369130577731\n",
      "train loss:0.3419162256285692\n",
      "train loss:0.4995547060563471\n",
      "train loss:0.5500992690208292\n",
      "train loss:0.736859224333012\n",
      "train loss:0.41607018072192653\n",
      "train loss:0.784758545523467\n",
      "train loss:0.6135603313483322\n",
      "train loss:0.4300995881069095\n",
      "train loss:0.7200121133251751\n",
      "train loss:0.7912658614286718\n",
      "train loss:0.7104081724706288\n",
      "train loss:0.6329931007679777\n",
      "train loss:0.47568193300435874\n",
      "train loss:0.3492439195480929\n",
      "train loss:0.4923483104902779\n",
      "train loss:0.6921755585118068\n",
      "train loss:0.4239957112327799\n",
      "train loss:0.7813722824277672\n",
      "train loss:0.8126884698664716\n",
      "train loss:0.7480564957462967\n",
      "train loss:0.6792988879424176\n",
      "train loss:0.7675251018318735\n",
      "train loss:0.6241668812410279\n",
      "train loss:0.611631556125519\n",
      "train loss:0.5359399465806659\n",
      "train loss:0.6959091253968692\n",
      "train loss:0.6357930413710979\n",
      "train loss:0.5641558800145189\n",
      "train loss:0.6707386572277455\n",
      "train loss:0.6658262886087474\n",
      "train loss:0.5307491853630943\n",
      "train loss:0.6050229644142608\n",
      "train loss:0.5524774317041793\n",
      "train loss:0.6203776547841782\n",
      "train loss:0.6945739933771923\n",
      "train loss:0.6408049296144055\n",
      "train loss:0.5508605248010701\n",
      "train loss:0.6199705661532567\n",
      "train loss:0.7035035805275015\n",
      "train loss:0.6375516233188503\n",
      "train loss:0.47762621864462396\n",
      "train loss:0.5715223359970657\n",
      "train loss:0.5280446674406966\n",
      "train loss:0.5101815213501425\n",
      "train loss:0.6628397222386122\n",
      "train loss:0.5205710463313078\n",
      "train loss:0.557295895498505\n",
      "train loss:0.4849572086955443\n",
      "train loss:0.5746597965150979\n",
      "train loss:0.47907752217850036\n",
      "train loss:0.5622474485461914\n",
      "train loss:0.59801995988801\n",
      "train loss:0.44669057271665125\n",
      "train loss:0.4430455679833189\n",
      "train loss:0.5497147429061926\n",
      "train loss:0.4766400790434181\n",
      "train loss:0.6943035901487543\n",
      "train loss:0.6812746347293273\n",
      "train loss:0.3580188064423351\n",
      "train loss:0.9090121437606069\n",
      "train loss:0.6770207149454736\n",
      "train loss:0.6565416827241332\n",
      "train loss:0.3542298591753429\n",
      "train loss:0.40524239683352903\n",
      "train loss:0.7961327563616074\n",
      "train loss:0.6878663669540762\n",
      "train loss:0.5635251466824224\n",
      "train loss:0.3749571018999528\n",
      "train loss:0.5830416352781739\n",
      "train loss:0.6931431987895117\n",
      "train loss:0.6699212812019626\n",
      "train loss:0.5890892399124131\n",
      "train loss:0.6563461449836651\n",
      "train loss:0.5196059975143779\n",
      "train loss:0.7014480789307095\n",
      "train loss:0.545742903398122\n",
      "train loss:0.4442097880389116\n",
      "train loss:0.493967579717345\n",
      "train loss:0.6032158221954325\n",
      "train loss:0.6682146683607247\n",
      "train loss:0.5670361501432291\n",
      "train loss:0.5956880344170848\n",
      "train loss:0.6322025740475655\n",
      "train loss:0.6397451281550361\n",
      "train loss:0.5033999572957675\n",
      "train loss:0.857267358861864\n",
      "train loss:0.6808708180326214\n",
      "train loss:0.5577757600337037\n",
      "train loss:0.3642812994771841\n",
      "train loss:0.4576895356185767\n",
      "train loss:0.5247205085061715\n",
      "train loss:0.7742141896362871\n",
      "train loss:0.47380699813697075\n",
      "train loss:0.3896079669609024\n",
      "train loss:0.5051924179719499\n",
      "train loss:0.3667598048708727\n",
      "train loss:0.41620267493368734\n",
      "train loss:0.49521244453816526\n",
      "train loss:0.38049688729856046\n",
      "train loss:0.3301759631744063\n",
      "train loss:0.33564076303773877\n",
      "train loss:0.6079492878641475\n",
      "train loss:0.31069928960304777\n",
      "train loss:0.807039791312334\n",
      "train loss:0.7772467895552478\n",
      "train loss:0.503596940448104\n",
      "train loss:0.6217644225448968\n",
      "train loss:0.5207857213682243\n",
      "train loss:0.38279267327860983\n",
      "train loss:0.6703770331589342\n",
      "train loss:0.6285690368558828\n",
      "train loss:0.5219425436902906\n",
      "train loss:0.4394560546387057\n",
      "train loss:0.852641735467623\n",
      "train loss:0.46665157723107586\n",
      "train loss:0.7578278636915857\n",
      "train loss:0.6217076070217809\n",
      "train loss:0.7683796182160666\n",
      "train loss:0.5097422480345882\n",
      "train loss:0.7398349782637604\n",
      "train loss:0.45640604593543965\n",
      "train loss:0.7369004153773234\n",
      "train loss:0.6334678990068382\n",
      "train loss:0.5284294029297893\n",
      "train loss:0.5507940980793911\n",
      "train loss:0.6540434583412154\n",
      "train loss:0.3548852476936232\n",
      "train loss:0.5400405888240415\n",
      "train loss:0.6358241467090752\n",
      "train loss:0.721942595401844\n",
      "=== epoch:10, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5952838952083741\n",
      "train loss:0.5902504711051282\n",
      "train loss:0.4968008240672378\n",
      "train loss:0.44369958766724754\n",
      "train loss:0.4426552022207154\n",
      "train loss:0.4134490948177512\n",
      "train loss:0.4683041463003422\n",
      "train loss:0.5898740629102263\n",
      "train loss:0.48824864080648656\n",
      "train loss:0.4404393396708053\n",
      "train loss:0.8452668433275503\n",
      "train loss:0.7299054835086678\n",
      "train loss:0.3923102957851993\n",
      "train loss:0.36770565269685973\n",
      "train loss:0.6687406480467107\n",
      "train loss:0.6333695275342596\n",
      "train loss:0.4852827952256007\n",
      "train loss:0.8215695728849963\n",
      "train loss:0.36958578019893384\n",
      "train loss:0.46643708149443885\n",
      "train loss:0.5547541402674574\n",
      "train loss:0.4210241491182535\n",
      "train loss:0.5725304804231162\n",
      "train loss:0.6696412150481007\n",
      "train loss:0.634092097488226\n",
      "train loss:0.5303976975539728\n",
      "train loss:0.514467637129876\n",
      "train loss:0.6381857781837914\n",
      "train loss:0.383718419717574\n",
      "train loss:0.4507971166486494\n",
      "train loss:0.5089897317987802\n",
      "train loss:0.5316530564607126\n",
      "train loss:0.4225875847540438\n",
      "train loss:0.47079858817355563\n",
      "train loss:0.5667593312798399\n",
      "train loss:0.41481249000811105\n",
      "train loss:0.5645770485448571\n",
      "train loss:0.21050166756802322\n",
      "train loss:0.5219361171676052\n",
      "train loss:0.4420679678486642\n",
      "train loss:0.2690965277239715\n",
      "train loss:0.6340186579557541\n",
      "train loss:0.9418643655824714\n",
      "train loss:0.5898287723922093\n",
      "train loss:0.6865123965896788\n",
      "train loss:1.0341742914289975\n",
      "train loss:0.7706983047379748\n",
      "train loss:0.5925041379351793\n",
      "train loss:0.6884966558112853\n",
      "train loss:0.7046599649056814\n",
      "train loss:0.47376761491119285\n",
      "train loss:0.48440348710661424\n",
      "train loss:0.6982504099519641\n",
      "train loss:0.6087548077257657\n",
      "train loss:0.5737232929425015\n",
      "train loss:0.6065300332510648\n",
      "train loss:0.575508146773481\n",
      "train loss:0.5481347591232055\n",
      "train loss:0.5566784539996997\n",
      "train loss:0.6703163836876999\n",
      "train loss:0.5891637181803174\n",
      "train loss:0.5369964792280929\n",
      "train loss:0.6255455805904415\n",
      "train loss:0.569343108617654\n",
      "train loss:0.49634454414488555\n",
      "train loss:0.6292968642032273\n",
      "train loss:0.6246583480967327\n",
      "train loss:0.3791539148634876\n",
      "train loss:0.7912596820702807\n",
      "train loss:0.537218071396298\n",
      "train loss:0.5701410205911075\n",
      "train loss:0.7119820816018687\n",
      "train loss:0.5664491601534153\n",
      "train loss:0.6282929186519904\n",
      "train loss:0.5863177242148782\n",
      "train loss:0.45072088374024977\n",
      "train loss:0.5315248533625437\n",
      "train loss:0.4310924273453991\n",
      "train loss:0.5026124519649299\n",
      "train loss:0.59344225548207\n",
      "train loss:0.48861620364886377\n",
      "train loss:0.2598663546074431\n",
      "train loss:0.6111996269036338\n",
      "train loss:0.24878770458872862\n",
      "train loss:0.621263105261731\n",
      "train loss:0.5676884769901764\n",
      "train loss:0.3652454007713524\n",
      "train loss:0.6750526335312015\n",
      "train loss:0.6043552071322182\n",
      "train loss:0.2440708826164863\n",
      "train loss:0.7058370166326545\n",
      "train loss:0.5034249361131891\n",
      "train loss:0.7087741455527016\n",
      "train loss:0.5238134715015134\n",
      "train loss:0.4978095654445471\n",
      "train loss:0.38371724946323277\n",
      "train loss:0.7763977603726688\n",
      "train loss:0.39010490710078677\n",
      "train loss:1.0367734959899741\n",
      "train loss:0.9487147740536118\n",
      "train loss:0.5824158530913411\n",
      "train loss:0.6528688800493844\n",
      "train loss:0.607051405786539\n",
      "train loss:0.5514699234456303\n",
      "train loss:0.6195389683701101\n",
      "train loss:0.6179632346343502\n",
      "train loss:0.47215187105988143\n",
      "train loss:0.6009958718234542\n",
      "train loss:0.5756576757362348\n",
      "train loss:0.5456291702612248\n",
      "train loss:0.6133705769448665\n",
      "train loss:0.5850168973638619\n",
      "train loss:0.5343701576838061\n",
      "train loss:0.6732968873037501\n",
      "train loss:0.4671675956852515\n",
      "train loss:0.4813568696938744\n",
      "train loss:0.5751869531220184\n",
      "train loss:0.4648993079645935\n",
      "train loss:0.705874113136943\n",
      "train loss:0.6032650799092785\n",
      "train loss:0.7653983871593669\n",
      "train loss:0.5159538735143843\n",
      "train loss:0.5560675235453025\n",
      "train loss:0.6118729475990466\n",
      "train loss:0.395472943932511\n",
      "train loss:0.6621707495789326\n",
      "train loss:0.5143634570571082\n",
      "train loss:0.6035944199986001\n",
      "train loss:0.4901016619111841\n",
      "train loss:0.40310277252251064\n",
      "train loss:0.29121959096647254\n",
      "train loss:0.6819969513969721\n",
      "train loss:0.4596946937182064\n",
      "train loss:0.4302154543410996\n",
      "train loss:0.8365441187596911\n",
      "train loss:0.9581641448518379\n",
      "train loss:0.7686049138923888\n",
      "train loss:0.588269991846958\n",
      "train loss:0.3947269513200084\n",
      "train loss:0.44682348058161636\n",
      "train loss:0.5578556447417364\n",
      "train loss:0.430069349324777\n",
      "train loss:0.5660214763670617\n",
      "train loss:0.6842618029080139\n",
      "train loss:0.45623839593079313\n",
      "train loss:0.686537322094541\n",
      "train loss:0.5017527949971942\n",
      "train loss:0.5000318134775561\n",
      "train loss:0.5968326006726696\n",
      "train loss:0.6300803956853434\n",
      "train loss:0.6890325721843846\n",
      "train loss:0.5087410802619852\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5450980392156862\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 9, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af54f9d7-561f-477d-bd52-fa9a4cf44da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6911790905644076\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6862119591150034\n",
      "train loss:0.685485367391495\n",
      "train loss:0.6866566568487992\n",
      "train loss:0.6799386859416932\n",
      "train loss:0.6153239405147104\n",
      "train loss:0.6505677435662344\n",
      "train loss:0.6808976735170668\n",
      "train loss:0.6382625442384753\n",
      "train loss:0.6718723114047063\n",
      "train loss:0.6311368060888252\n",
      "train loss:0.7512562774574695\n",
      "train loss:0.6053444623191321\n",
      "train loss:0.6270208769491107\n",
      "train loss:0.7287983904069598\n",
      "train loss:0.6636740860426523\n",
      "train loss:0.6962658227265536\n",
      "train loss:0.4197522307784426\n",
      "train loss:0.415372281763074\n",
      "train loss:0.44649365912308925\n",
      "train loss:0.5989849528247913\n",
      "train loss:0.6928272700375394\n",
      "train loss:0.5251137607121489\n",
      "train loss:0.47586585743311066\n",
      "train loss:0.5022311386375431\n",
      "train loss:0.9376745196431253\n",
      "train loss:0.5278982461190089\n",
      "train loss:0.6899388901309089\n",
      "train loss:0.5194091865926646\n",
      "train loss:0.5972306269917416\n",
      "train loss:0.6013795075147363\n",
      "train loss:0.6283295378648595\n",
      "train loss:0.6239956894022274\n",
      "train loss:0.6358466843071472\n",
      "train loss:0.46323710956613057\n",
      "train loss:0.6034327497500762\n",
      "train loss:0.7698080221825364\n",
      "train loss:0.7487495093313221\n",
      "train loss:0.5568986818625834\n",
      "train loss:0.4924243556263962\n",
      "train loss:0.6859191116897595\n",
      "train loss:0.6473502660976701\n",
      "train loss:0.666571215131143\n",
      "train loss:0.5076345653496263\n",
      "train loss:0.6364726808494152\n",
      "train loss:0.6093914034681577\n",
      "train loss:0.4773590119077952\n",
      "train loss:0.828958779106378\n",
      "train loss:0.8020412034285298\n",
      "train loss:0.5178464782076238\n",
      "train loss:0.6176882526025281\n",
      "train loss:0.5791016856458047\n",
      "train loss:0.6117356651809176\n",
      "train loss:0.6638509121038809\n",
      "train loss:0.39702224066331093\n",
      "train loss:0.5173316980382219\n",
      "train loss:0.5039252636240649\n",
      "train loss:0.6922710144112467\n",
      "train loss:0.292526038029706\n",
      "train loss:0.5329601405644169\n",
      "train loss:0.5653767302720827\n",
      "train loss:0.6574122434456161\n",
      "train loss:0.4600662511116897\n",
      "train loss:0.6499680361618615\n",
      "train loss:0.4996397952736502\n",
      "train loss:0.3498358627792869\n",
      "train loss:0.17369237603816595\n",
      "train loss:0.30590752953822686\n",
      "train loss:0.3569673311548631\n",
      "train loss:0.287381424904141\n",
      "train loss:1.1441625630001329\n",
      "train loss:0.5129855068567776\n",
      "train loss:0.8621037188561251\n",
      "train loss:0.16473419269255013\n",
      "train loss:0.7960491707698658\n",
      "train loss:0.7208178092676725\n",
      "train loss:0.6320859524474968\n",
      "train loss:0.5367120278155068\n",
      "train loss:0.7738643003674258\n",
      "train loss:0.5322561797871976\n",
      "train loss:0.6237959230274945\n",
      "train loss:0.636876778073596\n",
      "train loss:0.6662708229057211\n",
      "train loss:0.6338813502231279\n",
      "train loss:0.6641384809282574\n",
      "train loss:0.5974591890622746\n",
      "train loss:0.6288738398324332\n",
      "train loss:0.550343543682564\n",
      "train loss:0.633264789682859\n",
      "train loss:0.6740231713632742\n",
      "train loss:0.5745811221685242\n",
      "train loss:0.6757876899676543\n",
      "train loss:0.6790398700426856\n",
      "train loss:0.6352775444248688\n",
      "train loss:0.667464982140703\n",
      "train loss:0.5727759329935834\n",
      "train loss:0.6242837778123915\n",
      "train loss:0.5898618869638403\n",
      "train loss:0.5383050727948129\n",
      "train loss:0.680230139062556\n",
      "train loss:0.7067694450773743\n",
      "train loss:0.692528872089921\n",
      "train loss:0.5346633397153769\n",
      "train loss:0.6416911031616742\n",
      "train loss:0.8861928386113135\n",
      "train loss:0.5229135636297062\n",
      "train loss:0.673520225191242\n",
      "train loss:0.8400076734172343\n",
      "train loss:0.6258132215596255\n",
      "train loss:0.487618068995929\n",
      "train loss:0.5900646308251178\n",
      "train loss:0.6614113662259736\n",
      "train loss:0.6846453378237338\n",
      "train loss:0.5665739925937079\n",
      "train loss:0.6718845320403227\n",
      "train loss:0.6573539058358084\n",
      "train loss:0.6174113016991574\n",
      "train loss:0.6314166657582014\n",
      "train loss:0.7086733676790432\n",
      "train loss:0.6047395747665792\n",
      "train loss:0.6885908175198026\n",
      "train loss:0.6786169108839546\n",
      "train loss:0.5614209391663836\n",
      "train loss:0.620768135209436\n",
      "train loss:0.5362203693538057\n",
      "train loss:0.6135001448484019\n",
      "train loss:0.46290579648984737\n",
      "train loss:0.45801180448491313\n",
      "train loss:0.8027414192350516\n",
      "train loss:0.5530394075125084\n",
      "train loss:0.41959538130510127\n",
      "train loss:0.7128423819258749\n",
      "train loss:0.7094652472815913\n",
      "train loss:0.7333326493156572\n",
      "train loss:0.7483578265208204\n",
      "train loss:0.6092573252092295\n",
      "train loss:0.670024174651578\n",
      "train loss:0.5025187918535534\n",
      "train loss:0.730263303006471\n",
      "train loss:0.5435889090499493\n",
      "train loss:0.7624387594288047\n",
      "train loss:0.6807186946899246\n",
      "train loss:0.7762973416368224\n",
      "train loss:0.6583269872803985\n",
      "train loss:0.6789164080961738\n",
      "train loss:0.6261123455539834\n",
      "train loss:0.5844551414647996\n",
      "train loss:0.7324986438908092\n",
      "train loss:0.6708927714491939\n",
      "train loss:0.6299996884352372\n",
      "train loss:0.5256081937388579\n",
      "train loss:0.636573410078997\n",
      "train loss:0.7195430792516483\n",
      "train loss:0.6546186501282046\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6306082735980114\n",
      "train loss:0.5832218380523313\n",
      "train loss:0.5761290460792026\n",
      "train loss:0.5142404291373989\n",
      "train loss:0.5376524627733931\n",
      "train loss:0.7528093572075952\n",
      "train loss:0.5169872386557958\n",
      "train loss:0.6173580613404415\n",
      "train loss:0.5877506386690798\n",
      "train loss:0.8096055617263127\n",
      "train loss:0.6081940168365779\n",
      "train loss:0.3917299752323672\n",
      "train loss:0.526133630547102\n",
      "train loss:0.7125165792894425\n",
      "train loss:0.6316653725668735\n",
      "train loss:0.5303898655595402\n",
      "train loss:0.5894667213757342\n",
      "train loss:0.6742904397998415\n",
      "train loss:0.5392588178449489\n",
      "train loss:0.32324043303653893\n",
      "train loss:0.7410048612037552\n",
      "train loss:0.6846575714318799\n",
      "train loss:0.3185005489859625\n",
      "train loss:0.7001039680111916\n",
      "train loss:0.2874188154795617\n",
      "train loss:0.8591231323701525\n",
      "train loss:0.7400047099041062\n",
      "train loss:0.6207170003640494\n",
      "train loss:0.8945877264670937\n",
      "train loss:0.3731028614418205\n",
      "train loss:0.6193172419147235\n",
      "train loss:0.6760307287738544\n",
      "train loss:0.4486830330446602\n",
      "train loss:0.8400065480770074\n",
      "train loss:0.5536474964734281\n",
      "train loss:0.6047572357059925\n",
      "train loss:0.768947504794242\n",
      "train loss:0.560638093428019\n",
      "train loss:0.6018639381079042\n",
      "train loss:0.5333086655227559\n",
      "train loss:0.6658349776442896\n",
      "train loss:0.6090211161137299\n",
      "train loss:0.6127332133771042\n",
      "train loss:0.546285157647777\n",
      "train loss:0.47968520639295714\n",
      "train loss:0.7301993326812405\n",
      "train loss:0.5203856449305501\n",
      "train loss:0.36110469915736426\n",
      "train loss:0.44395200784333333\n",
      "train loss:0.43076936917658565\n",
      "train loss:0.355180741529614\n",
      "train loss:0.49071537790951425\n",
      "train loss:0.8010497409742877\n",
      "train loss:0.45594380763007053\n",
      "train loss:0.2977022602324846\n",
      "train loss:0.6962336739555494\n",
      "train loss:0.6957244149430158\n",
      "train loss:0.679934571551411\n",
      "train loss:0.7796614487665511\n",
      "train loss:0.33205168857152956\n",
      "train loss:0.7729959331436602\n",
      "train loss:0.7739670481696477\n",
      "train loss:0.6048201552993888\n",
      "train loss:0.408832707405297\n",
      "train loss:0.5204387188794961\n",
      "train loss:0.5939719087887857\n",
      "train loss:0.7002082901214968\n",
      "train loss:0.6128273962031463\n",
      "train loss:0.5371570994154619\n",
      "train loss:0.6895424028023074\n",
      "train loss:0.6949278693123585\n",
      "train loss:0.5537764542100414\n",
      "train loss:0.6324806140277527\n",
      "train loss:0.6724935580963673\n",
      "train loss:0.6286443040074954\n",
      "train loss:0.6169564249557195\n",
      "train loss:0.6198227063216382\n",
      "train loss:0.5862874503538327\n",
      "train loss:0.6356103419813988\n",
      "train loss:0.6250338558985186\n",
      "train loss:0.6266899579739504\n",
      "train loss:0.6236247502933219\n",
      "train loss:0.7411791038048261\n",
      "train loss:0.5509619615642721\n",
      "train loss:0.5655531363961634\n",
      "train loss:0.5413881675070742\n",
      "train loss:0.6135649619610686\n",
      "train loss:0.6084657070539989\n",
      "train loss:0.6787053838146936\n",
      "train loss:0.5287876498705834\n",
      "train loss:0.3964048179863907\n",
      "train loss:0.6922495061848574\n",
      "train loss:0.7074590417339194\n",
      "train loss:0.7039486878427439\n",
      "train loss:0.600889474985703\n",
      "train loss:0.5213624140937718\n",
      "train loss:0.5104695025234762\n",
      "train loss:0.599971823833563\n",
      "train loss:0.8408563816081461\n",
      "train loss:0.409146468885194\n",
      "train loss:0.4937553475506272\n",
      "train loss:0.8384491136746458\n",
      "train loss:0.706201277666383\n",
      "train loss:0.7833765900977209\n",
      "train loss:0.555787931688293\n",
      "train loss:0.6230891395866959\n",
      "train loss:0.5559319495736049\n",
      "train loss:0.6004442648789527\n",
      "train loss:0.5648745787607753\n",
      "train loss:0.4943149983166408\n",
      "train loss:0.6859739745476584\n",
      "train loss:0.6160887455007099\n",
      "train loss:0.60240059636956\n",
      "train loss:0.6089625083543481\n",
      "train loss:0.6177834994541522\n",
      "train loss:0.7842880057935936\n",
      "train loss:0.7586021268620703\n",
      "train loss:0.5234219729965546\n",
      "train loss:0.7736912401334438\n",
      "train loss:0.7543671424837037\n",
      "train loss:0.6108146553067801\n",
      "train loss:0.6323169270239128\n",
      "train loss:0.670369329458643\n",
      "train loss:0.582369479558295\n",
      "train loss:0.5178831880381718\n",
      "train loss:0.5754287371288251\n",
      "train loss:0.5590671082319459\n",
      "train loss:0.6668568966272372\n",
      "train loss:0.5745891371104932\n",
      "train loss:0.5488521149726979\n",
      "train loss:0.8236646250390539\n",
      "train loss:0.4653684653369898\n",
      "train loss:0.61537370016627\n",
      "train loss:0.35608686154413327\n",
      "train loss:0.7228769526414114\n",
      "train loss:0.708759865936126\n",
      "train loss:0.589162760012176\n",
      "train loss:0.5292804156850465\n",
      "train loss:0.7933443312611221\n",
      "train loss:0.7176112896375438\n",
      "train loss:0.42033698496718974\n",
      "train loss:0.6278654876136169\n",
      "train loss:0.6299947869511835\n",
      "train loss:0.8032940166672919\n",
      "train loss:0.6977926248006294\n",
      "train loss:0.6001331032536446\n",
      "train loss:0.6094384645758806\n",
      "train loss:0.6050678890692858\n",
      "train loss:0.6164068903829276\n",
      "train loss:0.7013145069513316\n",
      "train loss:0.4886133349927232\n",
      "train loss:0.7528837829084455\n",
      "train loss:0.5385426480852763\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.4938591352139162\n",
      "train loss:0.6200117745098267\n",
      "train loss:0.7545626164299597\n",
      "train loss:0.6920040286092488\n",
      "train loss:0.6639405791039944\n",
      "train loss:0.5594618643039315\n",
      "train loss:0.750642056260753\n",
      "train loss:0.5621129162500449\n",
      "train loss:0.46854665191272016\n",
      "train loss:0.6712612292566664\n",
      "train loss:0.7419293045272497\n",
      "train loss:0.5556592565943228\n",
      "train loss:0.619463796824744\n",
      "train loss:0.6650504220624145\n",
      "train loss:0.5285551450126403\n",
      "train loss:0.6248799602591427\n",
      "train loss:0.4662050274405841\n",
      "train loss:0.37095804956782386\n",
      "train loss:0.5957726574215015\n",
      "train loss:0.3016648637108247\n",
      "train loss:0.7640419410825791\n",
      "train loss:0.38597435249056283\n",
      "train loss:0.6335051203764293\n",
      "train loss:1.033978863260041\n",
      "train loss:0.6142591665437033\n",
      "train loss:0.6021356031542757\n",
      "train loss:0.7157459403024431\n",
      "train loss:0.43141235319601295\n",
      "train loss:0.47843239236926377\n",
      "train loss:0.7352051572492324\n",
      "train loss:0.6932934954024199\n",
      "train loss:0.7588496657613687\n",
      "train loss:0.6803531292515675\n",
      "train loss:0.7582977214089738\n",
      "train loss:0.7985424863658335\n",
      "train loss:0.723711533594482\n",
      "train loss:0.5711847190303977\n",
      "train loss:0.6215458046886837\n",
      "train loss:0.6714356157270871\n",
      "train loss:0.6669551170670043\n",
      "train loss:0.5621326151921975\n",
      "train loss:0.6029729478856265\n",
      "train loss:0.6734728389104155\n",
      "train loss:0.6401715276964399\n",
      "train loss:0.6673984074221135\n",
      "train loss:0.5990942333695572\n",
      "train loss:0.6238552236692612\n",
      "train loss:0.5338527960107038\n",
      "train loss:0.6792540108819736\n",
      "train loss:0.6202416505568704\n",
      "train loss:0.6573520728866944\n",
      "train loss:0.555268086656018\n",
      "train loss:0.5429620561657617\n",
      "train loss:0.6095410160061894\n",
      "train loss:0.614576128192853\n",
      "train loss:0.8476741276301356\n",
      "train loss:0.5258513341426821\n",
      "train loss:0.5076753516113656\n",
      "train loss:0.5982945587875161\n",
      "train loss:0.6786292298360082\n",
      "train loss:0.7152007873723938\n",
      "train loss:0.5014635192901801\n",
      "train loss:0.507520777381984\n",
      "train loss:0.7149857943761476\n",
      "train loss:0.38922348463196427\n",
      "train loss:0.4104520049866471\n",
      "train loss:0.8425447373557139\n",
      "train loss:0.6157502659326771\n",
      "train loss:0.8878122465433751\n",
      "train loss:0.502047505504836\n",
      "train loss:0.7167931493583214\n",
      "train loss:0.5121747241825061\n",
      "train loss:0.5134340017836234\n",
      "train loss:0.5239855873816802\n",
      "train loss:0.683112468636874\n",
      "train loss:0.7853695790149049\n",
      "train loss:0.6036962283694656\n",
      "train loss:0.6726986184211732\n",
      "train loss:0.5299309344036142\n",
      "train loss:0.5259635905879134\n",
      "train loss:0.6211497438198074\n",
      "train loss:0.6185069254393192\n",
      "train loss:0.4685337642696769\n",
      "train loss:0.4584400644230427\n",
      "train loss:0.3454386181631023\n",
      "train loss:0.4221174754686462\n",
      "train loss:0.5835889810015402\n",
      "train loss:0.3827738765418072\n",
      "train loss:0.4849937625371064\n",
      "train loss:0.6403867270072917\n",
      "train loss:0.36777823526334685\n",
      "train loss:0.6935484022389758\n",
      "train loss:0.46687559201549683\n",
      "train loss:0.9050385131355284\n",
      "train loss:0.34216925999700165\n",
      "train loss:0.5470490795287004\n",
      "train loss:0.8511148735300592\n",
      "train loss:0.6495007308582414\n",
      "train loss:0.5135067487507838\n",
      "train loss:0.7025123826650642\n",
      "train loss:0.8954029835426678\n",
      "train loss:0.7053906051791262\n",
      "train loss:0.5333507803024508\n",
      "train loss:0.5340826337520269\n",
      "train loss:0.561060019162053\n",
      "train loss:0.5747196561886341\n",
      "train loss:0.6174587340614245\n",
      "train loss:0.6235545230402955\n",
      "train loss:0.6811713386306654\n",
      "train loss:0.675732192139764\n",
      "train loss:0.7166073616600155\n",
      "train loss:0.5756175391037057\n",
      "train loss:0.6651980112034\n",
      "train loss:0.5777825314840771\n",
      "train loss:0.6743069288064445\n",
      "train loss:0.5280544827427975\n",
      "train loss:0.6197505969239705\n",
      "train loss:0.8267852213393754\n",
      "train loss:0.62194823398304\n",
      "train loss:0.667149055746307\n",
      "train loss:0.6223253840069065\n",
      "train loss:0.761811448157024\n",
      "train loss:0.5738589650786114\n",
      "train loss:0.4669963201228461\n",
      "train loss:0.6196291594387213\n",
      "train loss:0.561797687260156\n",
      "train loss:0.5510173209129526\n",
      "train loss:0.6226243823804706\n",
      "train loss:0.5495445191367246\n",
      "train loss:0.6804705044963274\n",
      "train loss:0.5997514066831539\n",
      "train loss:0.4171006274067078\n",
      "train loss:0.5216435782407864\n",
      "train loss:0.6171529117831003\n",
      "train loss:0.7201228075171666\n",
      "train loss:0.7369258285333157\n",
      "train loss:1.010992512122407\n",
      "train loss:0.4886231139623452\n",
      "train loss:0.6728468801514094\n",
      "train loss:0.6126997066408859\n",
      "train loss:0.3364492235051197\n",
      "train loss:0.531914078103702\n",
      "train loss:0.5623991751591954\n",
      "train loss:0.4172646792871988\n",
      "train loss:0.6236311414702246\n",
      "train loss:0.5262700578448624\n",
      "train loss:0.42113911854560193\n",
      "train loss:0.5060514626561436\n",
      "train loss:0.6254207749226753\n",
      "train loss:0.4676942852083812\n",
      "train loss:0.5327536479400403\n",
      "train loss:0.4083582580314376\n",
      "train loss:0.621377342264026\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5004771533788098\n",
      "train loss:0.7686589852808148\n",
      "train loss:0.8998309760513766\n",
      "train loss:0.7144580186886877\n",
      "train loss:0.7407623866481312\n",
      "train loss:0.5071720290171956\n",
      "train loss:0.4366352001789099\n",
      "train loss:0.52736822566299\n",
      "train loss:0.5246807933060806\n",
      "train loss:0.4392756129395554\n",
      "train loss:0.6192137631675058\n",
      "train loss:0.8348031000401777\n",
      "train loss:0.6077160476783783\n",
      "train loss:0.6865146715399101\n",
      "train loss:0.5485928080309723\n",
      "train loss:0.5263703887501613\n",
      "train loss:0.6706941056568977\n",
      "train loss:0.5389083148624402\n",
      "train loss:0.6121656009758971\n",
      "train loss:0.6670867514000582\n",
      "train loss:0.6763306115061657\n",
      "train loss:0.5579623694423128\n",
      "train loss:0.6010257321558214\n",
      "train loss:0.6058541601764901\n",
      "train loss:0.6078134197666964\n",
      "train loss:0.685944788441953\n",
      "train loss:0.6036690326851313\n",
      "train loss:0.8355222447733546\n",
      "train loss:0.6774879339091119\n",
      "train loss:0.47804897663084533\n",
      "train loss:0.6862805446032096\n",
      "train loss:0.5983229560715824\n",
      "train loss:0.39327800693107295\n",
      "train loss:0.4512201487744605\n",
      "train loss:0.4460478391403343\n",
      "train loss:0.5235856051418138\n",
      "train loss:0.5028582974862862\n",
      "train loss:0.5035851865444736\n",
      "train loss:0.7300079947795026\n",
      "train loss:0.3795570945117421\n",
      "train loss:0.5094580872959371\n",
      "train loss:0.6017054276607199\n",
      "train loss:0.4551940912854384\n",
      "train loss:0.5053441129607525\n",
      "train loss:0.38096672581902014\n",
      "train loss:1.0572291905140514\n",
      "train loss:1.1917007325697044\n",
      "train loss:0.6014062983533263\n",
      "train loss:0.8677620489545539\n",
      "train loss:0.49704032910648105\n",
      "train loss:0.3264160647017395\n",
      "train loss:0.4354397679586538\n",
      "train loss:0.6071046444347832\n",
      "train loss:0.5944619972161405\n",
      "train loss:0.43780043189831624\n",
      "train loss:0.5140376237150132\n",
      "train loss:0.5133858527028482\n",
      "train loss:0.6577203170413924\n",
      "train loss:0.43922332509685597\n",
      "train loss:0.4328913993073482\n",
      "train loss:0.504679935996689\n",
      "train loss:0.407549946607668\n",
      "train loss:0.5700648148890599\n",
      "train loss:0.9168326168357256\n",
      "train loss:0.4054740238430523\n",
      "train loss:0.7789909143113831\n",
      "train loss:0.5109700820650733\n",
      "train loss:0.7094601489918644\n",
      "train loss:0.8367935255268868\n",
      "train loss:0.41640787857310324\n",
      "train loss:0.6101103092549851\n",
      "train loss:0.6308605192374177\n",
      "train loss:0.5133652475763225\n",
      "train loss:0.4081420603184128\n",
      "train loss:0.5184514874124349\n",
      "train loss:0.7341881519204229\n",
      "train loss:0.4177577769219953\n",
      "train loss:0.4891779175987356\n",
      "train loss:0.4996723857875736\n",
      "train loss:0.64712771134546\n",
      "train loss:0.6049587434561603\n",
      "train loss:0.7237522198852264\n",
      "train loss:0.3065967959638821\n",
      "train loss:0.7366593692820325\n",
      "train loss:0.5022412286446218\n",
      "train loss:0.7029695824849902\n",
      "train loss:0.5143628206013691\n",
      "train loss:0.8907640092742891\n",
      "train loss:0.6275988225490643\n",
      "train loss:0.5067780425887701\n",
      "train loss:0.8832898007826879\n",
      "train loss:0.6861867776190012\n",
      "train loss:0.607984033137931\n",
      "train loss:0.5382303606987483\n",
      "train loss:0.6132637430524323\n",
      "train loss:0.6168498378620211\n",
      "train loss:0.6725663903781143\n",
      "train loss:0.749544081853044\n",
      "train loss:0.5500403274562295\n",
      "train loss:0.6120583665348113\n",
      "train loss:0.5021625745359238\n",
      "train loss:0.556109174140036\n",
      "train loss:0.5609794206711085\n",
      "train loss:0.5601299860588973\n",
      "train loss:0.5479174177506094\n",
      "train loss:0.6911581317751084\n",
      "train loss:0.5439585335627216\n",
      "train loss:0.5237425489471048\n",
      "train loss:0.4397187306468314\n",
      "train loss:0.605287089307679\n",
      "train loss:0.4390683003212398\n",
      "train loss:0.430424650924469\n",
      "train loss:0.8315657657731397\n",
      "train loss:0.7194563512664185\n",
      "train loss:0.7222211851213596\n",
      "train loss:0.7044693093925686\n",
      "train loss:0.4992145089359405\n",
      "train loss:0.5733273716764582\n",
      "train loss:0.5084416080010732\n",
      "train loss:0.5111614699610577\n",
      "train loss:0.7016240056260125\n",
      "train loss:0.30001767002381335\n",
      "train loss:0.9010364463432753\n",
      "train loss:0.7221465946341341\n",
      "train loss:0.716210456439073\n",
      "train loss:0.5981738516956093\n",
      "train loss:0.7768987323978823\n",
      "train loss:0.692259409404509\n",
      "train loss:0.45399842849092326\n",
      "train loss:0.8887032319165347\n",
      "train loss:0.5427707372943418\n",
      "train loss:0.47219564693186433\n",
      "train loss:0.4815616784093163\n",
      "train loss:0.6838355742628686\n",
      "train loss:0.6088895735025145\n",
      "train loss:0.4600027811250779\n",
      "train loss:0.6720145588656223\n",
      "train loss:0.4638073805947019\n",
      "train loss:0.6208290614491372\n",
      "train loss:0.5961524861202518\n",
      "train loss:0.6936290949403968\n",
      "train loss:0.7357415759348596\n",
      "train loss:0.8333289783375399\n",
      "train loss:0.755287127401749\n",
      "train loss:0.6761311891021304\n",
      "train loss:0.6085223429280286\n",
      "train loss:0.6806652085183664\n",
      "train loss:0.479017345486732\n",
      "train loss:0.5445914779870628\n",
      "train loss:0.7327645522201777\n",
      "train loss:0.617066024704186\n",
      "train loss:0.4880022655764128\n",
      "train loss:0.8195945698273974\n",
      "=== epoch:5, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.727004214123162\n",
      "train loss:0.7191448315375972\n",
      "train loss:0.441927627196926\n",
      "train loss:0.6663634381008136\n",
      "train loss:0.5982637808849068\n",
      "train loss:0.6182875313187731\n",
      "train loss:0.5399893570507183\n",
      "train loss:0.5298726644488908\n",
      "train loss:0.6149808971983283\n",
      "train loss:0.62744902024708\n",
      "train loss:0.45127854058665234\n",
      "train loss:0.6971437713755874\n",
      "train loss:0.4653140818186688\n",
      "train loss:0.5450218747116216\n",
      "train loss:0.6242541754795609\n",
      "train loss:0.6802174065716693\n",
      "train loss:0.6160045008265402\n",
      "train loss:0.6753756498476278\n",
      "train loss:0.5006315128414652\n",
      "train loss:0.7181327607440013\n",
      "train loss:0.6168073810394242\n",
      "train loss:0.7744484617594577\n",
      "train loss:0.5186023750238182\n",
      "train loss:0.5243937680560886\n",
      "train loss:0.6747156427139507\n",
      "train loss:0.6088946933995412\n",
      "train loss:0.6337314338607639\n",
      "train loss:0.5103063212638109\n",
      "train loss:0.5265920136272887\n",
      "train loss:0.5341302056697178\n",
      "train loss:0.706252970385002\n",
      "train loss:0.5685526871746133\n",
      "train loss:0.42349658796699574\n",
      "train loss:0.7069951091617268\n",
      "train loss:0.6096141821451712\n",
      "train loss:0.5719260814230986\n",
      "train loss:0.8719527696813362\n",
      "train loss:0.43496198493978255\n",
      "train loss:0.4274215220785071\n",
      "train loss:0.6792733079566228\n",
      "train loss:0.7103872029640403\n",
      "train loss:0.6267381730957624\n",
      "train loss:0.7073655815342889\n",
      "train loss:0.6506792852658163\n",
      "train loss:0.6010690231751452\n",
      "train loss:0.4342254998081031\n",
      "train loss:0.5338980599266768\n",
      "train loss:0.3576507202917333\n",
      "train loss:0.6007239748106353\n",
      "train loss:0.6168430198232795\n",
      "train loss:0.7976053585391426\n",
      "train loss:0.5838980912116125\n",
      "train loss:0.4932680409083824\n",
      "train loss:0.6344265711134359\n",
      "train loss:0.7176293483505698\n",
      "train loss:0.4348217943595742\n",
      "train loss:0.7707623050623954\n",
      "train loss:0.659127111017323\n",
      "train loss:0.5521539341845693\n",
      "train loss:0.5388579214303548\n",
      "train loss:0.8544624551597509\n",
      "train loss:0.6127331760152088\n",
      "train loss:0.7663667485907196\n",
      "train loss:0.5422890451581303\n",
      "train loss:0.5310332949720067\n",
      "train loss:0.6176934342064537\n",
      "train loss:0.6247106393032855\n",
      "train loss:0.5955632819479598\n",
      "train loss:0.6682210836992397\n",
      "train loss:0.6104668047453229\n",
      "train loss:0.5339868828546182\n",
      "train loss:0.5398703586224769\n",
      "train loss:0.6065278002481429\n",
      "train loss:0.455399604444493\n",
      "train loss:0.6888963015195692\n",
      "train loss:0.4622988457842753\n",
      "train loss:0.6071324157891285\n",
      "train loss:0.5367799416860694\n",
      "train loss:0.41905224404036323\n",
      "train loss:0.7064791144859945\n",
      "train loss:0.5374904005173003\n",
      "train loss:0.5050944773424184\n",
      "train loss:0.6402434007880692\n",
      "train loss:0.613618971504469\n",
      "train loss:0.8628665476827116\n",
      "train loss:0.7923504505226705\n",
      "train loss:0.7516297140815394\n",
      "train loss:0.44538963288542366\n",
      "train loss:0.6772138953228459\n",
      "train loss:0.7613480020411416\n",
      "train loss:0.47214360025554836\n",
      "train loss:0.6781275446169396\n",
      "train loss:0.7318965723551679\n",
      "train loss:0.615331693389399\n",
      "train loss:0.6585903740954722\n",
      "train loss:0.6502744843599774\n",
      "train loss:0.7239159620185586\n",
      "train loss:0.6643783769844889\n",
      "train loss:0.5393860179692573\n",
      "train loss:0.7070545861894376\n",
      "train loss:0.676478698617758\n",
      "train loss:0.6926002149726805\n",
      "train loss:0.7142221147725882\n",
      "train loss:0.5809103203380439\n",
      "train loss:0.6175867230214809\n",
      "train loss:0.556740177498673\n",
      "train loss:0.697585592549513\n",
      "train loss:0.6362931806824629\n",
      "train loss:0.6258928583276869\n",
      "train loss:0.7084290828355494\n",
      "train loss:0.6155242208917319\n",
      "train loss:0.6297074895031672\n",
      "train loss:0.7053614837306954\n",
      "train loss:0.5659514795213839\n",
      "train loss:0.5506042564869681\n",
      "train loss:0.5871628498392041\n",
      "train loss:0.7189829087020245\n",
      "train loss:0.5629236469484885\n",
      "train loss:0.5522076437964457\n",
      "train loss:0.53890980382371\n",
      "train loss:0.5249304049595772\n",
      "train loss:0.789466261808929\n",
      "train loss:0.6857764133306932\n",
      "train loss:0.5906044048868969\n",
      "train loss:0.5138248759072013\n",
      "train loss:0.669074906985226\n",
      "train loss:0.6191534869403277\n",
      "train loss:0.52687885198294\n",
      "train loss:0.6218482941125728\n",
      "train loss:0.72269387393144\n",
      "train loss:0.5164646381803129\n",
      "train loss:0.6595179867862822\n",
      "train loss:0.421503294387189\n",
      "train loss:0.527324864980663\n",
      "train loss:0.9048620160363742\n",
      "train loss:0.6921166189106535\n",
      "train loss:0.5491285858871656\n",
      "train loss:0.5763329082789028\n",
      "train loss:0.616164451682698\n",
      "train loss:0.7332197146174868\n",
      "train loss:0.6040566450702991\n",
      "train loss:0.6021627282453831\n",
      "train loss:0.6803950855062313\n",
      "train loss:0.5073516267244015\n",
      "train loss:0.6307448885532547\n",
      "train loss:0.6084346285621293\n",
      "train loss:0.4771377462597468\n",
      "train loss:0.7553593360868603\n",
      "train loss:0.5514863218040268\n",
      "train loss:0.7300020941297363\n",
      "train loss:0.6617388108935504\n",
      "train loss:0.5907258315646133\n",
      "train loss:0.8453254304221645\n",
      "=== epoch:6, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5413677510292576\n",
      "train loss:0.5461189062254602\n",
      "train loss:0.5344444160431185\n",
      "train loss:0.49660764948079733\n",
      "train loss:0.5355101213898966\n",
      "train loss:0.6658040967506355\n",
      "train loss:0.6041865608123291\n",
      "train loss:0.41554853060872343\n",
      "train loss:0.6218138478241283\n",
      "train loss:0.7011662212639156\n",
      "train loss:0.6320521709882283\n",
      "train loss:0.7074804017505519\n",
      "train loss:0.7249492931171088\n",
      "train loss:0.7699300853393434\n",
      "train loss:0.41918893583795347\n",
      "train loss:0.5268150786556752\n",
      "train loss:0.527995744162544\n",
      "train loss:0.5376253690281704\n",
      "train loss:0.6203168425311846\n",
      "train loss:0.5262109987790916\n",
      "train loss:0.9015799762004748\n",
      "train loss:0.702913462106664\n",
      "train loss:0.7034317719202325\n",
      "train loss:0.5087675515709417\n",
      "train loss:0.6949729528819086\n",
      "train loss:0.5220861327313406\n",
      "train loss:0.5882697129551666\n",
      "train loss:0.45516708996484156\n",
      "train loss:0.5348137822981615\n",
      "train loss:0.6808175171160847\n",
      "train loss:0.45162540327377537\n",
      "train loss:0.4264640071271467\n",
      "train loss:0.49009593855975203\n",
      "train loss:0.5427084655193327\n",
      "train loss:0.6132352602106667\n",
      "train loss:0.7253089902376257\n",
      "train loss:0.3967477257426628\n",
      "train loss:0.6163969708934787\n",
      "train loss:0.5770921141773429\n",
      "train loss:0.5017774482451494\n",
      "train loss:0.5006802876942282\n",
      "train loss:0.794404099299375\n",
      "train loss:0.7843230415680242\n",
      "train loss:0.6842715568178981\n",
      "train loss:0.5530204656495453\n",
      "train loss:0.7756084724409332\n",
      "train loss:0.43786001712790235\n",
      "train loss:0.5813021149563021\n",
      "train loss:0.5321541033303745\n",
      "train loss:0.5881177744293961\n",
      "train loss:0.47526500565175683\n",
      "train loss:0.4358799572652785\n",
      "train loss:0.6245653837038205\n",
      "train loss:0.7142233909369942\n",
      "train loss:0.4854011936265154\n",
      "train loss:0.5171944497960181\n",
      "train loss:0.6443034097789859\n",
      "train loss:0.7099750951831171\n",
      "train loss:0.5698082956399084\n",
      "train loss:0.7074213822906452\n",
      "train loss:0.4517278402438657\n",
      "train loss:0.2951875826508704\n",
      "train loss:0.5128712078293496\n",
      "train loss:0.7377689744185056\n",
      "train loss:0.7647945322426991\n",
      "train loss:0.4823805479538845\n",
      "train loss:0.7492676560534681\n",
      "train loss:0.537146370803923\n",
      "train loss:0.8190134246122677\n",
      "train loss:0.6626560386475739\n",
      "train loss:0.8346224510843372\n",
      "train loss:0.7988014190871742\n",
      "train loss:0.6150376959430537\n",
      "train loss:0.6139462039139687\n",
      "train loss:0.5106342175240574\n",
      "train loss:0.5560391300806521\n",
      "train loss:0.6044866471273754\n",
      "train loss:0.5478986390822563\n",
      "train loss:0.671642597509933\n",
      "train loss:0.5428074825446838\n",
      "train loss:0.7284113106177641\n",
      "train loss:0.6799660462985623\n",
      "train loss:0.5177849203429183\n",
      "train loss:0.5593247920096001\n",
      "train loss:0.6449784908907189\n",
      "train loss:0.6171948480214584\n",
      "train loss:0.4731007852385368\n",
      "train loss:0.6587961081733666\n",
      "train loss:0.5445325748532236\n",
      "train loss:0.6163381445299703\n",
      "train loss:0.7467312924000931\n",
      "train loss:0.46198928112526794\n",
      "train loss:0.6905148106253128\n",
      "train loss:0.4500687797030404\n",
      "train loss:0.5054535526773929\n",
      "train loss:0.6061445564840154\n",
      "train loss:0.6167170575062373\n",
      "train loss:0.3966462415856036\n",
      "train loss:0.7415643191306533\n",
      "train loss:0.6140798292096117\n",
      "train loss:0.5047838606458409\n",
      "train loss:0.387384427782811\n",
      "train loss:0.34825062051293065\n",
      "train loss:0.7791973342646503\n",
      "train loss:0.6179897779349626\n",
      "train loss:0.3617051949865486\n",
      "train loss:0.792302069485467\n",
      "train loss:0.5924453584211682\n",
      "train loss:0.8015186614748387\n",
      "train loss:0.7089203424924359\n",
      "train loss:0.5009429567679032\n",
      "train loss:0.565151663375546\n",
      "train loss:0.6004704495188429\n",
      "train loss:0.42773256573271706\n",
      "train loss:0.6066006111967077\n",
      "train loss:0.5846433125591106\n",
      "train loss:0.5623264909058623\n",
      "train loss:0.6778401256988282\n",
      "train loss:0.37606796728335906\n",
      "train loss:0.7032559417961404\n",
      "train loss:0.6773886044669272\n",
      "train loss:0.4463652027594661\n",
      "train loss:0.4894319659224104\n",
      "train loss:0.5883628477979074\n",
      "train loss:0.6882686835616632\n",
      "train loss:0.6003552535257272\n",
      "train loss:0.5261589710095057\n",
      "train loss:0.5311608324086486\n",
      "train loss:0.7047286171136439\n",
      "train loss:0.7444108690629239\n",
      "train loss:0.5687360526310908\n",
      "train loss:0.6311295605520321\n",
      "train loss:0.6847433589794762\n",
      "train loss:0.6807493707032777\n",
      "train loss:0.4534222443029511\n",
      "train loss:0.5119149538428976\n",
      "train loss:0.6820621230958929\n",
      "train loss:0.6217637933189749\n",
      "train loss:0.6472282540559959\n",
      "train loss:0.6505621254766202\n",
      "train loss:0.842278267288842\n",
      "train loss:0.6436273826883715\n",
      "train loss:0.6521067072716689\n",
      "train loss:0.6348958513058778\n",
      "train loss:0.603415965285248\n",
      "train loss:0.6373986546026736\n",
      "train loss:0.843444522874672\n",
      "train loss:0.607328460120468\n",
      "train loss:0.6788077376575987\n",
      "train loss:0.7054585126398355\n",
      "train loss:0.6318818111766553\n",
      "train loss:0.6545848745069429\n",
      "train loss:0.6367134357771727\n",
      "=== epoch:7, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6890181674478772\n",
      "train loss:0.7094568754088934\n",
      "train loss:0.5510966706846548\n",
      "train loss:0.6418138290416351\n",
      "train loss:0.5565793703943502\n",
      "train loss:0.47910513278988176\n",
      "train loss:0.6436416810488349\n",
      "train loss:0.648524910307452\n",
      "train loss:0.6752002739573051\n",
      "train loss:0.5893974389292361\n",
      "train loss:0.6328143193375946\n",
      "train loss:0.5554265823314793\n",
      "train loss:0.6985136738164364\n",
      "train loss:0.5422426520299356\n",
      "train loss:0.4965085442276549\n",
      "train loss:0.7431798528074082\n",
      "train loss:0.5266347513064953\n",
      "train loss:0.8074449294159054\n",
      "train loss:0.42091907049368693\n",
      "train loss:0.6213446348099914\n",
      "train loss:0.6073696097870738\n",
      "train loss:0.7087533871675241\n",
      "train loss:0.3996498496724325\n",
      "train loss:0.6556271411801677\n",
      "train loss:0.5147216660575301\n",
      "train loss:0.6792201510788769\n",
      "train loss:0.6679046964035542\n",
      "train loss:0.5049778175591567\n",
      "train loss:0.31879048215497535\n",
      "train loss:0.42688345975454045\n",
      "train loss:0.5924847301809354\n",
      "train loss:0.26824982677142406\n",
      "train loss:0.5875297741946598\n",
      "train loss:0.4523893362052875\n",
      "train loss:0.7730752920983365\n",
      "train loss:0.6287416209393948\n",
      "train loss:0.6506479861759843\n",
      "train loss:0.9392761347581317\n",
      "train loss:0.6989458053599108\n",
      "train loss:0.7176610692931316\n",
      "train loss:0.8761158950426967\n",
      "train loss:0.592117491583011\n",
      "train loss:0.665300669937444\n",
      "train loss:0.5981516706235568\n",
      "train loss:0.5407822224991744\n",
      "train loss:0.5863550927623215\n",
      "train loss:0.6279061888328449\n",
      "train loss:0.5705904953460672\n",
      "train loss:0.4950317590884345\n",
      "train loss:0.7466894766508643\n",
      "train loss:0.5331939531525197\n",
      "train loss:0.6369931121780865\n",
      "train loss:0.6085417090958282\n",
      "train loss:0.4978593604621935\n",
      "train loss:0.6131956177150324\n",
      "train loss:0.615462378398923\n",
      "train loss:0.6788997444763152\n",
      "train loss:0.5335796722797983\n",
      "train loss:0.5801526032324597\n",
      "train loss:0.7395096517202944\n",
      "train loss:0.5683965449440724\n",
      "train loss:0.6956377680760186\n",
      "train loss:0.6956643382161445\n",
      "train loss:0.39725077858646557\n",
      "train loss:0.5051542235097755\n",
      "train loss:0.49726189600465204\n",
      "train loss:0.5110441217876303\n",
      "train loss:0.6209363356174703\n",
      "train loss:0.7953617810006859\n",
      "train loss:0.6187940190672714\n",
      "train loss:0.5897262302381112\n",
      "train loss:0.5200746433069345\n",
      "train loss:0.36647110445582604\n",
      "train loss:0.3841204710955209\n",
      "train loss:0.24273965007032702\n",
      "train loss:0.31778971635436254\n",
      "train loss:0.7486098823649551\n",
      "train loss:1.085780344966063\n",
      "train loss:0.7338763856477918\n",
      "train loss:0.5386546689392695\n",
      "train loss:0.6022514493764616\n",
      "train loss:0.5659818881503178\n",
      "train loss:0.7784387651659046\n",
      "train loss:0.5277992599365428\n",
      "train loss:0.5397945785210743\n",
      "train loss:0.5916169241431476\n",
      "train loss:0.511489459313321\n",
      "train loss:0.5311040738119776\n",
      "train loss:0.6742900083858976\n",
      "train loss:0.5468767079889117\n",
      "train loss:0.5864882737030837\n",
      "train loss:0.4993156531499078\n",
      "train loss:0.5523460723139724\n",
      "train loss:0.5769527066963851\n",
      "train loss:0.53283440403228\n",
      "train loss:0.5234595007890566\n",
      "train loss:0.5902307635708491\n",
      "train loss:0.5945865027121775\n",
      "train loss:0.601433553366131\n",
      "train loss:0.7734698596889595\n",
      "train loss:0.505209400163707\n",
      "train loss:0.49118954042058743\n",
      "train loss:0.3712642812982522\n",
      "train loss:0.3781370227151425\n",
      "train loss:0.278548842903439\n",
      "train loss:0.7035436100691744\n",
      "train loss:0.7566641031408594\n",
      "train loss:0.6400335279676004\n",
      "train loss:0.28166195964793256\n",
      "train loss:0.26409805282863985\n",
      "train loss:0.536145888733804\n",
      "train loss:0.37249455884491633\n",
      "train loss:0.3279930823424095\n",
      "train loss:0.5720718273165034\n",
      "train loss:0.7092805770842593\n",
      "train loss:0.33407850977033104\n",
      "train loss:0.5006308438653692\n",
      "train loss:0.2882068027993452\n",
      "train loss:0.32222948954074176\n",
      "train loss:0.6371509454422188\n",
      "train loss:0.9595501880792409\n",
      "train loss:0.6520985048674347\n",
      "train loss:0.5659390274764502\n",
      "train loss:0.7210109543379091\n",
      "train loss:0.5514163344327427\n",
      "train loss:0.7246852890490032\n",
      "train loss:0.5344685482666036\n",
      "train loss:0.5190213431726367\n",
      "train loss:0.6904370191897472\n",
      "train loss:0.48461147372968433\n",
      "train loss:0.4414056575667728\n",
      "train loss:0.4904743874161294\n",
      "train loss:0.6245228727350794\n",
      "train loss:0.7197345245292714\n",
      "train loss:0.6267213801832012\n",
      "train loss:0.42117961309479623\n",
      "train loss:0.6949333235776647\n",
      "train loss:0.791141459167384\n",
      "train loss:0.6121940864900743\n",
      "train loss:0.48591768054299045\n",
      "train loss:0.7794130936819957\n",
      "train loss:0.6061657765370356\n",
      "train loss:0.5966044228123625\n",
      "train loss:0.7527380586276746\n",
      "train loss:0.5636475146355077\n",
      "train loss:0.611718557299723\n",
      "train loss:0.4510682786666969\n",
      "train loss:0.5320415046060175\n",
      "train loss:0.5312346525774527\n",
      "train loss:0.693554418737271\n",
      "train loss:0.691203048387908\n",
      "train loss:0.7169902973095873\n",
      "train loss:0.4870979303941015\n",
      "=== epoch:8, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6237344950724155\n",
      "train loss:0.4423604802087864\n",
      "train loss:0.517328681838271\n",
      "train loss:0.771894009442485\n",
      "train loss:0.43214521345583057\n",
      "train loss:0.64525689729861\n",
      "train loss:0.38417182554218265\n",
      "train loss:0.9074597572124528\n",
      "train loss:0.7432559827687086\n",
      "train loss:0.5152220834991017\n",
      "train loss:0.5432916591819861\n",
      "train loss:0.5215004001334036\n",
      "train loss:0.7185090942048027\n",
      "train loss:0.6143878936945123\n",
      "train loss:0.5752186223577685\n",
      "train loss:0.5065572174068919\n",
      "train loss:0.4102663520459635\n",
      "train loss:0.6377111629937886\n",
      "train loss:0.6777839071677522\n",
      "train loss:0.5246548845064073\n",
      "train loss:0.29772520491126553\n",
      "train loss:0.7301842024813862\n",
      "train loss:0.6423213745203683\n",
      "train loss:0.9084513940735593\n",
      "train loss:0.7469036374450839\n",
      "train loss:0.6049415797986267\n",
      "train loss:0.578793208383901\n",
      "train loss:0.527738494700895\n",
      "train loss:0.7466369757064942\n",
      "train loss:0.5901500841391285\n",
      "train loss:0.6690166377768836\n",
      "train loss:0.7949537194085522\n",
      "train loss:0.4910192003135506\n",
      "train loss:0.5816919412332937\n",
      "train loss:0.6643808741580128\n",
      "train loss:0.5213184031242001\n",
      "train loss:0.598270386411265\n",
      "train loss:0.5792499992694755\n",
      "train loss:0.7508996104548586\n",
      "train loss:0.5332492490872086\n",
      "train loss:0.5516040141283336\n",
      "train loss:0.6889358493674838\n",
      "train loss:0.5123163426537926\n",
      "train loss:0.6199369311893783\n",
      "train loss:0.5842944077572234\n",
      "train loss:0.8585571142762497\n",
      "train loss:0.7397373800860974\n",
      "train loss:0.666110512833751\n",
      "train loss:0.6770249030415985\n",
      "train loss:0.6749201802593123\n",
      "train loss:0.5229036020580375\n",
      "train loss:0.699216298741283\n",
      "train loss:0.6046986176131475\n",
      "train loss:0.4356529863112211\n",
      "train loss:0.5364429892951508\n",
      "train loss:0.4788862621893723\n",
      "train loss:0.8037701527236578\n",
      "train loss:0.7843549767904955\n",
      "train loss:0.5329130958784897\n",
      "train loss:0.5906471329553341\n",
      "train loss:0.6323845991927562\n",
      "train loss:0.6152029461630203\n",
      "train loss:0.6175523916608769\n",
      "train loss:0.5729520223477294\n",
      "train loss:0.7874239479191709\n",
      "train loss:0.6835840273989109\n",
      "train loss:0.7202740160394345\n",
      "train loss:0.6165354000218684\n",
      "train loss:0.6164435914917525\n",
      "train loss:0.5840043704681768\n",
      "train loss:0.5055100348633117\n",
      "train loss:0.42064513187398483\n",
      "train loss:0.6067331301936056\n",
      "train loss:0.5590334183093872\n",
      "train loss:0.573292184729379\n",
      "train loss:0.838408273690835\n",
      "train loss:0.7705371997022792\n",
      "train loss:0.6102337741386319\n",
      "train loss:0.49370061287141\n",
      "train loss:0.34662602304405243\n",
      "train loss:0.6635892178454882\n",
      "train loss:0.4937241201478356\n",
      "train loss:0.8467417416060117\n",
      "train loss:0.6098284047396892\n",
      "train loss:0.4791437625660738\n",
      "train loss:0.5467803000596568\n",
      "train loss:0.6375897903038725\n",
      "train loss:0.6321260927724194\n",
      "train loss:0.6153089520601448\n",
      "train loss:0.49610540192418207\n",
      "train loss:0.7081466398108528\n",
      "train loss:0.6750050863542613\n",
      "train loss:0.5359827893694022\n",
      "train loss:0.4840943697721841\n",
      "train loss:0.6193298741246911\n",
      "train loss:0.6131501678716301\n",
      "train loss:0.551970864266336\n",
      "train loss:0.6704684111369834\n",
      "train loss:0.6133032408695755\n",
      "train loss:0.47964683824056725\n",
      "train loss:0.44285540807992624\n",
      "train loss:0.5374345270506995\n",
      "train loss:0.4234329991808514\n",
      "train loss:0.38064297007310344\n",
      "train loss:0.8159223357846509\n",
      "train loss:0.4659726377938502\n",
      "train loss:0.3965898825877159\n",
      "train loss:0.6038590739913763\n",
      "train loss:0.26291472608985855\n",
      "train loss:0.7813426142775569\n",
      "train loss:0.2998212908466268\n",
      "train loss:0.7519888990336616\n",
      "train loss:0.7512830259135541\n",
      "train loss:0.49084752380376173\n",
      "train loss:0.6512161851420366\n",
      "train loss:0.6908192991577509\n",
      "train loss:0.6228325147563238\n",
      "train loss:0.5954432479750336\n",
      "train loss:0.9085965635376837\n",
      "train loss:0.5186400499584397\n",
      "train loss:0.5420266428793876\n",
      "train loss:0.524891049851969\n",
      "train loss:0.4744992288156104\n",
      "train loss:0.52664531425594\n",
      "train loss:0.6614209156272404\n",
      "train loss:0.5484063206430814\n",
      "train loss:0.5111545799992132\n",
      "train loss:0.36317306330287835\n",
      "train loss:0.4539281806225118\n",
      "train loss:0.5993113895665223\n",
      "train loss:0.5976450136694917\n",
      "train loss:0.38764970285545275\n",
      "train loss:0.5002627407138158\n",
      "train loss:0.5488019630524354\n",
      "train loss:0.8111226987018124\n",
      "train loss:0.6355448757502755\n",
      "train loss:0.6555346267209028\n",
      "train loss:0.37477999598259715\n",
      "train loss:0.7565986348286075\n",
      "train loss:0.7322059083242316\n",
      "train loss:0.4257156397485439\n",
      "train loss:0.3223465821763382\n",
      "train loss:0.5436267673907749\n",
      "train loss:0.3533749127734194\n",
      "train loss:0.5930682436529507\n",
      "train loss:0.5057204274657219\n",
      "train loss:0.5093903933515563\n",
      "train loss:0.4224468115650618\n",
      "train loss:0.5966254581197497\n",
      "train loss:0.5995081248578551\n",
      "train loss:0.47257943094682175\n",
      "train loss:0.34965205143779293\n",
      "train loss:0.3436555715839057\n",
      "=== epoch:9, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7570449515434485\n",
      "train loss:0.5591849749844957\n",
      "train loss:0.32555891483988175\n",
      "train loss:0.7545784730358764\n",
      "train loss:0.8151895780169303\n",
      "train loss:0.35090591631582313\n",
      "train loss:0.5849870122684901\n",
      "train loss:0.7749225012971648\n",
      "train loss:0.5623424167602789\n",
      "train loss:0.5879346349142811\n",
      "train loss:0.4035650932990123\n",
      "train loss:0.7321355630540503\n",
      "train loss:0.5339691062968113\n",
      "train loss:0.45524373833360665\n",
      "train loss:0.5185567213731129\n",
      "train loss:0.6981399272420686\n",
      "train loss:0.5006890606080429\n",
      "train loss:0.8960328209447725\n",
      "train loss:0.5300297555042173\n",
      "train loss:0.5501770000346724\n",
      "train loss:0.6826230392276412\n",
      "train loss:0.5749822322025671\n",
      "train loss:0.5584470249825962\n",
      "train loss:0.5699531703805417\n",
      "train loss:0.4976036991730008\n",
      "train loss:0.6910270568510322\n",
      "train loss:0.5918019271453397\n",
      "train loss:0.6213115390366459\n",
      "train loss:0.6411473982946492\n",
      "train loss:0.5161455134557877\n",
      "train loss:0.9719903946139565\n",
      "train loss:0.5611710757461442\n",
      "train loss:0.6205983679429156\n",
      "train loss:0.7211184154513978\n",
      "train loss:0.7329269101911514\n",
      "train loss:0.6861423946568096\n",
      "train loss:0.7368686760556129\n",
      "train loss:0.599476808131953\n",
      "train loss:0.5360846296989498\n",
      "train loss:0.6150775650747274\n",
      "train loss:0.6179335928877172\n",
      "train loss:0.6150370398003859\n",
      "train loss:0.5843592229106099\n",
      "train loss:0.5752212595257805\n",
      "train loss:0.5406279357272309\n",
      "train loss:0.4840092106608072\n",
      "train loss:0.6597924412305264\n",
      "train loss:0.7138384088957089\n",
      "train loss:0.773722746384084\n",
      "train loss:0.602589122492059\n",
      "train loss:0.6207959714883773\n",
      "train loss:0.5553669479665062\n",
      "train loss:0.6719259286905199\n",
      "train loss:0.5337729488050935\n",
      "train loss:0.6863371520653875\n",
      "train loss:0.5980542695413693\n",
      "train loss:0.6624511567506843\n",
      "train loss:0.6176143796515885\n",
      "train loss:0.583718674392699\n",
      "train loss:0.5480075966382087\n",
      "train loss:0.7302502877255729\n",
      "train loss:0.6745554128577818\n",
      "train loss:0.42846235513202374\n",
      "train loss:0.5073343486414175\n",
      "train loss:0.6561237433315569\n",
      "train loss:0.5813773056557439\n",
      "train loss:0.6084158952220332\n",
      "train loss:0.616279032638721\n",
      "train loss:0.42259061651234403\n",
      "train loss:0.3886116708218531\n",
      "train loss:0.5820930540134949\n",
      "train loss:0.3697568343931497\n",
      "train loss:0.4915058330406995\n",
      "train loss:0.8819790962453251\n",
      "train loss:0.21704919953897464\n",
      "train loss:0.3263667975409061\n",
      "train loss:0.7157187603757909\n",
      "train loss:0.20542234639804416\n",
      "train loss:0.8939681268605766\n",
      "train loss:0.44572059825630983\n",
      "train loss:0.2964622448418629\n",
      "train loss:0.7101877349199139\n",
      "train loss:0.5451780800483533\n",
      "train loss:0.4199327588498509\n",
      "train loss:0.4873542595248882\n",
      "train loss:0.47472585770120485\n",
      "train loss:0.4313734964884513\n",
      "train loss:0.7449751326641814\n",
      "train loss:0.392658098455271\n",
      "train loss:0.5015323851760093\n",
      "train loss:0.7667142228842609\n",
      "train loss:0.2961078487462985\n",
      "train loss:0.24822024306392593\n",
      "train loss:0.38204897783558045\n",
      "train loss:0.7309051966500442\n",
      "train loss:0.5239536086618976\n",
      "train loss:0.4119759643463735\n",
      "train loss:0.413397589924885\n",
      "train loss:0.3600459449837757\n",
      "train loss:0.5277761492289362\n",
      "train loss:0.5302064447040736\n",
      "train loss:0.8110080426694859\n",
      "train loss:0.19879196521667936\n",
      "train loss:0.7816008051248571\n",
      "train loss:0.6236800851467157\n",
      "train loss:0.48874981054285865\n",
      "train loss:0.6766091568950919\n",
      "train loss:0.6247386742669784\n",
      "train loss:0.47836564955374544\n",
      "train loss:0.5307593780987889\n",
      "train loss:0.6152559629969966\n",
      "train loss:0.6047701949302552\n",
      "train loss:0.6575399256064928\n",
      "train loss:0.7044223194617232\n",
      "train loss:0.5421484632319669\n",
      "train loss:0.8414189814935196\n",
      "train loss:0.4666441184580802\n",
      "train loss:0.630785495866588\n",
      "train loss:0.5226158449020227\n",
      "train loss:0.5981280699159225\n",
      "train loss:0.5625285979136033\n",
      "train loss:0.6420332238869991\n",
      "train loss:0.5718747246077911\n",
      "train loss:0.6232884608356529\n",
      "train loss:0.6708357360549196\n",
      "train loss:0.694867301279269\n",
      "train loss:0.7506248052653811\n",
      "train loss:0.6124739527464608\n",
      "train loss:0.4667511548010886\n",
      "train loss:0.7435013844329564\n",
      "train loss:0.6211101294886139\n",
      "train loss:0.5575552814309768\n",
      "train loss:0.6058448299637328\n",
      "train loss:0.6742603606667539\n",
      "train loss:0.39286917201175886\n",
      "train loss:0.6515742314159259\n",
      "train loss:0.6009520698631975\n",
      "train loss:0.6666169958668378\n",
      "train loss:0.4994579792911041\n",
      "train loss:0.45993402126326516\n",
      "train loss:0.49171835249311224\n",
      "train loss:0.6826906893064109\n",
      "train loss:0.5151557013264052\n",
      "train loss:0.4307157084615737\n",
      "train loss:0.5046745117939736\n",
      "train loss:0.4158265404052691\n",
      "train loss:0.5926561664542678\n",
      "train loss:0.7715197205747992\n",
      "train loss:0.7077454376485619\n",
      "train loss:0.4229606168233282\n",
      "train loss:0.6815075193984865\n",
      "train loss:0.5964185643216251\n",
      "train loss:0.5543072487836669\n",
      "=== epoch:10, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.3944743588909495\n",
      "train loss:0.5804106172263671\n",
      "train loss:0.6098751036423729\n",
      "train loss:0.5790151382137091\n",
      "train loss:0.42624666419198115\n",
      "train loss:0.34412838530023243\n",
      "train loss:0.2557611684414885\n",
      "train loss:0.7431940285777403\n",
      "train loss:0.9614489279844681\n",
      "train loss:0.4882760376566918\n",
      "train loss:0.47139265446773404\n",
      "train loss:0.7371058548980401\n",
      "train loss:0.4808499617259622\n",
      "train loss:0.509901680117818\n",
      "train loss:0.5538318923154174\n",
      "train loss:0.5566150986422891\n",
      "train loss:0.38429920782868193\n",
      "train loss:0.6108790597512985\n",
      "train loss:0.5507039087432295\n",
      "train loss:0.43105389240146363\n",
      "train loss:0.7017324596346893\n",
      "train loss:0.3180123998812793\n",
      "train loss:0.38127134843535543\n",
      "train loss:0.3861322399147973\n",
      "train loss:0.49828094069412254\n",
      "train loss:0.4447671586924303\n",
      "train loss:0.3812390924118441\n",
      "train loss:0.6246233404120922\n",
      "train loss:0.7096093210480918\n",
      "train loss:0.5237467202356556\n",
      "train loss:0.600518217361466\n",
      "train loss:0.21417388943866103\n",
      "train loss:0.9588458178015353\n",
      "train loss:0.6273027024076001\n",
      "train loss:0.38924492207625694\n",
      "train loss:0.5265605214817592\n",
      "train loss:0.7357454736880176\n",
      "train loss:0.6677610908705132\n",
      "train loss:0.6085526099389502\n",
      "train loss:0.6674083088106088\n",
      "train loss:0.4964695991501794\n",
      "train loss:0.5599816243152598\n",
      "train loss:0.644857488937534\n",
      "train loss:0.6032461801005675\n",
      "train loss:0.6924582455801357\n",
      "train loss:0.7452970101814314\n",
      "train loss:0.6219075856152799\n",
      "train loss:0.4840164576595095\n",
      "train loss:0.6401447117690682\n",
      "train loss:0.58640425874552\n",
      "train loss:0.6840568548366329\n",
      "train loss:0.574848444905087\n",
      "train loss:0.6275420292791849\n",
      "train loss:0.5600536906393448\n",
      "train loss:0.6226749282017463\n",
      "train loss:0.476390454907157\n",
      "train loss:0.5658990294777462\n",
      "train loss:0.73272887269948\n",
      "train loss:0.4736240918626202\n",
      "train loss:0.6655983645115334\n",
      "train loss:0.7483146562142375\n",
      "train loss:0.7016227453611725\n",
      "train loss:0.6411174526655768\n",
      "train loss:0.5814929912067187\n",
      "train loss:0.5872581814175172\n",
      "train loss:0.5957324694318181\n",
      "train loss:0.509078994471938\n",
      "train loss:0.5620409343385099\n",
      "train loss:0.6200089272031271\n",
      "train loss:0.777929340905849\n",
      "train loss:0.5903358975008184\n",
      "train loss:0.5341033680901065\n",
      "train loss:0.4264577530270814\n",
      "train loss:0.4409081411026288\n",
      "train loss:0.4394245049820723\n",
      "train loss:0.4540649083425049\n",
      "train loss:0.46853064168876307\n",
      "train loss:0.8946600483058409\n",
      "train loss:0.4172041377690272\n",
      "train loss:0.347564609723897\n",
      "train loss:0.6096967634925535\n",
      "train loss:0.7357185603283073\n",
      "train loss:0.35576200889404014\n",
      "train loss:0.7264613003644835\n",
      "train loss:0.8173454104831297\n",
      "train loss:0.5850598536579729\n",
      "train loss:0.3081700663005532\n",
      "train loss:0.5635704769597373\n",
      "train loss:0.5686214565131295\n",
      "train loss:0.9415738617077773\n",
      "train loss:0.5075622056771043\n",
      "train loss:0.4960173121764814\n",
      "train loss:0.5350131106398205\n",
      "train loss:0.5578873382074641\n",
      "train loss:0.5092842031079802\n",
      "train loss:0.5202230744506476\n",
      "train loss:0.4667376023774481\n",
      "train loss:0.49273123511474354\n",
      "train loss:0.510561630116698\n",
      "train loss:0.5997533156401932\n",
      "train loss:0.40135776203882323\n",
      "train loss:0.5078038728956747\n",
      "train loss:0.5613211922687216\n",
      "train loss:0.3369929683823344\n",
      "train loss:0.5405161282556054\n",
      "train loss:0.5517581365615327\n",
      "train loss:0.7811710566511134\n",
      "train loss:0.7460379535301805\n",
      "train loss:0.8595632641557458\n",
      "train loss:0.5983015649444032\n",
      "train loss:0.6259494617588733\n",
      "train loss:0.44264051461889214\n",
      "train loss:0.5052663994579772\n",
      "train loss:0.5598552885675951\n",
      "train loss:0.40575491991403567\n",
      "train loss:0.5357117647768107\n",
      "train loss:0.706777150857063\n",
      "train loss:0.49473579215673213\n",
      "train loss:0.5991791280395766\n",
      "train loss:0.41154013741532547\n",
      "train loss:0.42009932959004753\n",
      "train loss:0.45349943977645146\n",
      "train loss:0.43170632722387675\n",
      "train loss:0.3515741968950929\n",
      "train loss:0.8802124873595556\n",
      "train loss:0.5733171626988957\n",
      "train loss:0.6272434855460046\n",
      "train loss:0.4438651452206052\n",
      "train loss:0.5254935498164865\n",
      "train loss:0.5021340433332728\n",
      "train loss:0.5885612142168165\n",
      "train loss:0.42171474981355717\n",
      "train loss:0.4824555032948389\n",
      "train loss:0.5418946225231199\n",
      "train loss:0.8748676455729198\n",
      "train loss:0.5114778203777857\n",
      "train loss:0.509603333445971\n",
      "train loss:0.5128817039853337\n",
      "train loss:0.7494316455029919\n",
      "train loss:0.6023117266365394\n",
      "train loss:0.4220782220486356\n",
      "train loss:0.9212351026451667\n",
      "train loss:0.529504027873217\n",
      "train loss:0.5295242623891805\n",
      "train loss:0.560916901215078\n",
      "train loss:0.42732718649443735\n",
      "train loss:0.5291554634816805\n",
      "train loss:0.6158661211137334\n",
      "train loss:0.4648344162295611\n",
      "train loss:0.5123520056708184\n",
      "train loss:0.6991186835566816\n",
      "train loss:0.5667612653812173\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5490196078431373\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 11, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b4b90c6-f4f9-439c-a450-8394779a0e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6928510859104142\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6925172803634356\n",
      "train loss:0.6907468827427171\n",
      "train loss:0.6868809766851184\n",
      "train loss:0.6838392521143701\n",
      "train loss:0.6785353461274463\n",
      "train loss:0.681205216746674\n",
      "train loss:0.6741186135560235\n",
      "train loss:0.6653251296043586\n",
      "train loss:0.6109203954727258\n",
      "train loss:0.677804737613181\n",
      "train loss:0.6599351223474025\n",
      "train loss:0.5841992096592434\n",
      "train loss:0.756185082696882\n",
      "train loss:0.5683976200699709\n",
      "train loss:0.6862406893447898\n",
      "train loss:0.621512027927779\n",
      "train loss:0.46593635701721875\n",
      "train loss:0.5455247971029709\n",
      "train loss:0.41751109456403945\n",
      "train loss:0.6039450841106018\n",
      "train loss:0.2231483093208213\n",
      "train loss:0.6476002128821756\n",
      "train loss:0.5974805157937291\n",
      "train loss:0.5371490851053946\n",
      "train loss:0.5497884626728962\n",
      "train loss:0.5351897078438173\n",
      "train loss:0.31282555173231674\n",
      "train loss:0.7212114957448446\n",
      "train loss:0.6443047515016456\n",
      "train loss:0.5236525563988916\n",
      "train loss:0.5252572587887864\n",
      "train loss:0.6255014701834357\n",
      "train loss:0.5308177444450078\n",
      "train loss:0.9400286544802914\n",
      "train loss:0.6945685637545524\n",
      "train loss:0.3604525899572262\n",
      "train loss:0.6989925401171503\n",
      "train loss:0.37974454815985237\n",
      "train loss:0.6350662537374054\n",
      "train loss:0.5417641260493704\n",
      "train loss:0.4616724186479194\n",
      "train loss:0.585970826893578\n",
      "train loss:0.5263352671809711\n",
      "train loss:0.7572074721119278\n",
      "train loss:0.6813394813463203\n",
      "train loss:0.7728264775735683\n",
      "train loss:0.6998640845114135\n",
      "train loss:0.6432057810505716\n",
      "train loss:0.5407055169474333\n",
      "train loss:0.5510276717222675\n",
      "train loss:0.5347969340238224\n",
      "train loss:0.6053288801821287\n",
      "train loss:0.7056257654770425\n",
      "train loss:0.6790186721251258\n",
      "train loss:0.7044242121190549\n",
      "train loss:0.6204046636043802\n",
      "train loss:0.8052451881149111\n",
      "train loss:0.5060006509996293\n",
      "train loss:0.7486920633382611\n",
      "train loss:0.5603555504341745\n",
      "train loss:0.5629907824062892\n",
      "train loss:0.6904168215310331\n",
      "train loss:0.4684656194854602\n",
      "train loss:0.552415862265671\n",
      "train loss:0.6046459903775594\n",
      "train loss:0.5594546386089558\n",
      "train loss:0.683454644992392\n",
      "train loss:0.5591923030061233\n",
      "train loss:0.6104449271811596\n",
      "train loss:0.6947312660054349\n",
      "train loss:0.5360397020755183\n",
      "train loss:0.6965377881523043\n",
      "train loss:0.5040127333114368\n",
      "train loss:0.8850683701457379\n",
      "train loss:0.6237612835180468\n",
      "train loss:0.7693291454869404\n",
      "train loss:0.6161128338994993\n",
      "train loss:0.5116667427815396\n",
      "train loss:0.6763698275456562\n",
      "train loss:0.4642483256789408\n",
      "train loss:0.5401391713138252\n",
      "train loss:0.4438493499938265\n",
      "train loss:0.7754821739245443\n",
      "train loss:0.5426693224917314\n",
      "train loss:0.6154293491556914\n",
      "train loss:0.7780458026085301\n",
      "train loss:0.41540825915964835\n",
      "train loss:0.609374536415617\n",
      "train loss:0.4144454555642575\n",
      "train loss:0.5413112545028865\n",
      "train loss:0.5181849348546244\n",
      "train loss:0.5162258122566308\n",
      "train loss:0.5327685599502048\n",
      "train loss:0.5762531105614341\n",
      "train loss:0.7073295953487284\n",
      "train loss:0.6238690387901622\n",
      "train loss:0.364238551127842\n",
      "train loss:0.8554789099077225\n",
      "train loss:0.3696153488379642\n",
      "train loss:0.5776140966554026\n",
      "train loss:0.47606718438602613\n",
      "train loss:1.1471647895275392\n",
      "train loss:0.6113140802611066\n",
      "train loss:0.7981835083644085\n",
      "train loss:0.5309317839249165\n",
      "train loss:0.5015336346120528\n",
      "train loss:0.6223921463764526\n",
      "train loss:0.6118484675067236\n",
      "train loss:0.5523997286468786\n",
      "train loss:0.7478125842522975\n",
      "train loss:0.5412924369785149\n",
      "train loss:0.7021569224035497\n",
      "train loss:0.6667930030887944\n",
      "train loss:0.6779393211444701\n",
      "train loss:0.6139441362537682\n",
      "train loss:0.7460724021759686\n",
      "train loss:0.5639690996752418\n",
      "train loss:0.6074499223789321\n",
      "train loss:0.7388174976846236\n",
      "train loss:0.6258085488747809\n",
      "train loss:0.5803872279069201\n",
      "train loss:0.6712783057069007\n",
      "train loss:0.6739220641317464\n",
      "train loss:0.612953052236216\n",
      "train loss:0.7405708924135042\n",
      "train loss:0.6430615918684824\n",
      "train loss:0.576718572105519\n",
      "train loss:0.4689503990390668\n",
      "train loss:0.7371477864261129\n",
      "train loss:0.6172502925543634\n",
      "train loss:0.5039265592154374\n",
      "train loss:0.6916568069804597\n",
      "train loss:0.6039556733472524\n",
      "train loss:0.55613975553815\n",
      "train loss:0.5385720079011928\n",
      "train loss:0.7067421632162352\n",
      "train loss:0.5344465475588309\n",
      "train loss:0.6994553277978443\n",
      "train loss:0.7030618335735281\n",
      "train loss:0.60485084976474\n",
      "train loss:0.49789538909799236\n",
      "train loss:0.6892338755409189\n",
      "train loss:0.4208914415565446\n",
      "train loss:0.5038750133724907\n",
      "train loss:0.417772546975588\n",
      "train loss:0.38170419328296257\n",
      "train loss:0.39727330303793035\n",
      "train loss:0.6542039440065976\n",
      "train loss:1.1537221238103377\n",
      "train loss:0.6535658188409144\n",
      "train loss:0.6045793114552873\n",
      "train loss:0.8499602182955547\n",
      "train loss:0.41327228740696464\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6788427651422355\n",
      "train loss:0.5996224559893398\n",
      "train loss:0.48128199441537706\n",
      "train loss:0.5438195556467782\n",
      "train loss:0.6258676508004881\n",
      "train loss:0.5150940343421938\n",
      "train loss:0.6129475823474496\n",
      "train loss:0.44296868026871056\n",
      "train loss:0.9655845672308928\n",
      "train loss:0.4452253806938959\n",
      "train loss:0.5895489158061235\n",
      "train loss:0.4409430319149866\n",
      "train loss:0.7897682409397928\n",
      "train loss:0.6835362545003234\n",
      "train loss:0.5175142316262654\n",
      "train loss:0.5288659995335256\n",
      "train loss:0.6002845284358528\n",
      "train loss:0.6883973543159913\n",
      "train loss:0.6986919791360218\n",
      "train loss:0.6321688774127259\n",
      "train loss:0.6200665107749829\n",
      "train loss:0.6821589364887021\n",
      "train loss:0.6877903140327513\n",
      "train loss:0.6096530332471436\n",
      "train loss:0.6950717578476574\n",
      "train loss:0.5944555522110073\n",
      "train loss:0.7626640623687733\n",
      "train loss:0.6778286787371963\n",
      "train loss:0.6229592630689937\n",
      "train loss:0.5358362850695986\n",
      "train loss:0.48103936747869885\n",
      "train loss:0.6136691919430369\n",
      "train loss:0.6622617700507799\n",
      "train loss:0.47666413306904687\n",
      "train loss:0.607364973739658\n",
      "train loss:0.6764053274047603\n",
      "train loss:0.3985279474190495\n",
      "train loss:0.8042629328842367\n",
      "train loss:0.5433030508848444\n",
      "train loss:0.6202384399297137\n",
      "train loss:0.8995734270340814\n",
      "train loss:0.5275408345249882\n",
      "train loss:0.43088822493276735\n",
      "train loss:0.5179382796717509\n",
      "train loss:0.7129359246721352\n",
      "train loss:0.7032269815879479\n",
      "train loss:0.45027646678153055\n",
      "train loss:0.6296296005143157\n",
      "train loss:0.442129693732866\n",
      "train loss:0.6169156310936641\n",
      "train loss:0.42593548003860154\n",
      "train loss:0.8416991619053695\n",
      "train loss:0.5142433123819972\n",
      "train loss:0.7701613660081724\n",
      "train loss:0.814456097749811\n",
      "train loss:0.5115943376835029\n",
      "train loss:0.7053082425443779\n",
      "train loss:0.5332325513222915\n",
      "train loss:0.5158134061638222\n",
      "train loss:0.6372239281187168\n",
      "train loss:0.7725911487893701\n",
      "train loss:0.597979434407938\n",
      "train loss:0.6917709004752829\n",
      "train loss:0.5292461755185943\n",
      "train loss:0.5413094524268773\n",
      "train loss:0.6121895955366959\n",
      "train loss:0.6961550427795886\n",
      "train loss:0.6766635346901438\n",
      "train loss:0.6673823341191423\n",
      "train loss:0.46775721277602456\n",
      "train loss:0.4614266062931852\n",
      "train loss:0.9108738682386855\n",
      "train loss:0.5425731476658028\n",
      "train loss:0.5883292727712053\n",
      "train loss:0.6718287875490905\n",
      "train loss:0.604003544182531\n",
      "train loss:0.5541938649917444\n",
      "train loss:0.46387105820626456\n",
      "train loss:0.5175288459165118\n",
      "train loss:0.4978346530927366\n",
      "train loss:0.5163599207359847\n",
      "train loss:0.3066859239319915\n",
      "train loss:0.6309526320596142\n",
      "train loss:0.4802898970318997\n",
      "train loss:0.9710437075597881\n",
      "train loss:0.3643671455722104\n",
      "train loss:0.505299899747503\n",
      "train loss:0.3604604152498042\n",
      "train loss:0.7704642451862724\n",
      "train loss:0.6193502265473116\n",
      "train loss:0.7853383211986744\n",
      "train loss:0.4765284889039802\n",
      "train loss:0.6035198794871642\n",
      "train loss:0.6265405127154671\n",
      "train loss:0.5107227693321172\n",
      "train loss:0.8186642474026321\n",
      "train loss:0.7219304784392445\n",
      "train loss:0.43162357425367465\n",
      "train loss:0.8681141233495362\n",
      "train loss:0.6593798088036389\n",
      "train loss:0.5992268216801016\n",
      "train loss:0.3898858691475581\n",
      "train loss:0.6837106136919561\n",
      "train loss:0.4822218578587535\n",
      "train loss:0.7259130192940566\n",
      "train loss:0.6181614449115861\n",
      "train loss:0.7035180293711784\n",
      "train loss:0.5421304426490121\n",
      "train loss:0.6921573412925129\n",
      "train loss:0.563638580896588\n",
      "train loss:0.6221037423589119\n",
      "train loss:0.6886271053663018\n",
      "train loss:0.4825056539700343\n",
      "train loss:0.5395151682182633\n",
      "train loss:0.5300539572590448\n",
      "train loss:0.7575540964049311\n",
      "train loss:0.6438590816140718\n",
      "train loss:0.5267950069019032\n",
      "train loss:0.9801266495439416\n",
      "train loss:0.5500489415479463\n",
      "train loss:0.7530167388690454\n",
      "train loss:0.4795057876524522\n",
      "train loss:0.6031546697332277\n",
      "train loss:0.8220416760766712\n",
      "train loss:0.47721157579286677\n",
      "train loss:0.4739960607158807\n",
      "train loss:0.4720635946879502\n",
      "train loss:0.46109046700614564\n",
      "train loss:0.4462947148078104\n",
      "train loss:0.5133888784294803\n",
      "train loss:0.6393370564493711\n",
      "train loss:0.41706698823206817\n",
      "train loss:0.6101266787523084\n",
      "train loss:0.9362732678294851\n",
      "train loss:0.8912365796610684\n",
      "train loss:0.6369996659343302\n",
      "train loss:0.5921625947387883\n",
      "train loss:0.4334170233539541\n",
      "train loss:0.5003654907982391\n",
      "train loss:0.9982460164987241\n",
      "train loss:0.5250690663395577\n",
      "train loss:0.7580326269601969\n",
      "train loss:0.6348648340329628\n",
      "train loss:0.5508189738589744\n",
      "train loss:0.5300150474654622\n",
      "train loss:0.6140619478052056\n",
      "train loss:0.3706574822180718\n",
      "train loss:0.4631577932968807\n",
      "train loss:0.5300017433758063\n",
      "train loss:0.5027156880064944\n",
      "train loss:0.7238467423716164\n",
      "train loss:0.6196961187827263\n",
      "train loss:0.42584124590960226\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.9213844521388275\n",
      "train loss:0.7575081561266745\n",
      "train loss:0.6934781333892708\n",
      "train loss:0.5173864541379805\n",
      "train loss:0.45316794420957696\n",
      "train loss:0.6501802387389045\n",
      "train loss:0.6780057056064875\n",
      "train loss:0.7810266372013415\n",
      "train loss:0.6193982907382304\n",
      "train loss:0.784162715729655\n",
      "train loss:0.5480504462691342\n",
      "train loss:0.6483241035580443\n",
      "train loss:0.6649302324250846\n",
      "train loss:0.723232064421883\n",
      "train loss:0.5574991724930255\n",
      "train loss:0.6176999747755707\n",
      "train loss:0.6103286033491458\n",
      "train loss:0.6223513100994466\n",
      "train loss:0.6989605175625613\n",
      "train loss:0.5589582661607698\n",
      "train loss:0.565596917736714\n",
      "train loss:0.4971977275072508\n",
      "train loss:0.6894489739976011\n",
      "train loss:0.4873316476409352\n",
      "train loss:0.416407085793385\n",
      "train loss:0.6457169551345459\n",
      "train loss:0.5219754549789425\n",
      "train loss:0.5353734506413109\n",
      "train loss:0.553589403937521\n",
      "train loss:0.40965568501359284\n",
      "train loss:0.6217463853664199\n",
      "train loss:0.7014074433377709\n",
      "train loss:0.6835293183550284\n",
      "train loss:0.61658478257284\n",
      "train loss:0.49217298627463135\n",
      "train loss:0.8310690559252765\n",
      "train loss:0.6176524645056863\n",
      "train loss:0.5202325011429917\n",
      "train loss:0.6205940583453156\n",
      "train loss:0.5177357267933536\n",
      "train loss:0.5113317853009738\n",
      "train loss:0.7105490407802083\n",
      "train loss:0.513064172560977\n",
      "train loss:0.8387162121565119\n",
      "train loss:0.4329302655206602\n",
      "train loss:0.7007707850048623\n",
      "train loss:0.48904326484755706\n",
      "train loss:0.4199386761049408\n",
      "train loss:0.4155669443208977\n",
      "train loss:0.5969070517134036\n",
      "train loss:0.8189592477775198\n",
      "train loss:0.4909111421721823\n",
      "train loss:0.6024852998310735\n",
      "train loss:0.6223226749085087\n",
      "train loss:0.9974838217540853\n",
      "train loss:0.9314030407701146\n",
      "train loss:0.5983919381219047\n",
      "train loss:0.6770194182865035\n",
      "train loss:0.8248266756092958\n",
      "train loss:0.6139930993999216\n",
      "train loss:0.6108833789504037\n",
      "train loss:0.5709912074991832\n",
      "train loss:0.5240579260635939\n",
      "train loss:0.6746677813508433\n",
      "train loss:0.5782784712029189\n",
      "train loss:0.7147481145184178\n",
      "train loss:0.4814853536582396\n",
      "train loss:0.6168440640454426\n",
      "train loss:0.5700248597680092\n",
      "train loss:0.4625221210355484\n",
      "train loss:0.7381239880289238\n",
      "train loss:0.49319955515977637\n",
      "train loss:0.5482365553722818\n",
      "train loss:0.7558250313688554\n",
      "train loss:0.47869447414558464\n",
      "train loss:0.5389701814882691\n",
      "train loss:0.7012787820611714\n",
      "train loss:0.7539148497959821\n",
      "train loss:0.44710268591284796\n",
      "train loss:0.8667742112002607\n",
      "train loss:0.7583025380006093\n",
      "train loss:0.6231216810528083\n",
      "train loss:0.5357492487404588\n",
      "train loss:0.5352746261398283\n",
      "train loss:0.7018442497572898\n",
      "train loss:0.5226225958278122\n",
      "train loss:0.4341017196941553\n",
      "train loss:0.7792281683062511\n",
      "train loss:0.6963431319743361\n",
      "train loss:0.5172281661619149\n",
      "train loss:0.7162335547371173\n",
      "train loss:0.6889775802980785\n",
      "train loss:0.4299773635879983\n",
      "train loss:0.7691321578194987\n",
      "train loss:0.4375137056938316\n",
      "train loss:0.7010297956553527\n",
      "train loss:0.6045855437189084\n",
      "train loss:0.6990312777743372\n",
      "train loss:0.5823965884628721\n",
      "train loss:0.6066738026095437\n",
      "train loss:0.5178648821268772\n",
      "train loss:0.6250059511058456\n",
      "train loss:0.5342862549354095\n",
      "train loss:0.8066399108595951\n",
      "train loss:0.34007922704375665\n",
      "train loss:0.5142095205686131\n",
      "train loss:0.6995888672681848\n",
      "train loss:0.6087833631534021\n",
      "train loss:0.6221100776547235\n",
      "train loss:0.33822329230485615\n",
      "train loss:0.40335809041105397\n",
      "train loss:0.5266733572432198\n",
      "train loss:0.6226669324707166\n",
      "train loss:0.5059813193576301\n",
      "train loss:0.25704630625115155\n",
      "train loss:0.45582441238256033\n",
      "train loss:0.6271430547193765\n",
      "train loss:0.4854943855582688\n",
      "train loss:0.383321460142154\n",
      "train loss:0.7239037283987905\n",
      "train loss:1.1095549887945322\n",
      "train loss:0.7509164043628258\n",
      "train loss:0.2356414661577026\n",
      "train loss:0.7482839976384541\n",
      "train loss:0.7107779900844479\n",
      "train loss:0.6207445826049106\n",
      "train loss:0.48894668153820947\n",
      "train loss:0.5348564688964152\n",
      "train loss:0.6988897599142206\n",
      "train loss:0.4345122812330683\n",
      "train loss:0.5195513214905252\n",
      "train loss:0.7011200423107928\n",
      "train loss:0.5996293616736438\n",
      "train loss:0.7753174697029899\n",
      "train loss:0.6057638895549627\n",
      "train loss:0.6977009318237148\n",
      "train loss:0.6920894573407944\n",
      "train loss:0.6098264669766655\n",
      "train loss:0.5531748792751756\n",
      "train loss:0.6242885647875573\n",
      "train loss:0.5433261407608503\n",
      "train loss:0.4884568333530289\n",
      "train loss:0.690409590168003\n",
      "train loss:0.6870519741157347\n",
      "train loss:0.5392715928628751\n",
      "train loss:0.6763872860524354\n",
      "train loss:0.7714410475696007\n",
      "train loss:0.6691354519246693\n",
      "train loss:0.47739714477744555\n",
      "train loss:0.48176668026846936\n",
      "train loss:0.6356577102761901\n",
      "train loss:0.39300800326478763\n",
      "train loss:0.6878372030999321\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7891679027648315\n",
      "train loss:0.6083714264162916\n",
      "train loss:0.4595657934325598\n",
      "train loss:0.44243546197469286\n",
      "train loss:0.45270411514922476\n",
      "train loss:0.791324537198568\n",
      "train loss:0.7105426106070591\n",
      "train loss:0.6921369973077228\n",
      "train loss:0.4298703715383261\n",
      "train loss:0.5122979057112906\n",
      "train loss:0.9791480641248459\n",
      "train loss:0.6019569379174758\n",
      "train loss:0.7868217455266822\n",
      "train loss:0.6048832871918919\n",
      "train loss:0.6962307673654269\n",
      "train loss:0.5322318243942062\n",
      "train loss:0.5305994917142125\n",
      "train loss:0.6071874362900682\n",
      "train loss:0.6114339521306758\n",
      "train loss:0.6897697387477751\n",
      "train loss:0.6180078410290581\n",
      "train loss:0.7849770815827327\n",
      "train loss:0.73768880716424\n",
      "train loss:0.5501264398278987\n",
      "train loss:0.6052934399780857\n",
      "train loss:0.5361254534527042\n",
      "train loss:0.4777492793418541\n",
      "train loss:0.47760327646496126\n",
      "train loss:0.4653449077824143\n",
      "train loss:0.7476255299342307\n",
      "train loss:0.6129962810290039\n",
      "train loss:0.6964049393066511\n",
      "train loss:0.762914267214067\n",
      "train loss:0.6099668934285024\n",
      "train loss:0.550414938978704\n",
      "train loss:0.6223996648122416\n",
      "train loss:0.6839424634106315\n",
      "train loss:0.8186538753774478\n",
      "train loss:0.6892724662789546\n",
      "train loss:0.535337532012172\n",
      "train loss:0.6141136191544085\n",
      "train loss:0.6805342453639911\n",
      "train loss:0.6025048048876934\n",
      "train loss:0.5399484846132767\n",
      "train loss:0.6231786615354907\n",
      "train loss:0.8290919846418726\n",
      "train loss:0.6722547890967537\n",
      "train loss:0.48006531444595596\n",
      "train loss:0.535315204131751\n",
      "train loss:0.7312070745505275\n",
      "train loss:0.8074902268627995\n",
      "train loss:0.6045830943715504\n",
      "train loss:0.5600591639018836\n",
      "train loss:0.5515650307764147\n",
      "train loss:0.7396778807112129\n",
      "train loss:0.6829654658850979\n",
      "train loss:0.616626390120733\n",
      "train loss:0.5467695162976942\n",
      "train loss:0.49581569501620065\n",
      "train loss:0.6197334509588037\n",
      "train loss:0.5422617920568997\n",
      "train loss:0.6130968731360691\n",
      "train loss:0.6215555542066531\n",
      "train loss:0.5336691107988711\n",
      "train loss:0.45419835778648476\n",
      "train loss:0.8454919143999179\n",
      "train loss:0.5170867278745901\n",
      "train loss:0.504539605561277\n",
      "train loss:0.6136950527619603\n",
      "train loss:0.6163565621637553\n",
      "train loss:0.5166992481794577\n",
      "train loss:0.7140952394898084\n",
      "train loss:0.30663275910906695\n",
      "train loss:0.6252303395987198\n",
      "train loss:0.4872954506149352\n",
      "train loss:0.6051695763964043\n",
      "train loss:0.3895115572047797\n",
      "train loss:0.38597942365901433\n",
      "train loss:0.6347994943081025\n",
      "train loss:1.1086868294017793\n",
      "train loss:0.244212630015867\n",
      "train loss:0.4934259062279489\n",
      "train loss:1.024975010606561\n",
      "train loss:0.5051180473840985\n",
      "train loss:0.513551919739147\n",
      "train loss:0.6852584964356982\n",
      "train loss:0.7117415529136256\n",
      "train loss:0.6294884210378823\n",
      "train loss:0.8577005396798574\n",
      "train loss:0.5278455056808937\n",
      "train loss:0.6824682627103411\n",
      "train loss:0.6957903868870392\n",
      "train loss:0.47235158805610605\n",
      "train loss:0.6723797136358589\n",
      "train loss:0.6685182052019837\n",
      "train loss:0.4895065163071208\n",
      "train loss:0.6079660926300987\n",
      "train loss:0.6326347835235351\n",
      "train loss:0.557058689918632\n",
      "train loss:0.6046402743355483\n",
      "train loss:0.5493162774157418\n",
      "train loss:0.49100599249359195\n",
      "train loss:0.7262563249353614\n",
      "train loss:0.6184681567960277\n",
      "train loss:0.5315586737628152\n",
      "train loss:0.5292774977253549\n",
      "train loss:0.5129004483620159\n",
      "train loss:0.5197050492440229\n",
      "train loss:0.706521636714738\n",
      "train loss:0.5261670036281215\n",
      "train loss:0.6093528995199062\n",
      "train loss:0.4084989872216088\n",
      "train loss:0.609546611565496\n",
      "train loss:0.5028553017849436\n",
      "train loss:0.5961702188740333\n",
      "train loss:0.39261692655555397\n",
      "train loss:0.38073501378662056\n",
      "train loss:0.35513229213542497\n",
      "train loss:0.7792868979707117\n",
      "train loss:0.6250100670556201\n",
      "train loss:0.35820898601705486\n",
      "train loss:0.37583136055366795\n",
      "train loss:1.0779925369700916\n",
      "train loss:0.7914143480869875\n",
      "train loss:0.7490472703595604\n",
      "train loss:0.7418755286236628\n",
      "train loss:0.6988143943330221\n",
      "train loss:0.5987687960932803\n",
      "train loss:0.5222466917565849\n",
      "train loss:0.5853019743661297\n",
      "train loss:0.5115255498978131\n",
      "train loss:0.672454298365732\n",
      "train loss:0.6830393331192604\n",
      "train loss:0.7434391106457025\n",
      "train loss:0.6869341668765354\n",
      "train loss:0.6763426509966001\n",
      "train loss:0.5746789709199226\n",
      "train loss:0.5517486520591575\n",
      "train loss:0.5628488027177079\n",
      "train loss:0.56880224302214\n",
      "train loss:0.5602066666708935\n",
      "train loss:0.7292534031826612\n",
      "train loss:0.6772071817866271\n",
      "train loss:0.5116280810820562\n",
      "train loss:0.567354953142224\n",
      "train loss:0.553976383346908\n",
      "train loss:0.8024743479263409\n",
      "train loss:0.6633760909358867\n",
      "train loss:0.6653858394781906\n",
      "train loss:0.6277147816009746\n",
      "train loss:0.4835751973725236\n",
      "train loss:0.5527076279499213\n",
      "train loss:0.6828139637986024\n",
      "=== epoch:5, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5415079211156605\n",
      "train loss:0.6846996303912811\n",
      "train loss:0.5272188059434868\n",
      "train loss:0.7606095860764159\n",
      "train loss:0.5162970897587044\n",
      "train loss:0.6863568140885794\n",
      "train loss:0.5405772370705939\n",
      "train loss:0.5433809209748965\n",
      "train loss:0.6725094098004465\n",
      "train loss:0.5033103064397081\n",
      "train loss:0.5267623448076271\n",
      "train loss:0.7957356446669418\n",
      "train loss:0.7828888894668602\n",
      "train loss:0.6963079957267021\n",
      "train loss:0.5339129625577643\n",
      "train loss:0.5339013843894442\n",
      "train loss:0.6395326990347043\n",
      "train loss:0.585873154120977\n",
      "train loss:0.6721628936264765\n",
      "train loss:0.5386822394887097\n",
      "train loss:0.5203653958600668\n",
      "train loss:0.6642482869966575\n",
      "train loss:0.4322707383061591\n",
      "train loss:0.6271436821628265\n",
      "train loss:0.6189148431353303\n",
      "train loss:0.6729306576349121\n",
      "train loss:0.8038317040976507\n",
      "train loss:0.712032043919531\n",
      "train loss:0.6020040521288962\n",
      "train loss:0.6275168836041914\n",
      "train loss:0.5365758952698362\n",
      "train loss:0.44253089352518726\n",
      "train loss:0.7585775143793472\n",
      "train loss:0.6084125440823334\n",
      "train loss:0.5383957756294373\n",
      "train loss:0.6910817845049375\n",
      "train loss:0.6743611301237815\n",
      "train loss:0.6849613098496004\n",
      "train loss:0.6062693104146144\n",
      "train loss:0.5498878133107727\n",
      "train loss:0.6160616382248337\n",
      "train loss:0.5500079720477843\n",
      "train loss:0.613626030652079\n",
      "train loss:0.7573640991517256\n",
      "train loss:0.5272917162099418\n",
      "train loss:0.6519342273927858\n",
      "train loss:0.5431320589226517\n",
      "train loss:0.5390945724978977\n",
      "train loss:0.5228183542067455\n",
      "train loss:0.45178434787258154\n",
      "train loss:0.6974928098440929\n",
      "train loss:0.45901758033437334\n",
      "train loss:0.486263781225244\n",
      "train loss:0.7099297543112106\n",
      "train loss:0.5046814216369923\n",
      "train loss:0.2967913780070825\n",
      "train loss:0.5034538670474948\n",
      "train loss:0.3882308378831788\n",
      "train loss:0.612744289614515\n",
      "train loss:0.49952421732489727\n",
      "train loss:0.6589841959681262\n",
      "train loss:0.6514222388908205\n",
      "train loss:0.6490462460934492\n",
      "train loss:0.7746635953076989\n",
      "train loss:1.0045878301080031\n",
      "train loss:0.4980428827536829\n",
      "train loss:0.5241120937315511\n",
      "train loss:1.007709521941007\n",
      "train loss:0.6084555380686675\n",
      "train loss:0.6908812678394086\n",
      "train loss:0.4509444543227736\n",
      "train loss:0.44736735600145466\n",
      "train loss:0.6877799400741299\n",
      "train loss:0.596330858030095\n",
      "train loss:0.6150518320613536\n",
      "train loss:0.4691369515611637\n",
      "train loss:0.5370611962445176\n",
      "train loss:0.6100225794292411\n",
      "train loss:0.5264774806588942\n",
      "train loss:0.6749008406676362\n",
      "train loss:0.5883506952246985\n",
      "train loss:0.5295786403764555\n",
      "train loss:0.839013914445994\n",
      "train loss:0.6135512774237161\n",
      "train loss:0.4511791838366187\n",
      "train loss:0.6857368680712528\n",
      "train loss:0.6878173556443564\n",
      "train loss:0.6818671649020336\n",
      "train loss:0.5383061082439277\n",
      "train loss:0.5404439837311099\n",
      "train loss:0.5135968870628806\n",
      "train loss:0.7573194872288239\n",
      "train loss:0.6057269586288259\n",
      "train loss:0.5184190562610276\n",
      "train loss:0.776927036959461\n",
      "train loss:0.6027745538005101\n",
      "train loss:0.5294722793583916\n",
      "train loss:0.5441533120854738\n",
      "train loss:0.6020047169433985\n",
      "train loss:0.5361587849960331\n",
      "train loss:0.6196123231125065\n",
      "train loss:0.5173562360939565\n",
      "train loss:0.43418325212288317\n",
      "train loss:0.7968228771207523\n",
      "train loss:0.7044318625886393\n",
      "train loss:0.6053139420345445\n",
      "train loss:0.7188544593879327\n",
      "train loss:0.34022493967723416\n",
      "train loss:0.798342432536973\n",
      "train loss:0.4139886338613687\n",
      "train loss:0.7195319429915946\n",
      "train loss:0.7979888238744699\n",
      "train loss:0.4085133368467244\n",
      "train loss:0.702773380438143\n",
      "train loss:0.8672947887917676\n",
      "train loss:0.5224477566748604\n",
      "train loss:0.6193524718016536\n",
      "train loss:0.5328586932685757\n",
      "train loss:0.6004920797250367\n",
      "train loss:0.6319193244927441\n",
      "train loss:0.5402352412618815\n",
      "train loss:0.5228710406619447\n",
      "train loss:0.4320414919291541\n",
      "train loss:0.5283709614911787\n",
      "train loss:0.6180511184369687\n",
      "train loss:0.5135571661580183\n",
      "train loss:0.6923266851700344\n",
      "train loss:0.49758708322164125\n",
      "train loss:0.40992420247626205\n",
      "train loss:0.8107898243805185\n",
      "train loss:0.48288864418661426\n",
      "train loss:0.2883031514876808\n",
      "train loss:0.38174441388542074\n",
      "train loss:0.49366010448745745\n",
      "train loss:0.7384554770980449\n",
      "train loss:0.35945153320114487\n",
      "train loss:0.6678529277867126\n",
      "train loss:0.8122089217627334\n",
      "train loss:0.8833575104745321\n",
      "train loss:0.8772448810439567\n",
      "train loss:0.29098271054748476\n",
      "train loss:0.4987842584919885\n",
      "train loss:0.8003585202016789\n",
      "train loss:0.6286723619118172\n",
      "train loss:0.5782393460276184\n",
      "train loss:0.5943367362507722\n",
      "train loss:0.7243264295490827\n",
      "train loss:0.6093806739250216\n",
      "train loss:0.5386630233631369\n",
      "train loss:0.4344241572252465\n",
      "train loss:0.7459511217003383\n",
      "train loss:0.615489516192197\n",
      "train loss:0.6877151999273007\n",
      "=== epoch:6, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6881438401876785\n",
      "train loss:0.6757779566786493\n",
      "train loss:0.6619659707055588\n",
      "train loss:0.5989654587062636\n",
      "train loss:0.5058315486015499\n",
      "train loss:0.7033146745962975\n",
      "train loss:0.5557116588775267\n",
      "train loss:0.6831019214973868\n",
      "train loss:0.6771339884969378\n",
      "train loss:0.6297667931946456\n",
      "train loss:0.6310527615074302\n",
      "train loss:0.6674123830753318\n",
      "train loss:0.6330586754243454\n",
      "train loss:0.6632595672722577\n",
      "train loss:0.5118738504673097\n",
      "train loss:0.5779620522266201\n",
      "train loss:0.5947597549210779\n",
      "train loss:0.6929223092969792\n",
      "train loss:0.3920470940845059\n",
      "train loss:0.623679165064743\n",
      "train loss:0.5418881887330107\n",
      "train loss:0.505369607052973\n",
      "train loss:0.8625121046176168\n",
      "train loss:0.7568032935871998\n",
      "train loss:0.6105543255857379\n",
      "train loss:0.6874873464886456\n",
      "train loss:0.592296324140037\n",
      "train loss:0.6120561065240732\n",
      "train loss:0.6124244982479023\n",
      "train loss:0.6074760747072167\n",
      "train loss:0.7580100431890731\n",
      "train loss:0.4458378940277834\n",
      "train loss:0.515683759230853\n",
      "train loss:0.5962107642082299\n",
      "train loss:0.5325897632934723\n",
      "train loss:0.6259250805133376\n",
      "train loss:0.7851841916768975\n",
      "train loss:0.6648855929106279\n",
      "train loss:0.3550847919865708\n",
      "train loss:0.6200164968130905\n",
      "train loss:0.6993901434827791\n",
      "train loss:0.7162712134593986\n",
      "train loss:0.4309461294296856\n",
      "train loss:0.5408534658977773\n",
      "train loss:0.5389230337462264\n",
      "train loss:0.6922932191181262\n",
      "train loss:0.6347182985384651\n",
      "train loss:0.8682469813258384\n",
      "train loss:0.4103981708526271\n",
      "train loss:0.6541552300557179\n",
      "train loss:0.6270712699879756\n",
      "train loss:0.5207889228012832\n",
      "train loss:0.6033445104825037\n",
      "train loss:0.5885656128442629\n",
      "train loss:0.7230904110859014\n",
      "train loss:0.5980233132684315\n",
      "train loss:0.6804758774810721\n",
      "train loss:0.609737264358931\n",
      "train loss:0.5030969708549196\n",
      "train loss:0.5263094549751833\n",
      "train loss:0.6823651762141161\n",
      "train loss:0.7621831911277276\n",
      "train loss:0.5805617390614717\n",
      "train loss:0.6489598809148118\n",
      "train loss:0.5573476444312802\n",
      "train loss:0.7340126791475787\n",
      "train loss:0.7480655212786267\n",
      "train loss:0.5296352921939288\n",
      "train loss:0.5366082109165492\n",
      "train loss:0.4683060864929992\n",
      "train loss:0.6720965382404989\n",
      "train loss:0.4715277069287066\n",
      "train loss:0.5512381543394526\n",
      "train loss:0.6407809569867804\n",
      "train loss:0.5025369002928091\n",
      "train loss:0.604759274509252\n",
      "train loss:0.4286200757263236\n",
      "train loss:0.4872884733264433\n",
      "train loss:0.53048407266863\n",
      "train loss:0.592260949046959\n",
      "train loss:0.3910613775388079\n",
      "train loss:0.6468901256373306\n",
      "train loss:0.6475146019457313\n",
      "train loss:0.7008845726889015\n",
      "train loss:0.5998486305925596\n",
      "train loss:0.6455925479572909\n",
      "train loss:0.4836763797534407\n",
      "train loss:0.5091644763480268\n",
      "train loss:0.8424965336758105\n",
      "train loss:0.6859135299618389\n",
      "train loss:0.7016341333563423\n",
      "train loss:0.32591991234607337\n",
      "train loss:0.5055759843808817\n",
      "train loss:0.6212404789236678\n",
      "train loss:0.5775709225694501\n",
      "train loss:0.7848939662372915\n",
      "train loss:0.7152443670966621\n",
      "train loss:0.610892555107309\n",
      "train loss:0.7183690164654136\n",
      "train loss:0.5329527739291796\n",
      "train loss:0.37480948827289995\n",
      "train loss:0.5781365144272868\n",
      "train loss:0.43075034718441074\n",
      "train loss:0.779426530375241\n",
      "train loss:0.3619065583960271\n",
      "train loss:0.530046611477937\n",
      "train loss:0.5557210212060661\n",
      "train loss:0.42611353581606426\n",
      "train loss:0.5046224708364119\n",
      "train loss:0.7304308100545509\n",
      "train loss:0.597827309639605\n",
      "train loss:0.6291283445996093\n",
      "train loss:0.5067851338939942\n",
      "train loss:0.43125476218759823\n",
      "train loss:0.5779043752103646\n",
      "train loss:0.3199673093833445\n",
      "train loss:0.8813057579684715\n",
      "train loss:0.6347966540621583\n",
      "train loss:0.37031897264274394\n",
      "train loss:0.5870992666258898\n",
      "train loss:0.7457737187329208\n",
      "train loss:0.5276862072373988\n",
      "train loss:0.3811477237446361\n",
      "train loss:0.393989265567733\n",
      "train loss:0.5237481933838801\n",
      "train loss:0.6061348519012123\n",
      "train loss:0.7304531912163371\n",
      "train loss:0.5048440611415994\n",
      "train loss:0.8475683438377274\n",
      "train loss:0.5566994932715826\n",
      "train loss:0.27553479333527797\n",
      "train loss:0.8244168807218571\n",
      "train loss:0.44056863922388373\n",
      "train loss:0.6295208776553131\n",
      "train loss:0.6228607523843631\n",
      "train loss:0.43415190524445463\n",
      "train loss:0.7277528440082561\n",
      "train loss:0.6931732633229897\n",
      "train loss:0.7794133768202424\n",
      "train loss:0.5034457463565446\n",
      "train loss:0.43326081015043094\n",
      "train loss:0.43442108388210415\n",
      "train loss:0.5198506221640988\n",
      "train loss:0.7631896138255525\n",
      "train loss:0.7824719479011855\n",
      "train loss:0.43229694903546356\n",
      "train loss:0.6740447201900784\n",
      "train loss:0.7812905626933586\n",
      "train loss:0.4285496271900794\n",
      "train loss:0.5874530393393409\n",
      "train loss:0.5115478440369794\n",
      "train loss:0.5874538962717617\n",
      "train loss:0.6230985610758478\n",
      "=== epoch:7, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.8660471728162966\n",
      "train loss:0.5296854219817737\n",
      "train loss:0.5122621375681331\n",
      "train loss:0.8759529083792585\n",
      "train loss:0.5312670433954332\n",
      "train loss:0.5240059795612233\n",
      "train loss:0.6064355783712744\n",
      "train loss:0.5892497496599518\n",
      "train loss:0.5873160591969756\n",
      "train loss:0.4532044269610152\n",
      "train loss:0.5502790237741269\n",
      "train loss:0.5999798595404677\n",
      "train loss:0.6292206722174901\n",
      "train loss:0.7190048540868423\n",
      "train loss:0.6293639559842739\n",
      "train loss:0.43030373626041574\n",
      "train loss:0.6282786165078905\n",
      "train loss:0.5293240821732261\n",
      "train loss:0.6211403824256783\n",
      "train loss:0.5722524597051053\n",
      "train loss:0.7379118912627971\n",
      "train loss:0.2862270751037026\n",
      "train loss:0.6228852772084295\n",
      "train loss:0.7136083784292341\n",
      "train loss:0.5302435984807191\n",
      "train loss:0.765367998225552\n",
      "train loss:0.47187770933896384\n",
      "train loss:0.6849938192286125\n",
      "train loss:0.5014691722524074\n",
      "train loss:0.5265160163144762\n",
      "train loss:0.4296889385086546\n",
      "train loss:0.3791819106514322\n",
      "train loss:0.7228050940645458\n",
      "train loss:0.7242614602027655\n",
      "train loss:0.38407838968838826\n",
      "train loss:0.6079084771379554\n",
      "train loss:0.7087514260839873\n",
      "train loss:0.47506355957465285\n",
      "train loss:0.5094747537553523\n",
      "train loss:0.5616267622158956\n",
      "train loss:0.4910987495168782\n",
      "train loss:0.4016833895220205\n",
      "train loss:0.27331377580159677\n",
      "train loss:0.5161980721781594\n",
      "train loss:0.39397777703023823\n",
      "train loss:0.6341663501467376\n",
      "train loss:0.362756385974766\n",
      "train loss:0.6615759582021807\n",
      "train loss:0.831440501624785\n",
      "train loss:0.9187293295151479\n",
      "train loss:0.49348030030050916\n",
      "train loss:0.7478234996884596\n",
      "train loss:0.7681294090545778\n",
      "train loss:0.964118062835789\n",
      "train loss:0.44632494415139423\n",
      "train loss:0.6655965060877858\n",
      "train loss:0.512554665008235\n",
      "train loss:0.5391316523449097\n",
      "train loss:0.4481458082259426\n",
      "train loss:0.5751648365836824\n",
      "train loss:0.5623005913683046\n",
      "train loss:0.7050286979686253\n",
      "train loss:0.6218857845427899\n",
      "train loss:0.4913271372588893\n",
      "train loss:0.562746304696091\n",
      "train loss:0.47011563858861427\n",
      "train loss:0.5744632723315647\n",
      "train loss:0.47467801209838784\n",
      "train loss:0.7048485201764307\n",
      "train loss:0.5392317405127951\n",
      "train loss:0.5763203585103811\n",
      "train loss:0.6145058210369843\n",
      "train loss:0.4630363173957132\n",
      "train loss:0.5485044010053992\n",
      "train loss:0.6055506342311887\n",
      "train loss:0.5115183825348888\n",
      "train loss:0.8028863406188705\n",
      "train loss:0.7402933123253581\n",
      "train loss:0.5204655341242592\n",
      "train loss:0.6044058699846533\n",
      "train loss:0.5355646716758988\n",
      "train loss:0.6235097010463033\n",
      "train loss:0.4321698122175244\n",
      "train loss:0.3930795337406693\n",
      "train loss:0.4919422777978366\n",
      "train loss:0.508200518693463\n",
      "train loss:0.3882312077573117\n",
      "train loss:0.5072351424497082\n",
      "train loss:1.0790843948048798\n",
      "train loss:0.7264082387254865\n",
      "train loss:0.7293945169136725\n",
      "train loss:0.9667688934304508\n",
      "train loss:0.6079758093302718\n",
      "train loss:0.4277032008800826\n",
      "train loss:0.5294619509542893\n",
      "train loss:0.510541981321228\n",
      "train loss:0.4280798419422198\n",
      "train loss:0.5820239672197376\n",
      "train loss:0.5339127668279456\n",
      "train loss:0.5246141952159186\n",
      "train loss:0.5255218424023439\n",
      "train loss:0.6002843737261028\n",
      "train loss:0.38116478288006866\n",
      "train loss:0.632426545437175\n",
      "train loss:0.29448098793369476\n",
      "train loss:0.6178694783435744\n",
      "train loss:0.6539381685981989\n",
      "train loss:0.4960578840874502\n",
      "train loss:0.5313454255422775\n",
      "train loss:0.754427679252672\n",
      "train loss:0.6419722044922145\n",
      "train loss:0.39449676608023865\n",
      "train loss:0.44115899199721714\n",
      "train loss:0.5504489634711419\n",
      "train loss:0.7057638198773737\n",
      "train loss:0.5268953144059376\n",
      "train loss:0.624288197119662\n",
      "train loss:0.9194897002668153\n",
      "train loss:0.5273769985328934\n",
      "train loss:0.5058236457971873\n",
      "train loss:0.8075393563098354\n",
      "train loss:0.787288847992943\n",
      "train loss:0.6543755996439242\n",
      "train loss:0.72718729879934\n",
      "train loss:0.597738774070512\n",
      "train loss:0.4642534693667365\n",
      "train loss:0.5586425089036665\n",
      "train loss:0.6632818459830315\n",
      "train loss:0.6745024683745887\n",
      "train loss:0.5938849446511987\n",
      "train loss:0.6324397314189893\n",
      "train loss:0.6001370006233661\n",
      "train loss:0.47937835776183046\n",
      "train loss:0.631355161812212\n",
      "train loss:0.5956616763059246\n",
      "train loss:0.5108878964099673\n",
      "train loss:0.6818563336310136\n",
      "train loss:0.36523901309941686\n",
      "train loss:0.7572397385527021\n",
      "train loss:0.4998857323239149\n",
      "train loss:0.7680940196963745\n",
      "train loss:0.6949483737416731\n",
      "train loss:0.6041379662943751\n",
      "train loss:0.7792353469431769\n",
      "train loss:0.6777755254898705\n",
      "train loss:0.505133318741565\n",
      "train loss:0.5183462702155601\n",
      "train loss:0.7442340178166844\n",
      "train loss:0.6895016419871811\n",
      "train loss:0.5271047719416555\n",
      "train loss:0.627851100456678\n",
      "train loss:0.7355632652851571\n",
      "train loss:0.7257630125787806\n",
      "=== epoch:8, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6162262195034698\n",
      "train loss:0.4951707137774174\n",
      "train loss:0.6424787673421106\n",
      "train loss:0.8073330727760748\n",
      "train loss:0.510886757259626\n",
      "train loss:0.4857594685831287\n",
      "train loss:0.5366657229514348\n",
      "train loss:0.639833601123403\n",
      "train loss:0.676131540229738\n",
      "train loss:0.528631041593737\n",
      "train loss:0.7381144971993762\n",
      "train loss:0.4960334746064077\n",
      "train loss:0.6298612034878405\n",
      "train loss:0.6063865504365528\n",
      "train loss:0.6553152529941302\n",
      "train loss:0.6027946985494315\n",
      "train loss:0.610745076843014\n",
      "train loss:0.5202231748871438\n",
      "train loss:0.6812391866578904\n",
      "train loss:0.6545389180299621\n",
      "train loss:0.6957393298092064\n",
      "train loss:0.7426473593443608\n",
      "train loss:0.5600083875528672\n",
      "train loss:0.523570279559544\n",
      "train loss:0.7204469763696231\n",
      "train loss:0.5928080314460138\n",
      "train loss:0.539643266205713\n",
      "train loss:0.6060241804261769\n",
      "train loss:0.46717820746108024\n",
      "train loss:0.5663250917482963\n",
      "train loss:0.6590639472923558\n",
      "train loss:0.5328991377535867\n",
      "train loss:0.757875408672991\n",
      "train loss:0.6474501533200165\n",
      "train loss:0.5089530166850814\n",
      "train loss:0.4895409853557228\n",
      "train loss:0.6119099238859398\n",
      "train loss:0.6327917348067265\n",
      "train loss:0.6065092355727957\n",
      "train loss:0.45683529338064394\n",
      "train loss:0.5752979191711439\n",
      "train loss:0.6282119176280784\n",
      "train loss:0.6711020256169831\n",
      "train loss:0.7051708111620143\n",
      "train loss:0.5082365954082912\n",
      "train loss:0.4762804983340425\n",
      "train loss:0.5272489480592701\n",
      "train loss:0.5571311334390356\n",
      "train loss:0.5094840135131067\n",
      "train loss:0.5167040937978944\n",
      "train loss:0.5035592896371526\n",
      "train loss:0.7660479976467126\n",
      "train loss:0.8363985385259337\n",
      "train loss:0.692941072191219\n",
      "train loss:0.5963523799201796\n",
      "train loss:0.5224146765329197\n",
      "train loss:0.8208837403625294\n",
      "train loss:0.6945318007731968\n",
      "train loss:0.4832350695989692\n",
      "train loss:0.52122355378858\n",
      "train loss:0.6745654716941265\n",
      "train loss:0.7869392860773099\n",
      "train loss:0.6874645750686225\n",
      "train loss:0.5190184804242453\n",
      "train loss:0.5698361528734457\n",
      "train loss:0.5998680131568331\n",
      "train loss:0.7201744618461386\n",
      "train loss:0.7511001072065413\n",
      "train loss:0.5927426458795917\n",
      "train loss:0.5539106277130216\n",
      "train loss:0.49402712835954005\n",
      "train loss:0.4785996696581698\n",
      "train loss:0.5430137487246248\n",
      "train loss:0.7140375084755642\n",
      "train loss:0.5833166765082319\n",
      "train loss:0.5674699153447176\n",
      "train loss:0.5671239160577528\n",
      "train loss:0.6360991425517892\n",
      "train loss:0.7767286007600327\n",
      "train loss:0.5456842278744541\n",
      "train loss:0.6502091025224068\n",
      "train loss:0.36481227638919483\n",
      "train loss:0.5318935777377828\n",
      "train loss:0.6301341202632028\n",
      "train loss:0.5699887353400461\n",
      "train loss:0.5420944476791754\n",
      "train loss:0.4167666933142747\n",
      "train loss:0.627443774754159\n",
      "train loss:0.5314361275477653\n",
      "train loss:0.38302092407083754\n",
      "train loss:0.7081743100297606\n",
      "train loss:0.7624204957308607\n",
      "train loss:0.7576555470340407\n",
      "train loss:0.520694798649362\n",
      "train loss:0.7854901646202246\n",
      "train loss:0.6654116181912493\n",
      "train loss:0.30401665449807064\n",
      "train loss:0.404658613744919\n",
      "train loss:0.7642768784830779\n",
      "train loss:0.6329036355476896\n",
      "train loss:0.4140179947810676\n",
      "train loss:0.40158260274801033\n",
      "train loss:0.39728935253466624\n",
      "train loss:0.5029567405237128\n",
      "train loss:1.0195276251574577\n",
      "train loss:0.4759834898579685\n",
      "train loss:0.5926454078823218\n",
      "train loss:0.46306668097880477\n",
      "train loss:0.5292643500550891\n",
      "train loss:0.48109975034905145\n",
      "train loss:0.6133858914366935\n",
      "train loss:0.632186071660447\n",
      "train loss:0.7553589780065592\n",
      "train loss:0.5667698189535655\n",
      "train loss:0.46271054693012276\n",
      "train loss:0.6172355321395993\n",
      "train loss:0.7537336423081351\n",
      "train loss:0.6599736923041716\n",
      "train loss:0.6342070797342465\n",
      "train loss:0.4830976522477415\n",
      "train loss:0.6377741238522476\n",
      "train loss:0.5719917857467423\n",
      "train loss:0.4465955885579037\n",
      "train loss:0.5828042926939863\n",
      "train loss:0.6585252331767677\n",
      "train loss:0.5823738060224067\n",
      "train loss:0.589932674872824\n",
      "train loss:0.5360183612686443\n",
      "train loss:0.5556759624676242\n",
      "train loss:0.6581980068639639\n",
      "train loss:0.8493210528201974\n",
      "train loss:0.5222607829869925\n",
      "train loss:0.5681967254052454\n",
      "train loss:0.4137801674442011\n",
      "train loss:0.6703673893398777\n",
      "train loss:0.5744085237270938\n",
      "train loss:0.5698753194181665\n",
      "train loss:0.6064345004893968\n",
      "train loss:0.5684325799496418\n",
      "train loss:0.41845870914694094\n",
      "train loss:0.38480467110632127\n",
      "train loss:0.5723908137341075\n",
      "train loss:0.703998107791391\n",
      "train loss:0.3713949371626817\n",
      "train loss:0.5306707003519255\n",
      "train loss:0.5952235344982305\n",
      "train loss:0.7385358051152734\n",
      "train loss:0.35795160075605886\n",
      "train loss:0.4944917176451574\n",
      "train loss:0.776243840926789\n",
      "train loss:0.7702330755107718\n",
      "train loss:0.7425180526023276\n",
      "train loss:0.7780109762752894\n",
      "=== epoch:9, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.8562395834376954\n",
      "train loss:0.44148838630059517\n",
      "train loss:0.631757946522627\n",
      "train loss:0.4820507881063243\n",
      "train loss:0.41678305113778136\n",
      "train loss:0.5429915871897688\n",
      "train loss:0.5611652651828486\n",
      "train loss:0.668681105136278\n",
      "train loss:0.70137554535989\n",
      "train loss:0.4374981863172924\n",
      "train loss:0.5368089358777108\n",
      "train loss:0.7295858704365115\n",
      "train loss:0.7250271279051759\n",
      "train loss:0.34980038088990073\n",
      "train loss:0.7863284064589191\n",
      "train loss:0.6821635365516929\n",
      "train loss:0.6374636201945794\n",
      "train loss:0.4536427424693367\n",
      "train loss:0.7209984136604314\n",
      "train loss:0.5303085037508735\n",
      "train loss:0.521230250235482\n",
      "train loss:0.5551149503247325\n",
      "train loss:0.5148722616883845\n",
      "train loss:0.7127963608754397\n",
      "train loss:0.6940354551312836\n",
      "train loss:0.5404444624720007\n",
      "train loss:0.6865343213190969\n",
      "train loss:0.5275530132351096\n",
      "train loss:0.5661638055915696\n",
      "train loss:0.4297987917569611\n",
      "train loss:0.5716488718875531\n",
      "train loss:0.6006458030324425\n",
      "train loss:0.588083424471552\n",
      "train loss:0.6628921599080728\n",
      "train loss:0.5372524783030592\n",
      "train loss:0.41479337629462504\n",
      "train loss:0.4954265871593\n",
      "train loss:0.47073901478094876\n",
      "train loss:0.6387464329709127\n",
      "train loss:0.5965033621592927\n",
      "train loss:0.854332411347845\n",
      "train loss:0.6132994635150226\n",
      "train loss:0.84381402007207\n",
      "train loss:0.4413059892590402\n",
      "train loss:0.6389570214669293\n",
      "train loss:0.6116492432369046\n",
      "train loss:0.4577010833456153\n",
      "train loss:0.6788215338891651\n",
      "train loss:0.580265726486503\n",
      "train loss:0.6022885042693247\n",
      "train loss:0.7648877973693129\n",
      "train loss:0.5922535876924445\n",
      "train loss:0.474614006296456\n",
      "train loss:0.5948389705344466\n",
      "train loss:0.4925141033765157\n",
      "train loss:0.7672555981775394\n",
      "train loss:0.7387313450429128\n",
      "train loss:0.4400430972406458\n",
      "train loss:0.6898318280158835\n",
      "train loss:0.46662470044143606\n",
      "train loss:0.5914192873645444\n",
      "train loss:0.4851246845689149\n",
      "train loss:0.6712525518388037\n",
      "train loss:0.7074707639758268\n",
      "train loss:0.35856383054472196\n",
      "train loss:0.7460407687313214\n",
      "train loss:0.5580385597022042\n",
      "train loss:0.5899173702771877\n",
      "train loss:0.6127455630331291\n",
      "train loss:0.6684209116696109\n",
      "train loss:0.6366554873703965\n",
      "train loss:0.45378599630295147\n",
      "train loss:0.42741204772293917\n",
      "train loss:0.49887311502491666\n",
      "train loss:0.43920792288107197\n",
      "train loss:0.430724115576919\n",
      "train loss:0.6327958222328599\n",
      "train loss:0.4809832968897454\n",
      "train loss:0.47697231328006706\n",
      "train loss:0.6353907429737385\n",
      "train loss:0.37054339264132863\n",
      "train loss:0.7780198152669515\n",
      "train loss:0.3182828394527416\n",
      "train loss:0.2905779405106806\n",
      "train loss:0.6912947368724743\n",
      "train loss:0.7055847125629987\n",
      "train loss:0.6669808049857993\n",
      "train loss:0.23135496071103923\n",
      "train loss:1.0672090752597443\n",
      "train loss:0.528389962313824\n",
      "train loss:0.5141981663970451\n",
      "train loss:0.6265247710855647\n",
      "train loss:0.6091397800384246\n",
      "train loss:0.6417933315241908\n",
      "train loss:0.6716704482444301\n",
      "train loss:0.6334446319520244\n",
      "train loss:0.5889902152582949\n",
      "train loss:0.46940022255016867\n",
      "train loss:0.5288937432089106\n",
      "train loss:0.499375303013643\n",
      "train loss:0.6855466275348681\n",
      "train loss:0.5096593765709238\n",
      "train loss:0.4235431416485175\n",
      "train loss:0.5553072859834411\n",
      "train loss:0.7991945818273407\n",
      "train loss:0.5185836290045391\n",
      "train loss:0.5450686023966335\n",
      "train loss:0.5540627805905817\n",
      "train loss:0.797031160094802\n",
      "train loss:0.43845044541669453\n",
      "train loss:0.4353268750081044\n",
      "train loss:0.6015404905548994\n",
      "train loss:0.8043107441416151\n",
      "train loss:0.6511835676122442\n",
      "train loss:0.6365589411448598\n",
      "train loss:0.6861974016073253\n",
      "train loss:0.543918301285523\n",
      "train loss:0.4543031857651111\n",
      "train loss:0.48918988928524304\n",
      "train loss:0.787721548528069\n",
      "train loss:0.8366632471011952\n",
      "train loss:0.6958145904047165\n",
      "train loss:0.7266576745909683\n",
      "train loss:0.607044196233638\n",
      "train loss:0.9629021144466364\n",
      "train loss:0.5366058699639615\n",
      "train loss:0.5924893237191264\n",
      "train loss:0.4516248542581619\n",
      "train loss:0.3884145106736615\n",
      "train loss:0.5799815471736289\n",
      "train loss:0.4599483737500159\n",
      "train loss:0.5144020475351703\n",
      "train loss:0.5323557266769456\n",
      "train loss:0.5467033391893713\n",
      "train loss:0.6303995875790946\n",
      "train loss:0.4316555334497312\n",
      "train loss:0.4989073264738805\n",
      "train loss:0.5804660812108867\n",
      "train loss:0.6631473807199227\n",
      "train loss:0.9330415954528986\n",
      "train loss:0.7570632890143097\n",
      "train loss:0.8727991376380787\n",
      "train loss:0.4877694151367235\n",
      "train loss:0.6526405706266749\n",
      "train loss:0.7373680272063422\n",
      "train loss:0.6242249434419845\n",
      "train loss:0.3695518098346661\n",
      "train loss:0.5246115612780218\n",
      "train loss:0.42734847418467725\n",
      "train loss:0.6267123477034436\n",
      "train loss:0.5745983587517619\n",
      "train loss:0.4686849612472287\n",
      "train loss:0.7747777871698921\n",
      "=== epoch:10, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5282314627405557\n",
      "train loss:0.5697664128304626\n",
      "train loss:0.541375949856892\n",
      "train loss:0.5186667746799298\n",
      "train loss:0.5993403211741439\n",
      "train loss:0.5669152330686799\n",
      "train loss:0.7743663982378555\n",
      "train loss:0.7391588505137006\n",
      "train loss:0.6818743885539441\n",
      "train loss:0.5512565814038893\n",
      "train loss:0.4161869357985342\n",
      "train loss:0.630178039530592\n",
      "train loss:0.510633438988418\n",
      "train loss:0.7562220821011818\n",
      "train loss:0.5009692847554259\n",
      "train loss:0.3738841590617284\n",
      "train loss:0.7534090642608218\n",
      "train loss:0.5737983704639351\n",
      "train loss:0.4232960000850599\n",
      "train loss:0.5504750369465172\n",
      "train loss:0.8717557461661034\n",
      "train loss:0.7375090893036782\n",
      "train loss:0.7108181928893142\n",
      "train loss:0.6730740849277039\n",
      "train loss:0.7609716900478107\n",
      "train loss:0.5469989261803434\n",
      "train loss:0.666426124693339\n",
      "train loss:0.6125365729394928\n",
      "train loss:0.6306939654579511\n",
      "train loss:0.44127672282719355\n",
      "train loss:0.5806058956179209\n",
      "train loss:0.6683081579634848\n",
      "train loss:0.6078182810203893\n",
      "train loss:0.5035749199257762\n",
      "train loss:0.41075049927666407\n",
      "train loss:0.42435567375873634\n",
      "train loss:0.7154938375855429\n",
      "train loss:0.6065696830890334\n",
      "train loss:0.413967607239704\n",
      "train loss:0.39470662641765963\n",
      "train loss:0.5346782469991826\n",
      "train loss:0.7563921088840051\n",
      "train loss:0.832665968073593\n",
      "train loss:0.5142987699632369\n",
      "train loss:0.4795550825459939\n",
      "train loss:0.6232958307055527\n",
      "train loss:0.785439680762358\n",
      "train loss:0.8788428303546114\n",
      "train loss:0.5004839080591146\n",
      "train loss:0.3513666245537168\n",
      "train loss:0.5677873215374637\n",
      "train loss:0.7346682212221644\n",
      "train loss:0.48837226116999605\n",
      "train loss:0.749979521177049\n",
      "train loss:0.5299245130262892\n",
      "train loss:0.7379112945844256\n",
      "train loss:0.5636678978970889\n",
      "train loss:0.6602585891411628\n",
      "train loss:0.48450735938275447\n",
      "train loss:0.5387744471596747\n",
      "train loss:0.681139472430015\n",
      "train loss:0.4343630174040155\n",
      "train loss:0.7179606948575058\n",
      "train loss:0.5566029175654392\n",
      "train loss:0.5969880795524914\n",
      "train loss:0.7444986971818072\n",
      "train loss:0.6458728292493242\n",
      "train loss:0.6366055315893367\n",
      "train loss:0.5167878911133225\n",
      "train loss:0.44489355625534854\n",
      "train loss:0.7050847302181215\n",
      "train loss:0.5503098663164306\n",
      "train loss:0.6131204959915075\n",
      "train loss:0.6263293823058976\n",
      "train loss:0.5944857117990298\n",
      "train loss:0.5374093369475955\n",
      "train loss:0.5927337867749158\n",
      "train loss:0.7211358418412752\n",
      "train loss:0.7890354428998831\n",
      "train loss:0.42267259018605835\n",
      "train loss:0.6902586834309549\n",
      "train loss:0.62411040253017\n",
      "train loss:0.5030382552818882\n",
      "train loss:0.5808066767623183\n",
      "train loss:0.8644030106058574\n",
      "train loss:0.37143028859235\n",
      "train loss:0.7005567128958553\n",
      "train loss:0.44213532546912704\n",
      "train loss:0.5889296676559163\n",
      "train loss:0.7261832818902183\n",
      "train loss:0.41618427794698487\n",
      "train loss:0.5601062882102732\n",
      "train loss:0.6369752995200407\n",
      "train loss:0.8038775078196796\n",
      "train loss:0.5297268559035843\n",
      "train loss:0.5152105534523034\n",
      "train loss:0.37942677908296035\n",
      "train loss:0.5707941251580054\n",
      "train loss:0.6435334758450907\n",
      "train loss:0.39618100823222324\n",
      "train loss:0.5054359933157642\n",
      "train loss:0.7383092987776978\n",
      "train loss:0.5206935596409245\n",
      "train loss:0.5107866088636717\n",
      "train loss:0.7149930784712203\n",
      "train loss:0.5889628002301452\n",
      "train loss:0.6440178531032468\n",
      "train loss:0.6926499502081775\n",
      "train loss:0.6156043394969734\n",
      "train loss:0.6388224265969962\n",
      "train loss:0.3855268926971473\n",
      "train loss:0.441977554111307\n",
      "train loss:0.765060608888767\n",
      "train loss:0.7608880885603042\n",
      "train loss:0.6421366252687519\n",
      "train loss:0.551598559911192\n",
      "train loss:0.5436676893478517\n",
      "train loss:0.3859339580203852\n",
      "train loss:0.4638248333227392\n",
      "train loss:0.45508812391721587\n",
      "train loss:0.49011106466748994\n",
      "train loss:0.6336199602246133\n",
      "train loss:0.648804132780288\n",
      "train loss:0.7399949260295479\n",
      "train loss:0.49740335437375166\n",
      "train loss:0.7873059419521367\n",
      "train loss:0.49871098222938004\n",
      "train loss:0.6875409458610828\n",
      "train loss:0.5591349211425595\n",
      "train loss:0.6492046810696297\n",
      "train loss:0.5720846837313193\n",
      "train loss:0.7720141905307443\n",
      "train loss:0.7821237364103399\n",
      "train loss:0.7442353777538805\n",
      "train loss:0.6216853871628818\n",
      "train loss:0.8485897988982204\n",
      "train loss:0.6187865687545762\n",
      "train loss:0.6513155517637721\n",
      "train loss:0.7474444411287992\n",
      "train loss:0.6373922470051284\n",
      "train loss:0.6584701116511682\n",
      "train loss:0.4987682583779292\n",
      "train loss:0.6688997049538117\n",
      "train loss:0.5964032896383393\n",
      "train loss:0.6584401972155394\n",
      "train loss:0.5167410282596695\n",
      "train loss:0.5648451105878765\n",
      "train loss:0.6302877022878551\n",
      "train loss:0.5974878165055422\n",
      "train loss:0.781505671294739\n",
      "train loss:0.7613615525724131\n",
      "train loss:0.6611652972900095\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5490196078431373\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 10, 'filter_size': 7, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a402803b-ec48-4c97-85c0-0f5f88761340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6918437963654605\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.688096495473791\n",
      "train loss:0.6815928026559086\n",
      "train loss:0.6739298888440098\n",
      "train loss:0.6913868488340634\n",
      "train loss:0.6483725298130925\n",
      "train loss:0.6487364854494777\n",
      "train loss:0.7000266648225285\n",
      "train loss:0.583001378020084\n",
      "train loss:0.626131199720137\n",
      "train loss:0.5444416635908292\n",
      "train loss:0.617661673454801\n",
      "train loss:0.6662156085830981\n",
      "train loss:0.4844179653980313\n",
      "train loss:0.6278261816534106\n",
      "train loss:0.9701751093927333\n",
      "train loss:0.5944772042941276\n",
      "train loss:0.6134293852343675\n",
      "train loss:0.4998105983013835\n",
      "train loss:0.7072458497902258\n",
      "train loss:0.5362388182140709\n",
      "train loss:0.649059465150934\n",
      "train loss:0.3859065428027339\n",
      "train loss:0.619357723212177\n",
      "train loss:0.4664577050519891\n",
      "train loss:0.6095424250094357\n",
      "train loss:0.4981509880102347\n",
      "train loss:0.41301891306868593\n",
      "train loss:0.5914301223928917\n",
      "train loss:0.8384186949361455\n",
      "train loss:0.8481634409805847\n",
      "train loss:0.397746225436442\n",
      "train loss:0.513445750137867\n",
      "train loss:0.7303534221578808\n",
      "train loss:0.7342537786034221\n",
      "train loss:0.4071347299039211\n",
      "train loss:0.6234817631424371\n",
      "train loss:0.3335666611798499\n",
      "train loss:0.497370986764742\n",
      "train loss:0.5725592570486568\n",
      "train loss:0.5830639613737759\n",
      "train loss:0.6278489113514695\n",
      "train loss:0.478331284037388\n",
      "train loss:0.6443547388395584\n",
      "train loss:0.691346313253789\n",
      "train loss:0.7124294922701582\n",
      "train loss:0.5956356703070901\n",
      "train loss:0.5331674147567771\n",
      "train loss:0.6863515242056772\n",
      "train loss:0.49791947870473613\n",
      "train loss:0.7079920821622936\n",
      "train loss:0.5801621513083985\n",
      "train loss:0.4377685760923094\n",
      "train loss:0.9868498403689262\n",
      "train loss:0.45493890751361493\n",
      "train loss:0.635622340787726\n",
      "train loss:0.6219712703126825\n",
      "train loss:0.5071995730571057\n",
      "train loss:0.8735485259212699\n",
      "train loss:0.6965617436985572\n",
      "train loss:0.7366338809685438\n",
      "train loss:0.6970143212483889\n",
      "train loss:0.5921858839547527\n",
      "train loss:0.575829283622415\n",
      "train loss:0.7092714063874117\n",
      "train loss:0.5817788514344295\n",
      "train loss:0.5634387257399316\n",
      "train loss:0.5324529583661021\n",
      "train loss:0.6351855356177429\n",
      "train loss:0.6094427942887232\n",
      "train loss:0.6199305684364392\n",
      "train loss:0.5495903289100381\n",
      "train loss:0.5932999443270688\n",
      "train loss:0.8413037205843826\n",
      "train loss:0.4455148543939972\n",
      "train loss:0.5191299669582249\n",
      "train loss:0.5081360657414125\n",
      "train loss:0.312189570289158\n",
      "train loss:0.5120314887020179\n",
      "train loss:0.4932528891619574\n",
      "train loss:0.5144309670526985\n",
      "train loss:0.6832438004025212\n",
      "train loss:0.6663699393778535\n",
      "train loss:0.44405134770738447\n",
      "train loss:0.8180304964168142\n",
      "train loss:0.45933266076649487\n",
      "train loss:0.762866751046002\n",
      "train loss:0.5349303196467934\n",
      "train loss:0.3945752749783212\n",
      "train loss:0.5292955590272433\n",
      "train loss:0.6406232240954648\n",
      "train loss:0.560239680265493\n",
      "train loss:0.5177927821363364\n",
      "train loss:0.7061186083653918\n",
      "train loss:0.8464082591781882\n",
      "train loss:0.757249059149881\n",
      "train loss:0.5379863832147784\n",
      "train loss:0.6241516867051515\n",
      "train loss:0.5650550667617285\n",
      "train loss:0.6170253236466705\n",
      "train loss:0.658141387239022\n",
      "train loss:0.7214628768851545\n",
      "train loss:0.6301271138093257\n",
      "train loss:0.5186350045753831\n",
      "train loss:0.7273629990411767\n",
      "train loss:0.5780985497322954\n",
      "train loss:0.5739547539324502\n",
      "train loss:0.5567147758117682\n",
      "train loss:0.562175732756722\n",
      "train loss:0.5623156299847099\n",
      "train loss:0.46860222199138796\n",
      "train loss:0.5299362671719263\n",
      "train loss:0.6094566910449578\n",
      "train loss:0.4147002163797371\n",
      "train loss:0.811754502985572\n",
      "train loss:0.6250375182918029\n",
      "train loss:0.9741951246176314\n",
      "train loss:0.3945480136867071\n",
      "train loss:0.41251108028837874\n",
      "train loss:0.6169535691085553\n",
      "train loss:0.5456104625890893\n",
      "train loss:0.7410026968133919\n",
      "train loss:0.4007001765724147\n",
      "train loss:0.40533751406441515\n",
      "train loss:0.5346863418295483\n",
      "train loss:0.7336702091492119\n",
      "train loss:0.7288606441641481\n",
      "train loss:0.9090900202255614\n",
      "train loss:0.8100185166641699\n",
      "train loss:0.8572966366712184\n",
      "train loss:0.6696327320829709\n",
      "train loss:0.6189869361719351\n",
      "train loss:0.6623748508236018\n",
      "train loss:0.635488144052914\n",
      "train loss:0.5899562640784118\n",
      "train loss:0.6720892120179891\n",
      "train loss:0.6455156508967266\n",
      "train loss:0.6466753101660526\n",
      "train loss:0.6434949693526884\n",
      "train loss:0.7310882508076553\n",
      "train loss:0.6237893270074559\n",
      "train loss:0.6153299320274364\n",
      "train loss:0.6680046893033611\n",
      "train loss:0.6765036778715412\n",
      "train loss:0.5856177421245986\n",
      "train loss:0.6135032566076698\n",
      "train loss:0.6188942840263341\n",
      "train loss:0.596785489407189\n",
      "train loss:0.6683198586260405\n",
      "train loss:0.7574489707180633\n",
      "train loss:0.5406645806528295\n",
      "train loss:0.6244448910937815\n",
      "train loss:0.6292464231850008\n",
      "train loss:0.6604899466866434\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5607544094917681\n",
      "train loss:0.6313444605422645\n",
      "train loss:0.7320706730935838\n",
      "train loss:0.7106131614667931\n",
      "train loss:0.5455750257535439\n",
      "train loss:0.6063164302016572\n",
      "train loss:0.5424844565706668\n",
      "train loss:0.5966540671466555\n",
      "train loss:0.6708224287055566\n",
      "train loss:0.6960187544371178\n",
      "train loss:0.5198815680049977\n",
      "train loss:0.6875059672612145\n",
      "train loss:1.019696055731219\n",
      "train loss:0.5483613395317213\n",
      "train loss:0.7051541184979343\n",
      "train loss:0.5656836966434817\n",
      "train loss:0.6794902197659024\n",
      "train loss:0.6762520318131188\n",
      "train loss:0.6799895724080678\n",
      "train loss:0.43612608137657194\n",
      "train loss:0.42828217400968605\n",
      "train loss:0.8029121529965886\n",
      "train loss:0.6273516030053117\n",
      "train loss:0.6226440190022017\n",
      "train loss:0.47875973977394465\n",
      "train loss:0.547498916031656\n",
      "train loss:0.4562587114483856\n",
      "train loss:0.5351490007609366\n",
      "train loss:0.6211641730708898\n",
      "train loss:0.4224379523459941\n",
      "train loss:0.6258686665982427\n",
      "train loss:0.4039431651995141\n",
      "train loss:0.9467840315332003\n",
      "train loss:0.7966007297256288\n",
      "train loss:0.3531199254236762\n",
      "train loss:0.3907476562898067\n",
      "train loss:0.6700659017609908\n",
      "train loss:0.5905125374514163\n",
      "train loss:0.37374133593828296\n",
      "train loss:0.4865560022554957\n",
      "train loss:0.364373529079194\n",
      "train loss:0.9062665916769769\n",
      "train loss:0.5105880924838201\n",
      "train loss:0.4893092471119288\n",
      "train loss:0.6179608092848418\n",
      "train loss:0.6152369886196459\n",
      "train loss:0.5115690649181023\n",
      "train loss:0.7045918354820259\n",
      "train loss:0.6179158148965265\n",
      "train loss:0.5210872755989129\n",
      "train loss:0.5154628727696268\n",
      "train loss:0.6407306257801706\n",
      "train loss:0.7882622472109564\n",
      "train loss:0.5376685697835935\n",
      "train loss:0.6835379923615963\n",
      "train loss:0.6147715704419691\n",
      "train loss:0.6131880792478228\n",
      "train loss:0.5474050269029846\n",
      "train loss:0.6788853660968657\n",
      "train loss:0.5446050998959688\n",
      "train loss:0.5469315823686008\n",
      "train loss:0.6112558518155955\n",
      "train loss:0.6761300122827139\n",
      "train loss:0.5261406501092195\n",
      "train loss:0.6786402419517206\n",
      "train loss:0.704760443772726\n",
      "train loss:0.3535297603720959\n",
      "train loss:0.8324129644833931\n",
      "train loss:0.44183678540143\n",
      "train loss:0.6806517776379754\n",
      "train loss:0.5325674589615115\n",
      "train loss:0.6168858928286942\n",
      "train loss:0.5983605792140205\n",
      "train loss:0.5017329086104458\n",
      "train loss:0.6050497379966391\n",
      "train loss:0.7936562382804899\n",
      "train loss:0.7945842718301168\n",
      "train loss:0.6082624828927065\n",
      "train loss:0.759255662694047\n",
      "train loss:0.6044215250198575\n",
      "train loss:0.7887886201108417\n",
      "train loss:0.6106571361450199\n",
      "train loss:0.5082128923139606\n",
      "train loss:0.6241962177240084\n",
      "train loss:0.6293407957257118\n",
      "train loss:0.506439463413622\n",
      "train loss:0.5810632181660097\n",
      "train loss:0.7845530273044783\n",
      "train loss:0.662557277819394\n",
      "train loss:0.6314018545987057\n",
      "train loss:0.6049794972458286\n",
      "train loss:0.6743297935732641\n",
      "train loss:0.6708947455137644\n",
      "train loss:0.6800047094697044\n",
      "train loss:0.6170400498076847\n",
      "train loss:0.4976525914664148\n",
      "train loss:0.42599015944189855\n",
      "train loss:0.6761809594392938\n",
      "train loss:0.6931488150986505\n",
      "train loss:0.5277761455262965\n",
      "train loss:0.6124405199786327\n",
      "train loss:0.6085674711197818\n",
      "train loss:0.6898833666187789\n",
      "train loss:0.440499015994155\n",
      "train loss:0.7783684622533976\n",
      "train loss:0.528728672453554\n",
      "train loss:0.6145649500197303\n",
      "train loss:0.7734824969775489\n",
      "train loss:0.9509272463597893\n",
      "train loss:0.851627080214611\n",
      "train loss:0.6131877231841835\n",
      "train loss:0.3851671985546111\n",
      "train loss:0.4090084581882241\n",
      "train loss:0.4656568223370555\n",
      "train loss:0.6852577880828047\n",
      "train loss:0.6171495205139615\n",
      "train loss:0.5345769859631266\n",
      "train loss:0.7440758085922038\n",
      "train loss:0.6095999209973166\n",
      "train loss:0.7586708517828752\n",
      "train loss:0.5403649069587838\n",
      "train loss:0.43514609822832134\n",
      "train loss:0.4276714361871775\n",
      "train loss:0.7926336453615369\n",
      "train loss:0.6944294833562227\n",
      "train loss:0.44330345558139816\n",
      "train loss:0.5196348677837074\n",
      "train loss:0.594336349465937\n",
      "train loss:0.41706006370395804\n",
      "train loss:0.598813327168185\n",
      "train loss:0.8794813546539244\n",
      "train loss:0.6168567254971362\n",
      "train loss:1.010719151220251\n",
      "train loss:0.6855236444099111\n",
      "train loss:0.5390652116543706\n",
      "train loss:0.5325729157029598\n",
      "train loss:0.527233809198407\n",
      "train loss:0.3822524124983502\n",
      "train loss:0.6053391233588548\n",
      "train loss:0.6942824013773631\n",
      "train loss:0.6046218702026884\n",
      "train loss:0.5094803974690314\n",
      "train loss:0.517892057433755\n",
      "train loss:0.7935349665908841\n",
      "train loss:0.4425989324689955\n",
      "train loss:0.530626765906988\n",
      "train loss:0.33047124798615046\n",
      "train loss:0.5907144711897987\n",
      "train loss:0.3733556583497781\n",
      "train loss:0.7585990259145184\n",
      "train loss:0.5024635489666618\n",
      "train loss:0.7545099088632023\n",
      "train loss:0.8481930578834307\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.592034462330798\n",
      "train loss:0.5926632353650814\n",
      "train loss:0.6060918325513339\n",
      "train loss:0.41079049328176076\n",
      "train loss:0.47902957865418905\n",
      "train loss:0.48581943248634546\n",
      "train loss:0.6173019803409762\n",
      "train loss:0.9028270553105685\n",
      "train loss:0.5179803234126255\n",
      "train loss:0.5279165244713553\n",
      "train loss:0.7180864694094418\n",
      "train loss:0.5110610610193442\n",
      "train loss:0.762869303965287\n",
      "train loss:0.671170715855014\n",
      "train loss:0.5470499683907066\n",
      "train loss:0.38790129341084423\n",
      "train loss:0.6834930300178591\n",
      "train loss:0.7747116706579513\n",
      "train loss:0.6746513715947422\n",
      "train loss:0.5963825075382414\n",
      "train loss:0.7362684780928692\n",
      "train loss:0.5481467510477209\n",
      "train loss:0.6052624486958654\n",
      "train loss:0.6126306838728264\n",
      "train loss:0.4622320469788888\n",
      "train loss:0.5942984714259562\n",
      "train loss:0.544964135361962\n",
      "train loss:0.5800169152735797\n",
      "train loss:0.6087597601820443\n",
      "train loss:0.4328826638531771\n",
      "train loss:0.6000065523965937\n",
      "train loss:0.535449876019364\n",
      "train loss:0.7414263790906104\n",
      "train loss:0.6399480817182935\n",
      "train loss:0.7263502588011019\n",
      "train loss:0.7434469345528774\n",
      "train loss:0.6934063014861376\n",
      "train loss:0.6101898867811316\n",
      "train loss:0.5990588190168216\n",
      "train loss:0.6802135585634987\n",
      "train loss:0.6231178983531814\n",
      "train loss:0.825255142919157\n",
      "train loss:0.46819795507696876\n",
      "train loss:0.7293998642447781\n",
      "train loss:0.5937001803092615\n",
      "train loss:0.6316877677214567\n",
      "train loss:0.6801020897931775\n",
      "train loss:0.7326324536348316\n",
      "train loss:0.6241482970476671\n",
      "train loss:0.6293099076249674\n",
      "train loss:0.622841353747364\n",
      "train loss:0.7293567991264058\n",
      "train loss:0.49136453021314913\n",
      "train loss:0.519879049661608\n",
      "train loss:0.6300906552820129\n",
      "train loss:0.6283433032124321\n",
      "train loss:0.5622977058792856\n",
      "train loss:0.42208090537764953\n",
      "train loss:0.6160015700644983\n",
      "train loss:0.5964245230628016\n",
      "train loss:0.6709545523355167\n",
      "train loss:0.778438456568652\n",
      "train loss:0.6025305148505239\n",
      "train loss:0.606047351309795\n",
      "train loss:0.8615889521112278\n",
      "train loss:0.6139919421743464\n",
      "train loss:0.5245786417728462\n",
      "train loss:0.3382123568160326\n",
      "train loss:0.6043942214191645\n",
      "train loss:0.6073479839057049\n",
      "train loss:0.7166468334345587\n",
      "train loss:0.5842118800277543\n",
      "train loss:0.41535959779135434\n",
      "train loss:0.7259836014896628\n",
      "train loss:0.5773489584936408\n",
      "train loss:0.7180868534792213\n",
      "train loss:0.5027374544024463\n",
      "train loss:0.6280492049605514\n",
      "train loss:0.5365066372975082\n",
      "train loss:0.5219810617092269\n",
      "train loss:0.7817834767574501\n",
      "train loss:0.4954604688314907\n",
      "train loss:0.5363666027743923\n",
      "train loss:0.527432711028198\n",
      "train loss:0.5220670730030511\n",
      "train loss:0.8133729411119507\n",
      "train loss:0.6216480795725426\n",
      "train loss:0.6085534764895553\n",
      "train loss:0.7282707029670543\n",
      "train loss:0.49559597149627377\n",
      "train loss:0.697085436722669\n",
      "train loss:0.5177638007192004\n",
      "train loss:0.5258152221881717\n",
      "train loss:0.5454296943819228\n",
      "train loss:0.6196049728151234\n",
      "train loss:0.6700601759945949\n",
      "train loss:0.6631612539361432\n",
      "train loss:0.36478132436571853\n",
      "train loss:0.6268939394183753\n",
      "train loss:0.44325774164474313\n",
      "train loss:0.41677220789445213\n",
      "train loss:0.5080515787970308\n",
      "train loss:0.5135225516513406\n",
      "train loss:0.2459281817091617\n",
      "train loss:0.24258107680390645\n",
      "train loss:0.561466995143743\n",
      "train loss:0.5039866801914561\n",
      "train loss:0.7071333107247446\n",
      "train loss:0.823576192904355\n",
      "train loss:0.9441818575919738\n",
      "train loss:0.4766292917872432\n",
      "train loss:0.6060410336640175\n",
      "train loss:0.6057940447887866\n",
      "train loss:0.46247387561260567\n",
      "train loss:0.6143793523218679\n",
      "train loss:0.4656139255839828\n",
      "train loss:0.8378772233842957\n",
      "train loss:0.6376518308870083\n",
      "train loss:0.527818180985611\n",
      "train loss:0.6982710435736924\n",
      "train loss:0.6134763999464151\n",
      "train loss:0.6060285118961115\n",
      "train loss:0.7325678372623929\n",
      "train loss:0.5555456117831301\n",
      "train loss:0.4334963020215371\n",
      "train loss:0.6679962704039092\n",
      "train loss:0.5110180390932099\n",
      "train loss:0.6167906546085946\n",
      "train loss:0.6024453541839713\n",
      "train loss:0.5676381825849216\n",
      "train loss:0.6047301117535155\n",
      "train loss:0.6806747227602103\n",
      "train loss:0.6690341908181344\n",
      "train loss:0.6268460612334784\n",
      "train loss:0.5912643172101595\n",
      "train loss:0.539033811072323\n",
      "train loss:0.6447202732206472\n",
      "train loss:0.4372421266736456\n",
      "train loss:0.5351621207126329\n",
      "train loss:0.7994195936686361\n",
      "train loss:0.6642794819443777\n",
      "train loss:0.5960243701901597\n",
      "train loss:0.5060518834485725\n",
      "train loss:0.8063632248815773\n",
      "train loss:0.422498214213721\n",
      "train loss:0.42606144874700924\n",
      "train loss:0.612184059683462\n",
      "train loss:0.411059150691033\n",
      "train loss:0.8814583439109256\n",
      "train loss:0.6207407979978082\n",
      "train loss:0.6221139396150451\n",
      "train loss:0.3891831655099131\n",
      "train loss:0.7082771615301893\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6387456669781535\n",
      "train loss:0.6195458308455168\n",
      "train loss:0.6064772001479253\n",
      "train loss:0.5055735499348435\n",
      "train loss:0.5207528858719905\n",
      "train loss:0.5939275679047148\n",
      "train loss:0.6693341025861187\n",
      "train loss:0.6823038448025812\n",
      "train loss:0.6929775915319367\n",
      "train loss:0.630244617818229\n",
      "train loss:0.4278528218271167\n",
      "train loss:0.8345104317820123\n",
      "train loss:0.5443753417642537\n",
      "train loss:0.5942707734767805\n",
      "train loss:0.878677302295916\n",
      "train loss:0.7768268789203031\n",
      "train loss:0.6067623134449395\n",
      "train loss:0.5628921203559752\n",
      "train loss:0.5587950129690596\n",
      "train loss:0.5665538883049999\n",
      "train loss:0.6151122293175625\n",
      "train loss:0.5420848013144546\n",
      "train loss:0.5623856089172887\n",
      "train loss:0.5341754935741282\n",
      "train loss:0.6188103768917794\n",
      "train loss:0.48096758593439803\n",
      "train loss:0.6247888991817097\n",
      "train loss:0.45261685665278906\n",
      "train loss:0.5899546007337438\n",
      "train loss:0.684399525460041\n",
      "train loss:0.7074853073060478\n",
      "train loss:0.4267817572531108\n",
      "train loss:0.5779695732857312\n",
      "train loss:0.5730440283607101\n",
      "train loss:0.7823416563731874\n",
      "train loss:0.48108821165027366\n",
      "train loss:0.6427222587997582\n",
      "train loss:0.4803130471026849\n",
      "train loss:0.7702075164986157\n",
      "train loss:0.47780994587696224\n",
      "train loss:0.6153027936162929\n",
      "train loss:0.2921108747004455\n",
      "train loss:0.5843944784522396\n",
      "train loss:0.5057906757125249\n",
      "train loss:0.3869835408581067\n",
      "train loss:0.6473046693532656\n",
      "train loss:0.7316624169238514\n",
      "train loss:0.47879104413826595\n",
      "train loss:0.7551060582102984\n",
      "train loss:0.7002191751197133\n",
      "train loss:0.37742803658961455\n",
      "train loss:0.7857218585417783\n",
      "train loss:0.5187483640579394\n",
      "train loss:0.6824722795272684\n",
      "train loss:0.4356160250133764\n",
      "train loss:0.6313369624672108\n",
      "train loss:0.6921006543664328\n",
      "train loss:0.5934307248231496\n",
      "train loss:0.6852884622398766\n",
      "train loss:0.5351955405648675\n",
      "train loss:0.4758173481545526\n",
      "train loss:0.45896241669283383\n",
      "train loss:0.6224709173194011\n",
      "train loss:0.6537181169974036\n",
      "train loss:0.4453781310355736\n",
      "train loss:0.5155826184125213\n",
      "train loss:0.511312326027026\n",
      "train loss:0.5791859070573869\n",
      "train loss:0.5951525430038195\n",
      "train loss:0.5353169516624224\n",
      "train loss:0.5923878947075321\n",
      "train loss:0.3833163638239604\n",
      "train loss:0.49125647734997385\n",
      "train loss:0.8168997091478147\n",
      "train loss:0.4985874655844308\n",
      "train loss:0.47174965014269654\n",
      "train loss:0.7491023391093623\n",
      "train loss:0.5987587248171986\n",
      "train loss:0.467781998784308\n",
      "train loss:0.7169815597243543\n",
      "train loss:0.8095527571126112\n",
      "train loss:0.583441904309858\n",
      "train loss:0.4086894457927298\n",
      "train loss:0.6633594742124999\n",
      "train loss:0.6635853592624559\n",
      "train loss:0.6840309115215414\n",
      "train loss:0.5847644118627648\n",
      "train loss:0.5642405464486198\n",
      "train loss:0.6192847951346215\n",
      "train loss:0.4011103695945756\n",
      "train loss:0.5578077531222905\n",
      "train loss:0.6021016540301871\n",
      "train loss:0.6119048491819803\n",
      "train loss:0.5954069110316713\n",
      "train loss:0.9629592103989125\n",
      "train loss:0.7152309885465924\n",
      "train loss:0.4729842849489768\n",
      "train loss:0.6808092602755015\n",
      "train loss:0.6999382446895421\n",
      "train loss:0.6361558670485726\n",
      "train loss:0.7332952542100462\n",
      "train loss:0.6194473914487096\n",
      "train loss:0.591665203304075\n",
      "train loss:0.556657699952239\n",
      "train loss:0.591858200137804\n",
      "train loss:0.7092630631136672\n",
      "train loss:0.5046647378526761\n",
      "train loss:0.6854503792114441\n",
      "train loss:0.5181156440535315\n",
      "train loss:0.6131467175347327\n",
      "train loss:0.6446838572924227\n",
      "train loss:0.4894381460940204\n",
      "train loss:0.4594409326013487\n",
      "train loss:0.7903954329069343\n",
      "train loss:0.5690323056696557\n",
      "train loss:0.5179656527411851\n",
      "train loss:0.6203611464826675\n",
      "train loss:0.6120173790275854\n",
      "train loss:0.5963649168084835\n",
      "train loss:0.6146176669162252\n",
      "train loss:0.4411754547510115\n",
      "train loss:0.7332067030434416\n",
      "train loss:0.7351579513556976\n",
      "train loss:0.6106658469647476\n",
      "train loss:0.5010792103028998\n",
      "train loss:0.3820808503562839\n",
      "train loss:0.5241866528144167\n",
      "train loss:0.43265444685914545\n",
      "train loss:0.7205148631833824\n",
      "train loss:0.39603267312834894\n",
      "train loss:0.7274933218444181\n",
      "train loss:0.4844759162034074\n",
      "train loss:0.5942921044965136\n",
      "train loss:0.44291197104465957\n",
      "train loss:0.6278004784564526\n",
      "train loss:0.6271744363633792\n",
      "train loss:0.4978693801624683\n",
      "train loss:0.4094361094754301\n",
      "train loss:0.24547588695008807\n",
      "train loss:0.5839172220508397\n",
      "train loss:0.5867934044835532\n",
      "train loss:0.7722793080446786\n",
      "train loss:0.4969184435292033\n",
      "train loss:0.7120588493636864\n",
      "train loss:0.6448364587005411\n",
      "train loss:0.7148564518288071\n",
      "train loss:0.6231999850655001\n",
      "train loss:0.5230360281797793\n",
      "train loss:0.6492783933433374\n",
      "train loss:0.7577887768348108\n",
      "train loss:0.6502109466283077\n",
      "train loss:0.5083147372929437\n",
      "train loss:0.5183647303605461\n",
      "=== epoch:5, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.4884116741061069\n",
      "train loss:0.4999004665095795\n",
      "train loss:0.6467420870906814\n",
      "train loss:0.5340474474871844\n",
      "train loss:0.48883581486323846\n",
      "train loss:0.7924415504438304\n",
      "train loss:0.511645561701784\n",
      "train loss:0.507211631547921\n",
      "train loss:0.3133809754371927\n",
      "train loss:0.613935601746209\n",
      "train loss:0.6030274903136998\n",
      "train loss:0.7484217189208751\n",
      "train loss:0.40028439940980415\n",
      "train loss:0.8622731294178815\n",
      "train loss:0.6918487627057073\n",
      "train loss:0.5253942007614243\n",
      "train loss:0.7229642050466089\n",
      "train loss:0.382229346517267\n",
      "train loss:0.6154501312069771\n",
      "train loss:0.505336050152567\n",
      "train loss:0.6058195705051826\n",
      "train loss:0.6674401757550777\n",
      "train loss:0.5177069476865928\n",
      "train loss:0.721288503292989\n",
      "train loss:0.5227145866582579\n",
      "train loss:0.6872950533595853\n",
      "train loss:0.6542764602757165\n",
      "train loss:0.6958613207237834\n",
      "train loss:0.6605237413332345\n",
      "train loss:0.7045123502565775\n",
      "train loss:0.5936332209394648\n",
      "train loss:0.5852168520960939\n",
      "train loss:0.5502533099115731\n",
      "train loss:0.5611606792793682\n",
      "train loss:0.6817427525275165\n",
      "train loss:0.625098327546113\n",
      "train loss:0.6162067464356099\n",
      "train loss:0.5596007159644395\n",
      "train loss:0.5783917170326485\n",
      "train loss:0.5587552159716485\n",
      "train loss:0.6462594666794624\n",
      "train loss:0.630108502840814\n",
      "train loss:0.7831772758550765\n",
      "train loss:0.7348125157366016\n",
      "train loss:0.5507022094033731\n",
      "train loss:0.3904622419938516\n",
      "train loss:0.5545942300231299\n",
      "train loss:0.6631224819223727\n",
      "train loss:0.5129514178745214\n",
      "train loss:0.42689772155456407\n",
      "train loss:0.5985816010848973\n",
      "train loss:0.6259230535758684\n",
      "train loss:0.571706956393955\n",
      "train loss:0.46560391526462713\n",
      "train loss:0.39993628840381307\n",
      "train loss:0.6268867735641507\n",
      "train loss:0.9223412472407491\n",
      "train loss:0.48475770836760557\n",
      "train loss:0.26527091408108006\n",
      "train loss:0.7231826117220023\n",
      "train loss:0.36688801658336334\n",
      "train loss:0.4589309185513334\n",
      "train loss:0.6781103921531424\n",
      "train loss:0.5179473979007396\n",
      "train loss:0.5751210647950389\n",
      "train loss:0.7110555035174289\n",
      "train loss:0.3548770537430431\n",
      "train loss:0.8507704468415322\n",
      "train loss:0.6288605067093791\n",
      "train loss:0.3925877970872655\n",
      "train loss:0.5346861483207672\n",
      "train loss:0.3216572661591149\n",
      "train loss:0.5578209436497386\n",
      "train loss:0.7287570062033755\n",
      "train loss:0.42295832952038326\n",
      "train loss:0.8838895631881956\n",
      "train loss:0.7424276096505706\n",
      "train loss:0.5183870044130081\n",
      "train loss:0.6112581435204446\n",
      "train loss:0.6908964022615864\n",
      "train loss:0.8603752011476594\n",
      "train loss:0.45476478395958536\n",
      "train loss:0.6460909500035059\n",
      "train loss:0.5265845022684147\n",
      "train loss:0.7382315929889965\n",
      "train loss:0.617768940032777\n",
      "train loss:0.6274759970139517\n",
      "train loss:0.568524105150457\n",
      "train loss:0.5092356811326602\n",
      "train loss:0.6179114074331111\n",
      "train loss:0.6340875342746382\n",
      "train loss:0.6698069240100217\n",
      "train loss:0.735136987003761\n",
      "train loss:0.4975300067781082\n",
      "train loss:0.5522243388025455\n",
      "train loss:0.5999118039487759\n",
      "train loss:0.37694839655385676\n",
      "train loss:0.5665999159369075\n",
      "train loss:0.6222244685830944\n",
      "train loss:0.5740655690807264\n",
      "train loss:0.4523876543905884\n",
      "train loss:0.6455725333207252\n",
      "train loss:0.48215370304011007\n",
      "train loss:0.8638108612768132\n",
      "train loss:0.2813236485711928\n",
      "train loss:0.6005668980115357\n",
      "train loss:0.614240283421867\n",
      "train loss:0.6161466035271521\n",
      "train loss:0.7428309224699741\n",
      "train loss:0.94496625401652\n",
      "train loss:0.5534445282345657\n",
      "train loss:0.6085810139754455\n",
      "train loss:0.589893242826211\n",
      "train loss:0.6176051247362533\n",
      "train loss:0.8370597233519603\n",
      "train loss:0.7166896086402856\n",
      "train loss:0.6165553343649033\n",
      "train loss:0.7254664897917475\n",
      "train loss:0.5377318623580465\n",
      "train loss:0.5608320631046289\n",
      "train loss:0.7177796034987874\n",
      "train loss:0.6645664531043391\n",
      "train loss:0.5486475882592735\n",
      "train loss:0.5127695932261943\n",
      "train loss:0.6542587049659693\n",
      "train loss:0.47726454160384935\n",
      "train loss:0.4404128371466192\n",
      "train loss:0.5896463776636207\n",
      "train loss:0.7089269016995999\n",
      "train loss:0.5561321809934636\n",
      "train loss:0.39213328094169725\n",
      "train loss:0.540292664612335\n",
      "train loss:0.8872689638328207\n",
      "train loss:0.49170914503798224\n",
      "train loss:0.49287415210287466\n",
      "train loss:0.5085865593455113\n",
      "train loss:0.49578943892608607\n",
      "train loss:0.4102381809604207\n",
      "train loss:0.8811095324188118\n",
      "train loss:0.6972404728188332\n",
      "train loss:0.6212886053381366\n",
      "train loss:0.6520736334266342\n",
      "train loss:0.6334158836577781\n",
      "train loss:0.6715217200064096\n",
      "train loss:0.3197080732803315\n",
      "train loss:0.799304678864756\n",
      "train loss:0.8062225068269662\n",
      "train loss:0.4995236772689358\n",
      "train loss:0.5093884617564834\n",
      "train loss:0.620964872968334\n",
      "train loss:0.5561818218642999\n",
      "train loss:0.7729056715923356\n",
      "train loss:0.6198112621494803\n",
      "=== epoch:6, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.4766245413918814\n",
      "train loss:0.698517491123224\n",
      "train loss:0.6442427780906709\n",
      "train loss:0.6749167690914193\n",
      "train loss:0.5023418244233884\n",
      "train loss:0.6784088680376252\n",
      "train loss:0.43433956626135195\n",
      "train loss:0.551122377449172\n",
      "train loss:0.6904273442701918\n",
      "train loss:0.5436724015716882\n",
      "train loss:0.46060603982218645\n",
      "train loss:0.6224847410386551\n",
      "train loss:0.6014239741273806\n",
      "train loss:0.5167374942033518\n",
      "train loss:0.40914255235306374\n",
      "train loss:0.666202787408271\n",
      "train loss:0.6867121680056916\n",
      "train loss:0.5066261995935447\n",
      "train loss:0.7080433203783302\n",
      "train loss:0.4824440779448089\n",
      "train loss:0.4750549992695804\n",
      "train loss:0.4909272627509342\n",
      "train loss:0.6375727196730352\n",
      "train loss:0.8225221006886153\n",
      "train loss:0.5823912079890695\n",
      "train loss:0.6150061040769224\n",
      "train loss:0.530273406856004\n",
      "train loss:0.5138309166736745\n",
      "train loss:0.6902303501167211\n",
      "train loss:0.6799331326531521\n",
      "train loss:0.537908587211137\n",
      "train loss:0.5172461592003529\n",
      "train loss:0.5691401605250108\n",
      "train loss:0.498996524793874\n",
      "train loss:0.6273990468296675\n",
      "train loss:0.47080256242950147\n",
      "train loss:0.6553373731901562\n",
      "train loss:0.5741014713274482\n",
      "train loss:0.6129135676352423\n",
      "train loss:0.7568937215140431\n",
      "train loss:0.4934835733263432\n",
      "train loss:0.5940573090548165\n",
      "train loss:0.6527950858820334\n",
      "train loss:0.5450334170892315\n",
      "train loss:0.7193458632516683\n",
      "train loss:0.49875003382007244\n",
      "train loss:0.6854626964227201\n",
      "train loss:0.39896110626142356\n",
      "train loss:0.41548918175260086\n",
      "train loss:0.33438123338965614\n",
      "train loss:0.5003972159651034\n",
      "train loss:0.8300253477439989\n",
      "train loss:0.6509062095260267\n",
      "train loss:0.3862221199103636\n",
      "train loss:0.5083695373908644\n",
      "train loss:0.33476974587901503\n",
      "train loss:0.45627721986878605\n",
      "train loss:0.5354494402602685\n",
      "train loss:0.7553884083230064\n",
      "train loss:0.7895657513012284\n",
      "train loss:0.4697360713821803\n",
      "train loss:0.9051134664515486\n",
      "train loss:0.4328512111849733\n",
      "train loss:0.8505762882850677\n",
      "train loss:0.38554715227794556\n",
      "train loss:0.5640482529468536\n",
      "train loss:0.7350458847996425\n",
      "train loss:0.5455321874463193\n",
      "train loss:0.6754693778012134\n",
      "train loss:0.5568968217784158\n",
      "train loss:0.7690642691566671\n",
      "train loss:0.5825792895627377\n",
      "train loss:0.6543939225703183\n",
      "train loss:0.6947505736905257\n",
      "train loss:0.5006011760111334\n",
      "train loss:0.6567839816523955\n",
      "train loss:0.6034355925002521\n",
      "train loss:0.628583527165697\n",
      "train loss:0.650198427966177\n",
      "train loss:0.5761252946901536\n",
      "train loss:0.6303670992245877\n",
      "train loss:0.722481129276061\n",
      "train loss:0.7091717306959993\n",
      "train loss:0.6942061150525547\n",
      "train loss:0.8536828026033231\n",
      "train loss:0.480519532506566\n",
      "train loss:0.5854919301170438\n",
      "train loss:0.5300951513139187\n",
      "train loss:0.5934845840387994\n",
      "train loss:0.5607560950570452\n",
      "train loss:0.6158696255184004\n",
      "train loss:0.736410408976399\n",
      "train loss:0.5587509800387799\n",
      "train loss:0.5168564119692183\n",
      "train loss:0.5496628709962017\n",
      "train loss:0.6765440562041222\n",
      "train loss:0.34917153570564513\n",
      "train loss:0.7218674047847976\n",
      "train loss:0.5780732776643217\n",
      "train loss:0.6674059054851862\n",
      "train loss:0.7764165803027857\n",
      "train loss:0.6431310672127589\n",
      "train loss:0.7343195003327634\n",
      "train loss:0.7202636615398607\n",
      "train loss:0.8527381622111632\n",
      "train loss:0.571991624825943\n",
      "train loss:0.6970017278038889\n",
      "train loss:0.7361269150054909\n",
      "train loss:0.5848191173152135\n",
      "train loss:0.5469741534121567\n",
      "train loss:0.6437934623022507\n",
      "train loss:0.5180983541202138\n",
      "train loss:0.5829883593021397\n",
      "train loss:0.4895889170192457\n",
      "train loss:0.5794221691164366\n",
      "train loss:0.5424907887375039\n",
      "train loss:0.5814709445600634\n",
      "train loss:0.3840634049335513\n",
      "train loss:0.4096486748790058\n",
      "train loss:0.5777628319716939\n",
      "train loss:0.8188019911273072\n",
      "train loss:0.4459043645677415\n",
      "train loss:0.4601119841601746\n",
      "train loss:0.6536363087422532\n",
      "train loss:0.7790630670426111\n",
      "train loss:0.5412575882405879\n",
      "train loss:0.3039251529173811\n",
      "train loss:0.5858938934776085\n",
      "train loss:0.7385853560785381\n",
      "train loss:0.6224848752122212\n",
      "train loss:0.6097398813109796\n",
      "train loss:0.6295761397042589\n",
      "train loss:0.7927475284219044\n",
      "train loss:0.5073529388390257\n",
      "train loss:0.44659194036985567\n",
      "train loss:0.552688284998168\n",
      "train loss:0.6459043651840314\n",
      "train loss:0.5899839319203346\n",
      "train loss:0.439151250187314\n",
      "train loss:0.6155912732557451\n",
      "train loss:0.8873800692260071\n",
      "train loss:0.6068741389651917\n",
      "train loss:0.6432566708377336\n",
      "train loss:0.533233832432663\n",
      "train loss:0.6526421109516234\n",
      "train loss:0.6311168834471963\n",
      "train loss:0.5891725260925942\n",
      "train loss:0.46260163341414307\n",
      "train loss:0.47339981420820926\n",
      "train loss:0.6335439356875951\n",
      "train loss:0.651940498884459\n",
      "train loss:0.4912078179645598\n",
      "train loss:0.3862359914721879\n",
      "=== epoch:7, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6465254197082292\n",
      "train loss:0.5392730990124376\n",
      "train loss:0.7127812040542387\n",
      "train loss:0.7877030166434429\n",
      "train loss:0.6447755840310055\n",
      "train loss:0.43656239365489097\n",
      "train loss:0.32294295329641304\n",
      "train loss:0.6251826238873847\n",
      "train loss:0.6350440571655492\n",
      "train loss:0.7200505822343222\n",
      "train loss:0.6326216060205986\n",
      "train loss:0.5161867134496585\n",
      "train loss:0.7890863062763792\n",
      "train loss:0.3767681571432228\n",
      "train loss:0.6314837941126584\n",
      "train loss:0.5704226905020662\n",
      "train loss:0.48281615519207854\n",
      "train loss:0.630448050857948\n",
      "train loss:0.8178960108424802\n",
      "train loss:0.5785095608496952\n",
      "train loss:0.34082854026053483\n",
      "train loss:0.5593550976654017\n",
      "train loss:0.5871520863622874\n",
      "train loss:0.5044865844692739\n",
      "train loss:0.7009501703196812\n",
      "train loss:0.5050772191190194\n",
      "train loss:0.584663074011144\n",
      "train loss:0.7070880218988507\n",
      "train loss:0.3365776341458589\n",
      "train loss:0.6663671179247961\n",
      "train loss:0.5300130983619469\n",
      "train loss:0.7005300931241514\n",
      "train loss:0.47608833169376796\n",
      "train loss:0.5148741658603846\n",
      "train loss:0.7398174346297981\n",
      "train loss:0.28864132492190137\n",
      "train loss:0.8427730170086694\n",
      "train loss:0.554966079915075\n",
      "train loss:0.8161890586142393\n",
      "train loss:0.4879952429722295\n",
      "train loss:0.46001633600659797\n",
      "train loss:0.48447906865283413\n",
      "train loss:0.5501317394629375\n",
      "train loss:0.4198565297254664\n",
      "train loss:0.834203346212132\n",
      "train loss:0.6588160115080877\n",
      "train loss:0.5251224778518215\n",
      "train loss:0.5036421996243586\n",
      "train loss:0.5128488854736316\n",
      "train loss:1.0448825927983088\n",
      "train loss:0.5110827927736221\n",
      "train loss:0.5200266737870527\n",
      "train loss:0.67404628207076\n",
      "train loss:0.6216464100949222\n",
      "train loss:0.5171239516189923\n",
      "train loss:0.7745299528297415\n",
      "train loss:0.4464668365700642\n",
      "train loss:0.5237082380251489\n",
      "train loss:0.7574838938625066\n",
      "train loss:0.5592430899155848\n",
      "train loss:0.5119593505985666\n",
      "train loss:0.4899735995393085\n",
      "train loss:0.7189872916162019\n",
      "train loss:0.6732393602794016\n",
      "train loss:0.5793502761422661\n",
      "train loss:0.531312853092225\n",
      "train loss:0.6964063901400137\n",
      "train loss:0.6143754463296813\n",
      "train loss:0.600052614308179\n",
      "train loss:0.6333043624050245\n",
      "train loss:0.6765431549627969\n",
      "train loss:0.44491571379245265\n",
      "train loss:0.49524382585818183\n",
      "train loss:0.3391462250345405\n",
      "train loss:0.31929984152857843\n",
      "train loss:0.5901169389653984\n",
      "train loss:0.6129530005954865\n",
      "train loss:0.38830276662882324\n",
      "train loss:0.30315437792393884\n",
      "train loss:0.6674784342565171\n",
      "train loss:0.5345697003035976\n",
      "train loss:0.6041830205318732\n",
      "train loss:0.5564658138240298\n",
      "train loss:0.6463321971803253\n",
      "train loss:0.5269903405502141\n",
      "train loss:0.5226322306554164\n",
      "train loss:0.9375484104817222\n",
      "train loss:0.6740745061584125\n",
      "train loss:0.5633021745996383\n",
      "train loss:0.48593128337678965\n",
      "train loss:0.7082644256830625\n",
      "train loss:0.40843412518074873\n",
      "train loss:0.6740334755482007\n",
      "train loss:0.5142297089260341\n",
      "train loss:0.7947757719246915\n",
      "train loss:0.6268324323995272\n",
      "train loss:0.4046354173768788\n",
      "train loss:0.663009093370967\n",
      "train loss:0.5976930604521795\n",
      "train loss:0.5390102996179081\n",
      "train loss:0.6972619556699319\n",
      "train loss:0.6196726396396335\n",
      "train loss:0.6266326756768972\n",
      "train loss:0.5523136072085805\n",
      "train loss:0.6800683692432858\n",
      "train loss:0.5856300460229227\n",
      "train loss:0.7965702897834964\n",
      "train loss:0.6403499544450149\n",
      "train loss:0.5512294472501774\n",
      "train loss:0.6901403959771371\n",
      "train loss:0.7948020720908177\n",
      "train loss:0.6746165370606108\n",
      "train loss:0.5549253439212214\n",
      "train loss:0.5647461376319229\n",
      "train loss:0.6721273222247338\n",
      "train loss:0.7305239454269661\n",
      "train loss:0.4486829757242841\n",
      "train loss:0.5564664603040713\n",
      "train loss:0.5941910107785521\n",
      "train loss:0.7907013109849604\n",
      "train loss:0.5387728263184315\n",
      "train loss:0.6209693509250945\n",
      "train loss:0.6229818734809838\n",
      "train loss:0.6001531930083744\n",
      "train loss:0.6407863378073057\n",
      "train loss:0.577852195768742\n",
      "train loss:0.6055779583879783\n",
      "train loss:0.4637731931237273\n",
      "train loss:0.4459225518662121\n",
      "train loss:0.7315577684853\n",
      "train loss:0.5765434039293248\n",
      "train loss:0.6485573722532503\n",
      "train loss:0.5275758325130522\n",
      "train loss:0.6462363313213043\n",
      "train loss:0.5426102087272121\n",
      "train loss:0.6391697768552277\n",
      "train loss:0.5086241863321709\n",
      "train loss:0.5870769348702075\n",
      "train loss:0.8388592002353412\n",
      "train loss:0.532535815097837\n",
      "train loss:0.5845736668750516\n",
      "train loss:0.6387217959095093\n",
      "train loss:0.577104343685642\n",
      "train loss:0.6742532946810351\n",
      "train loss:0.4359502957145455\n",
      "train loss:0.5444904153789196\n",
      "train loss:0.5778451409757186\n",
      "train loss:0.5247794017233507\n",
      "train loss:0.6246072582793541\n",
      "train loss:0.3793577843369287\n",
      "train loss:0.9257491303779396\n",
      "train loss:0.6014782155153434\n",
      "train loss:0.4378866695725285\n",
      "=== epoch:8, train acc:0.73, test acc:0.69 ===\n",
      "train loss:0.49312930572192515\n",
      "train loss:0.5186366236045087\n",
      "train loss:0.5086760193806469\n",
      "train loss:0.4970249308171738\n",
      "train loss:0.28743186909221646\n",
      "train loss:0.5051401258679518\n",
      "train loss:1.036229985847036\n",
      "train loss:0.4942001842050998\n",
      "train loss:0.3871447745667153\n",
      "train loss:0.44300195092808536\n",
      "train loss:0.8149450996815675\n",
      "train loss:0.8696268686301412\n",
      "train loss:0.4221281854448252\n",
      "train loss:0.3556451698151132\n",
      "train loss:0.39524893852761456\n",
      "train loss:0.6294863076243985\n",
      "train loss:0.41026795437688046\n",
      "train loss:0.4291454752453858\n",
      "train loss:0.40564396015461135\n",
      "train loss:0.5216297018272241\n",
      "train loss:0.8380599857296867\n",
      "train loss:0.44506463055146905\n",
      "train loss:0.5353486286702882\n",
      "train loss:0.5404492828790615\n",
      "train loss:0.8381870941309778\n",
      "train loss:0.5862615019097502\n",
      "train loss:0.5928144476628304\n",
      "train loss:0.638267942432196\n",
      "train loss:0.5007362897601471\n",
      "train loss:0.5722369569734305\n",
      "train loss:0.5982744775378941\n",
      "train loss:0.4282699776512879\n",
      "train loss:0.5238354040560779\n",
      "train loss:0.6050056905773737\n",
      "train loss:0.6008746553801976\n",
      "train loss:0.6601527988702576\n",
      "train loss:0.5033606465421967\n",
      "train loss:0.6349057518957824\n",
      "train loss:0.4325402329213109\n",
      "train loss:0.47548336941403624\n",
      "train loss:0.4291960710164136\n",
      "train loss:0.7401167257495969\n",
      "train loss:0.7310757046772041\n",
      "train loss:0.3564710676826822\n",
      "train loss:0.45414090084714714\n",
      "train loss:0.7231054384421011\n",
      "train loss:0.5002923155831431\n",
      "train loss:0.5359616659886309\n",
      "train loss:0.8100668636595654\n",
      "train loss:0.7918450366932479\n",
      "train loss:0.5189403168996218\n",
      "train loss:0.49497018523034786\n",
      "train loss:0.652592057296242\n",
      "train loss:0.4889159573353087\n",
      "train loss:0.5252999262890571\n",
      "train loss:0.5832566481897303\n",
      "train loss:0.5836628756752258\n",
      "train loss:0.43799774294290056\n",
      "train loss:0.582230402163337\n",
      "train loss:0.6065399926208108\n",
      "train loss:0.6833153684970636\n",
      "train loss:0.6402185049832284\n",
      "train loss:0.4665586956854807\n",
      "train loss:0.64809296707836\n",
      "train loss:0.48909601188384\n",
      "train loss:0.5569084724702461\n",
      "train loss:0.5312160130387636\n",
      "train loss:0.49708445356886\n",
      "train loss:0.6455154270379555\n",
      "train loss:0.2714988118726588\n",
      "train loss:0.716267382790761\n",
      "train loss:0.7563246599294395\n",
      "train loss:0.4124719318312221\n",
      "train loss:0.49830543564675606\n",
      "train loss:0.8218835355336566\n",
      "train loss:0.42679827129780906\n",
      "train loss:0.5048913950738507\n",
      "train loss:0.6138470387577442\n",
      "train loss:0.5256553044590389\n",
      "train loss:0.4523412380781656\n",
      "train loss:0.6348839529079728\n",
      "train loss:0.5431706611200238\n",
      "train loss:0.48321009266581516\n",
      "train loss:0.43655725974655735\n",
      "train loss:0.5918831559858397\n",
      "train loss:0.4917834012889407\n",
      "train loss:0.3965743296579969\n",
      "train loss:0.5831431328656727\n",
      "train loss:0.8367118936198971\n",
      "train loss:0.8602327149181239\n",
      "train loss:0.6740773593972837\n",
      "train loss:0.531557955556853\n",
      "train loss:0.4734619761077835\n",
      "train loss:0.5255270606727304\n",
      "train loss:0.6003407787291957\n",
      "train loss:0.6119261528515134\n",
      "train loss:0.6933816856760614\n",
      "train loss:0.46789194292158554\n",
      "train loss:0.4741193530659923\n",
      "train loss:0.4509737465901925\n",
      "train loss:0.5168562213317085\n",
      "train loss:0.7462602623317124\n",
      "train loss:0.6366024809514592\n",
      "train loss:0.5274137147413841\n",
      "train loss:0.49862329315078585\n",
      "train loss:0.7348073463510815\n",
      "train loss:0.8156790468940015\n",
      "train loss:0.525687075458573\n",
      "train loss:0.6400381789465527\n",
      "train loss:0.4409998699447096\n",
      "train loss:0.7106885885885859\n",
      "train loss:0.7207998823604197\n",
      "train loss:0.5768236482332543\n",
      "train loss:0.5893592804356494\n",
      "train loss:0.5176476240500762\n",
      "train loss:0.6147456686350444\n",
      "train loss:0.6081209001816497\n",
      "train loss:0.7386895067822625\n",
      "train loss:0.5533921872086836\n",
      "train loss:0.3929559197298057\n",
      "train loss:0.7992293172464715\n",
      "train loss:0.7296919263499102\n",
      "train loss:0.6959931723224366\n",
      "train loss:0.5521681245754377\n",
      "train loss:0.5486947266640716\n",
      "train loss:0.7069068485579834\n",
      "train loss:0.5759857132436353\n",
      "train loss:0.4628471844491549\n",
      "train loss:0.4209255256006501\n",
      "train loss:0.5678218086345692\n",
      "train loss:0.699717278324703\n",
      "train loss:0.6010469017526544\n",
      "train loss:0.557527574726464\n",
      "train loss:0.7475411458280928\n",
      "train loss:0.4216423688672535\n",
      "train loss:0.6799560353734047\n",
      "train loss:0.6249960275366334\n",
      "train loss:0.6088424105992445\n",
      "train loss:0.8515165610251598\n",
      "train loss:0.5304443441542813\n",
      "train loss:0.4552752788771678\n",
      "train loss:0.7161042519963823\n",
      "train loss:0.6361577491366255\n",
      "train loss:0.5642020946207766\n",
      "train loss:0.6265845863691166\n",
      "train loss:0.6086358701191144\n",
      "train loss:0.5598030334763641\n",
      "train loss:0.8034876971745171\n",
      "train loss:0.6234352997754279\n",
      "train loss:0.5760080192602526\n",
      "train loss:0.5404175442513616\n",
      "train loss:0.6592847251849883\n",
      "train loss:0.6853311690302244\n",
      "=== epoch:9, train acc:0.77, test acc:0.7 ===\n",
      "train loss:0.5368383905057506\n",
      "train loss:0.47140135394001587\n",
      "train loss:0.7372361215074856\n",
      "train loss:0.6370985554090123\n",
      "train loss:0.5408332585805005\n",
      "train loss:0.5177332594922969\n",
      "train loss:0.5801296076135298\n",
      "train loss:0.5900332611857746\n",
      "train loss:0.48484047456925045\n",
      "train loss:0.6882416914881881\n",
      "train loss:0.4788628402090687\n",
      "train loss:0.6340101218062544\n",
      "train loss:0.8674847999961044\n",
      "train loss:0.5739241696597788\n",
      "train loss:0.46007992020178917\n",
      "train loss:0.6416259356990797\n",
      "train loss:0.6027354913129919\n",
      "train loss:0.48132019182121866\n",
      "train loss:0.5324378382398618\n",
      "train loss:0.47140887952925403\n",
      "train loss:0.5326575311674053\n",
      "train loss:0.7406558629204756\n",
      "train loss:0.5049345958458973\n",
      "train loss:0.44733155058528046\n",
      "train loss:0.6978371188560308\n",
      "train loss:0.554237273736656\n",
      "train loss:0.6621128645985245\n",
      "train loss:0.5890366957347637\n",
      "train loss:0.5180407425487914\n",
      "train loss:0.5048501504853216\n",
      "train loss:0.7670798649589394\n",
      "train loss:0.5495814299934778\n",
      "train loss:0.41944585954343677\n",
      "train loss:0.6765315143015594\n",
      "train loss:0.5226732057865731\n",
      "train loss:0.6741523735459592\n",
      "train loss:0.3440108378663128\n",
      "train loss:0.5840685756367984\n",
      "train loss:0.3508805030639512\n",
      "train loss:0.6220990436399046\n",
      "train loss:0.4600221876416727\n",
      "train loss:0.7485071699713054\n",
      "train loss:0.4855077644327251\n",
      "train loss:0.4883060458970666\n",
      "train loss:0.4271703821185008\n",
      "train loss:0.46252603095845696\n",
      "train loss:0.5164265326451681\n",
      "train loss:0.5556250698003777\n",
      "train loss:0.40032692224706967\n",
      "train loss:0.2487568983990794\n",
      "train loss:0.8269719348033611\n",
      "train loss:0.22423767914193768\n",
      "train loss:0.3992082371198157\n",
      "train loss:0.15235583029179567\n",
      "train loss:0.7703336181031972\n",
      "train loss:0.6001641024537726\n",
      "train loss:0.6041526365590446\n",
      "train loss:0.2675704778930222\n",
      "train loss:0.5393047519087343\n",
      "train loss:0.6042183984857511\n",
      "train loss:0.7868018726954982\n",
      "train loss:0.42389276143509713\n",
      "train loss:0.46449805240588765\n",
      "train loss:0.4446726542526712\n",
      "train loss:0.7390527912464583\n",
      "train loss:0.5961869824066734\n",
      "train loss:0.3623524866080743\n",
      "train loss:0.42592354283861455\n",
      "train loss:0.6403921856877857\n",
      "train loss:0.5361090023153101\n",
      "train loss:0.4673420383589083\n",
      "train loss:0.4892769995228428\n",
      "train loss:0.8898818202017752\n",
      "train loss:0.628147230548388\n",
      "train loss:0.7100989445711089\n",
      "train loss:0.6615064668719779\n",
      "train loss:0.4948373172065869\n",
      "train loss:0.5629454336235176\n",
      "train loss:0.4812595469954187\n",
      "train loss:0.4856700265603192\n",
      "train loss:0.5158441250500179\n",
      "train loss:0.6207087082099597\n",
      "train loss:0.47408281576433187\n",
      "train loss:0.7149300684498047\n",
      "train loss:0.38775167555630496\n",
      "train loss:0.7177816129391449\n",
      "train loss:0.4107190136779706\n",
      "train loss:0.4774215959227086\n",
      "train loss:0.3634735412532839\n",
      "train loss:0.1748871419247868\n",
      "train loss:0.26187458406752284\n",
      "train loss:0.7521562426160424\n",
      "train loss:0.7105083243500984\n",
      "train loss:0.5215596278055991\n",
      "train loss:0.35198001985073335\n",
      "train loss:0.3941158464247001\n",
      "train loss:0.7661314688775365\n",
      "train loss:0.37392991974963624\n",
      "train loss:0.6195209241846942\n",
      "train loss:0.5023631881816312\n",
      "train loss:0.729235477616209\n",
      "train loss:0.49948401477864407\n",
      "train loss:0.7186386860631847\n",
      "train loss:0.5233395585828478\n",
      "train loss:0.5491331619565625\n",
      "train loss:0.6325760436218217\n",
      "train loss:0.630006580520873\n",
      "train loss:0.5409158533892926\n",
      "train loss:0.41732548199955327\n",
      "train loss:0.7847489388617797\n",
      "train loss:0.6076659840398486\n",
      "train loss:0.556932580660812\n",
      "train loss:0.4207261602210222\n",
      "train loss:0.6131718192446296\n",
      "train loss:0.4423899258419105\n",
      "train loss:0.5156712777306577\n",
      "train loss:0.3971626324699874\n",
      "train loss:0.707005515746553\n",
      "train loss:0.5166272418154756\n",
      "train loss:0.6575652274450887\n",
      "train loss:0.5121711291359446\n",
      "train loss:0.56692148188064\n",
      "train loss:0.2852602611078867\n",
      "train loss:0.4900911853036233\n",
      "train loss:0.4882171570842761\n",
      "train loss:0.7759095779972168\n",
      "train loss:0.39056492964986705\n",
      "train loss:0.5131076184398548\n",
      "train loss:0.48280304758712245\n",
      "train loss:0.3786789411176723\n",
      "train loss:0.5942264212541012\n",
      "train loss:0.19451489895530658\n",
      "train loss:0.6920448932605638\n",
      "train loss:0.38812070281330724\n",
      "train loss:0.4627793438607218\n",
      "train loss:0.37905545812740327\n",
      "train loss:0.49487796593313893\n",
      "train loss:0.30867856428437845\n",
      "train loss:0.6958087360290107\n",
      "train loss:0.4955013888166381\n",
      "train loss:0.8326488803301141\n",
      "train loss:0.22177049750439984\n",
      "train loss:0.536881070507675\n",
      "train loss:0.5116000369125296\n",
      "train loss:0.49730372320079635\n",
      "train loss:0.43125540837724036\n",
      "train loss:0.6361291233665353\n",
      "train loss:0.9082296768586771\n",
      "train loss:0.4676863124231966\n",
      "train loss:0.697623854934276\n",
      "train loss:0.5683370866990192\n",
      "train loss:0.5027036062655347\n",
      "train loss:0.43752467643293586\n",
      "=== epoch:10, train acc:0.73, test acc:0.68 ===\n",
      "train loss:0.6446005728847888\n",
      "train loss:0.5197836657763464\n",
      "train loss:0.4801078729030879\n",
      "train loss:0.5971392700229783\n",
      "train loss:0.466871449402497\n",
      "train loss:0.3415891071881518\n",
      "train loss:0.628502515026242\n",
      "train loss:0.8432327088297645\n",
      "train loss:0.5153255048487814\n",
      "train loss:0.21852901428130397\n",
      "train loss:0.5533013248976507\n",
      "train loss:0.5798295773007475\n",
      "train loss:0.7597746400851585\n",
      "train loss:0.8459074164186473\n",
      "train loss:0.7185748609759873\n",
      "train loss:0.5871239298140937\n",
      "train loss:0.6462440311094013\n",
      "train loss:0.5364111795779174\n",
      "train loss:0.5468476351550462\n",
      "train loss:0.7365686905813819\n",
      "train loss:0.5433789629340449\n",
      "train loss:0.5534022632996115\n",
      "train loss:0.5893617444380034\n",
      "train loss:0.6188958323531425\n",
      "train loss:0.4496990466297821\n",
      "train loss:0.642313972235704\n",
      "train loss:0.4990830265370885\n",
      "train loss:0.36546058624792516\n",
      "train loss:0.6959659966934273\n",
      "train loss:0.5118736390953262\n",
      "train loss:0.5947888913812109\n",
      "train loss:0.32382455720590964\n",
      "train loss:0.4486956164407844\n",
      "train loss:0.7847682236521366\n",
      "train loss:0.691883171556412\n",
      "train loss:0.8879298293345599\n",
      "train loss:0.6276721201641217\n",
      "train loss:0.6500421735349835\n",
      "train loss:0.6142922144020215\n",
      "train loss:0.5049975201564652\n",
      "train loss:0.5612561717024127\n",
      "train loss:0.5500701542639757\n",
      "train loss:0.4900818527227397\n",
      "train loss:0.5351803075380108\n",
      "train loss:0.4726384690572223\n",
      "train loss:0.5028914271938659\n",
      "train loss:0.7616547678290566\n",
      "train loss:0.4720928766151428\n",
      "train loss:0.5890972722646349\n",
      "train loss:0.6111338964254636\n",
      "train loss:0.5833188792620476\n",
      "train loss:0.513069929129643\n",
      "train loss:0.7097620149043496\n",
      "train loss:0.5837813071612837\n",
      "train loss:0.5229247624578309\n",
      "train loss:0.7839902838739916\n",
      "train loss:0.4547175757613299\n",
      "train loss:0.6737621939486892\n",
      "train loss:0.41316669819774055\n",
      "train loss:0.47571322640106634\n",
      "train loss:0.27125369486138495\n",
      "train loss:0.3819447596332358\n",
      "train loss:0.4067863321203576\n",
      "train loss:0.5501746633500774\n",
      "train loss:0.34427183685744944\n",
      "train loss:0.48926249941745625\n",
      "train loss:0.30803301137213024\n",
      "train loss:0.6964649748152044\n",
      "train loss:0.9672319066582619\n",
      "train loss:0.601266616208921\n",
      "train loss:0.5087770310309632\n",
      "train loss:0.6738957268288963\n",
      "train loss:0.26628306576151267\n",
      "train loss:0.406603385579319\n",
      "train loss:0.7768861793126586\n",
      "train loss:0.6873444371872715\n",
      "train loss:0.409247929429809\n",
      "train loss:0.5906763133951329\n",
      "train loss:0.4497895244415179\n",
      "train loss:0.6967465441368599\n",
      "train loss:0.668035322595815\n",
      "train loss:0.7116755998468809\n",
      "train loss:0.3569663147782952\n",
      "train loss:0.5928942750369078\n",
      "train loss:0.6763971845794818\n",
      "train loss:0.6154142441557995\n",
      "train loss:0.6028101935024097\n",
      "train loss:0.5209830218103205\n",
      "train loss:0.5319854311908596\n",
      "train loss:0.33317606303432656\n",
      "train loss:0.5726268454869816\n",
      "train loss:0.5301414365040577\n",
      "train loss:0.6289647036021372\n",
      "train loss:0.5005814836236042\n",
      "train loss:0.5137350142858749\n",
      "train loss:0.6357007332148379\n",
      "train loss:0.6757526307501216\n",
      "train loss:0.535039804985068\n",
      "train loss:0.5359567129807519\n",
      "train loss:0.5312333399797596\n",
      "train loss:0.5365813019046136\n",
      "train loss:0.4946505662741603\n",
      "train loss:0.6276455303372306\n",
      "train loss:0.7842316136646639\n",
      "train loss:0.4757536729087762\n",
      "train loss:0.6188965524717187\n",
      "train loss:0.6883656942491644\n",
      "train loss:0.4219865914524674\n",
      "train loss:0.47423805829076227\n",
      "train loss:0.49502470842757323\n",
      "train loss:0.4212940371776653\n",
      "train loss:0.5115608167003365\n",
      "train loss:0.7074061722194479\n",
      "train loss:0.4445470388392968\n",
      "train loss:0.7716134623864556\n",
      "train loss:0.2947708437882225\n",
      "train loss:0.8456207469707604\n",
      "train loss:0.5141720695110459\n",
      "train loss:0.47817935228776864\n",
      "train loss:0.8378249427047713\n",
      "train loss:0.4912950179206971\n",
      "train loss:0.6690775988912356\n",
      "train loss:0.7725779589206083\n",
      "train loss:0.6274993217648583\n",
      "train loss:0.665218602522886\n",
      "train loss:0.4200290189544539\n",
      "train loss:0.482828925125797\n",
      "train loss:0.4538595972735682\n",
      "train loss:0.5405675458918684\n",
      "train loss:0.5516359418615997\n",
      "train loss:0.5895009117722353\n",
      "train loss:0.5274712103319961\n",
      "train loss:0.5360126486638844\n",
      "train loss:0.6421729030327396\n",
      "train loss:0.49777632078387424\n",
      "train loss:0.52246759443824\n",
      "train loss:0.6458188043394787\n",
      "train loss:0.7041031854880166\n",
      "train loss:0.5033208578940808\n",
      "train loss:0.7059496655681038\n",
      "train loss:0.6253306805782917\n",
      "train loss:0.5942267908087103\n",
      "train loss:0.6747140229368432\n",
      "train loss:0.3691341004898715\n",
      "train loss:0.582201476631935\n",
      "train loss:0.4538140124078775\n",
      "train loss:0.488493066672169\n",
      "train loss:0.48334452093250435\n",
      "train loss:0.42516156012311657\n",
      "train loss:0.36041382489797985\n",
      "train loss:0.4627629504338767\n",
      "train loss:0.6530765947778959\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5372549019607843\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 7, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30f897cf-1ed1-4204-aaac-09e305540c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6910262961089153\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6932117347161136\n",
      "train loss:0.6927957472169799\n",
      "train loss:0.6838038355492545\n",
      "train loss:0.6915215229204905\n",
      "train loss:0.6888102607750627\n",
      "train loss:0.7081585793113065\n",
      "train loss:0.6617782788384455\n",
      "train loss:0.6566745924811196\n",
      "train loss:0.676572951339313\n",
      "train loss:0.6134418661346764\n",
      "train loss:0.6993760873074464\n",
      "train loss:0.5384792727416399\n",
      "train loss:0.49485941453089544\n",
      "train loss:0.5442097114765259\n",
      "train loss:0.7097580140134762\n",
      "train loss:0.401159377122483\n",
      "train loss:0.5198604593699607\n",
      "train loss:0.9078129903065012\n",
      "train loss:0.9255405361425522\n",
      "train loss:0.6107370396800779\n",
      "train loss:0.6814750636124287\n",
      "train loss:0.6336389244008314\n",
      "train loss:0.44797153710528637\n",
      "train loss:0.6957794977729261\n",
      "train loss:0.554098995147964\n",
      "train loss:0.4736733257453666\n",
      "train loss:0.7094649761073322\n",
      "train loss:0.6084251538159193\n",
      "train loss:0.6381279182899864\n",
      "train loss:0.5177922969049334\n",
      "train loss:0.7924367389056659\n",
      "train loss:0.7409083801657088\n",
      "train loss:0.5245461488314329\n",
      "train loss:0.4419058473077196\n",
      "train loss:0.6044742409583261\n",
      "train loss:0.6240875100479839\n",
      "train loss:0.6086659494196446\n",
      "train loss:0.8232133534813546\n",
      "train loss:0.5538789182387467\n",
      "train loss:0.5939593840573317\n",
      "train loss:0.6160203621629996\n",
      "train loss:0.6040197370256566\n",
      "train loss:0.5222381742984428\n",
      "train loss:0.7422981692368402\n",
      "train loss:0.5076288529929789\n",
      "train loss:0.6102507552999111\n",
      "train loss:0.686468933497072\n",
      "train loss:0.5086917978301274\n",
      "train loss:0.7058956311606862\n",
      "train loss:0.5157787493057244\n",
      "train loss:0.4207320772532879\n",
      "train loss:0.5031547211759638\n",
      "train loss:0.5104691102918277\n",
      "train loss:0.48482099115489774\n",
      "train loss:0.591578385381738\n",
      "train loss:0.6508076233417132\n",
      "train loss:0.6452843816272343\n",
      "train loss:0.8465991812868552\n",
      "train loss:0.4770317421003664\n",
      "train loss:0.7139776149082655\n",
      "train loss:0.7032939708422628\n",
      "train loss:0.43410425205813113\n",
      "train loss:0.40721166763694727\n",
      "train loss:0.6133721972068413\n",
      "train loss:0.45708750302291357\n",
      "train loss:0.8612797353669635\n",
      "train loss:0.8399299635958111\n",
      "train loss:0.7004563020613379\n",
      "train loss:0.6632797437811802\n",
      "train loss:0.5274483825242425\n",
      "train loss:0.5682642805131095\n",
      "train loss:0.6602208195949777\n",
      "train loss:0.5636935368279369\n",
      "train loss:0.6016678625015895\n",
      "train loss:0.5616693125202786\n",
      "train loss:0.720364553069185\n",
      "train loss:0.5796956183892815\n",
      "train loss:0.7959851905360801\n",
      "train loss:0.5439291224508251\n",
      "train loss:0.5483665381784293\n",
      "train loss:0.534996835190295\n",
      "train loss:0.7783385564790141\n",
      "train loss:0.688632569697859\n",
      "train loss:0.6565783414182051\n",
      "train loss:0.7360173154903346\n",
      "train loss:0.5757016059980568\n",
      "train loss:0.48290834235511204\n",
      "train loss:0.5421650330492211\n",
      "train loss:0.613432989731062\n",
      "train loss:0.44496861456011017\n",
      "train loss:0.7719620283418107\n",
      "train loss:0.42780827258065807\n",
      "train loss:0.5107595581485179\n",
      "train loss:0.3914696930649938\n",
      "train loss:0.3811988117615702\n",
      "train loss:0.6124369163536878\n",
      "train loss:0.36133091043844634\n",
      "train loss:0.6889051187450985\n",
      "train loss:0.7714001738578337\n",
      "train loss:0.6332090255193366\n",
      "train loss:0.7964174926825034\n",
      "train loss:0.7153971000298759\n",
      "train loss:0.72404039493506\n",
      "train loss:0.42582912593666045\n",
      "train loss:0.6101622298330842\n",
      "train loss:0.5595109081342052\n",
      "train loss:0.6741985325199115\n",
      "train loss:0.5550357610186937\n",
      "train loss:0.569911960181076\n",
      "train loss:0.6603050243125649\n",
      "train loss:0.4422700237302303\n",
      "train loss:0.5443682357748202\n",
      "train loss:0.3896451714427228\n",
      "train loss:0.7980729873296221\n",
      "train loss:0.5971683322895789\n",
      "train loss:0.5100083894919152\n",
      "train loss:0.6977129776379812\n",
      "train loss:0.6250657700048026\n",
      "train loss:0.7205071653187182\n",
      "train loss:0.5892162918069216\n",
      "train loss:0.5267714965482625\n",
      "train loss:0.7173385988148606\n",
      "train loss:0.5672178718948621\n",
      "train loss:0.7937958930802332\n",
      "train loss:0.6288171303639946\n",
      "train loss:0.5925848763509943\n",
      "train loss:0.5370185376656821\n",
      "train loss:0.5248179187856732\n",
      "train loss:0.7428553385939226\n",
      "train loss:0.5583518153963529\n",
      "train loss:0.6683543550730346\n",
      "train loss:0.6015607727816688\n",
      "train loss:0.7408174520595425\n",
      "train loss:0.5338522882851536\n",
      "train loss:0.7439645606759481\n",
      "train loss:0.6850162961330402\n",
      "train loss:0.5474400971175434\n",
      "train loss:0.67156490308335\n",
      "train loss:0.6161046686581609\n",
      "train loss:0.6713676403029057\n",
      "train loss:0.8848562356593238\n",
      "train loss:0.5795733488044561\n",
      "train loss:0.5887859012881671\n",
      "train loss:0.6988699087255634\n",
      "train loss:0.6626135184422668\n",
      "train loss:0.5990943818815964\n",
      "train loss:0.6327561133844513\n",
      "train loss:0.6307912141212031\n",
      "train loss:0.5190826396645877\n",
      "train loss:0.7247388802869343\n",
      "train loss:0.6264603023676656\n",
      "train loss:0.6173797602092652\n",
      "train loss:0.62315254695382\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6580810238131664\n",
      "train loss:0.6840134575329413\n",
      "train loss:0.6890753654013275\n",
      "train loss:0.41955441919839415\n",
      "train loss:0.5451062524057997\n",
      "train loss:0.5340418214321836\n",
      "train loss:0.6834588974026812\n",
      "train loss:0.7778434529539927\n",
      "train loss:0.5646424295237462\n",
      "train loss:0.7783146704768912\n",
      "train loss:0.7647536243143165\n",
      "train loss:0.690503839146286\n",
      "train loss:0.5972694903584551\n",
      "train loss:0.6054382440876274\n",
      "train loss:0.54678568910766\n",
      "train loss:0.5276494473885303\n",
      "train loss:0.6648843169817464\n",
      "train loss:0.7290835609349012\n",
      "train loss:0.9458654324692792\n",
      "train loss:0.6615802534733511\n",
      "train loss:0.7714164344757386\n",
      "train loss:0.6733304355112667\n",
      "train loss:0.6336548804569455\n",
      "train loss:0.641931595049444\n",
      "train loss:0.5803955009307353\n",
      "train loss:0.7596771509572189\n",
      "train loss:0.6072240079266574\n",
      "train loss:0.656417765591834\n",
      "train loss:0.6449766038202372\n",
      "train loss:0.6268768210090682\n",
      "train loss:0.6122649716260551\n",
      "train loss:0.5429547192400763\n",
      "train loss:0.6043913103904581\n",
      "train loss:0.5844447986113595\n",
      "train loss:0.5793054392759137\n",
      "train loss:0.4876039006106561\n",
      "train loss:0.7084599544579568\n",
      "train loss:0.6929844168822912\n",
      "train loss:0.4256135916063757\n",
      "train loss:0.7912929210080268\n",
      "train loss:0.7507518851348082\n",
      "train loss:0.5837367398772664\n",
      "train loss:0.5158000918579714\n",
      "train loss:0.5906149491287648\n",
      "train loss:0.6072007750824594\n",
      "train loss:0.5196934340541266\n",
      "train loss:0.8215456479224154\n",
      "train loss:0.7344650583706269\n",
      "train loss:0.701310666174779\n",
      "train loss:0.5798267098767623\n",
      "train loss:0.81174771492989\n",
      "train loss:0.5684785069143812\n",
      "train loss:0.42085377849142225\n",
      "train loss:0.6297868427047189\n",
      "train loss:0.6078315644424175\n",
      "train loss:0.6773664007263753\n",
      "train loss:0.6881708346146248\n",
      "train loss:0.6302240320219508\n",
      "train loss:0.5179599750682046\n",
      "train loss:0.5418219496929576\n",
      "train loss:0.6242612057398159\n",
      "train loss:0.6597779530752674\n",
      "train loss:0.750552960060808\n",
      "train loss:0.8446290534776653\n",
      "train loss:0.5430459321278138\n",
      "train loss:0.7465368246207209\n",
      "train loss:0.6179132142495859\n",
      "train loss:0.575271186708262\n",
      "train loss:0.7227171391173723\n",
      "train loss:0.6735716339988516\n",
      "train loss:0.7395616491149534\n",
      "train loss:0.6264263342460105\n",
      "train loss:0.5786477265552762\n",
      "train loss:0.6248613751191525\n",
      "train loss:0.6620587343189169\n",
      "train loss:0.6722979811373306\n",
      "train loss:0.6188230379464407\n",
      "train loss:0.46859860232135675\n",
      "train loss:0.622921065564727\n",
      "train loss:0.6735394964715348\n",
      "train loss:0.4742732469433225\n",
      "train loss:0.6082705332797375\n",
      "train loss:0.7050366874258387\n",
      "train loss:0.6092348175240925\n",
      "train loss:0.6878840098207801\n",
      "train loss:0.6112076876391253\n",
      "train loss:0.5986689277839796\n",
      "train loss:0.3913148904977354\n",
      "train loss:0.5948010614766738\n",
      "train loss:0.8290252002561171\n",
      "train loss:0.6040339487294156\n",
      "train loss:0.7119309470821127\n",
      "train loss:0.5970937731303463\n",
      "train loss:0.6768415990779497\n",
      "train loss:0.4312955615476309\n",
      "train loss:0.6093330923962716\n",
      "train loss:0.6956868891390962\n",
      "train loss:0.5867524195861117\n",
      "train loss:0.4881377692826077\n",
      "train loss:0.7537385838608078\n",
      "train loss:0.5362210059808195\n",
      "train loss:0.7260058071421959\n",
      "train loss:0.7289928623712465\n",
      "train loss:0.5308025162630361\n",
      "train loss:0.638055968195715\n",
      "train loss:0.5879004076132052\n",
      "train loss:0.6570274238213056\n",
      "train loss:0.5438217389241037\n",
      "train loss:0.6106149220820545\n",
      "train loss:0.6245431807831686\n",
      "train loss:0.609772609137557\n",
      "train loss:0.6988833873704341\n",
      "train loss:0.743027546763577\n",
      "train loss:0.6183772064915595\n",
      "train loss:0.7437280302320282\n",
      "train loss:0.6273502591006158\n",
      "train loss:0.5734536245460655\n",
      "train loss:0.5633561937289043\n",
      "train loss:0.550987608008308\n",
      "train loss:0.6090206234877564\n",
      "train loss:0.6683551092894454\n",
      "train loss:0.6724630309634572\n",
      "train loss:0.5345229099501612\n",
      "train loss:0.6435178643579647\n",
      "train loss:0.686382204820617\n",
      "train loss:0.6641177480184782\n",
      "train loss:0.6215126859000651\n",
      "train loss:0.7756507903275864\n",
      "train loss:0.8023396084873866\n",
      "train loss:0.6186438727845234\n",
      "train loss:0.7516595165587222\n",
      "train loss:0.6789866345071607\n",
      "train loss:0.6021590073257512\n",
      "train loss:0.6443582690123882\n",
      "train loss:0.6138112879452813\n",
      "train loss:0.620279792234196\n",
      "train loss:0.6866055103948858\n",
      "train loss:0.5877764342423687\n",
      "train loss:0.6168690006422456\n",
      "train loss:0.5826367697518782\n",
      "train loss:0.5575613855592167\n",
      "train loss:0.544282201831661\n",
      "train loss:0.6566237130003068\n",
      "train loss:0.691820648434618\n",
      "train loss:0.6069496586578645\n",
      "train loss:0.6369539105875452\n",
      "train loss:0.7844920148193592\n",
      "train loss:0.5216589866687585\n",
      "train loss:0.5896713642190662\n",
      "train loss:0.6906656251989125\n",
      "train loss:0.5016994128340968\n",
      "train loss:0.6245259719220438\n",
      "train loss:0.5071081135508724\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5893859304462015\n",
      "train loss:0.653196559387617\n",
      "train loss:0.4212525095754459\n",
      "train loss:0.6178629046130739\n",
      "train loss:0.7176527810065704\n",
      "train loss:0.7168003924447774\n",
      "train loss:0.3154751226341389\n",
      "train loss:0.5236777757716695\n",
      "train loss:0.46888462127835995\n",
      "train loss:0.4757581190095939\n",
      "train loss:0.3480948085194069\n",
      "train loss:0.825600138753855\n",
      "train loss:0.7516530616810877\n",
      "train loss:0.7175922651261251\n",
      "train loss:0.4948331637150652\n",
      "train loss:0.5520170652617883\n",
      "train loss:0.4061420100161732\n",
      "train loss:0.6144276898400404\n",
      "train loss:0.4245263524473746\n",
      "train loss:0.377862316165118\n",
      "train loss:0.9467946946786148\n",
      "train loss:0.4134392771433283\n",
      "train loss:0.7159900009927839\n",
      "train loss:0.5644878104829949\n",
      "train loss:0.6006785485274853\n",
      "train loss:0.521916883360065\n",
      "train loss:0.5192356376571451\n",
      "train loss:0.5079794151422188\n",
      "train loss:0.746891253275627\n",
      "train loss:0.607963285801677\n",
      "train loss:0.5189923547217017\n",
      "train loss:0.48856995465094205\n",
      "train loss:0.5580512819419285\n",
      "train loss:0.7130381518331016\n",
      "train loss:0.6520843576764862\n",
      "train loss:0.6313524422047238\n",
      "train loss:0.6243297373158261\n",
      "train loss:0.5520058803041603\n",
      "train loss:0.7593595659834707\n",
      "train loss:0.6823361864995614\n",
      "train loss:0.7574712869742544\n",
      "train loss:0.6196639022612297\n",
      "train loss:0.5257005889657622\n",
      "train loss:0.6529020016400822\n",
      "train loss:0.5264785012303459\n",
      "train loss:0.551576771953048\n",
      "train loss:0.622790660564099\n",
      "train loss:0.5340238979041008\n",
      "train loss:0.45849905038845395\n",
      "train loss:0.4520184001788056\n",
      "train loss:0.5002960715081246\n",
      "train loss:0.7162514922842547\n",
      "train loss:0.7090965637724574\n",
      "train loss:0.4748807767284019\n",
      "train loss:0.37468716264669993\n",
      "train loss:0.5071467100349999\n",
      "train loss:0.662613979961659\n",
      "train loss:0.6373181923090426\n",
      "train loss:0.46054279684431015\n",
      "train loss:0.6485225447609173\n",
      "train loss:0.8256062650886165\n",
      "train loss:0.6651288371152674\n",
      "train loss:0.6790824093955038\n",
      "train loss:0.6487311686986799\n",
      "train loss:0.7543030321230899\n",
      "train loss:0.6000144263322433\n",
      "train loss:0.6737120595464001\n",
      "train loss:0.5334167563304699\n",
      "train loss:0.67405414879889\n",
      "train loss:0.6631387854229217\n",
      "train loss:0.677541398198821\n",
      "train loss:0.5526122083598\n",
      "train loss:0.6511858036105831\n",
      "train loss:0.628667423692734\n",
      "train loss:0.6658049660681311\n",
      "train loss:0.6080358158100078\n",
      "train loss:0.5128337073711712\n",
      "train loss:0.671273231805751\n",
      "train loss:0.640635786251781\n",
      "train loss:0.5388291989181045\n",
      "train loss:0.45605793043925213\n",
      "train loss:0.4371095062380964\n",
      "train loss:0.6104755617607653\n",
      "train loss:0.8193581067740009\n",
      "train loss:0.5970078850121352\n",
      "train loss:0.706057968246967\n",
      "train loss:0.6189744008389462\n",
      "train loss:0.6799677008124094\n",
      "train loss:0.6669297423915702\n",
      "train loss:0.3670299952100489\n",
      "train loss:0.5189091576193702\n",
      "train loss:0.5140969888986365\n",
      "train loss:0.6957320930105395\n",
      "train loss:0.6596696826841651\n",
      "train loss:0.490430132932781\n",
      "train loss:0.41718282511757154\n",
      "train loss:0.5387330179605826\n",
      "train loss:0.5150570788340108\n",
      "train loss:0.5632246166795059\n",
      "train loss:0.29505933006362167\n",
      "train loss:0.7611059483547684\n",
      "train loss:0.48087280617763434\n",
      "train loss:0.16561308279015857\n",
      "train loss:0.30679014412293226\n",
      "train loss:0.7114941750295447\n",
      "train loss:0.8608850696948893\n",
      "train loss:0.339915627147772\n",
      "train loss:0.3298979954716593\n",
      "train loss:0.6414458318219808\n",
      "train loss:0.8489945543934294\n",
      "train loss:0.5928885541713693\n",
      "train loss:0.5408113812156774\n",
      "train loss:0.6856656448658293\n",
      "train loss:0.7423128104947254\n",
      "train loss:0.6723822595660583\n",
      "train loss:0.635868476914718\n",
      "train loss:0.6359929687709218\n",
      "train loss:0.7625170157828108\n",
      "train loss:0.5975799564285024\n",
      "train loss:0.6185712279413751\n",
      "train loss:0.6075905688210785\n",
      "train loss:0.6772943589587969\n",
      "train loss:0.5930444303797796\n",
      "train loss:0.6315382323988166\n",
      "train loss:0.64271383568371\n",
      "train loss:0.7037793654976341\n",
      "train loss:0.5943062803614985\n",
      "train loss:0.6335622973827755\n",
      "train loss:0.6390690210207196\n",
      "train loss:0.6268628103201861\n",
      "train loss:0.7009120730664831\n",
      "train loss:0.6369200587044381\n",
      "train loss:0.5980156011711321\n",
      "train loss:0.5864537824632714\n",
      "train loss:0.6220777214187032\n",
      "train loss:0.507997974936301\n",
      "train loss:0.6189656758561288\n",
      "train loss:0.6844705901415641\n",
      "train loss:0.601206222103055\n",
      "train loss:0.5575828208643434\n",
      "train loss:0.6108258201082716\n",
      "train loss:0.588241303168745\n",
      "train loss:0.6943052346118316\n",
      "train loss:0.3790977835498784\n",
      "train loss:0.5175703908898969\n",
      "train loss:0.38433817141329363\n",
      "train loss:0.7023658096580583\n",
      "train loss:0.767308158176726\n",
      "train loss:0.5460736785516265\n",
      "train loss:0.7179898021046509\n",
      "train loss:0.7121093165252295\n",
      "train loss:0.7282024566436959\n",
      "train loss:0.7205412364960149\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7716421527198101\n",
      "train loss:0.6139011390776818\n",
      "train loss:0.46082072922750256\n",
      "train loss:0.675429823527959\n",
      "train loss:0.5047118538149777\n",
      "train loss:0.6634333699187815\n",
      "train loss:0.5631555240730489\n",
      "train loss:0.7074441117383329\n",
      "train loss:0.6612208409904207\n",
      "train loss:0.670763859111936\n",
      "train loss:0.6135973612662851\n",
      "train loss:0.6961384254525067\n",
      "train loss:0.47476251948810166\n",
      "train loss:0.7051377751391746\n",
      "train loss:0.6504366693520773\n",
      "train loss:0.6167835714490615\n",
      "train loss:0.6689722491661556\n",
      "train loss:0.7624773586645552\n",
      "train loss:0.5482254352541707\n",
      "train loss:0.5540245423051932\n",
      "train loss:0.661168122711914\n",
      "train loss:0.6282980291128717\n",
      "train loss:0.588669433632998\n",
      "train loss:0.7112215989548742\n",
      "train loss:0.5957253704975227\n",
      "train loss:0.5684266406458882\n",
      "train loss:0.6579564176085504\n",
      "train loss:0.6123514276020976\n",
      "train loss:0.5092322517891369\n",
      "train loss:0.6108500554834712\n",
      "train loss:0.5386596103947809\n",
      "train loss:0.8002716699916974\n",
      "train loss:0.7252115955202753\n",
      "train loss:0.5455061664658332\n",
      "train loss:0.44299805452951324\n",
      "train loss:0.7226630636297472\n",
      "train loss:0.7136594484955433\n",
      "train loss:0.33655726950305803\n",
      "train loss:0.4422057747774234\n",
      "train loss:0.5714891214989105\n",
      "train loss:0.6888624203165142\n",
      "train loss:0.5916038815362533\n",
      "train loss:0.7666491538760597\n",
      "train loss:0.7614735904414994\n",
      "train loss:0.8862795601745228\n",
      "train loss:0.5208678993317634\n",
      "train loss:0.5064017295883378\n",
      "train loss:0.5436465163968616\n",
      "train loss:0.5550934359912468\n",
      "train loss:0.6666269290158517\n",
      "train loss:0.4836862570941647\n",
      "train loss:0.703015736736649\n",
      "train loss:0.5661219369715484\n",
      "train loss:0.626615285340151\n",
      "train loss:0.6049560282814078\n",
      "train loss:0.5942450246956976\n",
      "train loss:0.373428405433863\n",
      "train loss:0.6970517437596533\n",
      "train loss:0.5174189881090109\n",
      "train loss:0.7507035476628776\n",
      "train loss:0.6529999346837138\n",
      "train loss:0.5330221703474717\n",
      "train loss:0.37696431191110247\n",
      "train loss:0.6716892250363091\n",
      "train loss:0.6054905738994454\n",
      "train loss:0.503292918761858\n",
      "train loss:0.6580120195702193\n",
      "train loss:0.9356606558672524\n",
      "train loss:0.453114542090038\n",
      "train loss:0.4400674876334304\n",
      "train loss:0.5113904103856326\n",
      "train loss:0.7324516508518505\n",
      "train loss:0.507985982154606\n",
      "train loss:0.5432248907564982\n",
      "train loss:0.5062827402461787\n",
      "train loss:0.5036905186626642\n",
      "train loss:0.499918687905923\n",
      "train loss:0.5239324040059359\n",
      "train loss:0.6979672966958126\n",
      "train loss:0.6295996262565595\n",
      "train loss:0.8775326560871137\n",
      "train loss:0.7773186948468418\n",
      "train loss:0.5146937237608478\n",
      "train loss:0.4850182492421484\n",
      "train loss:0.6952231452149553\n",
      "train loss:0.8246274465207039\n",
      "train loss:0.5789406662432899\n",
      "train loss:0.7519459153850273\n",
      "train loss:0.6590674791284997\n",
      "train loss:0.5274497997447818\n",
      "train loss:0.47993830498645906\n",
      "train loss:0.5132745379116254\n",
      "train loss:0.7608782611644743\n",
      "train loss:0.6697012828144625\n",
      "train loss:0.5677783073096411\n",
      "train loss:0.5349230675634591\n",
      "train loss:0.5592265852219088\n",
      "train loss:0.6491505306454262\n",
      "train loss:0.6688694696596031\n",
      "train loss:0.813226377235401\n",
      "train loss:0.6395196938985812\n",
      "train loss:0.7804304943921219\n",
      "train loss:0.5510273185153647\n",
      "train loss:0.5266298469238043\n",
      "train loss:0.46711433806314445\n",
      "train loss:0.5997230461509172\n",
      "train loss:0.6155745406648577\n",
      "train loss:0.5255064247096339\n",
      "train loss:0.6847610235821232\n",
      "train loss:0.6066188603924367\n",
      "train loss:0.5826212855873083\n",
      "train loss:0.6317721969348804\n",
      "train loss:0.5603731186583839\n",
      "train loss:0.6854276730466753\n",
      "train loss:0.42575525160921507\n",
      "train loss:0.5190602043843894\n",
      "train loss:0.3936024356693987\n",
      "train loss:0.7010606966994016\n",
      "train loss:0.5591625425327795\n",
      "train loss:0.6401583505872241\n",
      "train loss:0.6343746476473097\n",
      "train loss:0.6451337932155623\n",
      "train loss:0.49981876117801277\n",
      "train loss:0.6418658137549934\n",
      "train loss:0.7552328713233516\n",
      "train loss:0.6960895402545954\n",
      "train loss:0.5138944072993633\n",
      "train loss:0.4742812920120133\n",
      "train loss:0.5144133131370128\n",
      "train loss:0.6144837545636992\n",
      "train loss:0.7272060083848426\n",
      "train loss:0.7094529769612679\n",
      "train loss:0.48901658511812246\n",
      "train loss:0.6521859352512284\n",
      "train loss:0.7415748904916807\n",
      "train loss:0.5260422245249831\n",
      "train loss:0.3722073289520034\n",
      "train loss:0.655172777392486\n",
      "train loss:0.5074302271668579\n",
      "train loss:0.44393619988276833\n",
      "train loss:0.7289451060819033\n",
      "train loss:0.402334829735735\n",
      "train loss:0.5733864195414565\n",
      "train loss:0.6611437852849026\n",
      "train loss:0.4231338492581463\n",
      "train loss:0.7156145832191958\n",
      "train loss:0.6310988063766603\n",
      "train loss:0.6451208473613363\n",
      "train loss:0.6972772848121818\n",
      "train loss:0.6520660837808698\n",
      "train loss:0.6421393338253971\n",
      "train loss:0.5902926449919221\n",
      "train loss:0.5208980462581895\n",
      "=== epoch:5, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.4074790469837774\n",
      "train loss:0.7868877503772016\n",
      "train loss:0.44943027001275376\n",
      "train loss:0.7374533627233386\n",
      "train loss:0.6743902217263146\n",
      "train loss:0.5520283487148884\n",
      "train loss:0.5642266996587532\n",
      "train loss:0.5429961629381799\n",
      "train loss:0.6152654597477568\n",
      "train loss:0.6995043019175712\n",
      "train loss:0.5790647597190384\n",
      "train loss:0.6909395588343499\n",
      "train loss:0.5051006276105063\n",
      "train loss:0.64950838394992\n",
      "train loss:0.5384789370266069\n",
      "train loss:0.6965645550751344\n",
      "train loss:0.5579484478566776\n",
      "train loss:0.6781646823611414\n",
      "train loss:0.5233077472229135\n",
      "train loss:0.7121337896094027\n",
      "train loss:0.6116959659505321\n",
      "train loss:0.6518147742615477\n",
      "train loss:0.7786187658203367\n",
      "train loss:0.6109287975410675\n",
      "train loss:0.7199301975715009\n",
      "train loss:0.4924365269097377\n",
      "train loss:0.7280820861050937\n",
      "train loss:0.6578371557707383\n",
      "train loss:0.5650871355440826\n",
      "train loss:0.6373565209053427\n",
      "train loss:0.773976911144277\n",
      "train loss:0.6084693055057429\n",
      "train loss:0.5706442125043023\n",
      "train loss:0.6480860400480745\n",
      "train loss:0.7255443639595434\n",
      "train loss:0.5777468954197478\n",
      "train loss:0.5655614080487914\n",
      "train loss:0.6036593401166793\n",
      "train loss:0.5889846165777239\n",
      "train loss:0.6082670132833382\n",
      "train loss:0.5709921249049719\n",
      "train loss:0.6212995644590459\n",
      "train loss:0.4490169585124414\n",
      "train loss:0.678571190581255\n",
      "train loss:0.45341786738898965\n",
      "train loss:0.6229692276986671\n",
      "train loss:0.8213035989894291\n",
      "train loss:0.47797460009326703\n",
      "train loss:0.776594830100098\n",
      "train loss:0.48629054671175814\n",
      "train loss:0.5205465763571371\n",
      "train loss:0.5728844449622709\n",
      "train loss:0.6871297474212612\n",
      "train loss:0.525384843372466\n",
      "train loss:0.7521372736367021\n",
      "train loss:0.602971513331095\n",
      "train loss:0.6349510395667309\n",
      "train loss:0.6944216536988931\n",
      "train loss:0.7356153426048657\n",
      "train loss:0.480756021177708\n",
      "train loss:0.7089024322063155\n",
      "train loss:0.5078127449669884\n",
      "train loss:0.743168609933204\n",
      "train loss:0.681287437113107\n",
      "train loss:0.5552819792721461\n",
      "train loss:0.6241543053804008\n",
      "train loss:0.5480356879748645\n",
      "train loss:0.6720716347332087\n",
      "train loss:0.5651234941101198\n",
      "train loss:0.7534909565556861\n",
      "train loss:0.6360337936366474\n",
      "train loss:0.6042295814062738\n",
      "train loss:0.6756312291533026\n",
      "train loss:0.5814725428529424\n",
      "train loss:0.6564589985276041\n",
      "train loss:0.5805472465954067\n",
      "train loss:0.6884024478801616\n",
      "train loss:0.6588807683536285\n",
      "train loss:0.5670915868957607\n",
      "train loss:0.4582206679556524\n",
      "train loss:0.6360522524019605\n",
      "train loss:0.8599609664360817\n",
      "train loss:0.6290418722271738\n",
      "train loss:0.4484935569562752\n",
      "train loss:0.5859758761934495\n",
      "train loss:0.5313324914033023\n",
      "train loss:0.8116519061054452\n",
      "train loss:0.7216431827985998\n",
      "train loss:0.7603550666506426\n",
      "train loss:0.4665140361366203\n",
      "train loss:0.5213298412517312\n",
      "train loss:0.6069469420735523\n",
      "train loss:0.6086653740344954\n",
      "train loss:0.5245265002046728\n",
      "train loss:0.5195734692811287\n",
      "train loss:0.6086573994568326\n",
      "train loss:0.39211259965410195\n",
      "train loss:0.35527402173377426\n",
      "train loss:0.578765923237167\n",
      "train loss:0.6067312862314107\n",
      "train loss:0.5783897269219602\n",
      "train loss:0.7020747284930028\n",
      "train loss:0.544322827618761\n",
      "train loss:1.0247655596014658\n",
      "train loss:0.7021940695731232\n",
      "train loss:0.6283938718199586\n",
      "train loss:0.6598776075803384\n",
      "train loss:0.602487320098053\n",
      "train loss:0.42137122572867625\n",
      "train loss:0.5539826651342544\n",
      "train loss:0.5469882616132672\n",
      "train loss:0.4763244557574858\n",
      "train loss:0.5349469387055834\n",
      "train loss:0.4934449619462421\n",
      "train loss:0.7733633614232829\n",
      "train loss:0.5957832894654768\n",
      "train loss:0.5468423573673193\n",
      "train loss:0.5981018210168292\n",
      "train loss:0.7088251891030617\n",
      "train loss:0.7070110822544775\n",
      "train loss:0.6097138137110061\n",
      "train loss:0.43942213122697166\n",
      "train loss:0.5754129873703653\n",
      "train loss:0.6532507377727298\n",
      "train loss:0.85405629670057\n",
      "train loss:0.48404249567373947\n",
      "train loss:0.5204668717797539\n",
      "train loss:0.6034894825837265\n",
      "train loss:0.404662885366848\n",
      "train loss:0.5739921808716197\n",
      "train loss:0.6364087052626631\n",
      "train loss:0.8571855713675429\n",
      "train loss:0.5950289146358184\n",
      "train loss:0.4899997154640956\n",
      "train loss:0.60005755729627\n",
      "train loss:0.5408955305623678\n",
      "train loss:0.6459689043278478\n",
      "train loss:0.5215960058085404\n",
      "train loss:0.6565636978795998\n",
      "train loss:0.4967013518062296\n",
      "train loss:0.6244968053546232\n",
      "train loss:0.37843535814608065\n",
      "train loss:0.4658569325975643\n",
      "train loss:0.38730748985605534\n",
      "train loss:0.5038532351010806\n",
      "train loss:0.6982458294716608\n",
      "train loss:0.7372584246190501\n",
      "train loss:0.3251447258028521\n",
      "train loss:0.5192453720306347\n",
      "train loss:0.7279275754341641\n",
      "train loss:1.1353532928305887\n",
      "train loss:0.7176851910187413\n",
      "train loss:0.4314643474355554\n",
      "=== epoch:6, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5022974232658542\n",
      "train loss:0.6170993687672043\n",
      "train loss:0.7611523222862033\n",
      "train loss:0.704793524431129\n",
      "train loss:0.6090933778595924\n",
      "train loss:0.5489937410701635\n",
      "train loss:0.5724738728244662\n",
      "train loss:0.6546891644489516\n",
      "train loss:0.5586901691572121\n",
      "train loss:0.5711527940453626\n",
      "train loss:0.6084418623412098\n",
      "train loss:0.6520010686734737\n",
      "train loss:0.6167322743043276\n",
      "train loss:0.509350066418607\n",
      "train loss:0.5963505305580518\n",
      "train loss:0.6145730431143386\n",
      "train loss:0.6764405286050774\n",
      "train loss:0.5682949805768206\n",
      "train loss:0.5410784623740137\n",
      "train loss:0.6255158385675722\n",
      "train loss:0.5409184968748135\n",
      "train loss:0.46790587564281266\n",
      "train loss:0.4516101680488056\n",
      "train loss:0.3664990699540823\n",
      "train loss:0.37147792051433187\n",
      "train loss:0.35832647104667753\n",
      "train loss:0.6353932337471725\n",
      "train loss:0.26490276036886967\n",
      "train loss:0.6970977454005419\n",
      "train loss:0.5329288411023971\n",
      "train loss:0.692285424920246\n",
      "train loss:0.5007654091324054\n",
      "train loss:0.46262767062417637\n",
      "train loss:0.6710927461210009\n",
      "train loss:0.34368638575782984\n",
      "train loss:0.7644784569692652\n",
      "train loss:0.6221114615617436\n",
      "train loss:0.8546735722330039\n",
      "train loss:0.5112882591967115\n",
      "train loss:0.6118962792087791\n",
      "train loss:0.5222906000845204\n",
      "train loss:0.6503062488919855\n",
      "train loss:0.4667175335366241\n",
      "train loss:0.4157296974990451\n",
      "train loss:0.7296547009184423\n",
      "train loss:0.5500637629725593\n",
      "train loss:0.5528026664777619\n",
      "train loss:0.5660125303439698\n",
      "train loss:0.6320247179108642\n",
      "train loss:0.7340646979008912\n",
      "train loss:0.5477589493540447\n",
      "train loss:0.7891709489894945\n",
      "train loss:0.5973026189310293\n",
      "train loss:0.45078792772423376\n",
      "train loss:0.6008018820134704\n",
      "train loss:0.7836025469269596\n",
      "train loss:0.6205963486918507\n",
      "train loss:0.5445629945248257\n",
      "train loss:0.4770719623801867\n",
      "train loss:0.5032759820439525\n",
      "train loss:0.5784210750917124\n",
      "train loss:0.5721031371729263\n",
      "train loss:0.5514930121692513\n",
      "train loss:0.5752240560341114\n",
      "train loss:0.7240343698682269\n",
      "train loss:0.7412937808324636\n",
      "train loss:0.5648623893592111\n",
      "train loss:0.6062017491247306\n",
      "train loss:0.6942787630442615\n",
      "train loss:0.555736944973161\n",
      "train loss:0.7959573283364708\n",
      "train loss:0.5177293764890256\n",
      "train loss:0.43396261647796325\n",
      "train loss:0.4141717238828456\n",
      "train loss:0.36531823712717537\n",
      "train loss:0.5508526048313939\n",
      "train loss:0.4068026298854154\n",
      "train loss:0.5088099316905519\n",
      "train loss:0.38218505207053605\n",
      "train loss:0.39948452494030967\n",
      "train loss:0.9456930367792482\n",
      "train loss:0.7475607658890422\n",
      "train loss:0.9746388089601595\n",
      "train loss:0.41815741169171633\n",
      "train loss:0.7046010971867112\n",
      "train loss:0.5972393076139739\n",
      "train loss:0.6785529729288543\n",
      "train loss:0.6209298897465864\n",
      "train loss:0.6386736532342974\n",
      "train loss:0.5998120574051942\n",
      "train loss:0.547551881405858\n",
      "train loss:0.6694296913813856\n",
      "train loss:0.537158481036504\n",
      "train loss:0.639396240681641\n",
      "train loss:0.7730129515295392\n",
      "train loss:0.5297586868675939\n",
      "train loss:0.5585790628550477\n",
      "train loss:0.5960544683564178\n",
      "train loss:0.584546873806737\n",
      "train loss:0.5352122126888843\n",
      "train loss:0.6987853223753457\n",
      "train loss:0.6421802710688398\n",
      "train loss:0.787023176391586\n",
      "train loss:0.4246843201801048\n",
      "train loss:0.455567374279173\n",
      "train loss:0.39645147216390436\n",
      "train loss:0.5329829550540097\n",
      "train loss:0.49663117839153903\n",
      "train loss:0.48792260112666846\n",
      "train loss:0.4630921566913613\n",
      "train loss:0.7315782465852742\n",
      "train loss:0.7710354925103503\n",
      "train loss:0.4557321218708437\n",
      "train loss:0.8104006728340428\n",
      "train loss:0.5785199369777427\n",
      "train loss:0.4272565730673029\n",
      "train loss:0.4232511920210992\n",
      "train loss:0.538267051413711\n",
      "train loss:0.49471315659829235\n",
      "train loss:0.6810548729783213\n",
      "train loss:0.7060161598431449\n",
      "train loss:0.5867882668941596\n",
      "train loss:0.5840613293231675\n",
      "train loss:0.5956818021695913\n",
      "train loss:0.48711745483954283\n",
      "train loss:0.5962080789547085\n",
      "train loss:0.5643276940603459\n",
      "train loss:0.7403815269405426\n",
      "train loss:0.6393729153978785\n",
      "train loss:0.4257145300592784\n",
      "train loss:0.5490036583275388\n",
      "train loss:0.5912835735020351\n",
      "train loss:0.748284408520992\n",
      "train loss:0.6147321060743633\n",
      "train loss:0.7149559668698451\n",
      "train loss:0.5197621185149484\n",
      "train loss:0.3557018504373358\n",
      "train loss:0.5482317373439904\n",
      "train loss:0.4765178638308205\n",
      "train loss:0.3052752969046364\n",
      "train loss:0.4989110448646378\n",
      "train loss:0.6882139061228109\n",
      "train loss:0.3936756263853326\n",
      "train loss:0.4465569905634381\n",
      "train loss:0.467431646487985\n",
      "train loss:0.40971322531765636\n",
      "train loss:0.646053154649243\n",
      "train loss:0.6588810136727627\n",
      "train loss:0.564561436671232\n",
      "train loss:0.9264965645838972\n",
      "train loss:0.6965539874215275\n",
      "train loss:0.8036625945683648\n",
      "train loss:0.599146200621527\n",
      "=== epoch:7, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.609567708209091\n",
      "train loss:0.6282262171753888\n",
      "train loss:0.660257246730025\n",
      "train loss:0.438519979809967\n",
      "train loss:0.6239283850466586\n",
      "train loss:0.711894492861356\n",
      "train loss:0.6681362920989906\n",
      "train loss:0.6076225854876819\n",
      "train loss:0.6356530676976335\n",
      "train loss:0.5081626645047707\n",
      "train loss:0.5895941764147372\n",
      "train loss:0.6243925660571193\n",
      "train loss:0.7980075674666537\n",
      "train loss:0.6744610109358007\n",
      "train loss:0.49743128812602305\n",
      "train loss:0.5178872551741007\n",
      "train loss:0.5601324657210627\n",
      "train loss:0.4504254614962929\n",
      "train loss:0.4932645202995706\n",
      "train loss:0.5823718834052632\n",
      "train loss:0.571518086959893\n",
      "train loss:0.7920183107887213\n",
      "train loss:0.6761750489924072\n",
      "train loss:0.6348954388512168\n",
      "train loss:0.8848897089180282\n",
      "train loss:0.4724549122649274\n",
      "train loss:0.49539156188982386\n",
      "train loss:0.6081247635498956\n",
      "train loss:0.5886285622878575\n",
      "train loss:0.9215165963289511\n",
      "train loss:0.6965868812182616\n",
      "train loss:0.6474020855772625\n",
      "train loss:0.3402938591289343\n",
      "train loss:0.44995306313705974\n",
      "train loss:0.5331075203461768\n",
      "train loss:0.6231579778567082\n",
      "train loss:0.7129047019743562\n",
      "train loss:0.6787812370272839\n",
      "train loss:0.6736413402224236\n",
      "train loss:0.6842614981271048\n",
      "train loss:0.5220514443761762\n",
      "train loss:0.6119258531051928\n",
      "train loss:0.5359819717688696\n",
      "train loss:0.5776890807031021\n",
      "train loss:0.5041892155939109\n",
      "train loss:0.547878964891074\n",
      "train loss:0.45140203070092905\n",
      "train loss:0.716305166395763\n",
      "train loss:0.6474487362949656\n",
      "train loss:0.3294597700937446\n",
      "train loss:0.7358451345434175\n",
      "train loss:0.5325061130546106\n",
      "train loss:0.4445105792593626\n",
      "train loss:0.8539958242850428\n",
      "train loss:0.4382198092007562\n",
      "train loss:0.6537445056698247\n",
      "train loss:0.5645307479959688\n",
      "train loss:0.2576391803992375\n",
      "train loss:0.42235024865350124\n",
      "train loss:0.5472045863391142\n",
      "train loss:0.3528643320279986\n",
      "train loss:0.6476997208052228\n",
      "train loss:0.7928037446610412\n",
      "train loss:0.8902058329245847\n",
      "train loss:0.3668506567869626\n",
      "train loss:0.611743870490234\n",
      "train loss:0.6221869828232257\n",
      "train loss:0.6691514437762724\n",
      "train loss:0.4571087257059623\n",
      "train loss:0.7386086984220654\n",
      "train loss:0.6177356828047159\n",
      "train loss:0.6130246075325592\n",
      "train loss:0.4644625869813936\n",
      "train loss:0.6613948383053537\n",
      "train loss:0.6990684925700618\n",
      "train loss:0.49958547017475813\n",
      "train loss:0.5144820159431011\n",
      "train loss:0.595585333350736\n",
      "train loss:0.5759939287174912\n",
      "train loss:0.6073502345544101\n",
      "train loss:0.6058274178676595\n",
      "train loss:0.5236756726637819\n",
      "train loss:0.45164700410285574\n",
      "train loss:0.3692991079430311\n",
      "train loss:0.6110491204933939\n",
      "train loss:0.25919869322191996\n",
      "train loss:0.39030442247131103\n",
      "train loss:0.5093801522783352\n",
      "train loss:0.6737765015102474\n",
      "train loss:0.48958393617193574\n",
      "train loss:0.716982223370971\n",
      "train loss:0.547941055964913\n",
      "train loss:0.7915131680447944\n",
      "train loss:0.6126553637631709\n",
      "train loss:0.8725598359646789\n",
      "train loss:0.5069784665277903\n",
      "train loss:0.5323816107279152\n",
      "train loss:0.44648956810589907\n",
      "train loss:0.5301152447240861\n",
      "train loss:0.5121728904177152\n",
      "train loss:0.5327551696659849\n",
      "train loss:0.40706524622592816\n",
      "train loss:0.43318816899524676\n",
      "train loss:0.5848323764124705\n",
      "train loss:0.47615456503423825\n",
      "train loss:0.7363013203248798\n",
      "train loss:0.8547213790263584\n",
      "train loss:0.4505643601602179\n",
      "train loss:0.4276180045907993\n",
      "train loss:0.4917471760465782\n",
      "train loss:0.38001908224191283\n",
      "train loss:0.7750403223590717\n",
      "train loss:0.7424958679546643\n",
      "train loss:0.7655611080715344\n",
      "train loss:0.3859085077237506\n",
      "train loss:0.30202345783404827\n",
      "train loss:0.6085532277311283\n",
      "train loss:0.722564819557276\n",
      "train loss:0.5081454303447925\n",
      "train loss:0.6104527951745065\n",
      "train loss:0.5854967171937959\n",
      "train loss:0.5664631450308522\n",
      "train loss:0.26182761996095155\n",
      "train loss:0.4646115346991417\n",
      "train loss:0.3472994243289894\n",
      "train loss:0.35543087758578956\n",
      "train loss:0.36603548466381225\n",
      "train loss:0.6322601757873324\n",
      "train loss:0.4966104134743413\n",
      "train loss:0.7117734842199595\n",
      "train loss:0.462372881940656\n",
      "train loss:0.46802490499358285\n",
      "train loss:0.40809500358528156\n",
      "train loss:0.7509875939597527\n",
      "train loss:0.7559300478555209\n",
      "train loss:0.870550899792405\n",
      "train loss:0.5923271724063041\n",
      "train loss:0.6329890175668907\n",
      "train loss:0.6924070683265342\n",
      "train loss:0.5806449609103017\n",
      "train loss:0.6014861295185326\n",
      "train loss:0.47180322512882134\n",
      "train loss:0.7054845585419397\n",
      "train loss:0.48423940719417036\n",
      "train loss:0.6490333611011441\n",
      "train loss:0.6843744569651669\n",
      "train loss:0.6538169480262491\n",
      "train loss:0.6612437694660829\n",
      "train loss:0.5005287861057701\n",
      "train loss:0.5379712599666728\n",
      "train loss:0.657347199544071\n",
      "train loss:0.5873981497496181\n",
      "train loss:0.522666138469946\n",
      "=== epoch:8, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6025581144490989\n",
      "train loss:0.44247860807499617\n",
      "train loss:0.4506184152853539\n",
      "train loss:0.3776142902943387\n",
      "train loss:0.46576111067597664\n",
      "train loss:0.6260543154973296\n",
      "train loss:0.3877183564599242\n",
      "train loss:0.5844227223984577\n",
      "train loss:0.5701330572245831\n",
      "train loss:0.5000362611861371\n",
      "train loss:0.9144054909210706\n",
      "train loss:0.6604048605072771\n",
      "train loss:0.5071828977652969\n",
      "train loss:0.3806563517624021\n",
      "train loss:0.6091834899445202\n",
      "train loss:0.5392553805555823\n",
      "train loss:0.4935432708989719\n",
      "train loss:0.629613724161062\n",
      "train loss:0.4769010206449071\n",
      "train loss:0.6969142008901338\n",
      "train loss:0.7994134025319399\n",
      "train loss:0.4342619839209979\n",
      "train loss:0.5671258806077952\n",
      "train loss:0.569467335338911\n",
      "train loss:0.4505249410113346\n",
      "train loss:0.4726353116246053\n",
      "train loss:0.46171763758265366\n",
      "train loss:0.6466007870126776\n",
      "train loss:0.6650542419660302\n",
      "train loss:0.37873077186636606\n",
      "train loss:0.528317051529456\n",
      "train loss:0.37218421760673653\n",
      "train loss:0.48674372870134947\n",
      "train loss:0.5220008176610424\n",
      "train loss:0.5982199489832424\n",
      "train loss:0.6188892769777574\n",
      "train loss:0.5907459834484323\n",
      "train loss:0.2897390473153335\n",
      "train loss:0.6216212797841046\n",
      "train loss:0.6150990443023595\n",
      "train loss:0.7423209429485607\n",
      "train loss:0.22401158737193202\n",
      "train loss:0.757571949518485\n",
      "train loss:0.6250191074318582\n",
      "train loss:0.5040189411229308\n",
      "train loss:0.5072498319264909\n",
      "train loss:0.9518407224443605\n",
      "train loss:0.36412203575974644\n",
      "train loss:0.6122669517439677\n",
      "train loss:0.9371457697399315\n",
      "train loss:0.7725925602644033\n",
      "train loss:0.6384936693659358\n",
      "train loss:0.6037879072639847\n",
      "train loss:0.5603862557754052\n",
      "train loss:0.8858954022544665\n",
      "train loss:0.5481789034312745\n",
      "train loss:0.5479212875180737\n",
      "train loss:0.655969690173094\n",
      "train loss:0.6344890950581791\n",
      "train loss:0.7032405612256835\n",
      "train loss:0.685438251908195\n",
      "train loss:0.6477161302551689\n",
      "train loss:0.5524223430410574\n",
      "train loss:0.6675140215521649\n",
      "train loss:0.5982625892026453\n",
      "train loss:0.6091373662478976\n",
      "train loss:0.66490093904583\n",
      "train loss:0.5276040385882989\n",
      "train loss:0.6675260187471828\n",
      "train loss:0.6678444134594272\n",
      "train loss:0.5329620447711871\n",
      "train loss:0.5003225584289084\n",
      "train loss:0.6231385855449987\n",
      "train loss:0.5835277630954496\n",
      "train loss:0.8273613899646179\n",
      "train loss:0.7076640626580868\n",
      "train loss:0.5483495488623202\n",
      "train loss:0.6260021521890077\n",
      "train loss:0.6543479876043189\n",
      "train loss:0.6742818824912993\n",
      "train loss:0.5976809201326204\n",
      "train loss:0.5659139598620194\n",
      "train loss:0.7695332372987005\n",
      "train loss:0.4469208879983725\n",
      "train loss:0.5555839290902632\n",
      "train loss:0.44140339742883317\n",
      "train loss:0.63641645621944\n",
      "train loss:0.7416807438172517\n",
      "train loss:0.708936299614918\n",
      "train loss:0.6420819926691347\n",
      "train loss:0.47792544941055126\n",
      "train loss:0.32134146628354976\n",
      "train loss:0.6919825750998573\n",
      "train loss:0.8128320019304492\n",
      "train loss:0.4632063824058054\n",
      "train loss:0.5552930751514188\n",
      "train loss:0.6317697575374291\n",
      "train loss:0.7643801268303052\n",
      "train loss:0.6316229358080298\n",
      "train loss:0.5904365295398059\n",
      "train loss:0.543058794303813\n",
      "train loss:0.7495309930317996\n",
      "train loss:0.5078228567232396\n",
      "train loss:0.673550598239436\n",
      "train loss:0.6149595296513486\n",
      "train loss:0.493472047902008\n",
      "train loss:0.6949086015114101\n",
      "train loss:0.43745187566380733\n",
      "train loss:0.7329935891976473\n",
      "train loss:0.6664073551050583\n",
      "train loss:0.9053129012552914\n",
      "train loss:0.4792112761874395\n",
      "train loss:0.5820930153953519\n",
      "train loss:0.5559972573575539\n",
      "train loss:0.46551740797405083\n",
      "train loss:0.687706265659341\n",
      "train loss:0.7003739267040155\n",
      "train loss:0.5365720917335582\n",
      "train loss:0.587086001754616\n",
      "train loss:0.6306821941483387\n",
      "train loss:0.5483983441269845\n",
      "train loss:0.5583818423358398\n",
      "train loss:0.6284303018010661\n",
      "train loss:0.6165407478712659\n",
      "train loss:0.5445988557104354\n",
      "train loss:0.6972243880454334\n",
      "train loss:0.4476159207672009\n",
      "train loss:0.6872406894880051\n",
      "train loss:0.6064466598434459\n",
      "train loss:0.6671062268984196\n",
      "train loss:0.5488571600953557\n",
      "train loss:0.7661294864184031\n",
      "train loss:0.5813560506408099\n",
      "train loss:0.43118603055106897\n",
      "train loss:0.454680664242746\n",
      "train loss:0.7248971426181863\n",
      "train loss:0.6652291247498934\n",
      "train loss:0.653389772068485\n",
      "train loss:0.5162398649166502\n",
      "train loss:0.3217166212056679\n",
      "train loss:0.7920891998167028\n",
      "train loss:0.46917870894046254\n",
      "train loss:0.4732194715952415\n",
      "train loss:0.7188560208881649\n",
      "train loss:0.47958643215490254\n",
      "train loss:0.4721582056524606\n",
      "train loss:0.3430380707492115\n",
      "train loss:0.40138013291321073\n",
      "train loss:0.8425480421304167\n",
      "train loss:0.7307075621289323\n",
      "train loss:0.6311410667230716\n",
      "train loss:0.6312249840171511\n",
      "train loss:0.6051543888297214\n",
      "=== epoch:9, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6065000400901475\n",
      "train loss:0.6410731128871179\n",
      "train loss:0.6550404877016349\n",
      "train loss:0.5371534921174917\n",
      "train loss:0.48341938022583203\n",
      "train loss:0.47358590724798716\n",
      "train loss:0.6771590388875176\n",
      "train loss:0.7914640234275376\n",
      "train loss:0.5054335415325301\n",
      "train loss:0.6158156876589272\n",
      "train loss:0.5711252283399764\n",
      "train loss:0.817506511603491\n",
      "train loss:0.5357403960239729\n",
      "train loss:0.5873550761799151\n",
      "train loss:0.5344332157986644\n",
      "train loss:0.580538266040697\n",
      "train loss:0.6242211622984403\n",
      "train loss:0.5494798506710619\n",
      "train loss:0.525297196771937\n",
      "train loss:0.5845209356684625\n",
      "train loss:0.6977335689414451\n",
      "train loss:0.5305526213650527\n",
      "train loss:0.5428939875667427\n",
      "train loss:0.353338305250479\n",
      "train loss:0.4935984258584769\n",
      "train loss:0.6049078433432646\n",
      "train loss:0.5426429636113685\n",
      "train loss:0.46268691059220723\n",
      "train loss:0.5602333959381125\n",
      "train loss:0.47361986825952024\n",
      "train loss:0.3703359478799906\n",
      "train loss:0.22010500215010825\n",
      "train loss:0.2619453073944735\n",
      "train loss:0.7692083329874839\n",
      "train loss:0.5177177062800807\n",
      "train loss:0.5470711145953864\n",
      "train loss:0.4290813185347602\n",
      "train loss:0.32332706374908726\n",
      "train loss:0.5657755903035915\n",
      "train loss:0.4009375201829434\n",
      "train loss:0.9088658973939798\n",
      "train loss:0.33383936193714425\n",
      "train loss:0.799189111958243\n",
      "train loss:0.4055159700373686\n",
      "train loss:0.39362942723497035\n",
      "train loss:0.8474905992525036\n",
      "train loss:0.23342355512216412\n",
      "train loss:0.6644335347384577\n",
      "train loss:0.5279716146777484\n",
      "train loss:0.7603330604162574\n",
      "train loss:0.5873677328885262\n",
      "train loss:0.44700233435372977\n",
      "train loss:0.5461105397633647\n",
      "train loss:0.7469656831575682\n",
      "train loss:0.772028989699044\n",
      "train loss:0.6391114532618791\n",
      "train loss:0.6364724355733167\n",
      "train loss:0.7224944624688542\n",
      "train loss:0.6910322365017356\n",
      "train loss:0.5272736142919843\n",
      "train loss:0.46802918401118376\n",
      "train loss:0.6680129398241433\n",
      "train loss:0.5515658735576496\n",
      "train loss:0.5344295410770032\n",
      "train loss:0.5112047523592691\n",
      "train loss:0.5726799660550166\n",
      "train loss:0.4488613494661755\n",
      "train loss:0.5836461201724388\n",
      "train loss:0.4910543253896159\n",
      "train loss:0.4192627861659283\n",
      "train loss:0.4364723191120232\n",
      "train loss:0.4090892727674702\n",
      "train loss:0.5018677544841607\n",
      "train loss:0.3695351832353844\n",
      "train loss:0.7672014545768743\n",
      "train loss:0.7332962958794241\n",
      "train loss:0.6561192924401136\n",
      "train loss:0.7410555310615826\n",
      "train loss:0.4760294978950975\n",
      "train loss:0.6244796041897345\n",
      "train loss:0.865431878821464\n",
      "train loss:0.4738985485435395\n",
      "train loss:0.40569106290864665\n",
      "train loss:0.9109851057368898\n",
      "train loss:0.8530547030684458\n",
      "train loss:0.6227504671872405\n",
      "train loss:0.5931398261647904\n",
      "train loss:0.4854249077102253\n",
      "train loss:0.7033470939502644\n",
      "train loss:0.5372719965180648\n",
      "train loss:0.49222860341744845\n",
      "train loss:0.5192007650747257\n",
      "train loss:0.6135435437229517\n",
      "train loss:0.3842724867125465\n",
      "train loss:0.47982258517808596\n",
      "train loss:0.5624659617626357\n",
      "train loss:0.6157680105595726\n",
      "train loss:0.5117470241117689\n",
      "train loss:0.4557053284410027\n",
      "train loss:0.3994243698436326\n",
      "train loss:0.699188667079811\n",
      "train loss:0.6833863139870587\n",
      "train loss:0.9905021844147976\n",
      "train loss:0.6468993654211259\n",
      "train loss:0.5070055945651473\n",
      "train loss:0.46585306596457154\n",
      "train loss:0.5433101937176439\n",
      "train loss:0.4084246744639815\n",
      "train loss:0.46841459604559255\n",
      "train loss:0.6247689397709905\n",
      "train loss:0.5969481978032187\n",
      "train loss:0.7649797128010294\n",
      "train loss:0.803144705605957\n",
      "train loss:0.6279267658490896\n",
      "train loss:0.5380835498182448\n",
      "train loss:0.5497894920188064\n",
      "train loss:0.5202795319488551\n",
      "train loss:0.5335178246678198\n",
      "train loss:0.558955898367931\n",
      "train loss:0.5945860284084465\n",
      "train loss:0.7499729349990558\n",
      "train loss:0.5772708798751173\n",
      "train loss:0.71765843550589\n",
      "train loss:0.5936875030478883\n",
      "train loss:0.6017521323021968\n",
      "train loss:0.5411631548409541\n",
      "train loss:0.6479191871784417\n",
      "train loss:0.49004196129064415\n",
      "train loss:0.3717786557007903\n",
      "train loss:0.6351523891287236\n",
      "train loss:0.4203460427483073\n",
      "train loss:0.6247611607239505\n",
      "train loss:0.43363012130564493\n",
      "train loss:0.7520697232292637\n",
      "train loss:0.3194204232217135\n",
      "train loss:0.6141350834023613\n",
      "train loss:0.7054734888299643\n",
      "train loss:0.6014817186417096\n",
      "train loss:0.6128139402655939\n",
      "train loss:0.41334671858270255\n",
      "train loss:0.5818473041231245\n",
      "train loss:0.6061819981576815\n",
      "train loss:0.5531859726150052\n",
      "train loss:0.5242445194675263\n",
      "train loss:0.6784321313742625\n",
      "train loss:0.4090589893874907\n",
      "train loss:0.5298507234047315\n",
      "train loss:0.7236494141562962\n",
      "train loss:0.36365347239355866\n",
      "train loss:0.6039060629028239\n",
      "train loss:0.549984665001179\n",
      "train loss:0.6181791560599196\n",
      "train loss:0.37993553836845806\n",
      "=== epoch:10, train acc:0.73, test acc:0.69 ===\n",
      "train loss:0.5732552635366654\n",
      "train loss:0.5163688089975402\n",
      "train loss:0.5906945071555724\n",
      "train loss:0.7740919771111205\n",
      "train loss:0.43228826005342746\n",
      "train loss:0.5619502940426063\n",
      "train loss:0.3115552688306608\n",
      "train loss:0.5420228301717914\n",
      "train loss:0.6247408197260332\n",
      "train loss:0.5079326043190067\n",
      "train loss:0.5375292157422875\n",
      "train loss:0.6686349907483131\n",
      "train loss:0.6528847752459452\n",
      "train loss:0.5258110816461379\n",
      "train loss:0.7978204970368714\n",
      "train loss:0.5678601916590672\n",
      "train loss:0.6739814309774385\n",
      "train loss:0.6559651624293117\n",
      "train loss:0.63903038691113\n",
      "train loss:0.5083734738642891\n",
      "train loss:0.6586834177982649\n",
      "train loss:0.6715394468104604\n",
      "train loss:0.6002643118067369\n",
      "train loss:0.5827675525931408\n",
      "train loss:0.5527679782280752\n",
      "train loss:0.6068079792110936\n",
      "train loss:0.4916012047437578\n",
      "train loss:0.4984276534225378\n",
      "train loss:0.4308596272584132\n",
      "train loss:0.5137987186685694\n",
      "train loss:0.7853851255731489\n",
      "train loss:0.8106204295818269\n",
      "train loss:0.5062039934823959\n",
      "train loss:0.5973090696180555\n",
      "train loss:0.5112091802034191\n",
      "train loss:0.34684601988579866\n",
      "train loss:0.5270546074874186\n",
      "train loss:0.6499167017439001\n",
      "train loss:0.7169059903967581\n",
      "train loss:0.46804660257174086\n",
      "train loss:0.6855030383537452\n",
      "train loss:0.3295842011637003\n",
      "train loss:0.7690512918452973\n",
      "train loss:0.6511920538620307\n",
      "train loss:0.5226392196119487\n",
      "train loss:0.614345264486383\n",
      "train loss:0.5529403299513387\n",
      "train loss:0.7153022197346165\n",
      "train loss:0.565596212008984\n",
      "train loss:0.47052662303253767\n",
      "train loss:0.38462559087225123\n",
      "train loss:0.6827126109302879\n",
      "train loss:0.6667815508733165\n",
      "train loss:0.5592854818630656\n",
      "train loss:0.4288109928072383\n",
      "train loss:0.6727960347171116\n",
      "train loss:0.5604936536111504\n",
      "train loss:0.3372275441857397\n",
      "train loss:0.5658780327238597\n",
      "train loss:0.8495592960507888\n",
      "train loss:0.34481169904190045\n",
      "train loss:0.4914075897070779\n",
      "train loss:0.5052412750686368\n",
      "train loss:0.5178205318326791\n",
      "train loss:0.5456141795640428\n",
      "train loss:0.6157966912103194\n",
      "train loss:0.49416095463724546\n",
      "train loss:0.375603075515231\n",
      "train loss:0.4268202734521953\n",
      "train loss:0.6053011023116871\n",
      "train loss:0.29587933226743485\n",
      "train loss:0.66083775095311\n",
      "train loss:0.8165492806295536\n",
      "train loss:0.3608909490738792\n",
      "train loss:0.5960631407867164\n",
      "train loss:0.7447487810583975\n",
      "train loss:0.5265800945512813\n",
      "train loss:0.4958251210988413\n",
      "train loss:0.5148151485647773\n",
      "train loss:0.5424485879121776\n",
      "train loss:0.6082618009621099\n",
      "train loss:0.4752851102463497\n",
      "train loss:0.5578467256738652\n",
      "train loss:0.5654518194007407\n",
      "train loss:0.5791189295093431\n",
      "train loss:0.6899797872640339\n",
      "train loss:0.692002727857164\n",
      "train loss:0.49936474782731305\n",
      "train loss:0.6683535962482667\n",
      "train loss:0.6122255388629105\n",
      "train loss:0.5814461210512654\n",
      "train loss:0.6354590857045261\n",
      "train loss:0.5230491459717715\n",
      "train loss:0.41751712408319036\n",
      "train loss:0.6076209646679824\n",
      "train loss:0.53508984092791\n",
      "train loss:0.5905584480246263\n",
      "train loss:0.3946016299261811\n",
      "train loss:0.8292496736015682\n",
      "train loss:0.5678761250749628\n",
      "train loss:0.2281174091418885\n",
      "train loss:0.787122400418526\n",
      "train loss:0.5668947956914075\n",
      "train loss:0.47810833068532094\n",
      "train loss:0.5179218719672065\n",
      "train loss:0.3776920710540308\n",
      "train loss:0.6284829257251425\n",
      "train loss:0.8574171025806253\n",
      "train loss:0.5406807054104273\n",
      "train loss:0.3308987289657688\n",
      "train loss:0.39732527680474294\n",
      "train loss:0.35833143918301436\n",
      "train loss:0.40911276879553515\n",
      "train loss:0.5050442316224345\n",
      "train loss:0.6158219781870095\n",
      "train loss:0.5695742728652526\n",
      "train loss:0.7341759449497066\n",
      "train loss:0.3266272238403774\n",
      "train loss:0.4568825697920561\n",
      "train loss:0.4262629640280977\n",
      "train loss:0.7246742838000391\n",
      "train loss:0.4920029423709911\n",
      "train loss:0.25292054960788757\n",
      "train loss:0.3574780094769852\n",
      "train loss:0.3693601619665749\n",
      "train loss:0.4823236917942964\n",
      "train loss:0.7994886018265415\n",
      "train loss:0.72641395833002\n",
      "train loss:0.8632728995396983\n",
      "train loss:0.5529848133973861\n",
      "train loss:0.44821543004971076\n",
      "train loss:0.5212128771342865\n",
      "train loss:0.6919056325522173\n",
      "train loss:0.5369551957415928\n",
      "train loss:0.7171216024645493\n",
      "train loss:0.6503636664907767\n",
      "train loss:0.5351772865034384\n",
      "train loss:0.5970714061621542\n",
      "train loss:0.6233033803222683\n",
      "train loss:0.5608013426492232\n",
      "train loss:0.6445080550423311\n",
      "train loss:0.5499420281985493\n",
      "train loss:0.6425538102711654\n",
      "train loss:0.5245821421225273\n",
      "train loss:0.4931043187644216\n",
      "train loss:0.6000607557212173\n",
      "train loss:0.4761348015053531\n",
      "train loss:0.5895210714983061\n",
      "train loss:0.6169214921412407\n",
      "train loss:0.7570874189195849\n",
      "train loss:0.4596091650899957\n",
      "train loss:0.6951632460307018\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5294117647058824\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 50, 'filter_size': 7, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "457f68d1-d093-4960-b7e4-4643cee9722f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6911383436507729\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.691207470087991\n",
      "train loss:0.6867724778554467\n",
      "train loss:0.697156812827343\n",
      "train loss:0.683690610163096\n",
      "train loss:0.6848201740687052\n",
      "train loss:0.6740038392403739\n",
      "train loss:0.6737580115233769\n",
      "train loss:0.6523746665759894\n",
      "train loss:0.5941786864724034\n",
      "train loss:0.48558377750749937\n",
      "train loss:0.7403525481151885\n",
      "train loss:0.617217212578431\n",
      "train loss:0.4110856305647525\n",
      "train loss:0.8267959037753829\n",
      "train loss:0.5411663108564466\n",
      "train loss:0.48098689801687416\n",
      "train loss:0.530296148052061\n",
      "train loss:0.660507217563405\n",
      "train loss:0.8397987103276101\n",
      "train loss:0.8101840414063897\n",
      "train loss:0.9359122579167469\n",
      "train loss:0.5595108298158914\n",
      "train loss:0.5607255812651839\n",
      "train loss:0.7129124764993157\n",
      "train loss:0.6489128485522172\n",
      "train loss:0.7113401364395707\n",
      "train loss:0.6525790512543111\n",
      "train loss:0.6360480805974185\n",
      "train loss:0.6544045481247247\n",
      "train loss:0.6170643733054604\n",
      "train loss:0.6839171737690297\n",
      "train loss:0.6331798720208014\n",
      "train loss:0.6135088058734203\n",
      "train loss:0.6548537911005428\n",
      "train loss:0.5699986871139252\n",
      "train loss:0.6459802839050827\n",
      "train loss:0.6067960484575262\n",
      "train loss:0.5516533406641767\n",
      "train loss:0.5893817774619499\n",
      "train loss:0.6134459722576796\n",
      "train loss:0.6272745861587701\n",
      "train loss:0.5963630189114673\n",
      "train loss:0.6770127849072076\n",
      "train loss:0.6393013874879843\n",
      "train loss:0.8763158027732437\n",
      "train loss:0.7039379489115628\n",
      "train loss:0.601026553562528\n",
      "train loss:0.4381663158547015\n",
      "train loss:0.6024688362878344\n",
      "train loss:0.5075011559132336\n",
      "train loss:0.43142633083551657\n",
      "train loss:0.49841306480176656\n",
      "train loss:0.8198905234590134\n",
      "train loss:0.9135922850150052\n",
      "train loss:0.40126213651454884\n",
      "train loss:1.0072890094930234\n",
      "train loss:0.6938699321110433\n",
      "train loss:0.46471301790941766\n",
      "train loss:0.44482875316334614\n",
      "train loss:0.5961593478054653\n",
      "train loss:0.6191193612701953\n",
      "train loss:0.605046290229321\n",
      "train loss:0.5950068361439493\n",
      "train loss:0.6769707445594756\n",
      "train loss:0.5410642906786649\n",
      "train loss:0.5414948369306376\n",
      "train loss:0.624247459075372\n",
      "train loss:0.560802818907018\n",
      "train loss:0.835978005747368\n",
      "train loss:0.45119568175624813\n",
      "train loss:0.6966298781241045\n",
      "train loss:0.7567903879845568\n",
      "train loss:0.6012891455616981\n",
      "train loss:0.5540104992599086\n",
      "train loss:0.6625223842570198\n",
      "train loss:0.6851878370658684\n",
      "train loss:0.6873099850073328\n",
      "train loss:0.7519150917303367\n",
      "train loss:0.703271364785764\n",
      "train loss:0.641517969823132\n",
      "train loss:0.5732252731687606\n",
      "train loss:0.6361870647216422\n",
      "train loss:0.5036506119317642\n",
      "train loss:0.5718516277030516\n",
      "train loss:0.4912047978665671\n",
      "train loss:0.4062625011899927\n",
      "train loss:0.5539575733108164\n",
      "train loss:0.6637001097030313\n",
      "train loss:0.5112496508948886\n",
      "train loss:0.6995936322591975\n",
      "train loss:0.5068082985287927\n",
      "train loss:0.38121885977713027\n",
      "train loss:0.7683271858830442\n",
      "train loss:0.960541070127333\n",
      "train loss:0.587323505653398\n",
      "train loss:0.6046430280936435\n",
      "train loss:0.5123741282719179\n",
      "train loss:0.6359110779741017\n",
      "train loss:0.5253951102396508\n",
      "train loss:0.5063802174791645\n",
      "train loss:0.689815701291478\n",
      "train loss:0.5005510475790225\n",
      "train loss:0.5451800886469181\n",
      "train loss:0.6250239239182915\n",
      "train loss:0.5008011815354267\n",
      "train loss:0.5850640763906118\n",
      "train loss:0.41907099902093403\n",
      "train loss:0.7391130997244428\n",
      "train loss:0.5202718180381475\n",
      "train loss:0.746338345968841\n",
      "train loss:0.807548139653129\n",
      "train loss:0.6175298196199968\n",
      "train loss:0.6057884286951022\n",
      "train loss:0.5395666743605245\n",
      "train loss:0.6251838348413206\n",
      "train loss:0.46903947895071507\n",
      "train loss:0.5216633707062587\n",
      "train loss:0.5997541856711835\n",
      "train loss:0.6122400075342931\n",
      "train loss:0.6021942134862787\n",
      "train loss:0.5026903238969751\n",
      "train loss:0.579348733425744\n",
      "train loss:0.6005506767987605\n",
      "train loss:0.6206173540792466\n",
      "train loss:0.8354064107417102\n",
      "train loss:0.49715829715577087\n",
      "train loss:0.49525414314056776\n",
      "train loss:0.5873835386575117\n",
      "train loss:0.6158393390234405\n",
      "train loss:0.6980606566918622\n",
      "train loss:0.42052391602272554\n",
      "train loss:0.6141359420877741\n",
      "train loss:0.6601602090482469\n",
      "train loss:0.8092319004804704\n",
      "train loss:0.43836891176732207\n",
      "train loss:0.3172027721456491\n",
      "train loss:0.6872646122373669\n",
      "train loss:0.8475010241864084\n",
      "train loss:0.5501446631647825\n",
      "train loss:0.7477062593914195\n",
      "train loss:0.634071962439587\n",
      "train loss:0.7686296566169328\n",
      "train loss:0.673628759104299\n",
      "train loss:0.5635028463104731\n",
      "train loss:0.6124878183703224\n",
      "train loss:0.5080968825057185\n",
      "train loss:0.8254463430126358\n",
      "train loss:0.6351725817352224\n",
      "train loss:0.5768482833645894\n",
      "train loss:0.5819438076095209\n",
      "train loss:0.6818628715742389\n",
      "train loss:0.6741280568150682\n",
      "train loss:0.5226561851977247\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5946905834320022\n",
      "train loss:0.6717843045414972\n",
      "train loss:0.5497389411475361\n",
      "train loss:0.481668036081816\n",
      "train loss:0.6934066064210517\n",
      "train loss:0.6822458149274311\n",
      "train loss:0.3790306144489732\n",
      "train loss:0.33644198218437726\n",
      "train loss:0.679592084869256\n",
      "train loss:0.7105344144864969\n",
      "train loss:0.6343812640985365\n",
      "train loss:0.5230859766120748\n",
      "train loss:0.4712413322512726\n",
      "train loss:0.2566835330096241\n",
      "train loss:1.0147259903093413\n",
      "train loss:0.20900023163886527\n",
      "train loss:0.7560860249453182\n",
      "train loss:0.3664125217304261\n",
      "train loss:0.763684165135892\n",
      "train loss:0.7077867032371843\n",
      "train loss:0.5054781389775337\n",
      "train loss:0.5716120125432393\n",
      "train loss:0.7101377643506879\n",
      "train loss:0.6367980581661118\n",
      "train loss:0.5165582099070553\n",
      "train loss:0.43376167589486975\n",
      "train loss:0.7078980623690285\n",
      "train loss:0.6077777897945844\n",
      "train loss:0.776270817048919\n",
      "train loss:0.8372990315789426\n",
      "train loss:0.48973641102148535\n",
      "train loss:0.7277843534840771\n",
      "train loss:0.6970320390175163\n",
      "train loss:0.6723519740694752\n",
      "train loss:0.587517107609121\n",
      "train loss:0.6721636504215793\n",
      "train loss:0.5861022241079631\n",
      "train loss:0.5533814241183738\n",
      "train loss:0.6349548565587604\n",
      "train loss:0.582804575329447\n",
      "train loss:0.5814117903934886\n",
      "train loss:0.6283718931805502\n",
      "train loss:0.569964974836315\n",
      "train loss:0.5719960363739578\n",
      "train loss:0.5771834361959065\n",
      "train loss:0.618440702081344\n",
      "train loss:0.47578099761279996\n",
      "train loss:0.5921629505878048\n",
      "train loss:0.5297593959774841\n",
      "train loss:0.6054237239103248\n",
      "train loss:0.5184898252072676\n",
      "train loss:0.6454328855180327\n",
      "train loss:0.5179376638908243\n",
      "train loss:0.6220902425885125\n",
      "train loss:0.6347018703747891\n",
      "train loss:0.6004539098011941\n",
      "train loss:0.637099086648204\n",
      "train loss:0.7647013214545091\n",
      "train loss:0.6248874614714792\n",
      "train loss:0.5147469274999568\n",
      "train loss:0.4955590202558858\n",
      "train loss:0.42876700494162723\n",
      "train loss:0.7241378610001121\n",
      "train loss:0.6035365330143383\n",
      "train loss:0.5015497351713715\n",
      "train loss:0.502459451097353\n",
      "train loss:0.8079008893743215\n",
      "train loss:0.6056415379421558\n",
      "train loss:0.6068573299361477\n",
      "train loss:0.7042678006863594\n",
      "train loss:0.6153624158090788\n",
      "train loss:0.6155227274390951\n",
      "train loss:0.6879956298246463\n",
      "train loss:0.6720552802880693\n",
      "train loss:0.4894733175650082\n",
      "train loss:0.4707702429475232\n",
      "train loss:0.7364804421796219\n",
      "train loss:0.5422930364670215\n",
      "train loss:0.4815861142601835\n",
      "train loss:0.6039047370873808\n",
      "train loss:0.838480681632813\n",
      "train loss:0.4616755985084492\n",
      "train loss:0.6136280952243978\n",
      "train loss:0.5081057577434946\n",
      "train loss:0.8783651496663335\n",
      "train loss:0.6758010051748018\n",
      "train loss:0.6875137077545735\n",
      "train loss:0.6915115711093558\n",
      "train loss:0.6853021486256878\n",
      "train loss:0.6852272720200066\n",
      "train loss:0.6181244575408349\n",
      "train loss:0.6141345382138003\n",
      "train loss:0.7375583290475756\n",
      "train loss:0.67875751866318\n",
      "train loss:0.613997020963244\n",
      "train loss:0.6147600353726672\n",
      "train loss:0.6790833994049073\n",
      "train loss:0.573492559874546\n",
      "train loss:0.6715954887501713\n",
      "train loss:0.46296617092274267\n",
      "train loss:0.6210186212727539\n",
      "train loss:0.44467991198048784\n",
      "train loss:0.6760212932311843\n",
      "train loss:0.5456680154076092\n",
      "train loss:0.5365937928740125\n",
      "train loss:0.7513278930879338\n",
      "train loss:0.44707130506499126\n",
      "train loss:0.6067379218047323\n",
      "train loss:0.5139217855316611\n",
      "train loss:0.6025933722338503\n",
      "train loss:0.8250207857758338\n",
      "train loss:0.6156556977860699\n",
      "train loss:0.6247315917864522\n",
      "train loss:0.3966774348154706\n",
      "train loss:0.871867272741509\n",
      "train loss:0.6154400553290416\n",
      "train loss:0.5239557895300113\n",
      "train loss:0.6167599977779974\n",
      "train loss:0.9640615572073408\n",
      "train loss:0.6792162284409012\n",
      "train loss:0.8206201118156977\n",
      "train loss:0.6051499729874964\n",
      "train loss:0.6774122482795735\n",
      "train loss:0.6265719181786623\n",
      "train loss:0.5832871378027334\n",
      "train loss:0.5389790786571373\n",
      "train loss:0.5833126943815108\n",
      "train loss:0.6728907744723139\n",
      "train loss:0.7186098328663197\n",
      "train loss:0.6323476061088897\n",
      "train loss:0.629515753794771\n",
      "train loss:0.6329713053226763\n",
      "train loss:0.6763854192736509\n",
      "train loss:0.5858664696827736\n",
      "train loss:0.5729092052559467\n",
      "train loss:0.5692745456379068\n",
      "train loss:0.6204691909974225\n",
      "train loss:0.686119257044735\n",
      "train loss:0.552518638106436\n",
      "train loss:0.689532623460128\n",
      "train loss:0.6125040631845605\n",
      "train loss:0.43973239364374683\n",
      "train loss:0.601742619868585\n",
      "train loss:0.40673134972164054\n",
      "train loss:0.7424776710642744\n",
      "train loss:0.6300996712716478\n",
      "train loss:0.25338526573963466\n",
      "train loss:0.7654622119514543\n",
      "train loss:0.37792456802986296\n",
      "train loss:0.4833713558825097\n",
      "train loss:0.6956132300235435\n",
      "train loss:0.6357112725424552\n",
      "train loss:0.9838371341746444\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6230578288771398\n",
      "train loss:0.3963452751035552\n",
      "train loss:0.6267827135524844\n",
      "train loss:0.6148810950645085\n",
      "train loss:0.5119975911613885\n",
      "train loss:0.6029370241913512\n",
      "train loss:0.4872089315165372\n",
      "train loss:0.6167611995169583\n",
      "train loss:0.683399210736804\n",
      "train loss:0.6964460241225259\n",
      "train loss:0.46255543247782577\n",
      "train loss:0.44922658552929634\n",
      "train loss:0.515536018040289\n",
      "train loss:0.525282860568731\n",
      "train loss:0.5101890771179269\n",
      "train loss:0.5290340926074952\n",
      "train loss:0.6876655253425366\n",
      "train loss:0.6161269251612508\n",
      "train loss:0.537019397513905\n",
      "train loss:0.5197540861634387\n",
      "train loss:0.5009452117099623\n",
      "train loss:0.7465732767676149\n",
      "train loss:0.5350155616925955\n",
      "train loss:0.6150618763297755\n",
      "train loss:0.6274041126076668\n",
      "train loss:0.6962768563510566\n",
      "train loss:0.6301266575436311\n",
      "train loss:0.6168328106503089\n",
      "train loss:0.5165274401626782\n",
      "train loss:0.5098746225375013\n",
      "train loss:0.7924543661490453\n",
      "train loss:0.722317880389282\n",
      "train loss:0.7553221458664209\n",
      "train loss:0.6720174054217714\n",
      "train loss:0.6783979768719587\n",
      "train loss:0.6215630528391616\n",
      "train loss:0.5214629832637376\n",
      "train loss:0.46412268332118495\n",
      "train loss:0.5747076628808854\n",
      "train loss:0.6087105346826795\n",
      "train loss:0.5636650765926688\n",
      "train loss:0.551868163790219\n",
      "train loss:0.4752665595555884\n",
      "train loss:0.6826846371229591\n",
      "train loss:0.4625817270755487\n",
      "train loss:0.6791451994155635\n",
      "train loss:0.4255160787207222\n",
      "train loss:0.6231162399914048\n",
      "train loss:0.6016358733253775\n",
      "train loss:0.24821661823739744\n",
      "train loss:0.7684546054309991\n",
      "train loss:0.6610334711109785\n",
      "train loss:0.903142430280047\n",
      "train loss:0.5112934919527038\n",
      "train loss:0.7226812301043012\n",
      "train loss:0.5888655381875022\n",
      "train loss:0.5867638383643623\n",
      "train loss:0.49488714361804237\n",
      "train loss:0.8572141878311432\n",
      "train loss:0.6822708578956299\n",
      "train loss:0.45991542061939994\n",
      "train loss:0.5456988476754263\n",
      "train loss:0.6989040564915677\n",
      "train loss:0.6102287769997663\n",
      "train loss:0.5366873082258132\n",
      "train loss:0.6804840666811744\n",
      "train loss:0.6782469511388595\n",
      "train loss:0.5557864046215059\n",
      "train loss:0.7327134177503749\n",
      "train loss:0.6697768894575881\n",
      "train loss:0.6135932477123153\n",
      "train loss:0.6157168275352484\n",
      "train loss:0.4270076923375166\n",
      "train loss:0.4744204051818176\n",
      "train loss:0.8104778697221787\n",
      "train loss:0.7424845971363141\n",
      "train loss:0.46290305764302647\n",
      "train loss:0.6091319884169517\n",
      "train loss:0.600532909633217\n",
      "train loss:0.5461545118611717\n",
      "train loss:0.686899486425799\n",
      "train loss:0.5935444713731567\n",
      "train loss:0.6911792875267112\n",
      "train loss:0.42964697934188906\n",
      "train loss:0.4203006521912382\n",
      "train loss:0.5019554992218919\n",
      "train loss:0.7196160920650368\n",
      "train loss:0.6050613128913392\n",
      "train loss:0.5057030185523234\n",
      "train loss:0.7385915869223214\n",
      "train loss:0.7242829885767238\n",
      "train loss:0.3934350702056853\n",
      "train loss:0.7040386795229103\n",
      "train loss:0.49101577210909236\n",
      "train loss:0.6216861496003079\n",
      "train loss:0.3951511087121329\n",
      "train loss:0.8434549261907879\n",
      "train loss:0.3995967237354979\n",
      "train loss:0.5956585861652615\n",
      "train loss:0.7153352609342628\n",
      "train loss:0.601782716304134\n",
      "train loss:0.7013330688046178\n",
      "train loss:0.5720145301334948\n",
      "train loss:0.6089603975813677\n",
      "train loss:0.6194452312437593\n",
      "train loss:0.6266819807180928\n",
      "train loss:0.6106910013653142\n",
      "train loss:0.543636002674279\n",
      "train loss:0.6188367851930747\n",
      "train loss:0.5468571645563223\n",
      "train loss:0.5864267234429097\n",
      "train loss:0.6766224302467474\n",
      "train loss:0.5942341449996199\n",
      "train loss:0.5313884514035963\n",
      "train loss:0.5279662922024813\n",
      "train loss:0.5125350916935942\n",
      "train loss:0.6351349644544515\n",
      "train loss:0.775045745187278\n",
      "train loss:0.7962976490260187\n",
      "train loss:0.5967414843741012\n",
      "train loss:0.6644537420900072\n",
      "train loss:0.613037472918754\n",
      "train loss:0.5223486319843406\n",
      "train loss:0.4445911042368233\n",
      "train loss:0.5278652183434649\n",
      "train loss:0.8469201232694982\n",
      "train loss:0.7388469080689148\n",
      "train loss:0.37514407999930544\n",
      "train loss:0.6063061954178306\n",
      "train loss:0.5343972862270533\n",
      "train loss:0.4285400453202858\n",
      "train loss:0.7567742274444917\n",
      "train loss:0.5254392608869167\n",
      "train loss:0.7828234932634571\n",
      "train loss:0.6853266424212128\n",
      "train loss:0.6754700203566008\n",
      "train loss:0.7460527468910788\n",
      "train loss:0.4471523570024726\n",
      "train loss:0.5344322073782612\n",
      "train loss:0.6851608032738319\n",
      "train loss:0.6938797920967468\n",
      "train loss:0.6127121010498462\n",
      "train loss:0.6892781950009031\n",
      "train loss:0.7442608252270752\n",
      "train loss:0.7405114978762452\n",
      "train loss:0.5058859097910876\n",
      "train loss:0.5651470782180766\n",
      "train loss:0.7713114891484877\n",
      "train loss:0.6672424013779615\n",
      "train loss:0.4919191622166717\n",
      "train loss:0.5708077084402289\n",
      "train loss:0.6773370616367529\n",
      "train loss:0.6660299599970341\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6004620986226585\n",
      "train loss:0.6575476524103723\n",
      "train loss:0.6716448458008664\n",
      "train loss:0.607038386380725\n",
      "train loss:0.7223646988076873\n",
      "train loss:0.6769163474071537\n",
      "train loss:0.7381605610966192\n",
      "train loss:0.5448345828327095\n",
      "train loss:0.6016357453014914\n",
      "train loss:0.4931184291461923\n",
      "train loss:0.6529811011447454\n",
      "train loss:0.7266993081958643\n",
      "train loss:0.6215260882822224\n",
      "train loss:0.7436258950581853\n",
      "train loss:0.7241119599378152\n",
      "train loss:0.6794030453938795\n",
      "train loss:0.5605133652399774\n",
      "train loss:0.6999271074491369\n",
      "train loss:0.6284563218069403\n",
      "train loss:0.6231101777370058\n",
      "train loss:0.6592268385412119\n",
      "train loss:0.598262360006604\n",
      "train loss:0.7155683067637852\n",
      "train loss:0.5777525279832494\n",
      "train loss:0.551988629272391\n",
      "train loss:0.6116297297275343\n",
      "train loss:0.7237639698924359\n",
      "train loss:0.6368803489200651\n",
      "train loss:0.5634508515995235\n",
      "train loss:0.5122127840562892\n",
      "train loss:0.5548317990681148\n",
      "train loss:0.44595275842849336\n",
      "train loss:0.4830734375916329\n",
      "train loss:0.49969823336670804\n",
      "train loss:0.6054173227719117\n",
      "train loss:0.6168592185200678\n",
      "train loss:0.6220822557957887\n",
      "train loss:0.6460425692430662\n",
      "train loss:0.4708419892378502\n",
      "train loss:0.6880105891857504\n",
      "train loss:0.3645277610266472\n",
      "train loss:0.6104955489189414\n",
      "train loss:0.47947844229328024\n",
      "train loss:0.4638036834717301\n",
      "train loss:0.9470547325715006\n",
      "train loss:0.6183939940555108\n",
      "train loss:0.4996757497314393\n",
      "train loss:0.6319213581539604\n",
      "train loss:0.5322147624084975\n",
      "train loss:0.618912031168232\n",
      "train loss:0.7821251332320452\n",
      "train loss:0.5891465556768785\n",
      "train loss:0.7596033268727942\n",
      "train loss:0.5632163969912404\n",
      "train loss:0.6115549317636582\n",
      "train loss:0.5309257103739862\n",
      "train loss:0.7363642577273659\n",
      "train loss:0.643341411475687\n",
      "train loss:0.5365028352720062\n",
      "train loss:0.5576742442469915\n",
      "train loss:0.6764643859358684\n",
      "train loss:0.588976047846217\n",
      "train loss:0.5432436305971804\n",
      "train loss:0.6558746290337122\n",
      "train loss:0.6639702980110284\n",
      "train loss:0.5220921494833526\n",
      "train loss:0.48210119416011865\n",
      "train loss:0.5221922296834892\n",
      "train loss:0.4812253495407496\n",
      "train loss:0.5931496698307347\n",
      "train loss:0.6215889599808955\n",
      "train loss:0.6845953101439213\n",
      "train loss:0.8173595368500696\n",
      "train loss:0.29065448409417755\n",
      "train loss:0.4678661770291791\n",
      "train loss:0.5178710280389689\n",
      "train loss:0.6272714775134669\n",
      "train loss:0.7349852321325093\n",
      "train loss:0.3815514781060624\n",
      "train loss:0.5660485211143091\n",
      "train loss:0.6271258769032885\n",
      "train loss:0.4777391185020905\n",
      "train loss:0.40556907440689993\n",
      "train loss:0.6246637411402158\n",
      "train loss:0.8959583369623004\n",
      "train loss:0.6939565050098471\n",
      "train loss:0.5201019250502598\n",
      "train loss:0.52686199417484\n",
      "train loss:0.5957557318242109\n",
      "train loss:0.8415352475875976\n",
      "train loss:0.5428810411999305\n",
      "train loss:0.47082625579079246\n",
      "train loss:0.4894975066983531\n",
      "train loss:0.5623710240960159\n",
      "train loss:0.6106794773984066\n",
      "train loss:0.7930799927272598\n",
      "train loss:0.7741976154446999\n",
      "train loss:0.43597113741499627\n",
      "train loss:0.5773379483854203\n",
      "train loss:0.608508535833526\n",
      "train loss:0.41812925597409023\n",
      "train loss:0.6124858684734784\n",
      "train loss:0.49623325867066315\n",
      "train loss:0.3196926902403886\n",
      "train loss:0.6299146408179673\n",
      "train loss:0.9285039228948133\n",
      "train loss:0.5051870744130521\n",
      "train loss:0.9109894135615104\n",
      "train loss:0.5877774638144219\n",
      "train loss:0.7466634635921235\n",
      "train loss:0.5875057896616335\n",
      "train loss:0.6095923491794069\n",
      "train loss:0.837045229704429\n",
      "train loss:0.5759057553463356\n",
      "train loss:0.5725885393913044\n",
      "train loss:0.6179460791845413\n",
      "train loss:0.507833742175037\n",
      "train loss:0.5648077716607989\n",
      "train loss:0.6560128967646291\n",
      "train loss:0.5612141936154094\n",
      "train loss:0.5568710694297849\n",
      "train loss:0.7133502065311396\n",
      "train loss:0.6219247036631361\n",
      "train loss:0.6293807157734985\n",
      "train loss:0.7095434506173534\n",
      "train loss:0.5592334853326221\n",
      "train loss:0.4753153428754084\n",
      "train loss:0.6242727594681544\n",
      "train loss:0.5271502704400384\n",
      "train loss:0.555870735838558\n",
      "train loss:0.5047106662545396\n",
      "train loss:0.681140349738366\n",
      "train loss:0.2589293883360446\n",
      "train loss:0.5100261384740911\n",
      "train loss:0.8723622887631347\n",
      "train loss:0.5134976513340525\n",
      "train loss:0.48252156482048497\n",
      "train loss:0.563633969241905\n",
      "train loss:0.8156337929285126\n",
      "train loss:0.4902734598077636\n",
      "train loss:0.6911804458341556\n",
      "train loss:0.8349196883767416\n",
      "train loss:0.6057553472524024\n",
      "train loss:0.6319734363919299\n",
      "train loss:0.5360850754148786\n",
      "train loss:0.5411903064798886\n",
      "train loss:0.5544353015950969\n",
      "train loss:0.6067553247196606\n",
      "train loss:0.6069528823811161\n",
      "train loss:0.5259139988061025\n",
      "train loss:0.6056625358040127\n",
      "train loss:0.6101628958137719\n",
      "train loss:0.47134193600234164\n",
      "=== epoch:5, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5772697600258702\n",
      "train loss:0.41994778726302073\n",
      "train loss:0.4875706652370863\n",
      "train loss:0.3845312901042548\n",
      "train loss:0.5201163728924082\n",
      "train loss:0.3382549926639084\n",
      "train loss:0.9001143999339918\n",
      "train loss:0.6663670717780524\n",
      "train loss:0.816836234521363\n",
      "train loss:0.747617747228672\n",
      "train loss:0.5585352482289857\n",
      "train loss:0.605886532787554\n",
      "train loss:0.5954117083429564\n",
      "train loss:0.601661314764208\n",
      "train loss:0.6943711336730622\n",
      "train loss:0.598196609947962\n",
      "train loss:0.5505073711664783\n",
      "train loss:0.5674442546229199\n",
      "train loss:0.5451283273043976\n",
      "train loss:0.5538152404237988\n",
      "train loss:0.5536955229121601\n",
      "train loss:0.5857663000101019\n",
      "train loss:0.6628032226774933\n",
      "train loss:0.5654967040168477\n",
      "train loss:0.7110105582426748\n",
      "train loss:0.519767410977581\n",
      "train loss:0.6626478987012642\n",
      "train loss:0.3435609916721393\n",
      "train loss:0.34081283822768926\n",
      "train loss:0.6467740271650538\n",
      "train loss:0.3979498059415462\n",
      "train loss:0.3820652442903274\n",
      "train loss:0.5513915898535713\n",
      "train loss:0.3426949419448092\n",
      "train loss:0.30880901617620304\n",
      "train loss:0.9387245931909142\n",
      "train loss:0.7524634305367406\n",
      "train loss:0.5045777711792299\n",
      "train loss:1.0097413957156443\n",
      "train loss:0.3062489233201337\n",
      "train loss:0.5531369346445193\n",
      "train loss:0.5899010407035191\n",
      "train loss:0.7634227334631583\n",
      "train loss:0.5429260287474215\n",
      "train loss:0.5712283238284435\n",
      "train loss:0.6292344241025838\n",
      "train loss:0.5610775220117898\n",
      "train loss:0.5876161162206535\n",
      "train loss:0.4742582913091254\n",
      "train loss:0.5324857442258895\n",
      "train loss:0.608763876820088\n",
      "train loss:0.7119896513949306\n",
      "train loss:0.6196902890956192\n",
      "train loss:0.7450897281684669\n",
      "train loss:0.6358498193991426\n",
      "train loss:0.5483995780961707\n",
      "train loss:0.752081449777606\n",
      "train loss:0.7807835110772328\n",
      "train loss:0.8170328717156636\n",
      "train loss:0.6786243531703662\n",
      "train loss:0.6763818976680049\n",
      "train loss:0.6055809087450796\n",
      "train loss:0.7515925584584568\n",
      "train loss:0.6802380827850161\n",
      "train loss:0.5785773066028613\n",
      "train loss:0.5777140315944044\n",
      "train loss:0.6547410719656697\n",
      "train loss:0.6004776223635278\n",
      "train loss:0.6743721664667675\n",
      "train loss:0.61794684583125\n",
      "train loss:0.7002136526144722\n",
      "train loss:0.6519346739831879\n",
      "train loss:0.6973483350664188\n",
      "train loss:0.6945682204775773\n",
      "train loss:0.6523042955806219\n",
      "train loss:0.7149563166966452\n",
      "train loss:0.707209718869012\n",
      "train loss:0.639439831700042\n",
      "train loss:0.5801059344657441\n",
      "train loss:0.6269863507810065\n",
      "train loss:0.7436905346940594\n",
      "train loss:0.5555741536512885\n",
      "train loss:0.5026050731682468\n",
      "train loss:0.5383392035976359\n",
      "train loss:0.5523193255327065\n",
      "train loss:0.5992854377904696\n",
      "train loss:0.5524640095347297\n",
      "train loss:0.5506818117096708\n",
      "train loss:0.43662255863284327\n",
      "train loss:0.5660673217437772\n",
      "train loss:0.4220679678930737\n",
      "train loss:0.498464335308196\n",
      "train loss:0.7323776210655917\n",
      "train loss:0.28860556003518384\n",
      "train loss:0.5137608728846026\n",
      "train loss:0.38383650706175804\n",
      "train loss:0.3180314954938116\n",
      "train loss:0.9319731034765105\n",
      "train loss:0.7340632071940604\n",
      "train loss:0.11686498687692197\n",
      "train loss:0.557985301682502\n",
      "train loss:0.8889182759387211\n",
      "train loss:0.429427622593166\n",
      "train loss:1.2092263219765371\n",
      "train loss:0.547246825330406\n",
      "train loss:0.420119578564938\n",
      "train loss:0.611678268933661\n",
      "train loss:0.7060596959997342\n",
      "train loss:0.6807167803379915\n",
      "train loss:0.8555906359741181\n",
      "train loss:0.7113338275892259\n",
      "train loss:0.5643700506902241\n",
      "train loss:0.6238646873002744\n",
      "train loss:0.5794001573420842\n",
      "train loss:0.6903691302745045\n",
      "train loss:0.5606164744946251\n",
      "train loss:0.6661371240556447\n",
      "train loss:0.5593506112086326\n",
      "train loss:0.6461073745616603\n",
      "train loss:0.593025344607702\n",
      "train loss:0.6887119402499324\n",
      "train loss:0.7135149291377424\n",
      "train loss:0.5707662992754527\n",
      "train loss:0.6828453797100476\n",
      "train loss:0.6822076890089271\n",
      "train loss:0.6684539233933827\n",
      "train loss:0.6046907192816335\n",
      "train loss:0.5266029743927415\n",
      "train loss:0.5545878231452551\n",
      "train loss:0.6312611631048802\n",
      "train loss:0.6045117378394798\n",
      "train loss:0.6494430306728584\n",
      "train loss:0.5509115198185203\n",
      "train loss:0.509591241891968\n",
      "train loss:0.7717628297415645\n",
      "train loss:0.4275633788228047\n",
      "train loss:0.7491147128167116\n",
      "train loss:0.4900072238578118\n",
      "train loss:0.5305129583334995\n",
      "train loss:0.5705482491372555\n",
      "train loss:0.6341552238397763\n",
      "train loss:0.42344811617589356\n",
      "train loss:0.5054240161777607\n",
      "train loss:0.6216624478472003\n",
      "train loss:0.7503018316139343\n",
      "train loss:0.4675087877613723\n",
      "train loss:0.23935506965349146\n",
      "train loss:0.39037236930258584\n",
      "train loss:0.48529848932409675\n",
      "train loss:0.3243137157073626\n",
      "train loss:0.7009203323105667\n",
      "train loss:0.7749976973203336\n",
      "train loss:0.3362993176462764\n",
      "=== epoch:6, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7371080149348123\n",
      "train loss:1.143501558876386\n",
      "train loss:0.6050829105487197\n",
      "train loss:0.6625081666042554\n",
      "train loss:0.47172764784946636\n",
      "train loss:0.5405806311479596\n",
      "train loss:0.5235861596364864\n",
      "train loss:0.43361829902035665\n",
      "train loss:0.7081619988327508\n",
      "train loss:0.4442410934023493\n",
      "train loss:0.6979082155270679\n",
      "train loss:0.7990780899360466\n",
      "train loss:0.6088364728059361\n",
      "train loss:0.6209763906774712\n",
      "train loss:0.7856855469124281\n",
      "train loss:0.5673207188106051\n",
      "train loss:0.48403219148811855\n",
      "train loss:0.6108798797803753\n",
      "train loss:0.6195817017384421\n",
      "train loss:0.6016986908095878\n",
      "train loss:0.6357651502710024\n",
      "train loss:0.6579760559965897\n",
      "train loss:0.5776540088122857\n",
      "train loss:0.6366829060404974\n",
      "train loss:0.6428216400540452\n",
      "train loss:0.614968210023299\n",
      "train loss:0.43522188838156\n",
      "train loss:0.5398977560951049\n",
      "train loss:0.5301726679226467\n",
      "train loss:0.4377240824178858\n",
      "train loss:0.6729210343711045\n",
      "train loss:0.5638097382506205\n",
      "train loss:0.5606468581097833\n",
      "train loss:0.7446481260313207\n",
      "train loss:0.5291391735531316\n",
      "train loss:0.38194458154066113\n",
      "train loss:0.46097717619267725\n",
      "train loss:0.6726971163654161\n",
      "train loss:0.5532790075223086\n",
      "train loss:0.670388146808584\n",
      "train loss:0.5538000618316359\n",
      "train loss:0.7318907074368944\n",
      "train loss:0.4940518132507428\n",
      "train loss:0.45645706892335214\n",
      "train loss:0.6347129542716463\n",
      "train loss:0.742685494602829\n",
      "train loss:0.5233962789817106\n",
      "train loss:0.4914313614261877\n",
      "train loss:0.5920147877174766\n",
      "train loss:0.631421790673272\n",
      "train loss:0.4132091021002836\n",
      "train loss:0.7384196712683934\n",
      "train loss:0.6144339793605167\n",
      "train loss:0.43304669066498225\n",
      "train loss:0.6859596980828954\n",
      "train loss:0.4848609056521342\n",
      "train loss:0.32614289655646084\n",
      "train loss:0.38784318399091944\n",
      "train loss:0.6951187385677515\n",
      "train loss:0.6287438874400765\n",
      "train loss:0.4165344823831988\n",
      "train loss:0.6160858354678498\n",
      "train loss:0.5091831409992552\n",
      "train loss:0.5647442343866592\n",
      "train loss:0.6427710257495526\n",
      "train loss:0.8258003953749956\n",
      "train loss:0.3897574819579786\n",
      "train loss:0.72863222708314\n",
      "train loss:0.5181361895523009\n",
      "train loss:0.3792685488728082\n",
      "train loss:0.3580748936999197\n",
      "train loss:0.6739061415073153\n",
      "train loss:0.38570603731531444\n",
      "train loss:0.6234716644645292\n",
      "train loss:0.6237857358607339\n",
      "train loss:0.6195784397781421\n",
      "train loss:0.8143524974828777\n",
      "train loss:0.45410958601924756\n",
      "train loss:0.4412035704129867\n",
      "train loss:0.4239932584873496\n",
      "train loss:0.6300893402738845\n",
      "train loss:0.4712243058818091\n",
      "train loss:0.5423290448273335\n",
      "train loss:0.625734256475718\n",
      "train loss:0.4830500607877851\n",
      "train loss:0.707325189213279\n",
      "train loss:0.5836213906503759\n",
      "train loss:0.5046749403416254\n",
      "train loss:0.6546101966795435\n",
      "train loss:0.3958607434401152\n",
      "train loss:0.46755201018883985\n",
      "train loss:0.5348125648218722\n",
      "train loss:0.47497886957092456\n",
      "train loss:0.449379475363584\n",
      "train loss:0.4900307767002762\n",
      "train loss:1.0256865203786432\n",
      "train loss:0.8525105673564427\n",
      "train loss:0.43722213055195003\n",
      "train loss:0.5916431261995886\n",
      "train loss:0.6757743719051241\n",
      "train loss:0.7146774191749693\n",
      "train loss:0.39333290707116125\n",
      "train loss:0.3635866927948783\n",
      "train loss:0.35126506133390045\n",
      "train loss:0.6983704498099528\n",
      "train loss:0.5176565955997867\n",
      "train loss:0.6505846613603586\n",
      "train loss:0.3955348893240155\n",
      "train loss:0.7242273313810967\n",
      "train loss:0.4963917321193718\n",
      "train loss:0.4106949079631713\n",
      "train loss:0.6413242640074752\n",
      "train loss:0.730664523448491\n",
      "train loss:0.5732009160602071\n",
      "train loss:0.5941175970862587\n",
      "train loss:0.7146079472442943\n",
      "train loss:0.5356741748598871\n",
      "train loss:0.6592514231165536\n",
      "train loss:0.7667394766829589\n",
      "train loss:0.7528850760603487\n",
      "train loss:0.43731594389977246\n",
      "train loss:0.5453830373820344\n",
      "train loss:0.33125766969029957\n",
      "train loss:0.5583607235199795\n",
      "train loss:0.6672434998089036\n",
      "train loss:0.48698516192269625\n",
      "train loss:0.6967672000634527\n",
      "train loss:0.6921132860191621\n",
      "train loss:0.674904567850924\n",
      "train loss:0.7944949498226517\n",
      "train loss:0.7342308079764293\n",
      "train loss:0.5510727382990666\n",
      "train loss:0.4086802643334713\n",
      "train loss:0.5622014898254599\n",
      "train loss:0.5383159785108889\n",
      "train loss:0.6297186162207428\n",
      "train loss:0.5788355814000503\n",
      "train loss:0.6108870211664266\n",
      "train loss:0.4856921631052601\n",
      "train loss:0.47229213800809655\n",
      "train loss:0.39812442850140983\n",
      "train loss:0.5924640483107577\n",
      "train loss:0.6069012705402775\n",
      "train loss:0.49418121892531913\n",
      "train loss:0.8003700174054892\n",
      "train loss:0.6052572696857631\n",
      "train loss:0.8049804424802707\n",
      "train loss:0.6183117415675012\n",
      "train loss:0.7336750260401559\n",
      "train loss:0.42036029229138905\n",
      "train loss:0.6452064696411448\n",
      "train loss:0.7523505413962192\n",
      "train loss:0.7358777555489484\n",
      "=== epoch:7, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6478114040909206\n",
      "train loss:0.6390136080143765\n",
      "train loss:0.4607866385770681\n",
      "train loss:0.6418777091698795\n",
      "train loss:0.46421511118947467\n",
      "train loss:0.515835025464885\n",
      "train loss:0.525127447353122\n",
      "train loss:0.5776858551680532\n",
      "train loss:0.610755454981214\n",
      "train loss:0.6703138476198964\n",
      "train loss:0.5162879674748351\n",
      "train loss:0.6596876544437057\n",
      "train loss:0.5269747615796432\n",
      "train loss:0.5878347173903403\n",
      "train loss:0.6572419786100446\n",
      "train loss:0.3408700454464137\n",
      "train loss:0.7249357798205532\n",
      "train loss:0.691423031732038\n",
      "train loss:0.4039890556161835\n",
      "train loss:0.8165156975696227\n",
      "train loss:0.8815764126845143\n",
      "train loss:0.5308710962000582\n",
      "train loss:0.9091989094800198\n",
      "train loss:0.600738326133186\n",
      "train loss:0.6632488722034211\n",
      "train loss:0.6185591343979107\n",
      "train loss:0.44866049287064963\n",
      "train loss:0.4732064607103913\n",
      "train loss:0.6701570790277158\n",
      "train loss:0.5675632568952563\n",
      "train loss:0.5895369956017997\n",
      "train loss:0.5491358639977205\n",
      "train loss:0.4633514008026742\n",
      "train loss:0.5815538652182629\n",
      "train loss:0.6306942729738095\n",
      "train loss:0.7344877217947602\n",
      "train loss:0.6247491191894732\n",
      "train loss:0.42214577168985024\n",
      "train loss:0.5946843759504425\n",
      "train loss:0.49094618829284753\n",
      "train loss:0.6514208376416242\n",
      "train loss:0.5050184484892307\n",
      "train loss:0.31504992803050536\n",
      "train loss:0.5317922865811866\n",
      "train loss:0.7590751363540862\n",
      "train loss:0.6064402241898404\n",
      "train loss:0.3786668404591398\n",
      "train loss:0.5311587504305223\n",
      "train loss:0.37133191502019736\n",
      "train loss:0.7184445816837252\n",
      "train loss:0.6550033006461605\n",
      "train loss:0.7867742299432838\n",
      "train loss:0.5554998761164665\n",
      "train loss:0.37611505180571775\n",
      "train loss:0.6668198630572648\n",
      "train loss:0.8196559697264169\n",
      "train loss:0.6746242594355836\n",
      "train loss:0.6899079242607522\n",
      "train loss:0.6906200076961168\n",
      "train loss:0.6373170193650063\n",
      "train loss:0.48997717540642743\n",
      "train loss:0.6167697626658117\n",
      "train loss:0.6442514779519855\n",
      "train loss:0.6336095860564674\n",
      "train loss:0.6995022753143582\n",
      "train loss:0.4978556768823027\n",
      "train loss:0.7188974951162113\n",
      "train loss:0.6091166718265913\n",
      "train loss:0.5472842374941158\n",
      "train loss:0.5368709439708366\n",
      "train loss:0.5514999978239009\n",
      "train loss:0.6642613200692697\n",
      "train loss:0.49005382727297464\n",
      "train loss:0.7672272579483834\n",
      "train loss:0.49192531022401953\n",
      "train loss:0.7884038533331127\n",
      "train loss:0.6597702806093334\n",
      "train loss:0.6785679396731459\n",
      "train loss:0.49776455123596\n",
      "train loss:0.5012444817674362\n",
      "train loss:0.43708367543543203\n",
      "train loss:0.759378191453432\n",
      "train loss:0.5310086880939486\n",
      "train loss:0.501882933573863\n",
      "train loss:0.7897320063142202\n",
      "train loss:0.6226744746525984\n",
      "train loss:0.4240481146936337\n",
      "train loss:0.548295180138394\n",
      "train loss:0.45871978850685863\n",
      "train loss:0.4985143031823246\n",
      "train loss:0.7279691838632673\n",
      "train loss:0.420301188751368\n",
      "train loss:0.6000533586680008\n",
      "train loss:0.815393134205161\n",
      "train loss:0.5057635079407505\n",
      "train loss:0.6755953184135337\n",
      "train loss:0.528863709336685\n",
      "train loss:0.3719874725411577\n",
      "train loss:0.6005239231118097\n",
      "train loss:0.5259804056142677\n",
      "train loss:0.7624177728020802\n",
      "train loss:0.614501338000532\n",
      "train loss:0.7308340327279185\n",
      "train loss:0.5970667167617576\n",
      "train loss:0.6035998051368521\n",
      "train loss:0.6329932551763737\n",
      "train loss:0.5804022521057456\n",
      "train loss:0.7187543736787656\n",
      "train loss:0.6035480674053122\n",
      "train loss:0.7391839301831735\n",
      "train loss:0.45817085586149775\n",
      "train loss:0.5962219262796429\n",
      "train loss:0.6406104906223612\n",
      "train loss:0.5447847507027243\n",
      "train loss:0.42583978175627585\n",
      "train loss:0.734813615038378\n",
      "train loss:0.5780117829280318\n",
      "train loss:0.4987218973983191\n",
      "train loss:0.6402230443240896\n",
      "train loss:0.777056710423111\n",
      "train loss:0.415203799276256\n",
      "train loss:0.4517994471308529\n",
      "train loss:0.641935024869938\n",
      "train loss:0.7027815832630102\n",
      "train loss:0.46278445131194845\n",
      "train loss:0.4675413433304708\n",
      "train loss:0.6724584618824884\n",
      "train loss:0.5267013530041035\n",
      "train loss:0.5906841888868147\n",
      "train loss:0.6849179512727287\n",
      "train loss:0.24922695663193611\n",
      "train loss:0.6937310910371496\n",
      "train loss:0.5710005330238641\n",
      "train loss:0.7091602110944993\n",
      "train loss:0.637266140616183\n",
      "train loss:0.476392909414043\n",
      "train loss:0.5072635270634981\n",
      "train loss:0.6804873161099099\n",
      "train loss:0.4119431577985894\n",
      "train loss:0.7267676691012086\n",
      "train loss:0.5367868186472733\n",
      "train loss:0.48107115197560263\n",
      "train loss:0.59731003662504\n",
      "train loss:0.5453107710757622\n",
      "train loss:0.7516844442843353\n",
      "train loss:0.427104622555583\n",
      "train loss:0.4819583100669206\n",
      "train loss:0.318356960742709\n",
      "train loss:0.3705913959555414\n",
      "train loss:0.5250534132485749\n",
      "train loss:0.635989149050251\n",
      "train loss:0.631029175429044\n",
      "train loss:0.364985427657329\n",
      "=== epoch:8, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6095046177448262\n",
      "train loss:0.6348923382815211\n",
      "train loss:0.9353514514221972\n",
      "train loss:0.6876666607380647\n",
      "train loss:0.7038910928315993\n",
      "train loss:0.7444365088832864\n",
      "train loss:0.5043299035553763\n",
      "train loss:0.5762134768837426\n",
      "train loss:0.5914393389761163\n",
      "train loss:0.7095299832709452\n",
      "train loss:0.48123669054110907\n",
      "train loss:0.7081898598381752\n",
      "train loss:0.5463865281527525\n",
      "train loss:0.5198762059539759\n",
      "train loss:0.49613505307067146\n",
      "train loss:0.7503759100280384\n",
      "train loss:0.5732656099199027\n",
      "train loss:0.52589595887807\n",
      "train loss:0.40884343478904217\n",
      "train loss:0.5718930132833553\n",
      "train loss:0.49724491641899393\n",
      "train loss:0.4493535577877835\n",
      "train loss:0.5453326892884903\n",
      "train loss:0.7087795299882387\n",
      "train loss:0.3277126708310503\n",
      "train loss:0.827138305935798\n",
      "train loss:0.557916827497659\n",
      "train loss:0.5356561480847529\n",
      "train loss:0.7028218185998558\n",
      "train loss:0.4784214052231205\n",
      "train loss:0.396879692703716\n",
      "train loss:0.4894289641946151\n",
      "train loss:0.5283218530332883\n",
      "train loss:0.85524748978136\n",
      "train loss:0.3549818546524255\n",
      "train loss:0.4573655339071113\n",
      "train loss:0.8353102137639438\n",
      "train loss:0.5498834455762482\n",
      "train loss:0.6622350963424047\n",
      "train loss:0.6538882683523511\n",
      "train loss:0.2232840692332645\n",
      "train loss:0.7824546537967635\n",
      "train loss:0.2643541981673899\n",
      "train loss:0.6694052531035455\n",
      "train loss:0.6295713961880414\n",
      "train loss:0.7416402208836603\n",
      "train loss:0.7684544538958048\n",
      "train loss:0.5531303443066807\n",
      "train loss:0.5969440846294911\n",
      "train loss:0.6345051127005066\n",
      "train loss:0.6537299682745985\n",
      "train loss:0.43652247755522744\n",
      "train loss:0.5842987691840913\n",
      "train loss:0.4846072863281112\n",
      "train loss:0.609645617585773\n",
      "train loss:0.6196379547297075\n",
      "train loss:0.5377181792061361\n",
      "train loss:0.45652755610581075\n",
      "train loss:0.5962490102445125\n",
      "train loss:0.6289454399626611\n",
      "train loss:0.6446979106789833\n",
      "train loss:0.5813730023532591\n",
      "train loss:0.4212576042108493\n",
      "train loss:0.5245852472693674\n",
      "train loss:0.49939148050964144\n",
      "train loss:0.689413437047458\n",
      "train loss:0.5065455927258776\n",
      "train loss:0.767894333545664\n",
      "train loss:0.9368007127661434\n",
      "train loss:0.7115962980508814\n",
      "train loss:0.554642403020721\n",
      "train loss:0.5018766025242456\n",
      "train loss:0.8231847198597876\n",
      "train loss:0.41990091198749446\n",
      "train loss:0.5049919864464344\n",
      "train loss:0.49813757379188334\n",
      "train loss:0.6229434117776214\n",
      "train loss:0.5742293133422379\n",
      "train loss:0.7095258666966002\n",
      "train loss:0.45318256287328473\n",
      "train loss:0.5463585296735658\n",
      "train loss:0.7115616441289329\n",
      "train loss:0.5315674805535304\n",
      "train loss:0.4270626173256895\n",
      "train loss:0.795962091771023\n",
      "train loss:0.545904599463638\n",
      "train loss:0.41354634262426293\n",
      "train loss:0.5512772338761567\n",
      "train loss:0.9432390467205183\n",
      "train loss:0.46475872909571914\n",
      "train loss:0.7025445187124499\n",
      "train loss:0.25979264017635656\n",
      "train loss:0.42982593569236877\n",
      "train loss:0.4967217205757491\n",
      "train loss:0.4993822330959422\n",
      "train loss:0.4316961791910002\n",
      "train loss:0.641492545281483\n",
      "train loss:0.5149323747353948\n",
      "train loss:0.368635342909971\n",
      "train loss:0.6225838724852113\n",
      "train loss:0.39940861661781324\n",
      "train loss:0.7576302845825696\n",
      "train loss:0.9363231660147532\n",
      "train loss:0.6441922958666125\n",
      "train loss:0.7163656736535722\n",
      "train loss:0.4903085092690228\n",
      "train loss:0.6545476840166796\n",
      "train loss:0.5533876983547508\n",
      "train loss:0.5045664780954218\n",
      "train loss:0.709287636170107\n",
      "train loss:0.47485850985351474\n",
      "train loss:0.4360658912603138\n",
      "train loss:0.5454370754914727\n",
      "train loss:0.576284890897881\n",
      "train loss:0.44325752668414664\n",
      "train loss:0.4987110572578507\n",
      "train loss:0.35814533663909964\n",
      "train loss:0.4840233262509502\n",
      "train loss:0.6551831254987787\n",
      "train loss:0.20884415735097664\n",
      "train loss:0.8015534265557802\n",
      "train loss:0.485594516185708\n",
      "train loss:0.5290668148115063\n",
      "train loss:0.7402515115908094\n",
      "train loss:0.5699847844493267\n",
      "train loss:0.7193429335967987\n",
      "train loss:0.5992323119761993\n",
      "train loss:0.5241419229023302\n",
      "train loss:0.5232526792288879\n",
      "train loss:0.5946269884542937\n",
      "train loss:0.5232747041528947\n",
      "train loss:0.6225907940908897\n",
      "train loss:0.555473359687362\n",
      "train loss:0.41692837068238786\n",
      "train loss:0.7246635014930259\n",
      "train loss:0.6146104479868135\n",
      "train loss:0.5656357826273714\n",
      "train loss:0.4945695825259982\n",
      "train loss:0.4304479965777922\n",
      "train loss:0.7064446080557182\n",
      "train loss:0.2910992709717698\n",
      "train loss:0.3765195297534479\n",
      "train loss:0.8489672676056541\n",
      "train loss:0.46629107911449863\n",
      "train loss:0.6908898566622235\n",
      "train loss:0.6034211741741339\n",
      "train loss:0.7477874964705398\n",
      "train loss:0.49655905418618096\n",
      "train loss:0.6327931288550079\n",
      "train loss:0.5686311874800878\n",
      "train loss:0.2769091961385994\n",
      "train loss:0.6077269054164396\n",
      "train loss:0.7482348522234313\n",
      "=== epoch:9, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.4161475312188988\n",
      "train loss:0.5063537791372139\n",
      "train loss:0.6916724050631146\n",
      "train loss:0.6486132189951588\n",
      "train loss:0.5162084266279556\n",
      "train loss:0.5080395234727729\n",
      "train loss:0.6896462464987675\n",
      "train loss:0.4556152642389938\n",
      "train loss:0.5981534520471483\n",
      "train loss:0.46959150252499804\n",
      "train loss:0.5250741241976974\n",
      "train loss:0.7585969243854921\n",
      "train loss:0.6107834117070295\n",
      "train loss:0.6752875151273992\n",
      "train loss:0.6508104447656641\n",
      "train loss:0.6216098763042588\n",
      "train loss:0.5138732275250786\n",
      "train loss:0.5462611865609941\n",
      "train loss:0.5902427309125818\n",
      "train loss:0.4383419020143834\n",
      "train loss:0.4259958458692902\n",
      "train loss:0.7337722890654031\n",
      "train loss:0.4358893291173017\n",
      "train loss:0.6943755035024475\n",
      "train loss:0.39976843037671506\n",
      "train loss:0.6306150854162597\n",
      "train loss:0.40762995354650633\n",
      "train loss:0.4652037670085673\n",
      "train loss:0.5337016235401206\n",
      "train loss:0.3825217349303454\n",
      "train loss:0.6770977041714056\n",
      "train loss:0.40693278340194416\n",
      "train loss:0.5188942552717135\n",
      "train loss:0.665855420988497\n",
      "train loss:0.6161705419228419\n",
      "train loss:0.7419351747364705\n",
      "train loss:0.4382595572816362\n",
      "train loss:0.6530047298130647\n",
      "train loss:0.5773197200980205\n",
      "train loss:0.48316473933506143\n",
      "train loss:0.4996926598709722\n",
      "train loss:0.48606021099079655\n",
      "train loss:0.46824289189771395\n",
      "train loss:0.6017257694476814\n",
      "train loss:0.5083027148413193\n",
      "train loss:0.4864707959438287\n",
      "train loss:0.6155104857613277\n",
      "train loss:0.6661879758019957\n",
      "train loss:0.49256597972907723\n",
      "train loss:0.5321590760005364\n",
      "train loss:0.4680597770790115\n",
      "train loss:0.6012767916559669\n",
      "train loss:0.5439630987588615\n",
      "train loss:0.6094735794100634\n",
      "train loss:0.41403826951929795\n",
      "train loss:0.5540500371698894\n",
      "train loss:0.4746200456679044\n",
      "train loss:0.685647430965166\n",
      "train loss:0.5603390002502007\n",
      "train loss:0.24419460826194914\n",
      "train loss:0.4087650328062605\n",
      "train loss:0.98644962203641\n",
      "train loss:0.5301035446980358\n",
      "train loss:0.4993786941657682\n",
      "train loss:0.599530528387595\n",
      "train loss:0.675235706368237\n",
      "train loss:0.6663941174290863\n",
      "train loss:0.5792895652997669\n",
      "train loss:0.6017523335043758\n",
      "train loss:0.49359309707347415\n",
      "train loss:0.5666426667832031\n",
      "train loss:0.570162529813256\n",
      "train loss:0.6359847386600797\n",
      "train loss:0.576459232643199\n",
      "train loss:0.6446202981579382\n",
      "train loss:0.5472739466178373\n",
      "train loss:0.46309269701139416\n",
      "train loss:0.5464482875546668\n",
      "train loss:0.3446910164049942\n",
      "train loss:0.3966261178636241\n",
      "train loss:0.7349600163644656\n",
      "train loss:0.4282624710296309\n",
      "train loss:0.4525873186709125\n",
      "train loss:0.7544729532611074\n",
      "train loss:0.803356478207597\n",
      "train loss:0.45474580105211626\n",
      "train loss:0.5954699996347816\n",
      "train loss:0.5087508458149568\n",
      "train loss:0.5259399285679475\n",
      "train loss:0.6346625834347801\n",
      "train loss:0.7819302442273959\n",
      "train loss:0.8251327851623639\n",
      "train loss:0.5664918690515336\n",
      "train loss:0.5221095307671899\n",
      "train loss:0.5910656187602129\n",
      "train loss:0.520782967248129\n",
      "train loss:0.6139875959263058\n",
      "train loss:0.6534748131819169\n",
      "train loss:0.6383874722093689\n",
      "train loss:0.4956240783318414\n",
      "train loss:0.4868899664834313\n",
      "train loss:0.5426139574688659\n",
      "train loss:0.6730758749940364\n",
      "train loss:0.4372984241425926\n",
      "train loss:0.60842622676773\n",
      "train loss:0.531995272070191\n",
      "train loss:0.5359790558601006\n",
      "train loss:0.485104850570895\n",
      "train loss:0.5744956573600396\n",
      "train loss:0.7055549489410337\n",
      "train loss:0.5499684735996448\n",
      "train loss:0.4037900558021194\n",
      "train loss:0.6992877504592319\n",
      "train loss:0.48130516781618765\n",
      "train loss:0.6859137294553659\n",
      "train loss:0.35143414849234694\n",
      "train loss:0.7128175332760258\n",
      "train loss:0.5616802037222881\n",
      "train loss:0.489230658542385\n",
      "train loss:0.7101517368619902\n",
      "train loss:0.4930543795918732\n",
      "train loss:0.6132805855371648\n",
      "train loss:0.3356601068256717\n",
      "train loss:0.7835694077622597\n",
      "train loss:0.5004468189488059\n",
      "train loss:0.37836647496895603\n",
      "train loss:0.32748507802900917\n",
      "train loss:0.3847031474686212\n",
      "train loss:0.6305973790172973\n",
      "train loss:0.40343792042213994\n",
      "train loss:0.522255405943706\n",
      "train loss:0.6734586779088383\n",
      "train loss:0.7075892923573435\n",
      "train loss:0.45130071757749113\n",
      "train loss:0.6001325707160557\n",
      "train loss:0.2819060155113079\n",
      "train loss:0.6149374564796495\n",
      "train loss:0.6883057686510206\n",
      "train loss:0.4542893935793136\n",
      "train loss:0.4898649421666774\n",
      "train loss:0.3705259284668389\n",
      "train loss:0.19336840397854899\n",
      "train loss:0.470265127411571\n",
      "train loss:0.3907296002856929\n",
      "train loss:0.6027312804279809\n",
      "train loss:0.6862510215581394\n",
      "train loss:0.43957086823792446\n",
      "train loss:0.5929694887772831\n",
      "train loss:0.7326291319203733\n",
      "train loss:0.27712697563149413\n",
      "train loss:0.2931468678215919\n",
      "train loss:0.3580335900573482\n",
      "train loss:0.4850247699941068\n",
      "=== epoch:10, train acc:0.75, test acc:0.69 ===\n",
      "train loss:0.49674822649182315\n",
      "train loss:0.5151499056082833\n",
      "train loss:0.4880923537252255\n",
      "train loss:0.5997759710125387\n",
      "train loss:0.22535090299785435\n",
      "train loss:0.6036095007287527\n",
      "train loss:0.6352500361168534\n",
      "train loss:0.6278885395422338\n",
      "train loss:0.5218644148455074\n",
      "train loss:0.6193879711421826\n",
      "train loss:0.7954175909611501\n",
      "train loss:0.4946083554675216\n",
      "train loss:0.36253729482483016\n",
      "train loss:0.6594021621726914\n",
      "train loss:0.5552592533644665\n",
      "train loss:0.8865902092352034\n",
      "train loss:0.6257168701422917\n",
      "train loss:0.42735578192377366\n",
      "train loss:0.6107822317078522\n",
      "train loss:0.7207433337679223\n",
      "train loss:0.5835593355278652\n",
      "train loss:0.660290231062193\n",
      "train loss:0.4792478469929846\n",
      "train loss:0.5320744747100742\n",
      "train loss:0.4708506825915345\n",
      "train loss:0.6474044948897911\n",
      "train loss:0.37689125925955014\n",
      "train loss:0.5363348053285572\n",
      "train loss:0.49221719187798635\n",
      "train loss:0.6870832185403446\n",
      "train loss:0.3882705144199775\n",
      "train loss:0.5600622147336058\n",
      "train loss:0.528508943836845\n",
      "train loss:0.41603737094713233\n",
      "train loss:0.5080891395232844\n",
      "train loss:0.40550727544166537\n",
      "train loss:0.4789479918774404\n",
      "train loss:0.2961021079732569\n",
      "train loss:0.4830305615487642\n",
      "train loss:0.8559405033806202\n",
      "train loss:0.4967387187167249\n",
      "train loss:0.6027024832878153\n",
      "train loss:0.6224451340955395\n",
      "train loss:0.591093313232111\n",
      "train loss:0.6818777866954695\n",
      "train loss:0.5393158142319004\n",
      "train loss:0.8379931857017437\n",
      "train loss:0.6312112998549624\n",
      "train loss:0.5554889779043755\n",
      "train loss:0.6380281564309598\n",
      "train loss:0.6744929100214312\n",
      "train loss:0.573697925946828\n",
      "train loss:0.5380417458318759\n",
      "train loss:0.5016957977880845\n",
      "train loss:0.7899135770313152\n",
      "train loss:0.3809244598094391\n",
      "train loss:0.6538669796765995\n",
      "train loss:0.5946737242415707\n",
      "train loss:0.5984948200437847\n",
      "train loss:0.6089279059531438\n",
      "train loss:0.6445977862582446\n",
      "train loss:0.5768088983970504\n",
      "train loss:0.533008261017803\n",
      "train loss:0.4925198221053389\n",
      "train loss:0.6720450239249244\n",
      "train loss:0.6065417745837077\n",
      "train loss:0.6444432317257992\n",
      "train loss:0.6928768202241118\n",
      "train loss:0.5830113278142479\n",
      "train loss:0.5835812983367434\n",
      "train loss:0.6152981713339389\n",
      "train loss:0.594290891884205\n",
      "train loss:0.3867485379887639\n",
      "train loss:0.8705389306523387\n",
      "train loss:0.6473241177930265\n",
      "train loss:0.6769992515820508\n",
      "train loss:0.6339572581909854\n",
      "train loss:0.6164602991084942\n",
      "train loss:0.5269968511146897\n",
      "train loss:0.7044753089862028\n",
      "train loss:0.5460478604024894\n",
      "train loss:0.4678370299171871\n",
      "train loss:0.6998106651883106\n",
      "train loss:0.7607535860826576\n",
      "train loss:0.5469516499337893\n",
      "train loss:0.7280148609624957\n",
      "train loss:0.48532661750689454\n",
      "train loss:0.6059708950166429\n",
      "train loss:0.4165641828243201\n",
      "train loss:0.6476085192143917\n",
      "train loss:0.5487017802570751\n",
      "train loss:0.6284599609242034\n",
      "train loss:0.49847750626800735\n",
      "train loss:0.3851708259633065\n",
      "train loss:0.3754077115222045\n",
      "train loss:0.40215849126998016\n",
      "train loss:0.5311329815081167\n",
      "train loss:0.6223628115808104\n",
      "train loss:0.3902076818720248\n",
      "train loss:0.5442868858741184\n",
      "train loss:0.509480077096246\n",
      "train loss:0.7255938227661971\n",
      "train loss:0.5550336082385213\n",
      "train loss:0.36033689243836625\n",
      "train loss:0.6936264027007987\n",
      "train loss:0.33353862522290834\n",
      "train loss:0.5827414928330252\n",
      "train loss:0.77260311962798\n",
      "train loss:0.5407834917299661\n",
      "train loss:0.6372889837549834\n",
      "train loss:0.6430937007429989\n",
      "train loss:0.5562060086518918\n",
      "train loss:0.4796925735404961\n",
      "train loss:0.4884834109529563\n",
      "train loss:0.6120159183410049\n",
      "train loss:0.5134390934487691\n",
      "train loss:0.6693036249817421\n",
      "train loss:0.6649354764927699\n",
      "train loss:0.6132646986804422\n",
      "train loss:0.6995403505634945\n",
      "train loss:0.6735148245230118\n",
      "train loss:0.41791982323496696\n",
      "train loss:0.572277748118489\n",
      "train loss:0.5707136859533144\n",
      "train loss:0.584398013339384\n",
      "train loss:0.7343659548117071\n",
      "train loss:0.6426638437015744\n",
      "train loss:0.6845543436238419\n",
      "train loss:0.608384504947919\n",
      "train loss:0.6079703495622902\n",
      "train loss:0.46044299014421775\n",
      "train loss:0.7322553678829103\n",
      "train loss:0.4846570239195268\n",
      "train loss:0.4660330202794604\n",
      "train loss:0.454619562008802\n",
      "train loss:0.5903367132725612\n",
      "train loss:0.6606097826692063\n",
      "train loss:0.5254267874998486\n",
      "train loss:0.451844770413868\n",
      "train loss:0.5740927736405321\n",
      "train loss:0.4620253188960307\n",
      "train loss:0.5733225595057064\n",
      "train loss:0.6398627698511591\n",
      "train loss:0.5576825418321166\n",
      "train loss:0.3892904634834845\n",
      "train loss:0.7800055617418895\n",
      "train loss:0.5860353715840083\n",
      "train loss:0.20759432780454282\n",
      "train loss:0.7725112525992901\n",
      "train loss:0.4071623238511455\n",
      "train loss:0.5203474907776003\n",
      "train loss:0.8864589332742216\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5490196078431373\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 70, 'filter_size': 7, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86f8c7e2-dc7f-427e-acf0-3649102c33c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6924779943161419\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6926909914378856\n",
      "train loss:0.6855293725598582\n",
      "train loss:0.6715346159879181\n",
      "train loss:0.6870807108399647\n",
      "train loss:0.6799992836912911\n",
      "train loss:0.6835026797128686\n",
      "train loss:0.6146721804176252\n",
      "train loss:0.47733728410253545\n",
      "train loss:0.5366248140884652\n",
      "train loss:0.6347613570054921\n",
      "train loss:0.5332917612792856\n",
      "train loss:0.4748212410779208\n",
      "train loss:0.7113705690544088\n",
      "train loss:0.9457437921535401\n",
      "train loss:0.3246243326625943\n",
      "train loss:0.6232017580181923\n",
      "train loss:0.6214753345509948\n",
      "train loss:0.8313905261094948\n",
      "train loss:0.7299656958797537\n",
      "train loss:0.5118277536857944\n",
      "train loss:0.5155842789833246\n",
      "train loss:0.6912544399953968\n",
      "train loss:0.6272446658556643\n",
      "train loss:0.6357370783327737\n",
      "train loss:0.6302118909663544\n",
      "train loss:0.6124971604271378\n",
      "train loss:0.5951785901076458\n",
      "train loss:0.6424014961105735\n",
      "train loss:0.5966199286452678\n",
      "train loss:0.6665045889607091\n",
      "train loss:0.6310947789361199\n",
      "train loss:0.5691271209878097\n",
      "train loss:0.6752519949396849\n",
      "train loss:0.62016575878871\n",
      "train loss:0.5615514478639885\n",
      "train loss:0.5150330361000257\n",
      "train loss:0.5357641961021069\n",
      "train loss:0.5139412779904846\n",
      "train loss:0.5232206081613818\n",
      "train loss:0.4873368680290316\n",
      "train loss:1.046112177150026\n",
      "train loss:0.4742167352986619\n",
      "train loss:0.9430845916179578\n",
      "train loss:0.6128453244116644\n",
      "train loss:0.612779431154727\n",
      "train loss:0.6060723576472639\n",
      "train loss:0.5339765992317995\n",
      "train loss:0.6034858581646982\n",
      "train loss:0.5513582637686902\n",
      "train loss:0.5554015852671256\n",
      "train loss:0.7733982195323561\n",
      "train loss:0.6812515702670348\n",
      "train loss:0.7376954930194759\n",
      "train loss:0.49734828821354127\n",
      "train loss:0.6756365310791584\n",
      "train loss:0.6213481416176864\n",
      "train loss:0.6242763970794118\n",
      "train loss:0.7246123678589295\n",
      "train loss:0.7144627165905161\n",
      "train loss:0.6256766563088072\n",
      "train loss:0.4654741133017863\n",
      "train loss:0.7861373749748859\n",
      "train loss:0.6274095535624349\n",
      "train loss:0.7752319193040875\n",
      "train loss:0.6747249138411374\n",
      "train loss:0.6659427311386714\n",
      "train loss:0.6805361357575399\n",
      "train loss:0.670215592680035\n",
      "train loss:0.5396302923944771\n",
      "train loss:0.680006422305685\n",
      "train loss:0.4868424049990573\n",
      "train loss:0.6314961779988054\n",
      "train loss:0.6151308195796775\n",
      "train loss:0.598748606754079\n",
      "train loss:0.6675383108577306\n",
      "train loss:0.39116790258057527\n",
      "train loss:0.5142567688725308\n",
      "train loss:0.7087739645620312\n",
      "train loss:0.7367847040178835\n",
      "train loss:0.40742507457612076\n",
      "train loss:0.6390405195652739\n",
      "train loss:0.6440303068900406\n",
      "train loss:0.8381650047600708\n",
      "train loss:0.8842608662547656\n",
      "train loss:0.6656230155002052\n",
      "train loss:0.3699088813955048\n",
      "train loss:0.6149184368744598\n",
      "train loss:0.5316168357202842\n",
      "train loss:0.6116720592815832\n",
      "train loss:0.6896751943219572\n",
      "train loss:0.45879767998068843\n",
      "train loss:0.6382686061720999\n",
      "train loss:0.5957930942683634\n",
      "train loss:0.5466397595264821\n",
      "train loss:0.6754776779589885\n",
      "train loss:0.6742805038561914\n",
      "train loss:0.5887205635079791\n",
      "train loss:0.6157626124997727\n",
      "train loss:0.5331064227759866\n",
      "train loss:0.5170249038633108\n",
      "train loss:0.612752806002999\n",
      "train loss:0.495511938635614\n",
      "train loss:0.4871276208589478\n",
      "train loss:0.5174584533014742\n",
      "train loss:0.6265163259650315\n",
      "train loss:0.4036411719577213\n",
      "train loss:0.6137451815147622\n",
      "train loss:0.48681506177404615\n",
      "train loss:0.6497718255117502\n",
      "train loss:0.6517382651064155\n",
      "train loss:0.5225484734487116\n",
      "train loss:0.641738450802099\n",
      "train loss:0.7380856834930551\n",
      "train loss:0.8881295209343933\n",
      "train loss:0.5151791663436659\n",
      "train loss:0.7428436537577972\n",
      "train loss:0.6136497028030066\n",
      "train loss:0.6237582860597469\n",
      "train loss:0.5846380882057531\n",
      "train loss:0.7178925665419045\n",
      "train loss:0.6687542803024213\n",
      "train loss:0.6366141888411508\n",
      "train loss:0.5749420391020397\n",
      "train loss:0.5387986343155744\n",
      "train loss:0.6640897439665792\n",
      "train loss:0.6302432590999245\n",
      "train loss:0.6687407212385174\n",
      "train loss:0.5907776435958787\n",
      "train loss:0.6906729845169176\n",
      "train loss:0.7174168561437331\n",
      "train loss:0.5319241993134194\n",
      "train loss:0.5651951930075656\n",
      "train loss:0.6124383245863886\n",
      "train loss:0.6833895370960712\n",
      "train loss:0.6870359744629699\n",
      "train loss:0.4581272280689349\n",
      "train loss:0.5915569090359647\n",
      "train loss:0.5397279694548222\n",
      "train loss:0.6247772105236582\n",
      "train loss:0.7343324874635703\n",
      "train loss:0.6915150321227915\n",
      "train loss:0.620890568781939\n",
      "train loss:0.6995571225425589\n",
      "train loss:0.6353185061551958\n",
      "train loss:0.7153997533411192\n",
      "train loss:0.4572824625187373\n",
      "train loss:0.6126366076001006\n",
      "train loss:0.6956392305554889\n",
      "train loss:0.7513303650991612\n",
      "train loss:0.7442340315274858\n",
      "train loss:0.5624673453338253\n",
      "train loss:0.6175530739638161\n",
      "train loss:0.5041032361672206\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6188796619423977\n",
      "train loss:0.5633386471025876\n",
      "train loss:0.6757741724430388\n",
      "train loss:0.6835954025459763\n",
      "train loss:0.48992354907268953\n",
      "train loss:0.5432373220181572\n",
      "train loss:0.5313589328515266\n",
      "train loss:0.5222917574899281\n",
      "train loss:0.7144609227987829\n",
      "train loss:0.6377930175678935\n",
      "train loss:0.6025469721608628\n",
      "train loss:0.710722985787145\n",
      "train loss:0.7077419710564368\n",
      "train loss:0.30292414896853287\n",
      "train loss:0.7119646784595084\n",
      "train loss:0.6171531424559554\n",
      "train loss:0.6256408967365588\n",
      "train loss:0.5224043562688944\n",
      "train loss:0.5437669735185324\n",
      "train loss:0.7760018205462047\n",
      "train loss:0.5462418673327424\n",
      "train loss:0.7747885883133887\n",
      "train loss:0.7453218331610509\n",
      "train loss:0.7520325438432451\n",
      "train loss:0.5057209005127145\n",
      "train loss:0.5637071988171093\n",
      "train loss:0.6225979738637819\n",
      "train loss:0.5093907815190173\n",
      "train loss:0.5562927404011029\n",
      "train loss:0.42078575878006336\n",
      "train loss:0.6214333235632908\n",
      "train loss:0.5327215616208566\n",
      "train loss:0.6102198065026426\n",
      "train loss:0.49981620349391687\n",
      "train loss:0.6357698170610259\n",
      "train loss:0.4915468869369201\n",
      "train loss:0.8673165796191051\n",
      "train loss:0.49171959198260096\n",
      "train loss:0.8383825404406794\n",
      "train loss:0.7247375463504805\n",
      "train loss:0.6246342368045923\n",
      "train loss:0.8010149508351014\n",
      "train loss:0.6945615957085589\n",
      "train loss:0.7428233711538935\n",
      "train loss:0.6287059770744101\n",
      "train loss:0.6617028035278104\n",
      "train loss:0.6637934162551938\n",
      "train loss:0.6392267905175579\n",
      "train loss:0.6670207375694865\n",
      "train loss:0.6454296340840495\n",
      "train loss:0.6167574808836158\n",
      "train loss:0.6407568674421118\n",
      "train loss:0.6399271006909999\n",
      "train loss:0.641869302507267\n",
      "train loss:0.6452345746324447\n",
      "train loss:0.6989361071769576\n",
      "train loss:0.6027447457064843\n",
      "train loss:0.6041208783602391\n",
      "train loss:0.6747306298300166\n",
      "train loss:0.7028909034794045\n",
      "train loss:0.6289661981087338\n",
      "train loss:0.6747350979338947\n",
      "train loss:0.6663262001377823\n",
      "train loss:0.6741986432002989\n",
      "train loss:0.6175508034872722\n",
      "train loss:0.565867292759691\n",
      "train loss:0.6709713212106048\n",
      "train loss:0.671206212019668\n",
      "train loss:0.6767183564071715\n",
      "train loss:0.619991256939882\n",
      "train loss:0.7426567847347623\n",
      "train loss:0.5489269923904334\n",
      "train loss:0.6770306350217259\n",
      "train loss:0.5286625559229111\n",
      "train loss:0.6767130202328911\n",
      "train loss:0.5964029966703023\n",
      "train loss:0.5338250761737905\n",
      "train loss:0.7578571386836547\n",
      "train loss:0.6144480459057722\n",
      "train loss:0.7127967249165488\n",
      "train loss:0.6798435653527835\n",
      "train loss:0.6893825825312788\n",
      "train loss:0.555525570393961\n",
      "train loss:0.608196019508949\n",
      "train loss:0.39444175426777023\n",
      "train loss:0.5273143927031396\n",
      "train loss:0.7960065949751229\n",
      "train loss:0.6586344664989806\n",
      "train loss:0.6252080359862899\n",
      "train loss:0.6052856390482806\n",
      "train loss:0.44036783980566385\n",
      "train loss:0.43852290411171035\n",
      "train loss:0.7129935364937078\n",
      "train loss:0.4216801661408728\n",
      "train loss:0.5982915051534629\n",
      "train loss:0.8544148571855608\n",
      "train loss:0.729487195459558\n",
      "train loss:0.5008622574943594\n",
      "train loss:0.5162359472245852\n",
      "train loss:0.6086690496739959\n",
      "train loss:0.5304458349034029\n",
      "train loss:0.628743436227432\n",
      "train loss:0.41692061734232383\n",
      "train loss:0.5133733405738704\n",
      "train loss:0.4938770208773352\n",
      "train loss:0.38312911809845285\n",
      "train loss:0.5857324168272857\n",
      "train loss:0.881664575646556\n",
      "train loss:0.7228889227042617\n",
      "train loss:0.7065970297757816\n",
      "train loss:0.8161286129449271\n",
      "train loss:0.7548378704778154\n",
      "train loss:0.6895122671079773\n",
      "train loss:0.6278496419281253\n",
      "train loss:0.6701337890658897\n",
      "train loss:0.586262686902251\n",
      "train loss:0.5940057384575412\n",
      "train loss:0.5886019158617571\n",
      "train loss:0.6412681434444762\n",
      "train loss:0.6979482104344206\n",
      "train loss:0.5847110617080362\n",
      "train loss:0.6239162577810223\n",
      "train loss:0.7552570137922866\n",
      "train loss:0.5383471494645937\n",
      "train loss:0.5336823196547374\n",
      "train loss:0.6260491515087246\n",
      "train loss:0.6189975839800703\n",
      "train loss:0.5474527254866288\n",
      "train loss:0.6800456360500426\n",
      "train loss:0.8804355929005279\n",
      "train loss:0.7474956069892374\n",
      "train loss:0.4723572808780963\n",
      "train loss:0.6742698808432237\n",
      "train loss:0.6190162510065289\n",
      "train loss:0.6887831555855675\n",
      "train loss:0.4702754063880391\n",
      "train loss:0.5388403158728152\n",
      "train loss:0.6732229810845105\n",
      "train loss:0.45715090826473953\n",
      "train loss:0.4128259053338167\n",
      "train loss:0.7340215551956788\n",
      "train loss:0.28601187737549594\n",
      "train loss:0.7236329469079912\n",
      "train loss:0.862175982983225\n",
      "train loss:0.35542486523116257\n",
      "train loss:0.9406190021963399\n",
      "train loss:0.5060953705161478\n",
      "train loss:0.5181469110824757\n",
      "train loss:0.6056964038052745\n",
      "train loss:0.5189338561850295\n",
      "train loss:0.5112572505554378\n",
      "train loss:0.5102272626931146\n",
      "train loss:0.6127792846234712\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6360704944197348\n",
      "train loss:0.40787151696023943\n",
      "train loss:0.530644562182236\n",
      "train loss:0.7229103844909097\n",
      "train loss:0.3942825910182124\n",
      "train loss:0.893160606484438\n",
      "train loss:0.7267334357133542\n",
      "train loss:0.5039745506638098\n",
      "train loss:0.48831564322836807\n",
      "train loss:0.42132090703026603\n",
      "train loss:0.6295267139281495\n",
      "train loss:0.4227510061151187\n",
      "train loss:0.6217410380551949\n",
      "train loss:0.8852873997741065\n",
      "train loss:0.6441671752673845\n",
      "train loss:0.6037671600168162\n",
      "train loss:0.8796721681417514\n",
      "train loss:0.602764075329385\n",
      "train loss:0.5537221537370961\n",
      "train loss:0.6142507017851093\n",
      "train loss:0.6740518677362778\n",
      "train loss:0.5815225686271441\n",
      "train loss:0.7086921595107514\n",
      "train loss:0.647417862872725\n",
      "train loss:0.6672888610556811\n",
      "train loss:0.6225745735566885\n",
      "train loss:0.5947111700689736\n",
      "train loss:0.7212938272827059\n",
      "train loss:0.69562307618744\n",
      "train loss:0.5234978620359738\n",
      "train loss:0.5394075343707254\n",
      "train loss:0.5719183979632803\n",
      "train loss:0.4830818417463488\n",
      "train loss:0.52405847892894\n",
      "train loss:0.4135954017771092\n",
      "train loss:0.5116311023956579\n",
      "train loss:0.4008755059631959\n",
      "train loss:0.7565304676960347\n",
      "train loss:0.6020952096785026\n",
      "train loss:0.49936708498154125\n",
      "train loss:0.7625318855414258\n",
      "train loss:0.4937275807008472\n",
      "train loss:0.49536210685329085\n",
      "train loss:0.4762808396848054\n",
      "train loss:0.34661683770558127\n",
      "train loss:0.4924201426776011\n",
      "train loss:0.9237002437210051\n",
      "train loss:0.6383871176597606\n",
      "train loss:0.5547699233820558\n",
      "train loss:0.6085373367360349\n",
      "train loss:0.5812590062311245\n",
      "train loss:0.5380345370847388\n",
      "train loss:0.5248526331048017\n",
      "train loss:0.688547478304782\n",
      "train loss:0.5487518810416688\n",
      "train loss:0.5560172234548558\n",
      "train loss:0.723763577744689\n",
      "train loss:0.5462265350609892\n",
      "train loss:0.7433755114574574\n",
      "train loss:0.6057910155918674\n",
      "train loss:0.46540998539423073\n",
      "train loss:0.5203741682453842\n",
      "train loss:0.6205853259314271\n",
      "train loss:0.5318085506574974\n",
      "train loss:0.6229968635480585\n",
      "train loss:0.5575983580568857\n",
      "train loss:0.5093988044392534\n",
      "train loss:0.5491416301433745\n",
      "train loss:0.27139507732905377\n",
      "train loss:0.7921629007534017\n",
      "train loss:0.48869797054185626\n",
      "train loss:0.7330833017775271\n",
      "train loss:0.6296104816537282\n",
      "train loss:0.5671585328430877\n",
      "train loss:0.4782393379478971\n",
      "train loss:0.8458952770507014\n",
      "train loss:0.5941689913664194\n",
      "train loss:0.49512350477183337\n",
      "train loss:0.6249665129712735\n",
      "train loss:0.6063769499030799\n",
      "train loss:0.5228738068984153\n",
      "train loss:0.6903549444513113\n",
      "train loss:0.6877281336825783\n",
      "train loss:0.45797249286963915\n",
      "train loss:0.656368865917995\n",
      "train loss:0.5271330796667237\n",
      "train loss:0.7549780649453868\n",
      "train loss:0.6845131489730268\n",
      "train loss:0.7487342484712298\n",
      "train loss:0.5963589332738379\n",
      "train loss:0.49321772770249084\n",
      "train loss:0.5505668947699961\n",
      "train loss:0.6675547124373795\n",
      "train loss:0.608825174805791\n",
      "train loss:0.6013984144757386\n",
      "train loss:0.6717637915663013\n",
      "train loss:0.5878938571762167\n",
      "train loss:0.5237835695555989\n",
      "train loss:0.7075247508417019\n",
      "train loss:0.4551672485286583\n",
      "train loss:0.5207632543939372\n",
      "train loss:0.7638071481076295\n",
      "train loss:0.864317937359312\n",
      "train loss:0.6873916270032536\n",
      "train loss:0.5268034296190746\n",
      "train loss:0.5134250924199092\n",
      "train loss:0.6877655425392417\n",
      "train loss:0.6054751204849811\n",
      "train loss:0.53835627510444\n",
      "train loss:0.3524607352632731\n",
      "train loss:0.6186193922529256\n",
      "train loss:0.5332621054902555\n",
      "train loss:0.5052972795736045\n",
      "train loss:0.47229994500835676\n",
      "train loss:0.4010064844486109\n",
      "train loss:0.6407512421169856\n",
      "train loss:0.6166678151245775\n",
      "train loss:0.5110477332676895\n",
      "train loss:0.3263726756857997\n",
      "train loss:0.45542010076965883\n",
      "train loss:0.6658597880019179\n",
      "train loss:0.2820636414041111\n",
      "train loss:0.6715318483928009\n",
      "train loss:0.6237007450522905\n",
      "train loss:0.6513224423846816\n",
      "train loss:0.48892129936852635\n",
      "train loss:1.1020766467136929\n",
      "train loss:0.504729355981872\n",
      "train loss:0.5364573939122227\n",
      "train loss:0.8054433167145654\n",
      "train loss:0.4234887591020652\n",
      "train loss:0.5606081906920215\n",
      "train loss:0.6668281302140892\n",
      "train loss:0.6611636702254194\n",
      "train loss:0.6295872979246173\n",
      "train loss:0.6253509368990329\n",
      "train loss:0.6865882135910505\n",
      "train loss:0.6117959595071681\n",
      "train loss:0.6793909931086721\n",
      "train loss:0.6277059981636121\n",
      "train loss:0.635834951757644\n",
      "train loss:0.562808450135577\n",
      "train loss:0.5318076599268907\n",
      "train loss:0.6087656009529718\n",
      "train loss:0.7123304859185036\n",
      "train loss:0.6238973400070154\n",
      "train loss:0.6576739020454841\n",
      "train loss:0.6349834018915403\n",
      "train loss:0.651934307410987\n",
      "train loss:0.6761139969059513\n",
      "train loss:0.7490461131897526\n",
      "train loss:0.6527230913083641\n",
      "train loss:0.5720699279711539\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.604035305788384\n",
      "train loss:0.5512900822329513\n",
      "train loss:0.4696032236599764\n",
      "train loss:0.6336656661042763\n",
      "train loss:0.6097366527543185\n",
      "train loss:0.5086125048129805\n",
      "train loss:0.8024093872612321\n",
      "train loss:0.6193540622572218\n",
      "train loss:0.5558493319686353\n",
      "train loss:0.38811578443733896\n",
      "train loss:0.4860151641870106\n",
      "train loss:0.630482240841446\n",
      "train loss:0.7321811121460056\n",
      "train loss:0.3783795003813501\n",
      "train loss:0.5074968417039833\n",
      "train loss:0.6418586795204635\n",
      "train loss:0.7409189160786593\n",
      "train loss:0.5147138886282006\n",
      "train loss:0.8252983410558945\n",
      "train loss:0.6364935363502998\n",
      "train loss:0.7473936765947584\n",
      "train loss:0.5046856520290487\n",
      "train loss:0.685354336979158\n",
      "train loss:0.6313591084256989\n",
      "train loss:0.5449252738851843\n",
      "train loss:0.5459980410139281\n",
      "train loss:0.7046896983198476\n",
      "train loss:0.5914504101037141\n",
      "train loss:0.46440332503656745\n",
      "train loss:0.5520944103580361\n",
      "train loss:0.563718537346432\n",
      "train loss:0.4934418706737168\n",
      "train loss:0.4444825438514967\n",
      "train loss:0.8881036945741488\n",
      "train loss:0.4135302656279558\n",
      "train loss:0.5251363661987367\n",
      "train loss:0.6788366131594861\n",
      "train loss:0.4973731551251574\n",
      "train loss:0.7393093544909932\n",
      "train loss:0.621443090892334\n",
      "train loss:0.9362981979212937\n",
      "train loss:0.5356527463652757\n",
      "train loss:0.5097835000477112\n",
      "train loss:0.6710956142807257\n",
      "train loss:0.7037476083905415\n",
      "train loss:0.6117428895367378\n",
      "train loss:0.5975741604288789\n",
      "train loss:0.5010056154671126\n",
      "train loss:0.5400950271081003\n",
      "train loss:0.3915794944966176\n",
      "train loss:0.49341408151169475\n",
      "train loss:0.6494188283164066\n",
      "train loss:0.6756274221464651\n",
      "train loss:0.7356664561259671\n",
      "train loss:0.712440985013272\n",
      "train loss:0.8461115863205843\n",
      "train loss:0.4506343824382736\n",
      "train loss:0.7202465661565165\n",
      "train loss:0.7352220024878964\n",
      "train loss:0.5487403614397948\n",
      "train loss:0.6165172297743663\n",
      "train loss:0.6186084603962158\n",
      "train loss:0.6444118725569942\n",
      "train loss:0.6254237441725503\n",
      "train loss:0.6420112792581425\n",
      "train loss:0.6288592945019371\n",
      "train loss:0.4666438169090523\n",
      "train loss:0.663878568245803\n",
      "train loss:0.45612557594560094\n",
      "train loss:0.4956878458197723\n",
      "train loss:0.4688543441119803\n",
      "train loss:0.389208434836502\n",
      "train loss:0.5155742738175726\n",
      "train loss:0.6524563130901593\n",
      "train loss:0.7840341708269806\n",
      "train loss:0.7288456570067081\n",
      "train loss:0.6139495012862984\n",
      "train loss:0.4695728931214195\n",
      "train loss:0.5403492532085012\n",
      "train loss:0.5031603452427923\n",
      "train loss:0.37501990992674694\n",
      "train loss:0.6223483775034899\n",
      "train loss:0.5078829136545823\n",
      "train loss:0.6873062298252914\n",
      "train loss:0.7563183700694436\n",
      "train loss:0.8279213501197514\n",
      "train loss:0.7040658428918387\n",
      "train loss:0.5049447290569136\n",
      "train loss:0.44394820917293726\n",
      "train loss:0.7707241540935341\n",
      "train loss:0.6886962089910665\n",
      "train loss:0.6480261092494358\n",
      "train loss:0.6446960489198181\n",
      "train loss:0.6071189758152811\n",
      "train loss:0.600439626471909\n",
      "train loss:0.5744294578731286\n",
      "train loss:0.5806206231209037\n",
      "train loss:0.7389286134225876\n",
      "train loss:0.6334724512228084\n",
      "train loss:0.6806429328057195\n",
      "train loss:0.6849037953885964\n",
      "train loss:0.6075611353685388\n",
      "train loss:0.46359403952532724\n",
      "train loss:0.4173015597417275\n",
      "train loss:0.5942088297559498\n",
      "train loss:0.4888256954150224\n",
      "train loss:0.7122987938826443\n",
      "train loss:0.35159264189201844\n",
      "train loss:0.3167914433625343\n",
      "train loss:0.5014448120835642\n",
      "train loss:0.6361149359086504\n",
      "train loss:0.6802524708639016\n",
      "train loss:0.5158436849791657\n",
      "train loss:0.7718790968305075\n",
      "train loss:0.6570721223394951\n",
      "train loss:0.3959801294394857\n",
      "train loss:0.7495570589645514\n",
      "train loss:0.6360197698356317\n",
      "train loss:0.6629969319261291\n",
      "train loss:0.5811470643930801\n",
      "train loss:0.48242064654607353\n",
      "train loss:0.5935921388043083\n",
      "train loss:0.42023896155385876\n",
      "train loss:0.6523966243570947\n",
      "train loss:0.6632541208088545\n",
      "train loss:0.551844486411748\n",
      "train loss:0.5716314021057112\n",
      "train loss:0.6797756024581084\n",
      "train loss:0.6286894559802527\n",
      "train loss:0.6842483381245991\n",
      "train loss:0.49134748796585886\n",
      "train loss:0.39802506809173527\n",
      "train loss:0.6226404378380321\n",
      "train loss:0.672672872617171\n",
      "train loss:0.7421126627330323\n",
      "train loss:0.6089350066523542\n",
      "train loss:0.4920589272608299\n",
      "train loss:0.771283876058138\n",
      "train loss:0.7266888628910564\n",
      "train loss:0.49572668782133766\n",
      "train loss:0.48611129657152297\n",
      "train loss:0.5228307793826299\n",
      "train loss:0.41082048632309176\n",
      "train loss:0.6698526602558694\n",
      "train loss:0.40344328064046053\n",
      "train loss:0.6426340069197657\n",
      "train loss:0.5815882033533207\n",
      "train loss:0.7246053030912768\n",
      "train loss:0.7597090832267237\n",
      "train loss:0.5473961773786905\n",
      "train loss:0.3117827122362725\n",
      "train loss:0.4422496712796214\n",
      "train loss:0.7279984043078584\n",
      "=== epoch:5, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5935303376895127\n",
      "train loss:0.9573123988011798\n",
      "train loss:0.570594820363165\n",
      "train loss:0.6158389544528668\n",
      "train loss:0.5904458189212908\n",
      "train loss:0.5607523879807143\n",
      "train loss:0.6851535917365524\n",
      "train loss:0.7112066415324795\n",
      "train loss:0.7449199470798675\n",
      "train loss:0.48713240621091014\n",
      "train loss:0.6307319986059129\n",
      "train loss:0.5978063522109202\n",
      "train loss:0.653467248728376\n",
      "train loss:0.4732947231399699\n",
      "train loss:0.7163323597081772\n",
      "train loss:0.5704476983180411\n",
      "train loss:0.6776086561364043\n",
      "train loss:0.7367865096096801\n",
      "train loss:0.4566113670044388\n",
      "train loss:0.48263026358613503\n",
      "train loss:0.6217492447675433\n",
      "train loss:0.47466060837131135\n",
      "train loss:0.574488475089751\n",
      "train loss:0.6318606793131194\n",
      "train loss:0.7309247761938338\n",
      "train loss:0.8268001600942905\n",
      "train loss:0.5905860691866084\n",
      "train loss:0.4416248021122529\n",
      "train loss:0.6523478201450499\n",
      "train loss:0.6155933060234863\n",
      "train loss:0.6145450368106222\n",
      "train loss:0.6072416843833048\n",
      "train loss:0.6967109458059617\n",
      "train loss:0.4132071194082815\n",
      "train loss:0.6877411976817684\n",
      "train loss:0.5028747222823702\n",
      "train loss:0.43213278471992334\n",
      "train loss:0.5805969670996909\n",
      "train loss:0.5423132670143368\n",
      "train loss:0.5662764478661242\n",
      "train loss:0.38162369175316047\n",
      "train loss:0.9706085246551\n",
      "train loss:0.42287036350823753\n",
      "train loss:0.7051357889972197\n",
      "train loss:0.5949289961185104\n",
      "train loss:0.5079048351419957\n",
      "train loss:0.8660752151503859\n",
      "train loss:0.4101601166409\n",
      "train loss:0.5327670141925236\n",
      "train loss:0.647459105683505\n",
      "train loss:0.4822244319477285\n",
      "train loss:0.5307634104667709\n",
      "train loss:0.7379524484552885\n",
      "train loss:0.6255663398893816\n",
      "train loss:0.451638254995718\n",
      "train loss:0.7846077646127358\n",
      "train loss:0.6755419579353367\n",
      "train loss:0.578781223130167\n",
      "train loss:0.5906376147151888\n",
      "train loss:0.596134614515813\n",
      "train loss:0.42014997865123077\n",
      "train loss:0.6192297323483527\n",
      "train loss:0.381562024137806\n",
      "train loss:0.6021393159105969\n",
      "train loss:0.5623993542906782\n",
      "train loss:0.3293563826559336\n",
      "train loss:0.41629545375870247\n",
      "train loss:0.48999720849188\n",
      "train loss:0.6385952000712912\n",
      "train loss:0.24666962787333974\n",
      "train loss:0.555239250475582\n",
      "train loss:0.2821312677886646\n",
      "train loss:0.7267581510468627\n",
      "train loss:0.6100520187171663\n",
      "train loss:0.5270043601038317\n",
      "train loss:1.0017713834797481\n",
      "train loss:0.44930920203636643\n",
      "train loss:0.49754533070319296\n",
      "train loss:0.42247240570845024\n",
      "train loss:0.5529064992901385\n",
      "train loss:0.499996803338577\n",
      "train loss:0.6324354966580535\n",
      "train loss:0.5783768901900176\n",
      "train loss:0.5014999468531354\n",
      "train loss:0.6288051948261233\n",
      "train loss:0.4241838870372745\n",
      "train loss:0.45209500257242385\n",
      "train loss:0.6867497652731466\n",
      "train loss:0.5320489132598175\n",
      "train loss:0.7780347936742958\n",
      "train loss:0.5034346759292331\n",
      "train loss:0.48960891886111657\n",
      "train loss:0.2948385074856835\n",
      "train loss:0.39621427470848614\n",
      "train loss:0.9307317923977478\n",
      "train loss:0.5755081063366515\n",
      "train loss:0.6281897444016511\n",
      "train loss:0.28808698597235416\n",
      "train loss:0.2866570124660637\n",
      "train loss:0.813213010309982\n",
      "train loss:0.7077668498599478\n",
      "train loss:0.7380170727946277\n",
      "train loss:0.6245210914654965\n",
      "train loss:0.8778568444849011\n",
      "train loss:0.3971972579685857\n",
      "train loss:0.5450110323252614\n",
      "train loss:0.5847306987806088\n",
      "train loss:0.49485873911313066\n",
      "train loss:0.5157359431908987\n",
      "train loss:0.6898362906372659\n",
      "train loss:0.564520554634864\n",
      "train loss:0.5194642241951604\n",
      "train loss:0.6080541221199193\n",
      "train loss:0.775903526092736\n",
      "train loss:0.6140728521448371\n",
      "train loss:0.5431374343566187\n",
      "train loss:0.6576535138696626\n",
      "train loss:0.808828207482349\n",
      "train loss:0.5625641315967461\n",
      "train loss:0.5257660599519732\n",
      "train loss:0.748803654345257\n",
      "train loss:0.702030938460737\n",
      "train loss:0.6323681143897428\n",
      "train loss:0.5592744156682479\n",
      "train loss:0.6403244481852799\n",
      "train loss:0.6223323742786617\n",
      "train loss:0.5354223939480922\n",
      "train loss:0.6807721657832293\n",
      "train loss:0.5893872449893905\n",
      "train loss:0.7116390919097927\n",
      "train loss:0.497291704006917\n",
      "train loss:0.5624756937101947\n",
      "train loss:0.4074152544547348\n",
      "train loss:0.6874418112313594\n",
      "train loss:0.5555762078636591\n",
      "train loss:0.5460859578599787\n",
      "train loss:0.4581489906336267\n",
      "train loss:0.45884430168833423\n",
      "train loss:0.3517815911033756\n",
      "train loss:0.6442924479581965\n",
      "train loss:0.42028650335274476\n",
      "train loss:0.5957755015019776\n",
      "train loss:0.7264753718631154\n",
      "train loss:0.623983497341334\n",
      "train loss:0.3820301900982961\n",
      "train loss:0.21901876949550148\n",
      "train loss:0.3819031962403993\n",
      "train loss:0.36158762850697684\n",
      "train loss:0.3018563679576915\n",
      "train loss:1.1358284777650813\n",
      "train loss:0.480284035123382\n",
      "train loss:0.2590552626237255\n",
      "train loss:0.518204076164363\n",
      "=== epoch:6, train acc:0.74, test acc:0.69 ===\n",
      "train loss:0.6537271853881553\n",
      "train loss:0.5141118954631517\n",
      "train loss:0.6090721530191965\n",
      "train loss:0.23367284566949204\n",
      "train loss:0.5754642153400658\n",
      "train loss:0.5627651970289886\n",
      "train loss:0.9387716212258285\n",
      "train loss:0.3632305951796331\n",
      "train loss:0.9123231736086345\n",
      "train loss:0.5483732829312136\n",
      "train loss:0.6427951526544886\n",
      "train loss:0.6651309322032042\n",
      "train loss:0.7079586441777141\n",
      "train loss:0.6773118064752361\n",
      "train loss:0.6474935715346188\n",
      "train loss:0.46595061012878614\n",
      "train loss:0.5649582478977311\n",
      "train loss:0.5695423413330577\n",
      "train loss:0.7346956829031205\n",
      "train loss:0.579971412522337\n",
      "train loss:0.6250634063736041\n",
      "train loss:0.8656982263654134\n",
      "train loss:0.8423979662652241\n",
      "train loss:0.8617905829614537\n",
      "train loss:0.6923717722312066\n",
      "train loss:0.4497626172953426\n",
      "train loss:0.5017484393269385\n",
      "train loss:0.5375474560446457\n",
      "train loss:0.4111837572497009\n",
      "train loss:0.3889644199853949\n",
      "train loss:0.5374623327021195\n",
      "train loss:0.6205990441573884\n",
      "train loss:0.5414622585364713\n",
      "train loss:0.4995049828614782\n",
      "train loss:0.46643456952576406\n",
      "train loss:0.6595870585072268\n",
      "train loss:0.5381260744353564\n",
      "train loss:0.5418526818527887\n",
      "train loss:0.3863579524589893\n",
      "train loss:0.35765787329115045\n",
      "train loss:0.68309182248823\n",
      "train loss:0.7394671446018075\n",
      "train loss:0.7122018794036482\n",
      "train loss:0.46991501712117223\n",
      "train loss:0.47631249077760895\n",
      "train loss:0.6284536922871193\n",
      "train loss:0.7314433709740153\n",
      "train loss:0.4441832639072314\n",
      "train loss:0.5147657372349943\n",
      "train loss:0.9242836159405646\n",
      "train loss:0.5448302672311982\n",
      "train loss:0.3993309313246511\n",
      "train loss:0.5971774913255812\n",
      "train loss:0.5197765163189503\n",
      "train loss:0.7817791408232677\n",
      "train loss:0.4707480454169038\n",
      "train loss:0.4059518407053715\n",
      "train loss:0.6039653262398252\n",
      "train loss:0.5517923184622883\n",
      "train loss:0.6316747805024515\n",
      "train loss:0.4105716394175973\n",
      "train loss:0.5427890857684587\n",
      "train loss:0.7245936760648335\n",
      "train loss:0.5208807335931768\n",
      "train loss:0.35073188432553715\n",
      "train loss:0.9200372252913895\n",
      "train loss:0.41040837118612056\n",
      "train loss:0.6490629187937588\n",
      "train loss:0.4723229251179923\n",
      "train loss:0.6272374175834285\n",
      "train loss:0.29106301802417583\n",
      "train loss:0.5168004282930075\n",
      "train loss:0.7062261946581352\n",
      "train loss:0.5954952777268826\n",
      "train loss:0.7840325896318316\n",
      "train loss:0.39821239099789996\n",
      "train loss:0.4016702763777678\n",
      "train loss:0.5879790563940249\n",
      "train loss:0.4862875752010529\n",
      "train loss:0.511375611446713\n",
      "train loss:0.496420895903257\n",
      "train loss:0.5247271669128005\n",
      "train loss:0.6460754316260304\n",
      "train loss:0.8518699118202531\n",
      "train loss:0.3594721311084778\n",
      "train loss:0.34551724630177977\n",
      "train loss:0.5575520663374123\n",
      "train loss:0.44832696963371327\n",
      "train loss:0.5192680393083855\n",
      "train loss:0.38523215720541165\n",
      "train loss:0.7390250596682363\n",
      "train loss:0.3165470115800634\n",
      "train loss:0.8092871673519074\n",
      "train loss:0.4553653074105467\n",
      "train loss:0.5504161085833673\n",
      "train loss:0.5871797930428209\n",
      "train loss:0.3664614708561685\n",
      "train loss:0.5370089278307039\n",
      "train loss:0.4381208904007609\n",
      "train loss:0.38629216368654384\n",
      "train loss:0.5163120566723466\n",
      "train loss:0.557083250904766\n",
      "train loss:0.6319137932109167\n",
      "train loss:0.7535490550468266\n",
      "train loss:0.3462655325110658\n",
      "train loss:0.8331021588078713\n",
      "train loss:0.7589059211264912\n",
      "train loss:0.38043605694395405\n",
      "train loss:0.7629140289332096\n",
      "train loss:0.2921933397892732\n",
      "train loss:0.5863762062779257\n",
      "train loss:0.5058999564595903\n",
      "train loss:0.6268096111135507\n",
      "train loss:0.3339724393351625\n",
      "train loss:0.28150601132875225\n",
      "train loss:0.6618226981386648\n",
      "train loss:0.5580139443406033\n",
      "train loss:0.41163998125814116\n",
      "train loss:0.40298528865463057\n",
      "train loss:0.614290768848605\n",
      "train loss:0.8338723685907103\n",
      "train loss:0.6738618135996634\n",
      "train loss:0.6226222061908213\n",
      "train loss:0.5057262101483575\n",
      "train loss:0.6120320366483232\n",
      "train loss:0.4162603665282332\n",
      "train loss:0.6743123938962975\n",
      "train loss:0.6419084090971278\n",
      "train loss:0.5104444361946829\n",
      "train loss:0.41656231769704605\n",
      "train loss:0.6101245069584099\n",
      "train loss:0.2799596166401689\n",
      "train loss:0.3305757778380086\n",
      "train loss:0.6816021806294001\n",
      "train loss:0.5482683472061378\n",
      "train loss:0.8099908154054175\n",
      "train loss:0.4963999621682246\n",
      "train loss:0.3983199835353489\n",
      "train loss:0.5967700964030513\n",
      "train loss:0.793539591067721\n",
      "train loss:0.49341334766377914\n",
      "train loss:0.510628704713558\n",
      "train loss:0.7514785359964004\n",
      "train loss:0.606823992725395\n",
      "train loss:0.46807429239695414\n",
      "train loss:0.4660066792838073\n",
      "train loss:0.5739718244856479\n",
      "train loss:0.7082436820634478\n",
      "train loss:0.4824395711934993\n",
      "train loss:0.5355802227030134\n",
      "train loss:0.4400082159763453\n",
      "train loss:0.42048764091999524\n",
      "train loss:0.5107494791022741\n",
      "=== epoch:7, train acc:0.74, test acc:0.7 ===\n",
      "train loss:0.5584491465103024\n",
      "train loss:0.8720695275341089\n",
      "train loss:0.5336820233278657\n",
      "train loss:0.5765049771407229\n",
      "train loss:0.5822791211274596\n",
      "train loss:0.6856719975408694\n",
      "train loss:0.7192217460868585\n",
      "train loss:0.4355250108952128\n",
      "train loss:0.6364217758788671\n",
      "train loss:0.551240935422989\n",
      "train loss:0.6804252067279538\n",
      "train loss:0.549262137485557\n",
      "train loss:0.49271762180005396\n",
      "train loss:0.3939036331901042\n",
      "train loss:0.7003644440782728\n",
      "train loss:0.4324433495948969\n",
      "train loss:0.7396226307391303\n",
      "train loss:0.5530756704976192\n",
      "train loss:0.582445504570342\n",
      "train loss:0.5416677666206802\n",
      "train loss:0.5414124177386822\n",
      "train loss:0.6113569526001896\n",
      "train loss:0.4770440401119537\n",
      "train loss:0.5749181992719773\n",
      "train loss:0.6029616598183761\n",
      "train loss:0.7117221933702975\n",
      "train loss:0.5384157470418833\n",
      "train loss:0.6115637122846185\n",
      "train loss:0.5645622918796852\n",
      "train loss:0.5023458160007489\n",
      "train loss:0.4514211748469964\n",
      "train loss:0.5894395628537705\n",
      "train loss:0.7167363256334318\n",
      "train loss:0.572444163312078\n",
      "train loss:0.7189321985566972\n",
      "train loss:0.3598892286428346\n",
      "train loss:0.6838637183860932\n",
      "train loss:0.5495972314641504\n",
      "train loss:0.5611736462523418\n",
      "train loss:0.6087834614564063\n",
      "train loss:0.6930942785361214\n",
      "train loss:0.4806593745761285\n",
      "train loss:0.6880970049493411\n",
      "train loss:0.6279913476189909\n",
      "train loss:0.6125109533703654\n",
      "train loss:0.6122732701044777\n",
      "train loss:0.5272584356247219\n",
      "train loss:0.48525495820176073\n",
      "train loss:0.6122595745588622\n",
      "train loss:0.7508116662852792\n",
      "train loss:0.5154058360213896\n",
      "train loss:0.707771229339014\n",
      "train loss:0.6201590111914532\n",
      "train loss:0.4298985683162443\n",
      "train loss:0.5940061634805839\n",
      "train loss:0.5816614033260809\n",
      "train loss:0.5638770021863648\n",
      "train loss:0.5663625202227032\n",
      "train loss:0.6215927685305442\n",
      "train loss:0.5933923841525705\n",
      "train loss:0.426819353353267\n",
      "train loss:0.6236899804731402\n",
      "train loss:0.5266471469776935\n",
      "train loss:0.59902287824454\n",
      "train loss:0.4007167196201224\n",
      "train loss:0.5538438053132573\n",
      "train loss:0.47021152494409935\n",
      "train loss:0.3772585358285184\n",
      "train loss:0.6307729058715984\n",
      "train loss:0.7630677545922642\n",
      "train loss:0.4181175534754284\n",
      "train loss:0.7117975218584665\n",
      "train loss:0.25651421372230615\n",
      "train loss:0.42562061863693346\n",
      "train loss:0.4432422736287429\n",
      "train loss:0.3815112275792557\n",
      "train loss:0.893899417941839\n",
      "train loss:0.42505319102869893\n",
      "train loss:0.39107487333098756\n",
      "train loss:0.3896290863056504\n",
      "train loss:0.6760345267101482\n",
      "train loss:0.7408014380668735\n",
      "train loss:0.5099102626654094\n",
      "train loss:0.7332721783569791\n",
      "train loss:0.44190584043621317\n",
      "train loss:0.3481353635681817\n",
      "train loss:0.4675489755588667\n",
      "train loss:0.37582940888467087\n",
      "train loss:0.7449950771665088\n",
      "train loss:0.493410059293546\n",
      "train loss:0.5697521579820019\n",
      "train loss:0.4582595241618909\n",
      "train loss:0.7124351790995773\n",
      "train loss:0.6506648444472098\n",
      "train loss:0.49140087886574263\n",
      "train loss:0.5550873530385395\n",
      "train loss:0.4333865210297594\n",
      "train loss:0.513528501552545\n",
      "train loss:0.33130776082141344\n",
      "train loss:0.4885010414940879\n",
      "train loss:0.7592525508811946\n",
      "train loss:0.29085235289721595\n",
      "train loss:0.28283999023331363\n",
      "train loss:0.5266802956500634\n",
      "train loss:0.46918230241946\n",
      "train loss:0.6191207760751577\n",
      "train loss:0.93970682132131\n",
      "train loss:0.5454049423459268\n",
      "train loss:0.7137354285451637\n",
      "train loss:0.5150454027702442\n",
      "train loss:0.6397076217690817\n",
      "train loss:0.5350894799001013\n",
      "train loss:0.2582354642866163\n",
      "train loss:0.5489720939694223\n",
      "train loss:0.7303171330968917\n",
      "train loss:0.4027501970735575\n",
      "train loss:0.5625580588458702\n",
      "train loss:0.6821456148970078\n",
      "train loss:0.5508740563223745\n",
      "train loss:0.6820617281513406\n",
      "train loss:0.5750537544997435\n",
      "train loss:0.5467640661301578\n",
      "train loss:0.34902846638522195\n",
      "train loss:0.39462172109886456\n",
      "train loss:0.5427166749923262\n",
      "train loss:0.9040601339597476\n",
      "train loss:0.4973317968865594\n",
      "train loss:0.4384711797594328\n",
      "train loss:0.6038597261740831\n",
      "train loss:0.48326128542590946\n",
      "train loss:0.5176660631254371\n",
      "train loss:0.2835932029582853\n",
      "train loss:0.3666013851397242\n",
      "train loss:0.6307754464723836\n",
      "train loss:0.7199535265090637\n",
      "train loss:0.8804801440327216\n",
      "train loss:0.452928248534535\n",
      "train loss:0.6005116363799027\n",
      "train loss:0.44754845736971954\n",
      "train loss:0.4480826006150753\n",
      "train loss:0.5025428460356148\n",
      "train loss:0.3853211208495001\n",
      "train loss:0.38568272954453453\n",
      "train loss:0.4068291122203556\n",
      "train loss:0.7099624243770497\n",
      "train loss:0.3756206296704803\n",
      "train loss:0.4245253695457151\n",
      "train loss:0.7171812724389246\n",
      "train loss:0.3901911238789199\n",
      "train loss:0.5040543682550964\n",
      "train loss:0.49338617855489186\n",
      "train loss:0.8143860229982749\n",
      "train loss:0.2399291178847493\n",
      "=== epoch:8, train acc:0.76, test acc:0.7 ===\n",
      "train loss:0.5338069285823905\n",
      "train loss:0.3847503677942134\n",
      "train loss:0.40027109092410507\n",
      "train loss:0.5436327798659941\n",
      "train loss:0.5329561553177306\n",
      "train loss:0.7400015507917682\n",
      "train loss:0.469353906941756\n",
      "train loss:0.26276125797151045\n",
      "train loss:0.5885736564581967\n",
      "train loss:0.7555845936845181\n",
      "train loss:0.5156079178082222\n",
      "train loss:0.304044043833836\n",
      "train loss:0.36298172789774663\n",
      "train loss:0.375569985297131\n",
      "train loss:0.2328307858140611\n",
      "train loss:0.5787561538098448\n",
      "train loss:0.5065925225720719\n",
      "train loss:0.6174358894096165\n",
      "train loss:0.43139617233666927\n",
      "train loss:0.6186803729158001\n",
      "train loss:0.7613643262585847\n",
      "train loss:0.5236691465840894\n",
      "train loss:0.3812470925199308\n",
      "train loss:0.6773828784704551\n",
      "train loss:0.7303731947389956\n",
      "train loss:0.7380028709812856\n",
      "train loss:0.6540607099345893\n",
      "train loss:0.5137263471098092\n",
      "train loss:0.32961148474704083\n",
      "train loss:0.40361882830217555\n",
      "train loss:0.48073568886460094\n",
      "train loss:0.5278733272327808\n",
      "train loss:0.5384336047518727\n",
      "train loss:0.7526714726155299\n",
      "train loss:0.4976081986541506\n",
      "train loss:0.5830383027577205\n",
      "train loss:0.5673812368596585\n",
      "train loss:0.5344800796301545\n",
      "train loss:0.5605910219974627\n",
      "train loss:0.5932517586065199\n",
      "train loss:0.41649661704490415\n",
      "train loss:0.4277309176236992\n",
      "train loss:0.7797554419373092\n",
      "train loss:0.6371156187708664\n",
      "train loss:0.550092676903522\n",
      "train loss:0.38056022395001887\n",
      "train loss:0.8834443008703159\n",
      "train loss:0.4191080118590656\n",
      "train loss:0.37818273259798324\n",
      "train loss:0.4586136366901908\n",
      "train loss:0.5195667109453191\n",
      "train loss:0.4850653638136668\n",
      "train loss:0.5048824130973307\n",
      "train loss:0.36703787377736746\n",
      "train loss:0.7272127092053582\n",
      "train loss:0.4117347433062209\n",
      "train loss:0.61763351209004\n",
      "train loss:0.5524824654876941\n",
      "train loss:0.6100261171417746\n",
      "train loss:0.3923475171390006\n",
      "train loss:0.6197992619442023\n",
      "train loss:0.553710275203783\n",
      "train loss:0.6384951564699397\n",
      "train loss:0.43352161032655856\n",
      "train loss:0.47627004795996986\n",
      "train loss:0.5981004236952879\n",
      "train loss:0.5540917903551653\n",
      "train loss:0.5127681989725402\n",
      "train loss:0.4869972261300052\n",
      "train loss:0.5791395331744867\n",
      "train loss:0.5291693274381993\n",
      "train loss:0.6792887838765258\n",
      "train loss:0.5189580993390541\n",
      "train loss:0.40084533563022423\n",
      "train loss:0.5856359139105336\n",
      "train loss:0.7127284031752708\n",
      "train loss:0.7025485639109349\n",
      "train loss:0.628817247617179\n",
      "train loss:0.5597655145376226\n",
      "train loss:0.5131474523216772\n",
      "train loss:0.6433664825375136\n",
      "train loss:0.42690724649173956\n",
      "train loss:0.6633164294630385\n",
      "train loss:0.4868900260099133\n",
      "train loss:0.4419237960775635\n",
      "train loss:0.42940363390472297\n",
      "train loss:0.4929377572725754\n",
      "train loss:0.41663844681513895\n",
      "train loss:0.4855648725790186\n",
      "train loss:0.47162639634621656\n",
      "train loss:0.3884191971658605\n",
      "train loss:0.5143920656785723\n",
      "train loss:0.31138920011676013\n",
      "train loss:0.4824381518393176\n",
      "train loss:0.9375747157228028\n",
      "train loss:0.4817840123097339\n",
      "train loss:0.5308884118125419\n",
      "train loss:0.5214785678468313\n",
      "train loss:0.5057839907337162\n",
      "train loss:0.5153971312907313\n",
      "train loss:0.6261074378922731\n",
      "train loss:0.7013542561013794\n",
      "train loss:0.5681252871834486\n",
      "train loss:0.6188548627629931\n",
      "train loss:0.3235828007391034\n",
      "train loss:0.6533873426278761\n",
      "train loss:0.5198796004086776\n",
      "train loss:0.4225169926928974\n",
      "train loss:0.7201421309929226\n",
      "train loss:0.5945680253038603\n",
      "train loss:0.4847827389918919\n",
      "train loss:0.3524082269375547\n",
      "train loss:0.36527774661280754\n",
      "train loss:0.33536363265912705\n",
      "train loss:0.4579410826505562\n",
      "train loss:0.741199785952896\n",
      "train loss:0.6727995603534227\n",
      "train loss:0.6095121966388815\n",
      "train loss:0.29466235509646277\n",
      "train loss:0.24648757191025572\n",
      "train loss:0.7874609977181987\n",
      "train loss:0.585307992006929\n",
      "train loss:0.9024079702422603\n",
      "train loss:0.5014122227987422\n",
      "train loss:0.5018820158565761\n",
      "train loss:0.5931226582135997\n",
      "train loss:0.45197579152433026\n",
      "train loss:0.6650204723841286\n",
      "train loss:0.500234870127386\n",
      "train loss:0.4165651822813842\n",
      "train loss:0.5284697721812844\n",
      "train loss:0.6274819542656496\n",
      "train loss:0.6243267459307994\n",
      "train loss:0.46987961848953985\n",
      "train loss:0.4795071607247108\n",
      "train loss:0.6819264738687825\n",
      "train loss:0.4705757204764664\n",
      "train loss:0.6041246475471861\n",
      "train loss:0.4182975468227787\n",
      "train loss:0.19614072840904803\n",
      "train loss:0.20322850412241253\n",
      "train loss:0.5160550244141454\n",
      "train loss:0.4315015059458397\n",
      "train loss:0.8455790163987216\n",
      "train loss:0.6482449004353034\n",
      "train loss:0.403451080219552\n",
      "train loss:0.7334433201335953\n",
      "train loss:1.0130278917493152\n",
      "train loss:0.7091119466900693\n",
      "train loss:0.4276320827012623\n",
      "train loss:0.6000156690614392\n",
      "train loss:0.482628669630924\n",
      "train loss:0.5432116234714747\n",
      "=== epoch:9, train acc:0.73, test acc:0.7 ===\n",
      "train loss:0.39831995503777984\n",
      "train loss:0.5044333621682366\n",
      "train loss:0.6919915350955386\n",
      "train loss:0.4888365699603131\n",
      "train loss:0.4463003705858652\n",
      "train loss:0.5107538393010611\n",
      "train loss:0.6582272117239435\n",
      "train loss:0.8349190326399054\n",
      "train loss:0.34196584859407386\n",
      "train loss:0.5361440442026084\n",
      "train loss:0.7060549282725409\n",
      "train loss:0.5978195409342312\n",
      "train loss:0.43699108869127745\n",
      "train loss:0.5110924796318103\n",
      "train loss:0.7143668209834491\n",
      "train loss:0.6784450428072295\n",
      "train loss:0.5298881020616742\n",
      "train loss:0.7287180248357892\n",
      "train loss:0.5753834022834015\n",
      "train loss:0.7485642389761311\n",
      "train loss:0.5411667452110941\n",
      "train loss:0.6899747680624592\n",
      "train loss:0.6119146230969661\n",
      "train loss:0.5003828476813691\n",
      "train loss:0.6539654372202196\n",
      "train loss:0.42590406765672906\n",
      "train loss:0.46850759577815226\n",
      "train loss:0.591524614770406\n",
      "train loss:0.5018935348916769\n",
      "train loss:0.5769929527609201\n",
      "train loss:0.5963054009244305\n",
      "train loss:0.5080259160455302\n",
      "train loss:0.3798162535200119\n",
      "train loss:0.2867838799155498\n",
      "train loss:0.39478762797860806\n",
      "train loss:0.5128792531985039\n",
      "train loss:0.539619631260471\n",
      "train loss:0.5285311017303969\n",
      "train loss:0.8199678434172387\n",
      "train loss:0.3476272049115478\n",
      "train loss:0.7494086462349622\n",
      "train loss:0.48824388946458586\n",
      "train loss:0.2956085266307341\n",
      "train loss:0.871175781992046\n",
      "train loss:0.3480850449904926\n",
      "train loss:0.3443978325065612\n",
      "train loss:0.3778065181632594\n",
      "train loss:0.31778795481826627\n",
      "train loss:0.23839445920097968\n",
      "train loss:0.7711115809428536\n",
      "train loss:0.8253605708938068\n",
      "train loss:0.48515229682286226\n",
      "train loss:0.6292003469732428\n",
      "train loss:0.44276395801312446\n",
      "train loss:0.34514573665772175\n",
      "train loss:0.5626786099661196\n",
      "train loss:0.4380903096990901\n",
      "train loss:0.59731708719204\n",
      "train loss:0.5236903018792016\n",
      "train loss:0.756170983692177\n",
      "train loss:0.43418400524214346\n",
      "train loss:0.5206395457952482\n",
      "train loss:0.6572033529102626\n",
      "train loss:0.449686963033548\n",
      "train loss:0.7993807964185556\n",
      "train loss:0.5350390704594833\n",
      "train loss:0.3786734919536524\n",
      "train loss:0.542165095278212\n",
      "train loss:0.5925568466775699\n",
      "train loss:0.6607231646560494\n",
      "train loss:0.4725471403165081\n",
      "train loss:0.36815625658609746\n",
      "train loss:0.5360814641588434\n",
      "train loss:0.4189830608109754\n",
      "train loss:0.5020610990310084\n",
      "train loss:0.32348158191393706\n",
      "train loss:0.7025261430553607\n",
      "train loss:0.5386222842057942\n",
      "train loss:0.5964907732702222\n",
      "train loss:0.5890476190608395\n",
      "train loss:0.3314160830836305\n",
      "train loss:0.600356030774448\n",
      "train loss:0.7947946066915852\n",
      "train loss:0.5469839625515001\n",
      "train loss:0.826138486823465\n",
      "train loss:0.5438511604413864\n",
      "train loss:0.4322609975611466\n",
      "train loss:0.45663986905938636\n",
      "train loss:0.5706792510336578\n",
      "train loss:0.41478623801629366\n",
      "train loss:0.5288526021216955\n",
      "train loss:0.32873921777857384\n",
      "train loss:0.5733260378099382\n",
      "train loss:0.639859585432284\n",
      "train loss:0.4922274576679596\n",
      "train loss:0.49001444265599153\n",
      "train loss:0.7229339209848017\n",
      "train loss:0.4737875205564177\n",
      "train loss:0.3957933684185559\n",
      "train loss:1.0581235677276035\n",
      "train loss:0.6767590072134438\n",
      "train loss:0.42597240182949997\n",
      "train loss:0.5136819159516158\n",
      "train loss:0.5342516204511549\n",
      "train loss:0.7073773978657547\n",
      "train loss:0.6777153134426545\n",
      "train loss:0.7262791418126613\n",
      "train loss:0.5717613603713201\n",
      "train loss:0.5410385195830515\n",
      "train loss:0.6537871092639783\n",
      "train loss:0.5309725277546843\n",
      "train loss:0.5545491846049321\n",
      "train loss:0.5260562975446396\n",
      "train loss:0.4612725138999993\n",
      "train loss:0.5734502964059722\n",
      "train loss:0.46342510186927255\n",
      "train loss:0.32288920079630323\n",
      "train loss:0.6420205822133368\n",
      "train loss:0.42715478608833307\n",
      "train loss:0.5263112531100155\n",
      "train loss:0.6973710050294264\n",
      "train loss:0.7740987943493216\n",
      "train loss:0.5039149632007319\n",
      "train loss:0.5946590960005247\n",
      "train loss:0.2928819628807139\n",
      "train loss:0.6665309684142295\n",
      "train loss:0.783644560865685\n",
      "train loss:0.685035787043672\n",
      "train loss:0.6347721438700697\n",
      "train loss:0.6803185316939687\n",
      "train loss:0.5627867570752175\n",
      "train loss:0.5338284725066675\n",
      "train loss:0.6001902836644478\n",
      "train loss:0.5331044085623339\n",
      "train loss:0.6333184233785124\n",
      "train loss:0.7002625981327195\n",
      "train loss:0.670834600232943\n",
      "train loss:0.3887068790868354\n",
      "train loss:0.477751692171777\n",
      "train loss:0.39469174177193606\n",
      "train loss:0.42489065329491743\n",
      "train loss:0.6413303589900143\n",
      "train loss:0.47131736401170377\n",
      "train loss:0.5225103786581335\n",
      "train loss:0.45737459234797945\n",
      "train loss:0.5029898340594559\n",
      "train loss:0.6492017093233079\n",
      "train loss:0.672638306254291\n",
      "train loss:0.6999328334669741\n",
      "train loss:0.3833563004898488\n",
      "train loss:0.4693541758060179\n",
      "train loss:0.7698083929709703\n",
      "train loss:0.5094694148466116\n",
      "=== epoch:10, train acc:0.74, test acc:0.7 ===\n",
      "train loss:0.4103311113754284\n",
      "train loss:0.7928079688998603\n",
      "train loss:0.6483703486828982\n",
      "train loss:0.31568036098328445\n",
      "train loss:0.5543035567593451\n",
      "train loss:0.3819958156329093\n",
      "train loss:0.3822706238342244\n",
      "train loss:0.3800839882671448\n",
      "train loss:0.39757192912199674\n",
      "train loss:0.620537821892947\n",
      "train loss:0.4965384396991685\n",
      "train loss:0.7406674938326124\n",
      "train loss:0.8958244536087518\n",
      "train loss:0.6843638555734632\n",
      "train loss:0.5005078578635376\n",
      "train loss:0.6400403409993234\n",
      "train loss:0.6253396014102216\n",
      "train loss:0.5705404333143634\n",
      "train loss:0.5483820774251258\n",
      "train loss:0.4445577173644385\n",
      "train loss:0.5654596818845254\n",
      "train loss:0.3899249049831365\n",
      "train loss:0.5081555586409405\n",
      "train loss:0.5885776072677861\n",
      "train loss:0.5019436540298864\n",
      "train loss:0.6350669788710829\n",
      "train loss:0.5175450476894565\n",
      "train loss:0.32738503745640124\n",
      "train loss:0.4583245779740966\n",
      "train loss:0.39721795211302685\n",
      "train loss:0.5361785660357741\n",
      "train loss:0.7717620640193059\n",
      "train loss:0.38701206324047377\n",
      "train loss:0.4616295656211505\n",
      "train loss:0.3850978472943894\n",
      "train loss:0.37874177476009885\n",
      "train loss:0.45919364329685736\n",
      "train loss:0.6360957212385928\n",
      "train loss:0.40636430538485946\n",
      "train loss:0.4305443637685894\n",
      "train loss:0.7676388488746135\n",
      "train loss:0.34151540835526895\n",
      "train loss:0.48868858697255824\n",
      "train loss:0.6941665720386748\n",
      "train loss:0.5888224665349051\n",
      "train loss:0.6611471674605733\n",
      "train loss:0.5732603557876771\n",
      "train loss:0.24123144995681\n",
      "train loss:0.5376417881934461\n",
      "train loss:0.7009896535817276\n",
      "train loss:0.6410781338355134\n",
      "train loss:0.7492963906656311\n",
      "train loss:0.29963387865187696\n",
      "train loss:0.3334759014012506\n",
      "train loss:0.4083203623449558\n",
      "train loss:0.5010990486342115\n",
      "train loss:0.5814210048224648\n",
      "train loss:0.4192994882509125\n",
      "train loss:0.4900279712628512\n",
      "train loss:0.5037125381144475\n",
      "train loss:0.5729079869508874\n",
      "train loss:0.32352569031020595\n",
      "train loss:0.4054198204873722\n",
      "train loss:0.4651140466348724\n",
      "train loss:0.5112940273927098\n",
      "train loss:0.6016265774576659\n",
      "train loss:0.5060832322479061\n",
      "train loss:0.3845589445525498\n",
      "train loss:0.3974963708483478\n",
      "train loss:0.5337871790944673\n",
      "train loss:0.4676847128917802\n",
      "train loss:0.6510481825148691\n",
      "train loss:0.31052744457889914\n",
      "train loss:0.4248804568916141\n",
      "train loss:0.4915741116160529\n",
      "train loss:0.2889804259376184\n",
      "train loss:0.12090133214608131\n",
      "train loss:0.7533865986393982\n",
      "train loss:0.15973108220719207\n",
      "train loss:0.4574142308333058\n",
      "train loss:0.5173584747143546\n",
      "train loss:0.7285833773075898\n",
      "train loss:0.15972929997299773\n",
      "train loss:0.31539001214073564\n",
      "train loss:0.7090409987171399\n",
      "train loss:0.497960692327989\n",
      "train loss:0.6824303138798334\n",
      "train loss:0.9218077766413\n",
      "train loss:0.4149461787876338\n",
      "train loss:0.4589989760730916\n",
      "train loss:0.5605535712838403\n",
      "train loss:0.6484964917200358\n",
      "train loss:0.6597443174294767\n",
      "train loss:0.5282712215790165\n",
      "train loss:0.29308243526212024\n",
      "train loss:0.8124365955124164\n",
      "train loss:0.5934279273403404\n",
      "train loss:0.5329203287846941\n",
      "train loss:0.5254549829798626\n",
      "train loss:0.6304686907457232\n",
      "train loss:0.6463412924472729\n",
      "train loss:0.6276906615302\n",
      "train loss:0.5071022928132407\n",
      "train loss:0.47283990489052574\n",
      "train loss:0.6011740648276989\n",
      "train loss:0.6118877016990893\n",
      "train loss:0.5134436475760977\n",
      "train loss:0.4543470130148572\n",
      "train loss:0.7616902039213698\n",
      "train loss:0.667759353463176\n",
      "train loss:0.5987295217257943\n",
      "train loss:0.5278545306696688\n",
      "train loss:0.5797176139086521\n",
      "train loss:0.45970490947700204\n",
      "train loss:0.2910251977816028\n",
      "train loss:0.437687692284184\n",
      "train loss:0.7386229363744825\n",
      "train loss:0.5601050584733995\n",
      "train loss:0.77712319753011\n",
      "train loss:0.6173129637355919\n",
      "train loss:0.5234815479007182\n",
      "train loss:0.438778073274547\n",
      "train loss:0.3886597783165634\n",
      "train loss:0.6395671715732723\n",
      "train loss:0.4676216982224978\n",
      "train loss:0.6025725513007921\n",
      "train loss:0.45565651509841876\n",
      "train loss:0.24563396855924227\n",
      "train loss:0.4099649495444826\n",
      "train loss:0.607066599526617\n",
      "train loss:0.47851554666268636\n",
      "train loss:0.7649989833624332\n",
      "train loss:0.23647426524090812\n",
      "train loss:0.6533110311247856\n",
      "train loss:0.9909633594437983\n",
      "train loss:0.31340657447324977\n",
      "train loss:0.4030275750869697\n",
      "train loss:0.480478253027593\n",
      "train loss:0.3994364718954919\n",
      "train loss:0.5985782881055255\n",
      "train loss:0.37769677091441467\n",
      "train loss:0.7532970906391594\n",
      "train loss:0.37281305489965744\n",
      "train loss:0.6885687370072637\n",
      "train loss:0.5029642523834441\n",
      "train loss:0.5787643738151711\n",
      "train loss:0.9211548640844042\n",
      "train loss:0.616980038746074\n",
      "train loss:0.6225974425931113\n",
      "train loss:0.7280363859364898\n",
      "train loss:0.4856632826045214\n",
      "train loss:0.8376094206194165\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5372549019607843\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 90, 'filter_size': 7, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ddb4be6-80ab-4984-b6ac-8ce28670b5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6869525648098208\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6834615286589522\n",
      "train loss:0.6729306193701378\n",
      "train loss:0.6597401735831118\n",
      "train loss:0.6389340485999555\n",
      "train loss:0.6006829584520961\n",
      "train loss:0.6067286135443066\n",
      "train loss:0.48863436202251914\n",
      "train loss:0.6602840345244683\n",
      "train loss:0.6422248257610947\n",
      "train loss:0.4974618129244286\n",
      "train loss:0.4971678745112892\n",
      "train loss:0.4914407639562469\n",
      "train loss:0.810338026456086\n",
      "train loss:0.4854029786436332\n",
      "train loss:0.7257892211830947\n",
      "train loss:0.5535576350783502\n",
      "train loss:0.5417486691612836\n",
      "train loss:0.7950234079051154\n",
      "train loss:0.6267350912397437\n",
      "train loss:0.6827541088122507\n",
      "train loss:0.5912213267202008\n",
      "train loss:0.595306281973968\n",
      "train loss:0.5548173263684502\n",
      "train loss:0.5976804623125731\n",
      "train loss:0.5082244418714369\n",
      "train loss:0.4973135197543835\n",
      "train loss:0.6550430425238998\n",
      "train loss:0.6879591490150107\n",
      "train loss:0.3057740958926156\n",
      "train loss:0.604443049887983\n",
      "train loss:0.9915699767005147\n",
      "train loss:0.800297138486324\n",
      "train loss:0.8983429710007073\n",
      "train loss:0.4569173899286838\n",
      "train loss:0.8409558894351266\n",
      "train loss:0.5491720936132025\n",
      "train loss:0.56474310937461\n",
      "train loss:0.6564250137480057\n",
      "train loss:0.642788109503239\n",
      "train loss:0.6167715769486121\n",
      "train loss:0.7112817249585854\n",
      "train loss:0.6579773709734995\n",
      "train loss:0.5786207312005105\n",
      "train loss:0.6337173681041162\n",
      "train loss:0.5372834759292935\n",
      "train loss:0.6532659028624576\n",
      "train loss:0.6752319471156428\n",
      "train loss:0.5825330831976976\n",
      "train loss:0.728894372142151\n",
      "train loss:0.6754959011307815\n",
      "train loss:0.6808219595195873\n",
      "train loss:0.7305647513347127\n",
      "train loss:0.6856101516622042\n",
      "train loss:0.5749889521255641\n",
      "train loss:0.7263219180276772\n",
      "train loss:0.6190118297269099\n",
      "train loss:0.6742843392356972\n",
      "train loss:0.6288784296981644\n",
      "train loss:0.6283058661271552\n",
      "train loss:0.5110724165013865\n",
      "train loss:0.7319168034257397\n",
      "train loss:0.5395787904555813\n",
      "train loss:0.5451954298396235\n",
      "train loss:0.5380499830174331\n",
      "train loss:0.3461460935443188\n",
      "train loss:0.6986526640405586\n",
      "train loss:0.7348673594900672\n",
      "train loss:0.5344173032649913\n",
      "train loss:0.634977361700094\n",
      "train loss:0.6079688954142185\n",
      "train loss:1.0461228262194016\n",
      "train loss:0.697522075918854\n",
      "train loss:0.6901185156757382\n",
      "train loss:0.6694785884225616\n",
      "train loss:0.7001719205329713\n",
      "train loss:0.6786240806515873\n",
      "train loss:0.6339336941868493\n",
      "train loss:0.5566299334214222\n",
      "train loss:0.5936273243087832\n",
      "train loss:0.6689147418108649\n",
      "train loss:0.7354696977419757\n",
      "train loss:0.6646406831210149\n",
      "train loss:0.634000759251897\n",
      "train loss:0.66746077523976\n",
      "train loss:0.5722596070183508\n",
      "train loss:0.6112739155389503\n",
      "train loss:0.6061292760744134\n",
      "train loss:0.6771106782328137\n",
      "train loss:0.587244616736732\n",
      "train loss:0.6658989568774022\n",
      "train loss:0.5762573188100736\n",
      "train loss:0.7182531135723673\n",
      "train loss:0.4155978988414173\n",
      "train loss:0.6118402960690785\n",
      "train loss:0.5998999852110245\n",
      "train loss:0.6067595092616674\n",
      "train loss:0.5154328328887956\n",
      "train loss:0.5300006322189519\n",
      "train loss:0.502537412235829\n",
      "train loss:0.8741355508627986\n",
      "train loss:0.6434190868431622\n",
      "train loss:0.7084438276540628\n",
      "train loss:0.5170529094516702\n",
      "train loss:0.3924599887790695\n",
      "train loss:0.5010056406614292\n",
      "train loss:0.4046839457961074\n",
      "train loss:0.5992721561068765\n",
      "train loss:0.7550875456102282\n",
      "train loss:0.6460508516584251\n",
      "train loss:0.48558597919013646\n",
      "train loss:0.7838466184770881\n",
      "train loss:0.8084251002764844\n",
      "train loss:0.5385839647569105\n",
      "train loss:0.369081836272918\n",
      "train loss:0.609844228652657\n",
      "train loss:0.4430272081487995\n",
      "train loss:0.5391798216560412\n",
      "train loss:0.6600518747143083\n",
      "train loss:0.4136087117827635\n",
      "train loss:0.6171430317124325\n",
      "train loss:0.7638180140263623\n",
      "train loss:0.4952906566960755\n",
      "train loss:0.7318963240051766\n",
      "train loss:0.2891061430438554\n",
      "train loss:0.7158311092375808\n",
      "train loss:0.7587409037588837\n",
      "train loss:0.5117445636312558\n",
      "train loss:0.7690832085114041\n",
      "train loss:0.5884750898477782\n",
      "train loss:0.4257706632974876\n",
      "train loss:0.42042289712350955\n",
      "train loss:0.6052533919386464\n",
      "train loss:0.5190130829899804\n",
      "train loss:0.48537490110033427\n",
      "train loss:0.587258886747942\n",
      "train loss:0.8384538574502756\n",
      "train loss:0.47675377769756827\n",
      "train loss:0.5712402722422565\n",
      "train loss:0.930858303623135\n",
      "train loss:0.5036667155713381\n",
      "train loss:0.6160042679924098\n",
      "train loss:0.6109013010020763\n",
      "train loss:0.43003738187362767\n",
      "train loss:0.522949308235961\n",
      "train loss:0.45887250924275796\n",
      "train loss:0.4956954277992279\n",
      "train loss:0.5157619392734851\n",
      "train loss:0.41201456430454514\n",
      "train loss:0.6699747192859549\n",
      "train loss:0.8471002339161021\n",
      "train loss:0.4770812437219675\n",
      "train loss:0.8365369825573469\n",
      "train loss:0.4842486261548703\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5173366879619665\n",
      "train loss:0.5371485718573659\n",
      "train loss:0.48718728698298336\n",
      "train loss:0.6096932607960135\n",
      "train loss:0.7044486713283552\n",
      "train loss:0.40723676111230017\n",
      "train loss:0.8359387936284026\n",
      "train loss:0.7674755497864147\n",
      "train loss:0.5199668176368838\n",
      "train loss:0.7294660105914521\n",
      "train loss:0.6897058180477679\n",
      "train loss:0.6020035443382601\n",
      "train loss:0.6800028210132406\n",
      "train loss:0.6663120946882644\n",
      "train loss:0.56727401956324\n",
      "train loss:0.5723379039907823\n",
      "train loss:0.6748580005059107\n",
      "train loss:0.6550562559645149\n",
      "train loss:0.5783578198720558\n",
      "train loss:0.6184567084025444\n",
      "train loss:0.7147708910034674\n",
      "train loss:0.6749989641839067\n",
      "train loss:0.6082809530013789\n",
      "train loss:0.44046599026950106\n",
      "train loss:0.549998998463997\n",
      "train loss:0.6558288111720455\n",
      "train loss:0.6409356325121667\n",
      "train loss:0.6214007355171217\n",
      "train loss:0.774845967694278\n",
      "train loss:0.36907085246872784\n",
      "train loss:0.5165324418880279\n",
      "train loss:0.7165704032942699\n",
      "train loss:0.6093925564296399\n",
      "train loss:0.6962017605329596\n",
      "train loss:0.7281934522637402\n",
      "train loss:0.605781663139472\n",
      "train loss:0.6906064087412466\n",
      "train loss:0.8509180738893812\n",
      "train loss:0.5322954384913905\n",
      "train loss:0.610259486537736\n",
      "train loss:0.6538814322155032\n",
      "train loss:0.4751524669006678\n",
      "train loss:0.761084918026899\n",
      "train loss:0.4856423643503892\n",
      "train loss:0.6075179887095812\n",
      "train loss:0.596871199619704\n",
      "train loss:0.531447095590026\n",
      "train loss:0.5268906962663513\n",
      "train loss:0.3586338942045584\n",
      "train loss:0.5067963172731058\n",
      "train loss:0.6008359024090663\n",
      "train loss:0.49590673201573826\n",
      "train loss:0.5102713971966432\n",
      "train loss:0.6661702433722472\n",
      "train loss:1.0911274232232455\n",
      "train loss:0.6066354812772685\n",
      "train loss:0.5749335348629661\n",
      "train loss:0.39511013138859247\n",
      "train loss:0.7401071623190607\n",
      "train loss:0.6718633222967747\n",
      "train loss:0.536564509803304\n",
      "train loss:0.7541271196401885\n",
      "train loss:0.6267102297566475\n",
      "train loss:0.6741629919151927\n",
      "train loss:0.553322122473037\n",
      "train loss:0.44310322579874584\n",
      "train loss:0.549462031855994\n",
      "train loss:0.6382892849048961\n",
      "train loss:0.6259607455612468\n",
      "train loss:0.5828354001570526\n",
      "train loss:0.5567413434352677\n",
      "train loss:0.6803565084103818\n",
      "train loss:0.6651499653559354\n",
      "train loss:0.6465301011332842\n",
      "train loss:0.45754729780353987\n",
      "train loss:0.4689299105920197\n",
      "train loss:0.6272414836127741\n",
      "train loss:0.6629478518487624\n",
      "train loss:0.5088271316393767\n",
      "train loss:0.3783143829119691\n",
      "train loss:0.7301291699923451\n",
      "train loss:0.7197681586716834\n",
      "train loss:0.724619038348336\n",
      "train loss:0.9602769344261061\n",
      "train loss:0.6148203699085241\n",
      "train loss:0.44785242473705666\n",
      "train loss:0.6239628217142377\n",
      "train loss:0.6071361560830077\n",
      "train loss:0.9279640713748758\n",
      "train loss:0.6673277996138215\n",
      "train loss:0.7070663942203281\n",
      "train loss:0.6027768519812127\n",
      "train loss:0.6336458330845252\n",
      "train loss:0.7075914154805137\n",
      "train loss:0.6736163348877591\n",
      "train loss:0.7136167545835999\n",
      "train loss:0.6553319664945757\n",
      "train loss:0.5949944835529482\n",
      "train loss:0.6379885310089484\n",
      "train loss:0.6654461940907047\n",
      "train loss:0.6354278867290942\n",
      "train loss:0.6358478672753813\n",
      "train loss:0.7477463009547975\n",
      "train loss:0.5804380900133943\n",
      "train loss:0.5383001136985811\n",
      "train loss:0.5727364707843958\n",
      "train loss:0.6060107073667222\n",
      "train loss:0.8438641161625396\n",
      "train loss:0.5499546461264344\n",
      "train loss:0.7668491017211354\n",
      "train loss:0.6875124102393574\n",
      "train loss:0.5939884127527064\n",
      "train loss:0.5860211875802117\n",
      "train loss:0.5459748402668518\n",
      "train loss:0.7676887317097453\n",
      "train loss:0.38448336875025435\n",
      "train loss:0.4336704148070867\n",
      "train loss:0.49093359169273576\n",
      "train loss:0.505424895336509\n",
      "train loss:0.6370220364653181\n",
      "train loss:0.35199983280434466\n",
      "train loss:0.5109657937716732\n",
      "train loss:0.6221931372874605\n",
      "train loss:0.5107776682819275\n",
      "train loss:0.6812125352190938\n",
      "train loss:0.48031806058803583\n",
      "train loss:0.6457221266381439\n",
      "train loss:0.3386906564977183\n",
      "train loss:0.5807367656491647\n",
      "train loss:0.34878817371815957\n",
      "train loss:0.7582570283849422\n",
      "train loss:0.8548427168180319\n",
      "train loss:0.8120739642239523\n",
      "train loss:0.7075791482349958\n",
      "train loss:0.6159594091263504\n",
      "train loss:0.5448716709378639\n",
      "train loss:0.5674688418317304\n",
      "train loss:0.7264882532338772\n",
      "train loss:0.6306786434128399\n",
      "train loss:0.6305607446187157\n",
      "train loss:0.6708432574350847\n",
      "train loss:0.5568475732190273\n",
      "train loss:0.6360135712785138\n",
      "train loss:0.658569875697445\n",
      "train loss:0.6750244761943105\n",
      "train loss:0.6414478988857447\n",
      "train loss:0.6328475800249554\n",
      "train loss:0.6277975030477968\n",
      "train loss:0.5393675719178724\n",
      "train loss:0.6711092689316801\n",
      "train loss:0.611792899827454\n",
      "train loss:0.6180780917848897\n",
      "train loss:0.6200854443865429\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.667892552327748\n",
      "train loss:0.6044165536190114\n",
      "train loss:0.5457486821854036\n",
      "train loss:0.8234060223620169\n",
      "train loss:0.6064400609717956\n",
      "train loss:0.6129077982689817\n",
      "train loss:0.6201999593827182\n",
      "train loss:0.5282825293319616\n",
      "train loss:0.6753023501806502\n",
      "train loss:0.6228114117529155\n",
      "train loss:0.5129029428346621\n",
      "train loss:1.0281001724532022\n",
      "train loss:0.3575200493114118\n",
      "train loss:0.5870662418612229\n",
      "train loss:0.6843044817861046\n",
      "train loss:0.4426374556913282\n",
      "train loss:0.7085785042838632\n",
      "train loss:0.5853189767978724\n",
      "train loss:0.604641225115509\n",
      "train loss:0.4423900620762935\n",
      "train loss:0.4221455934271555\n",
      "train loss:0.608097000607484\n",
      "train loss:0.7064721200220727\n",
      "train loss:0.8657864674702236\n",
      "train loss:0.6124439388168207\n",
      "train loss:0.5225969109099762\n",
      "train loss:0.641335690056685\n",
      "train loss:0.736661312938977\n",
      "train loss:0.5844503310559364\n",
      "train loss:0.6262247867046035\n",
      "train loss:0.5001278098613099\n",
      "train loss:0.7473161631187599\n",
      "train loss:0.44651294444379896\n",
      "train loss:0.5766238212649518\n",
      "train loss:0.7952477962203682\n",
      "train loss:0.614270748191893\n",
      "train loss:0.4567347304858157\n",
      "train loss:0.592089917956651\n",
      "train loss:0.525694497598663\n",
      "train loss:0.5171456546050972\n",
      "train loss:0.5741574041958669\n",
      "train loss:0.41722745386021814\n",
      "train loss:0.4825501078609218\n",
      "train loss:0.6102106534565327\n",
      "train loss:0.5866097727297943\n",
      "train loss:0.4909802519579628\n",
      "train loss:0.5072624256162255\n",
      "train loss:0.6054969222022766\n",
      "train loss:0.6342497176093265\n",
      "train loss:0.48411104524830684\n",
      "train loss:0.6433008833384821\n",
      "train loss:0.6450059283222094\n",
      "train loss:0.24900334885715955\n",
      "train loss:0.3550959615697572\n",
      "train loss:0.36389213065757364\n",
      "train loss:0.38801263266063624\n",
      "train loss:0.7137725625143273\n",
      "train loss:0.6303150322212207\n",
      "train loss:0.5381650919515695\n",
      "train loss:0.5526189221316258\n",
      "train loss:0.8106666641954021\n",
      "train loss:0.79991886586805\n",
      "train loss:0.49667175547340003\n",
      "train loss:0.37202752716041326\n",
      "train loss:0.5937464125193836\n",
      "train loss:0.6320180799525859\n",
      "train loss:0.6951666936479122\n",
      "train loss:0.5475169438854095\n",
      "train loss:0.6050341671162716\n",
      "train loss:0.7386457995042649\n",
      "train loss:0.5890779467683498\n",
      "train loss:0.4177733620656913\n",
      "train loss:0.6572748110376951\n",
      "train loss:0.9655157883754706\n",
      "train loss:0.7241793777681164\n",
      "train loss:0.6514674340245663\n",
      "train loss:0.6555753958531023\n",
      "train loss:0.5101015742884852\n",
      "train loss:0.6296622182409386\n",
      "train loss:0.7376456187540881\n",
      "train loss:0.6258922343705066\n",
      "train loss:0.5826761740427732\n",
      "train loss:0.5761750359546863\n",
      "train loss:0.6230271637846697\n",
      "train loss:0.6741447124081943\n",
      "train loss:0.6330680810032803\n",
      "train loss:0.658266388968891\n",
      "train loss:0.595167718928265\n",
      "train loss:0.6078802775543994\n",
      "train loss:0.6767072820161603\n",
      "train loss:0.566871682740887\n",
      "train loss:0.4732560384858372\n",
      "train loss:0.7458320632447977\n",
      "train loss:0.7807053453241973\n",
      "train loss:0.372592201762839\n",
      "train loss:0.8264051936433866\n",
      "train loss:0.4676410639015776\n",
      "train loss:0.5751947273889122\n",
      "train loss:0.49821111692264486\n",
      "train loss:0.5651659828777165\n",
      "train loss:0.6566565306119767\n",
      "train loss:0.5296959229074076\n",
      "train loss:0.878140447569413\n",
      "train loss:0.6951082368386992\n",
      "train loss:0.6306173322031352\n",
      "train loss:0.3310929212855617\n",
      "train loss:0.6953771881659403\n",
      "train loss:0.5410633517806137\n",
      "train loss:0.3237089409717589\n",
      "train loss:0.6852975283400065\n",
      "train loss:0.6292206795818861\n",
      "train loss:0.40701847132085034\n",
      "train loss:0.5868286722622822\n",
      "train loss:0.4892036030422527\n",
      "train loss:0.5235053260486398\n",
      "train loss:0.4959348964500229\n",
      "train loss:0.8441284409804659\n",
      "train loss:0.2620844852626819\n",
      "train loss:0.8966343088941082\n",
      "train loss:0.3776179683230347\n",
      "train loss:0.5073029909952297\n",
      "train loss:0.7029057530489171\n",
      "train loss:0.4951179114977851\n",
      "train loss:0.3808377446290559\n",
      "train loss:0.6488825499416808\n",
      "train loss:0.5938433957726432\n",
      "train loss:0.7997186668199336\n",
      "train loss:0.6047480433055926\n",
      "train loss:0.5660261390254401\n",
      "train loss:0.5231220841626156\n",
      "train loss:0.4906843003643841\n",
      "train loss:0.7738973810190346\n",
      "train loss:0.6827451741436661\n",
      "train loss:0.6074074796472619\n",
      "train loss:0.5385030734594768\n",
      "train loss:0.5844982659948779\n",
      "train loss:0.6292773246215115\n",
      "train loss:0.5797300391249335\n",
      "train loss:0.6400038288234263\n",
      "train loss:0.8061931263851576\n",
      "train loss:0.5438419451888679\n",
      "train loss:0.45758882094102893\n",
      "train loss:0.6534561643733594\n",
      "train loss:0.7312056849846795\n",
      "train loss:0.6986409216483016\n",
      "train loss:0.5679996856754517\n",
      "train loss:0.7292216368151369\n",
      "train loss:0.7137165666062006\n",
      "train loss:0.5816806352413961\n",
      "train loss:0.5479476265733434\n",
      "train loss:0.551074865060234\n",
      "train loss:0.6641536147672852\n",
      "train loss:0.5920140453545122\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.48621732955766406\n",
      "train loss:0.6355816754653048\n",
      "train loss:0.5238385324802943\n",
      "train loss:0.4325891773190421\n",
      "train loss:0.41171173267749783\n",
      "train loss:0.623506612790402\n",
      "train loss:0.6393505782518476\n",
      "train loss:0.8115892684279921\n",
      "train loss:0.45055263839390103\n",
      "train loss:0.27750609965424927\n",
      "train loss:0.5139443774703872\n",
      "train loss:0.3391781523362192\n",
      "train loss:0.5172018958510604\n",
      "train loss:0.45777205267441223\n",
      "train loss:0.7004378969914089\n",
      "train loss:0.6682753716462028\n",
      "train loss:0.6092719038541169\n",
      "train loss:0.5299366980047586\n",
      "train loss:0.7371231468708479\n",
      "train loss:0.2645039241180544\n",
      "train loss:0.46614012351515977\n",
      "train loss:0.5639880963798454\n",
      "train loss:0.6912407656839202\n",
      "train loss:0.6682766896388228\n",
      "train loss:0.7339934152942004\n",
      "train loss:0.4602452913820493\n",
      "train loss:0.6428034065821094\n",
      "train loss:0.6194692859151816\n",
      "train loss:0.6004978074513875\n",
      "train loss:0.48818341131396714\n",
      "train loss:0.5850596681072865\n",
      "train loss:0.6511937571513026\n",
      "train loss:0.542348196029394\n",
      "train loss:0.5267412696676276\n",
      "train loss:0.7090256107701716\n",
      "train loss:0.7550108100010023\n",
      "train loss:0.5584868299492182\n",
      "train loss:0.43904892203621315\n",
      "train loss:0.60226604490099\n",
      "train loss:0.6206929883773246\n",
      "train loss:0.7960977826944186\n",
      "train loss:0.5362373809438985\n",
      "train loss:0.6205828207969742\n",
      "train loss:0.402257895050946\n",
      "train loss:0.8457940440392478\n",
      "train loss:0.7654801253231239\n",
      "train loss:0.4548180628829853\n",
      "train loss:0.48482548971023814\n",
      "train loss:0.6902836710141866\n",
      "train loss:0.5519185471271584\n",
      "train loss:0.6064006018179758\n",
      "train loss:0.7939669400734864\n",
      "train loss:0.4575529621709576\n",
      "train loss:0.7402234744524054\n",
      "train loss:0.4876326678641016\n",
      "train loss:0.5347699461299664\n",
      "train loss:0.7377278076406899\n",
      "train loss:0.35860758092997636\n",
      "train loss:0.9692011150668444\n",
      "train loss:0.5096479895227051\n",
      "train loss:0.5083597995323551\n",
      "train loss:0.6235122081948229\n",
      "train loss:0.48088673739165877\n",
      "train loss:0.5098560872368811\n",
      "train loss:0.6915426829554281\n",
      "train loss:0.49607277351560547\n",
      "train loss:0.5575018634140539\n",
      "train loss:0.8605760956823012\n",
      "train loss:0.29074020321247235\n",
      "train loss:0.7063451915870996\n",
      "train loss:0.6231097404079227\n",
      "train loss:0.7718061464236345\n",
      "train loss:0.6170005181161493\n",
      "train loss:0.7034405636112523\n",
      "train loss:0.5740496775903315\n",
      "train loss:0.5765231056412979\n",
      "train loss:0.61489565507897\n",
      "train loss:0.6203227513715379\n",
      "train loss:0.7430211919165576\n",
      "train loss:0.5558531765988307\n",
      "train loss:0.5781528137545054\n",
      "train loss:0.6771066495979257\n",
      "train loss:0.4786753278344508\n",
      "train loss:0.6795042579360218\n",
      "train loss:0.5683446008366264\n",
      "train loss:0.4681780283227438\n",
      "train loss:0.5804517254457342\n",
      "train loss:0.4720822747705552\n",
      "train loss:0.7524285128417187\n",
      "train loss:0.3773822780098481\n",
      "train loss:0.4353814516624091\n",
      "train loss:0.7407116974678296\n",
      "train loss:0.7827410784773039\n",
      "train loss:0.3813507306641427\n",
      "train loss:0.5145103588960156\n",
      "train loss:0.81810230481023\n",
      "train loss:0.628469756393901\n",
      "train loss:0.5414675919419307\n",
      "train loss:0.5142875871175956\n",
      "train loss:0.9335238844460975\n",
      "train loss:0.5999172744645852\n",
      "train loss:0.8215759536417254\n",
      "train loss:0.6142649167331025\n",
      "train loss:0.6009085622922458\n",
      "train loss:0.6406415917574559\n",
      "train loss:0.6850950115171583\n",
      "train loss:0.7051122205196456\n",
      "train loss:0.5939573813745482\n",
      "train loss:0.5822396481345078\n",
      "train loss:0.6864501753302008\n",
      "train loss:0.6453454398910526\n",
      "train loss:0.502753019584034\n",
      "train loss:0.6927217939202706\n",
      "train loss:0.7008752854951352\n",
      "train loss:0.6089939764836619\n",
      "train loss:0.6837989314208615\n",
      "train loss:0.49465652096278667\n",
      "train loss:0.37564700969242404\n",
      "train loss:0.42939031764111596\n",
      "train loss:0.4382371066381343\n",
      "train loss:0.5362040646331004\n",
      "train loss:1.1905577076988143\n",
      "train loss:0.4796613636712534\n",
      "train loss:0.6532874940113431\n",
      "train loss:0.8602923588254348\n",
      "train loss:0.5418821563192564\n",
      "train loss:0.5654707442909589\n",
      "train loss:0.5056346489771587\n",
      "train loss:0.5468689172711196\n",
      "train loss:0.5937907542049793\n",
      "train loss:0.8092822494903242\n",
      "train loss:0.569255472495146\n",
      "train loss:0.5611781539703348\n",
      "train loss:0.6927810598511016\n",
      "train loss:0.4872974012565782\n",
      "train loss:0.548814612548072\n",
      "train loss:0.5501048530390908\n",
      "train loss:0.43343190758601435\n",
      "train loss:0.6440019776693816\n",
      "train loss:0.3862836308766906\n",
      "train loss:0.48238114110173713\n",
      "train loss:0.819034512560382\n",
      "train loss:0.7859947587797693\n",
      "train loss:0.22637226291036744\n",
      "train loss:0.6207055064615822\n",
      "train loss:0.6566317682417491\n",
      "train loss:0.7823720655241143\n",
      "train loss:0.2813062041975966\n",
      "train loss:0.7674933173451067\n",
      "train loss:0.23212229432211004\n",
      "train loss:0.8125101521816189\n",
      "train loss:0.46924270907248833\n",
      "train loss:1.0496808984093968\n",
      "=== epoch:5, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.509983716344212\n",
      "train loss:0.6061438852574434\n",
      "train loss:0.5511968644691894\n",
      "train loss:0.6872421262277935\n",
      "train loss:0.384667940992671\n",
      "train loss:0.6958274910558445\n",
      "train loss:0.716713888618972\n",
      "train loss:0.629978380348119\n",
      "train loss:0.6748182700428862\n",
      "train loss:0.5958241850255617\n",
      "train loss:0.6399726812627264\n",
      "train loss:0.5772634494601052\n",
      "train loss:0.6480907233855524\n",
      "train loss:0.7207348745583684\n",
      "train loss:0.8171156222947753\n",
      "train loss:0.5869163598135712\n",
      "train loss:0.5844519366890888\n",
      "train loss:0.5654733632683313\n",
      "train loss:0.6738643503475253\n",
      "train loss:0.6720011102531666\n",
      "train loss:0.5049403930084099\n",
      "train loss:0.5961599546660148\n",
      "train loss:0.6240460495610659\n",
      "train loss:0.6080943861321175\n",
      "train loss:0.5804963353834943\n",
      "train loss:0.4220253572699445\n",
      "train loss:0.35035459707140265\n",
      "train loss:0.4440442928800158\n",
      "train loss:0.6186266960051572\n",
      "train loss:0.4988888574027449\n",
      "train loss:0.5850642939743173\n",
      "train loss:0.8769482519411094\n",
      "train loss:0.484759885809137\n",
      "train loss:0.2836569335494567\n",
      "train loss:0.6170035603744987\n",
      "train loss:0.7664722946733805\n",
      "train loss:0.491211490022721\n",
      "train loss:0.7274090911740881\n",
      "train loss:0.3121426836588281\n",
      "train loss:0.8308748055044324\n",
      "train loss:0.6714905061395166\n",
      "train loss:0.6397899507288943\n",
      "train loss:0.5632912913023633\n",
      "train loss:0.5986769590540286\n",
      "train loss:0.5213043701043099\n",
      "train loss:0.6098062337091379\n",
      "train loss:0.6382314908279051\n",
      "train loss:0.5585700005546733\n",
      "train loss:0.5811287054167001\n",
      "train loss:0.5718822321435586\n",
      "train loss:0.42680301700456413\n",
      "train loss:0.6748304619332779\n",
      "train loss:0.539544281011224\n",
      "train loss:0.6891401525254488\n",
      "train loss:0.6421955645363665\n",
      "train loss:0.5336384033931553\n",
      "train loss:0.49431004290099184\n",
      "train loss:0.33818463481563865\n",
      "train loss:0.6014752065942293\n",
      "train loss:0.4566163104149439\n",
      "train loss:0.5480295119857596\n",
      "train loss:0.6185613641403817\n",
      "train loss:0.35438844574682943\n",
      "train loss:0.3945885397830772\n",
      "train loss:0.42332801986079804\n",
      "train loss:0.4939553448800188\n",
      "train loss:0.7001856181900465\n",
      "train loss:0.5267661997204305\n",
      "train loss:0.44790252685999593\n",
      "train loss:0.49028015079745335\n",
      "train loss:0.8100451217745096\n",
      "train loss:0.31676843512701464\n",
      "train loss:0.3451995430609037\n",
      "train loss:0.5010286830455459\n",
      "train loss:0.5695089717754236\n",
      "train loss:0.6103014170028161\n",
      "train loss:0.8396577733686067\n",
      "train loss:0.44954354231826166\n",
      "train loss:0.5835879514529052\n",
      "train loss:0.6098417620355152\n",
      "train loss:0.5467181015668826\n",
      "train loss:0.5562443052517244\n",
      "train loss:0.7149960680299187\n",
      "train loss:0.4711966985113709\n",
      "train loss:0.4294723352065814\n",
      "train loss:0.7431998458338949\n",
      "train loss:0.6305145670047053\n",
      "train loss:0.5785336253263884\n",
      "train loss:0.41832560701970023\n",
      "train loss:0.44476687510623447\n",
      "train loss:0.6874426306784848\n",
      "train loss:0.5973093800384557\n",
      "train loss:0.6390824517357523\n",
      "train loss:0.6095934516416361\n",
      "train loss:0.7509761801029715\n",
      "train loss:0.6542093254642178\n",
      "train loss:0.5404686763431554\n",
      "train loss:0.6332795088063572\n",
      "train loss:0.7638982107168757\n",
      "train loss:0.5305571283429089\n",
      "train loss:0.7938152209558118\n",
      "train loss:0.6390364358861144\n",
      "train loss:0.6354264701578755\n",
      "train loss:0.592699642338028\n",
      "train loss:0.7261498886704082\n",
      "train loss:0.6778754526700903\n",
      "train loss:0.654772264855097\n",
      "train loss:0.5195721439087572\n",
      "train loss:0.5596955417886018\n",
      "train loss:0.6324038605068837\n",
      "train loss:0.7356912698992089\n",
      "train loss:0.6776232345505946\n",
      "train loss:0.5976264342643234\n",
      "train loss:0.5530821019893777\n",
      "train loss:0.6379545250604197\n",
      "train loss:0.6151951727738396\n",
      "train loss:0.6485494061525354\n",
      "train loss:0.4900689055475612\n",
      "train loss:0.7130669691787509\n",
      "train loss:0.6384320117224126\n",
      "train loss:0.3621462340654464\n",
      "train loss:0.818550701816789\n",
      "train loss:0.45155396948518584\n",
      "train loss:0.5386801943611641\n",
      "train loss:0.7197622112773968\n",
      "train loss:0.7031742168729005\n",
      "train loss:0.5861397708355192\n",
      "train loss:0.7268488695355059\n",
      "train loss:0.4796258389515186\n",
      "train loss:0.6153558376762774\n",
      "train loss:0.6826068730696254\n",
      "train loss:0.6358830897977523\n",
      "train loss:0.7248609628917833\n",
      "train loss:0.5638624746734604\n",
      "train loss:0.6615680155147874\n",
      "train loss:0.7721240921435749\n",
      "train loss:0.4654049923680222\n",
      "train loss:0.3424216555567026\n",
      "train loss:0.4533768051607193\n",
      "train loss:0.7171231906402223\n",
      "train loss:0.6160358227899019\n",
      "train loss:0.3473636730423638\n",
      "train loss:0.3346986011117319\n",
      "train loss:0.8998815021536342\n",
      "train loss:0.3794422630679267\n",
      "train loss:0.6743519579326157\n",
      "train loss:0.5232584931219035\n",
      "train loss:0.6236768898345179\n",
      "train loss:0.8189707504096632\n",
      "train loss:0.3437841250430676\n",
      "train loss:0.3606286725114659\n",
      "train loss:0.729470308528771\n",
      "train loss:0.6139549493807921\n",
      "=== epoch:6, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.4935678055629782\n",
      "train loss:0.5143873067908473\n",
      "train loss:0.8818282258454083\n",
      "train loss:0.6342977876166345\n",
      "train loss:0.5466212576625442\n",
      "train loss:0.4057022719537119\n",
      "train loss:0.6346561781872359\n",
      "train loss:0.7776114041971361\n",
      "train loss:0.5376302837365645\n",
      "train loss:0.4480162468914438\n",
      "train loss:0.6034513724301995\n",
      "train loss:0.48696923948879156\n",
      "train loss:0.5892093835070653\n",
      "train loss:0.6535989098137691\n",
      "train loss:0.7642938400456173\n",
      "train loss:0.5850047252201633\n",
      "train loss:0.6459607977227819\n",
      "train loss:0.5979955146653961\n",
      "train loss:0.8284328953952491\n",
      "train loss:0.6852828880071344\n",
      "train loss:0.642435788677289\n",
      "train loss:0.5397004617258383\n",
      "train loss:0.6095330654485516\n",
      "train loss:0.6570426211526137\n",
      "train loss:0.570498684082308\n",
      "train loss:0.6917822179064991\n",
      "train loss:0.5871385831271753\n",
      "train loss:0.6246492725373284\n",
      "train loss:0.6382542010570617\n",
      "train loss:0.49834708690467977\n",
      "train loss:0.5788477848784878\n",
      "train loss:0.5734367207233468\n",
      "train loss:0.6882458372522041\n",
      "train loss:0.6001981478844671\n",
      "train loss:0.45737253857075555\n",
      "train loss:0.5972339001408492\n",
      "train loss:0.8326221356815863\n",
      "train loss:0.78661190305186\n",
      "train loss:0.5499885709911638\n",
      "train loss:0.6123798343707916\n",
      "train loss:0.4624664016692727\n",
      "train loss:0.4884400176741305\n",
      "train loss:0.647392529058878\n",
      "train loss:0.7095608254987932\n",
      "train loss:0.4359880072606771\n",
      "train loss:0.3448287260743133\n",
      "train loss:0.3609934739336199\n",
      "train loss:0.6284048873060593\n",
      "train loss:0.48963474244342403\n",
      "train loss:0.6894245862965867\n",
      "train loss:0.492836486112681\n",
      "train loss:0.6922542009880317\n",
      "train loss:0.36701909289533974\n",
      "train loss:0.5161118371854141\n",
      "train loss:0.5855644975812123\n",
      "train loss:0.7729283238706695\n",
      "train loss:0.41415522412929073\n",
      "train loss:0.3342545746893023\n",
      "train loss:0.7021292063497795\n",
      "train loss:0.5501860942867917\n",
      "train loss:0.5332188237931013\n",
      "train loss:0.38446540880361446\n",
      "train loss:0.4995637511387575\n",
      "train loss:0.6618943165117528\n",
      "train loss:0.5743545958385244\n",
      "train loss:0.5412387074673958\n",
      "train loss:0.37556983623618717\n",
      "train loss:0.7334383449543828\n",
      "train loss:0.6585207894755359\n",
      "train loss:0.5662051606022932\n",
      "train loss:0.4396866883640641\n",
      "train loss:0.4603606821544387\n",
      "train loss:0.6618613910863318\n",
      "train loss:0.6180711541152755\n",
      "train loss:0.6860093279806543\n",
      "train loss:0.5412233684690302\n",
      "train loss:0.5443859192395019\n",
      "train loss:0.4582994271536746\n",
      "train loss:0.6035127340468178\n",
      "train loss:0.6045667437884492\n",
      "train loss:0.6497060332921347\n",
      "train loss:0.5285046452958242\n",
      "train loss:0.6237878414835563\n",
      "train loss:0.6228592292916927\n",
      "train loss:0.581049178612983\n",
      "train loss:0.49716492801820794\n",
      "train loss:0.5588448712119866\n",
      "train loss:0.4769848653467542\n",
      "train loss:0.40193899053224946\n",
      "train loss:0.4582980017167494\n",
      "train loss:0.4770894028308927\n",
      "train loss:0.38995649520118353\n",
      "train loss:0.3704377471449439\n",
      "train loss:0.3576381973291539\n",
      "train loss:0.5593489655203898\n",
      "train loss:0.6986117890018034\n",
      "train loss:0.550033861780815\n",
      "train loss:0.49111190058277615\n",
      "train loss:0.8057996751356145\n",
      "train loss:0.3631410140952466\n",
      "train loss:0.4787310015796626\n",
      "train loss:0.5895171752123579\n",
      "train loss:0.5610394839424366\n",
      "train loss:0.5877246089040412\n",
      "train loss:0.6878684004237583\n",
      "train loss:0.42654182816944386\n",
      "train loss:0.464787675906301\n",
      "train loss:0.3441052462230643\n",
      "train loss:0.3201669954576086\n",
      "train loss:0.2537547216521718\n",
      "train loss:0.40783082281076244\n",
      "train loss:0.8180857985077983\n",
      "train loss:0.6445062092243758\n",
      "train loss:0.37499282010893303\n",
      "train loss:0.7087519656278496\n",
      "train loss:0.703631806383289\n",
      "train loss:0.6226763860823159\n",
      "train loss:0.5430827627144132\n",
      "train loss:0.791970900688628\n",
      "train loss:0.5325336037083966\n",
      "train loss:0.5039151720936615\n",
      "train loss:0.5184048583763409\n",
      "train loss:0.5394536105314204\n",
      "train loss:0.4811416631075375\n",
      "train loss:0.5678316787859754\n",
      "train loss:0.466708323743506\n",
      "train loss:0.4472672710978938\n",
      "train loss:0.4845212880744196\n",
      "train loss:0.4080074116037983\n",
      "train loss:0.6518837080958796\n",
      "train loss:0.62139386027252\n",
      "train loss:0.7363675486698152\n",
      "train loss:0.6970348250545413\n",
      "train loss:0.6218690526290602\n",
      "train loss:0.5042947275365877\n",
      "train loss:0.4839997222764227\n",
      "train loss:0.7339406744359557\n",
      "train loss:0.5652688993919647\n",
      "train loss:0.6139345279141601\n",
      "train loss:0.45851022326610086\n",
      "train loss:0.547902475298898\n",
      "train loss:0.6657504816709403\n",
      "train loss:0.5409944854654093\n",
      "train loss:0.4790408791129475\n",
      "train loss:0.6388030026029659\n",
      "train loss:0.7215536452911051\n",
      "train loss:0.7699629099723367\n",
      "train loss:0.4050698728429084\n",
      "train loss:0.7419620507922126\n",
      "train loss:0.7341075703137906\n",
      "train loss:0.5110836574681459\n",
      "train loss:0.7184302533916831\n",
      "train loss:0.5592821939396211\n",
      "=== epoch:7, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5627563007280841\n",
      "train loss:0.5355039961480047\n",
      "train loss:0.7649462434753476\n",
      "train loss:0.47671673455145475\n",
      "train loss:0.45475877322348957\n",
      "train loss:0.6930116250168469\n",
      "train loss:0.72385671266157\n",
      "train loss:0.5507543762536009\n",
      "train loss:0.6078845899030074\n",
      "train loss:0.44693415266742775\n",
      "train loss:0.6427755653209009\n",
      "train loss:0.5382581767637153\n",
      "train loss:0.5330150447246377\n",
      "train loss:0.5362295459932194\n",
      "train loss:0.6570179203344643\n",
      "train loss:0.4771649791111616\n",
      "train loss:0.5095326428841208\n",
      "train loss:0.4552043380935748\n",
      "train loss:0.26316932683475136\n",
      "train loss:0.36542247171357667\n",
      "train loss:0.6272678180238805\n",
      "train loss:0.6066633973916198\n",
      "train loss:0.5540007597692387\n",
      "train loss:0.5414156220879836\n",
      "train loss:0.7763774798082954\n",
      "train loss:0.5332432436687802\n",
      "train loss:0.8034496084550437\n",
      "train loss:0.6692014874284758\n",
      "train loss:0.48185852112620353\n",
      "train loss:0.6767825199570804\n",
      "train loss:0.5983296333640833\n",
      "train loss:0.55388335359678\n",
      "train loss:0.7880585478321153\n",
      "train loss:0.5590434832820141\n",
      "train loss:0.5422105098837015\n",
      "train loss:0.41939188022546975\n",
      "train loss:0.46404423952042534\n",
      "train loss:0.5405239660864933\n",
      "train loss:0.5602008534594652\n",
      "train loss:0.3672283994384139\n",
      "train loss:0.7432074937907209\n",
      "train loss:0.7120824406145376\n",
      "train loss:0.5122659327597072\n",
      "train loss:0.7241554164201268\n",
      "train loss:0.7443345077286058\n",
      "train loss:0.42163042492351976\n",
      "train loss:0.6394832648262041\n",
      "train loss:0.5063564556952426\n",
      "train loss:0.7460363270062549\n",
      "train loss:0.4026930828437772\n",
      "train loss:0.6065369675748161\n",
      "train loss:0.4476520596991663\n",
      "train loss:0.4785546672763147\n",
      "train loss:0.6069724295254886\n",
      "train loss:0.38300660993671387\n",
      "train loss:0.5307708010968746\n",
      "train loss:0.5869516128074675\n",
      "train loss:0.5133567600438373\n",
      "train loss:0.8354286297612449\n",
      "train loss:0.5809440408567301\n",
      "train loss:0.6273497144861154\n",
      "train loss:0.5479682092167935\n",
      "train loss:0.43487694215098527\n",
      "train loss:0.592512776118564\n",
      "train loss:0.6826507676321223\n",
      "train loss:0.4889726028998749\n",
      "train loss:0.6040217904237366\n",
      "train loss:0.7008070528263167\n",
      "train loss:0.4537167128762377\n",
      "train loss:0.5742342516788461\n",
      "train loss:0.5858312866045841\n",
      "train loss:0.7309858225620176\n",
      "train loss:0.5609643814352301\n",
      "train loss:0.6388914000185173\n",
      "train loss:0.42882697799558533\n",
      "train loss:0.34872665243161177\n",
      "train loss:0.5113770444223102\n",
      "train loss:0.6909779565980625\n",
      "train loss:0.4954951780854208\n",
      "train loss:0.430894804437073\n",
      "train loss:0.7203003663855531\n",
      "train loss:0.7031556611434319\n",
      "train loss:0.5273574613725959\n",
      "train loss:0.7003632973322766\n",
      "train loss:0.38890479288661967\n",
      "train loss:0.5716036432755421\n",
      "train loss:0.5167077915023626\n",
      "train loss:0.4513376309626735\n",
      "train loss:0.39925880206371545\n",
      "train loss:0.5092115695444435\n",
      "train loss:0.966315803675163\n",
      "train loss:0.4005892540574165\n",
      "train loss:0.5268800011213897\n",
      "train loss:0.5081718487000692\n",
      "train loss:0.8136521860623681\n",
      "train loss:0.7611406240803712\n",
      "train loss:0.7061320401276834\n",
      "train loss:0.4975216752360677\n",
      "train loss:0.4080466785303292\n",
      "train loss:0.6304042102518401\n",
      "train loss:0.5016021184355766\n",
      "train loss:0.6292299574042427\n",
      "train loss:0.6136678107338842\n",
      "train loss:0.6516426727912104\n",
      "train loss:0.4882364963986702\n",
      "train loss:0.7828784757981726\n",
      "train loss:0.611207262939279\n",
      "train loss:0.6549420893651453\n",
      "train loss:0.4357658078520924\n",
      "train loss:0.6716831789923392\n",
      "train loss:0.5527519105634601\n",
      "train loss:0.713774812941433\n",
      "train loss:0.5477890236873018\n",
      "train loss:0.643893458771075\n",
      "train loss:0.4750635199792341\n",
      "train loss:0.631839119458134\n",
      "train loss:0.6978804514537255\n",
      "train loss:0.45838705666584884\n",
      "train loss:0.5005760573552711\n",
      "train loss:0.6394592380562681\n",
      "train loss:0.4751668818420218\n",
      "train loss:0.29524413754571943\n",
      "train loss:0.44132936208830387\n",
      "train loss:0.8631452658262326\n",
      "train loss:0.517617474855792\n",
      "train loss:0.6370020618682328\n",
      "train loss:0.5798746458362215\n",
      "train loss:0.3281222802335541\n",
      "train loss:0.3410853043171047\n",
      "train loss:0.34681523913280604\n",
      "train loss:0.6464278599722789\n",
      "train loss:0.35215592530434037\n",
      "train loss:0.5018126940000327\n",
      "train loss:0.30936362055064204\n",
      "train loss:0.9059972546439041\n",
      "train loss:0.4931755082115289\n",
      "train loss:0.18342042186233257\n",
      "train loss:0.4870164308677317\n",
      "train loss:0.5076960450279749\n",
      "train loss:0.7715930040967158\n",
      "train loss:0.7612602384339786\n",
      "train loss:0.5455756278597899\n",
      "train loss:0.6245534367303165\n",
      "train loss:0.637267615329168\n",
      "train loss:0.4973950201482973\n",
      "train loss:0.40159867329202825\n",
      "train loss:0.4455024139238418\n",
      "train loss:0.7824206403329258\n",
      "train loss:0.5305265288841171\n",
      "train loss:0.5903489389842697\n",
      "train loss:0.5254675545689691\n",
      "train loss:0.5938473245211745\n",
      "train loss:0.6394486233627802\n",
      "=== epoch:8, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.45406412355436343\n",
      "train loss:0.6864421965244727\n",
      "train loss:0.6316110004892632\n",
      "train loss:0.583732062730496\n",
      "train loss:0.39067779179823764\n",
      "train loss:0.6183285382833204\n",
      "train loss:0.5515417067497383\n",
      "train loss:0.4984438979542287\n",
      "train loss:0.49179930216930445\n",
      "train loss:0.6602546064749468\n",
      "train loss:0.3676448794680976\n",
      "train loss:0.6405155305242368\n",
      "train loss:0.41504344603724774\n",
      "train loss:0.6167589364146533\n",
      "train loss:0.8542571032727257\n",
      "train loss:0.4956755471260973\n",
      "train loss:0.36070186020886263\n",
      "train loss:0.22231703598364305\n",
      "train loss:0.584715567701922\n",
      "train loss:0.46641930171366736\n",
      "train loss:0.6180109803081301\n",
      "train loss:0.6172434859871203\n",
      "train loss:0.39594345296927685\n",
      "train loss:0.6241565840158628\n",
      "train loss:0.6081600096696823\n",
      "train loss:0.3278741961309243\n",
      "train loss:0.5325404052442881\n",
      "train loss:0.5468391956591421\n",
      "train loss:0.5317272660331046\n",
      "train loss:0.6530639832941222\n",
      "train loss:0.5287714349296936\n",
      "train loss:0.5807245877519251\n",
      "train loss:0.7377544088384409\n",
      "train loss:0.3763051604553831\n",
      "train loss:0.5376889648429771\n",
      "train loss:0.5915027754322548\n",
      "train loss:0.5961771888512847\n",
      "train loss:0.6152542647554415\n",
      "train loss:0.6175936315517665\n",
      "train loss:0.7616048092537847\n",
      "train loss:0.5555888920227011\n",
      "train loss:0.4779941899836836\n",
      "train loss:0.7262813124994538\n",
      "train loss:0.633994015243573\n",
      "train loss:0.674625176357486\n",
      "train loss:0.535448254833285\n",
      "train loss:0.5828471869638501\n",
      "train loss:0.6496680959236306\n",
      "train loss:0.5043234809695513\n",
      "train loss:0.6419729281100148\n",
      "train loss:0.37336746015867667\n",
      "train loss:0.5431818828901814\n",
      "train loss:0.6270616750202245\n",
      "train loss:0.6304332344830739\n",
      "train loss:0.5842455963751504\n",
      "train loss:0.6817146237780565\n",
      "train loss:0.7019138779039293\n",
      "train loss:0.5158740372916308\n",
      "train loss:0.5895597339755227\n",
      "train loss:0.5688554306916521\n",
      "train loss:0.4843893706995742\n",
      "train loss:0.470034016524896\n",
      "train loss:0.37221657426493604\n",
      "train loss:0.5033908914694064\n",
      "train loss:0.6339052018752154\n",
      "train loss:0.6721705382944767\n",
      "train loss:0.6593907057882407\n",
      "train loss:0.5351382399435348\n",
      "train loss:0.6688867134956075\n",
      "train loss:1.0472140786485773\n",
      "train loss:0.7265475865201384\n",
      "train loss:0.3759346997094369\n",
      "train loss:0.5988151574043127\n",
      "train loss:0.5964829114336132\n",
      "train loss:0.621139962155015\n",
      "train loss:0.6270005317229645\n",
      "train loss:0.6228753288977471\n",
      "train loss:0.5158363393854566\n",
      "train loss:0.5258270115834012\n",
      "train loss:0.4953269900302292\n",
      "train loss:0.4414858118846955\n",
      "train loss:0.531546927910083\n",
      "train loss:0.7067517338154556\n",
      "train loss:0.5708122237654638\n",
      "train loss:0.5827298604700837\n",
      "train loss:0.5623036486579231\n",
      "train loss:0.6434141728776649\n",
      "train loss:0.7604763215390815\n",
      "train loss:0.41849019572782636\n",
      "train loss:0.37835256740241724\n",
      "train loss:0.6900508865592035\n",
      "train loss:0.43473675137339984\n",
      "train loss:0.3895463611859031\n",
      "train loss:0.3815995787705925\n",
      "train loss:0.7943121968719294\n",
      "train loss:0.33384265697498683\n",
      "train loss:0.4859471415646345\n",
      "train loss:0.5155540152230428\n",
      "train loss:0.6628836133713742\n",
      "train loss:0.4556295991181857\n",
      "train loss:0.43951356374961764\n",
      "train loss:0.6972111174544494\n",
      "train loss:0.6840605210327138\n",
      "train loss:0.29620061701104783\n",
      "train loss:0.5791630765346368\n",
      "train loss:0.44753108456405977\n",
      "train loss:0.5317012209275458\n",
      "train loss:0.5039178829137997\n",
      "train loss:0.5217765265631726\n",
      "train loss:0.6486737084246778\n",
      "train loss:0.4423665237974541\n",
      "train loss:0.6212351368484683\n",
      "train loss:0.7073447640468282\n",
      "train loss:0.6121060069406971\n",
      "train loss:0.5628177706395567\n",
      "train loss:0.5219497487536195\n",
      "train loss:0.44498267902539357\n",
      "train loss:0.5751338432449178\n",
      "train loss:0.67383217513651\n",
      "train loss:0.5241513717326691\n",
      "train loss:0.5748512074684873\n",
      "train loss:0.6346132061047219\n",
      "train loss:0.4851109302325659\n",
      "train loss:0.7290884977698308\n",
      "train loss:0.40196907272148313\n",
      "train loss:0.44789667384844717\n",
      "train loss:0.5873885992074821\n",
      "train loss:0.6219347658536882\n",
      "train loss:0.454552527940721\n",
      "train loss:0.4401280656261483\n",
      "train loss:0.32405348083874913\n",
      "train loss:0.4923112419493895\n",
      "train loss:0.7364312710268646\n",
      "train loss:0.7341192747079428\n",
      "train loss:0.41270451391127877\n",
      "train loss:0.5807657284859109\n",
      "train loss:0.48432879299872483\n",
      "train loss:0.6146491429472849\n",
      "train loss:0.7340242851834555\n",
      "train loss:0.8293430189344001\n",
      "train loss:0.3277651836555001\n",
      "train loss:0.4817333795180617\n",
      "train loss:0.6154901968651172\n",
      "train loss:0.43487280929807925\n",
      "train loss:0.8166042281976749\n",
      "train loss:0.43979395084465783\n",
      "train loss:0.3906239518819835\n",
      "train loss:0.6490366691176392\n",
      "train loss:0.6238110369001223\n",
      "train loss:0.3723507653649925\n",
      "train loss:0.704400282593513\n",
      "train loss:0.4956983412889736\n",
      "train loss:0.7830859376173971\n",
      "=== epoch:9, train acc:0.75, test acc:0.69 ===\n",
      "train loss:0.3741709363484198\n",
      "train loss:0.4303229287613295\n",
      "train loss:0.2886179241830721\n",
      "train loss:0.6620955675179677\n",
      "train loss:0.3284700175841159\n",
      "train loss:0.900059666291772\n",
      "train loss:0.6335084787382647\n",
      "train loss:0.7500942102168449\n",
      "train loss:0.31117404599520054\n",
      "train loss:0.4076810559701814\n",
      "train loss:0.3867111546062126\n",
      "train loss:0.4862215410101743\n",
      "train loss:0.50965546887979\n",
      "train loss:0.7272247485350065\n",
      "train loss:0.46056057199978895\n",
      "train loss:0.48038697678586495\n",
      "train loss:0.7339049952921641\n",
      "train loss:0.714313413337681\n",
      "train loss:0.8256769187638777\n",
      "train loss:0.625616847404095\n",
      "train loss:0.4850157256424115\n",
      "train loss:0.535133705867304\n",
      "train loss:0.5390375240008269\n",
      "train loss:0.6676281857359487\n",
      "train loss:0.5854633512824952\n",
      "train loss:0.5949761256464287\n",
      "train loss:0.588732191122438\n",
      "train loss:0.5300525358025978\n",
      "train loss:0.44090495184221706\n",
      "train loss:0.5765018407456981\n",
      "train loss:0.4528553610267874\n",
      "train loss:0.42178949291831325\n",
      "train loss:0.5405622152357141\n",
      "train loss:0.7159082762865447\n",
      "train loss:0.45932197481710346\n",
      "train loss:0.5309596569695593\n",
      "train loss:0.28288836345725354\n",
      "train loss:0.7209601664446805\n",
      "train loss:0.681562931426719\n",
      "train loss:0.5910889786271227\n",
      "train loss:0.3818405193727793\n",
      "train loss:0.3577558185583277\n",
      "train loss:0.3121007268373094\n",
      "train loss:0.6169866852470103\n",
      "train loss:0.5118560828474891\n",
      "train loss:0.35042187311983336\n",
      "train loss:0.35727268309570304\n",
      "train loss:0.5847765179805167\n",
      "train loss:0.7583142015710795\n",
      "train loss:0.49174799339125086\n",
      "train loss:0.6511550764264087\n",
      "train loss:0.6243930568752394\n",
      "train loss:0.24763566137996312\n",
      "train loss:0.5114901979152533\n",
      "train loss:0.7823204063591105\n",
      "train loss:0.6026207831992011\n",
      "train loss:0.6447624283932333\n",
      "train loss:0.5209630604885424\n",
      "train loss:0.46217643892551796\n",
      "train loss:0.7806275127369788\n",
      "train loss:0.5661775416960715\n",
      "train loss:0.42003398533210945\n",
      "train loss:0.7959412685939562\n",
      "train loss:0.6152880230731934\n",
      "train loss:0.5212067635153498\n",
      "train loss:0.6172355929282907\n",
      "train loss:0.5534700390358991\n",
      "train loss:0.4038306242201732\n",
      "train loss:0.46691092490682184\n",
      "train loss:0.3987077010516528\n",
      "train loss:0.5083032712080086\n",
      "train loss:0.47712360404235576\n",
      "train loss:0.49500025858810004\n",
      "train loss:0.34021020013489833\n",
      "train loss:0.4503840022872968\n",
      "train loss:0.9319340808764863\n",
      "train loss:0.5182108168355913\n",
      "train loss:0.5196703979876576\n",
      "train loss:0.6915224643599501\n",
      "train loss:0.38908456650905887\n",
      "train loss:0.4710177600544057\n",
      "train loss:0.5925386792950951\n",
      "train loss:0.8050210473378094\n",
      "train loss:0.6018754488727236\n",
      "train loss:0.571460552880003\n",
      "train loss:0.6875892960834511\n",
      "train loss:0.39394310287451684\n",
      "train loss:0.5130440178525609\n",
      "train loss:0.49825382455146566\n",
      "train loss:0.4174784802422395\n",
      "train loss:0.5596064689390262\n",
      "train loss:0.6098162195290323\n",
      "train loss:0.46256270842727903\n",
      "train loss:0.5806547640796781\n",
      "train loss:0.9954355046715758\n",
      "train loss:0.6757084524488148\n",
      "train loss:0.4260836874596152\n",
      "train loss:0.6051937856261194\n",
      "train loss:0.4610879309943538\n",
      "train loss:0.5701699516051362\n",
      "train loss:0.39741421169244084\n",
      "train loss:0.30955074289260776\n",
      "train loss:0.4987829848058637\n",
      "train loss:0.6209711179462357\n",
      "train loss:0.3667454068669646\n",
      "train loss:0.35326754949473954\n",
      "train loss:0.5326244174474817\n",
      "train loss:0.6713469721102462\n",
      "train loss:0.575803605004755\n",
      "train loss:0.35312620525144733\n",
      "train loss:0.7306920875885174\n",
      "train loss:0.6438393252548484\n",
      "train loss:0.6074814333794711\n",
      "train loss:0.7707519935482848\n",
      "train loss:0.5080516381498568\n",
      "train loss:0.6694090083409817\n",
      "train loss:0.6100640123487114\n",
      "train loss:0.7253891535548717\n",
      "train loss:0.5775258155151108\n",
      "train loss:0.5628324373058158\n",
      "train loss:0.5625623848033264\n",
      "train loss:0.5257588046777973\n",
      "train loss:0.6264364269194661\n",
      "train loss:0.5736171212543538\n",
      "train loss:0.3620801331185198\n",
      "train loss:0.5097864958458257\n",
      "train loss:0.5842331581282311\n",
      "train loss:0.6515602186420816\n",
      "train loss:0.6078216490014849\n",
      "train loss:0.5214785096900244\n",
      "train loss:0.4795331289243877\n",
      "train loss:0.56273612513418\n",
      "train loss:0.689166540974985\n",
      "train loss:0.5417298438901589\n",
      "train loss:0.4953833901499037\n",
      "train loss:0.4856454786249381\n",
      "train loss:0.5041739538997342\n",
      "train loss:0.6222590038131445\n",
      "train loss:0.6225960855709785\n",
      "train loss:0.46121395134795673\n",
      "train loss:0.512403769970551\n",
      "train loss:0.6668624886297116\n",
      "train loss:0.5856776547192937\n",
      "train loss:0.9991396824596356\n",
      "train loss:0.6616371666321736\n",
      "train loss:0.5454580347940824\n",
      "train loss:0.45916951179826243\n",
      "train loss:0.4970591127216898\n",
      "train loss:0.5358529601609927\n",
      "train loss:0.4569358961245958\n",
      "train loss:0.582507600307387\n",
      "train loss:0.5221179516126971\n",
      "train loss:0.5086692392906559\n",
      "=== epoch:10, train acc:0.73, test acc:0.69 ===\n",
      "train loss:0.7015754292116992\n",
      "train loss:0.804789100345699\n",
      "train loss:0.5746183539737802\n",
      "train loss:0.4412337369938838\n",
      "train loss:0.6036396497689946\n",
      "train loss:0.4769261869506458\n",
      "train loss:0.616370784367656\n",
      "train loss:0.5271979838865343\n",
      "train loss:0.4542842657564393\n",
      "train loss:0.3821814095226377\n",
      "train loss:0.7743766056877991\n",
      "train loss:0.8621687712501214\n",
      "train loss:0.7511187874962154\n",
      "train loss:0.4442357945991403\n",
      "train loss:0.5692756340429922\n",
      "train loss:0.5317041774193656\n",
      "train loss:0.57143870352636\n",
      "train loss:0.6092021632276448\n",
      "train loss:0.7507048732931301\n",
      "train loss:0.6284459267864901\n",
      "train loss:0.5280517260905849\n",
      "train loss:0.6206178111482811\n",
      "train loss:0.6855875639168543\n",
      "train loss:0.5715551742839198\n",
      "train loss:0.4720950508213152\n",
      "train loss:0.5056393994480312\n",
      "train loss:0.5722950198442799\n",
      "train loss:0.7957359309489942\n",
      "train loss:0.49590639564062444\n",
      "train loss:0.6369062326333678\n",
      "train loss:0.594483397471575\n",
      "train loss:0.7174060457756262\n",
      "train loss:0.7286265101275549\n",
      "train loss:0.5877053745406388\n",
      "train loss:0.5670413519072326\n",
      "train loss:0.4288818751462221\n",
      "train loss:0.5211144055166249\n",
      "train loss:0.537701785226524\n",
      "train loss:0.45950507526808193\n",
      "train loss:0.438349451613728\n",
      "train loss:0.29711392397440606\n",
      "train loss:0.523435579374295\n",
      "train loss:0.685201277970147\n",
      "train loss:0.6847753252871311\n",
      "train loss:0.551164087279715\n",
      "train loss:0.47271600859301943\n",
      "train loss:0.534066263062934\n",
      "train loss:0.46076734219705245\n",
      "train loss:0.3390472280310003\n",
      "train loss:0.5021550238775236\n",
      "train loss:0.6533588495955788\n",
      "train loss:0.22955167501411808\n",
      "train loss:0.5090223641153766\n",
      "train loss:0.587949536536151\n",
      "train loss:0.5950556812045595\n",
      "train loss:0.560316897574156\n",
      "train loss:0.4509163881245156\n",
      "train loss:0.69046770448019\n",
      "train loss:0.4930279262925355\n",
      "train loss:0.2778063467705915\n",
      "train loss:0.49929425083802415\n",
      "train loss:0.7288928280383962\n",
      "train loss:0.516129523331782\n",
      "train loss:0.48127582306259126\n",
      "train loss:0.36507633496629766\n",
      "train loss:0.5299271169759999\n",
      "train loss:0.4734373042720117\n",
      "train loss:0.7284016357441251\n",
      "train loss:0.2505939995387326\n",
      "train loss:0.5087174565878124\n",
      "train loss:0.5136631240332771\n",
      "train loss:0.6014178208465972\n",
      "train loss:0.583261405471719\n",
      "train loss:0.6980891837897368\n",
      "train loss:0.2423787928884323\n",
      "train loss:0.7864993503910773\n",
      "train loss:0.4684481384544942\n",
      "train loss:0.4543163632616635\n",
      "train loss:0.7498688627119942\n",
      "train loss:0.47727315836107176\n",
      "train loss:0.7178092788653624\n",
      "train loss:0.41093564718000836\n",
      "train loss:0.5912444637401144\n",
      "train loss:0.4494176790647136\n",
      "train loss:0.4413882718970946\n",
      "train loss:0.6979035410925308\n",
      "train loss:0.6922843500160761\n",
      "train loss:0.49283088346021053\n",
      "train loss:0.5926694566252932\n",
      "train loss:0.5296225576333551\n",
      "train loss:0.39861597940358917\n",
      "train loss:0.6122978128747281\n",
      "train loss:0.3880403127968947\n",
      "train loss:0.48535524958328136\n",
      "train loss:0.606273626325248\n",
      "train loss:0.5586274976105151\n",
      "train loss:0.7080096830791435\n",
      "train loss:0.5438471650120371\n",
      "train loss:0.6976612056969687\n",
      "train loss:0.576824888400141\n",
      "train loss:0.7448886100824068\n",
      "train loss:0.5847675746056614\n",
      "train loss:0.5018173671331632\n",
      "train loss:0.5096097410874127\n",
      "train loss:0.3983478443206242\n",
      "train loss:0.37019931061721023\n",
      "train loss:0.567265375727815\n",
      "train loss:0.2876097925653921\n",
      "train loss:0.5598750065489834\n",
      "train loss:0.7900450693140568\n",
      "train loss:0.3861720643846308\n",
      "train loss:0.6228561360172975\n",
      "train loss:0.6262941544678157\n",
      "train loss:0.7207678878185579\n",
      "train loss:0.6498205762495168\n",
      "train loss:0.5037292402535518\n",
      "train loss:0.654964177154506\n",
      "train loss:0.46058117454291203\n",
      "train loss:0.6258327367594134\n",
      "train loss:0.7223864899397171\n",
      "train loss:0.586160027704099\n",
      "train loss:0.6567508371270528\n",
      "train loss:0.5872497660156852\n",
      "train loss:0.6417932096927765\n",
      "train loss:0.5998196776458422\n",
      "train loss:0.5337698881696388\n",
      "train loss:0.5957600586984608\n",
      "train loss:0.6106634872617654\n",
      "train loss:0.5356779474068981\n",
      "train loss:0.40164378122600974\n",
      "train loss:0.6973092381270312\n",
      "train loss:0.5126213580040656\n",
      "train loss:0.560743754848369\n",
      "train loss:0.6518805540450675\n",
      "train loss:0.45171805097295703\n",
      "train loss:0.5743767674898559\n",
      "train loss:0.39875860335275426\n",
      "train loss:0.249911200739092\n",
      "train loss:0.23422256484645804\n",
      "train loss:0.4320394546893914\n",
      "train loss:0.7268254094132197\n",
      "train loss:0.4527461495773125\n",
      "train loss:0.3448707950929075\n",
      "train loss:0.9238708683282295\n",
      "train loss:0.34279089030822285\n",
      "train loss:0.48934511696546173\n",
      "train loss:0.49555226378679257\n",
      "train loss:0.8184612763766008\n",
      "train loss:0.4704642624288414\n",
      "train loss:0.598981588982604\n",
      "train loss:0.48779130972611506\n",
      "train loss:0.5265469191475763\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5450980392156862\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 110, 'filter_size': 7, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9947de55-1982-483f-9911-0f226e622bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6911749339625759\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6903336096113669\n",
      "train loss:0.6795833979723682\n",
      "train loss:0.6926419595421216\n",
      "train loss:0.6410342170984362\n",
      "train loss:0.6169564096246152\n",
      "train loss:0.673437594305627\n",
      "train loss:0.5616706981997166\n",
      "train loss:0.6916094283574282\n",
      "train loss:0.6860281997290893\n",
      "train loss:0.4207828419232099\n",
      "train loss:0.626932099861168\n",
      "train loss:0.8259540896017\n",
      "train loss:0.4323317688561974\n",
      "train loss:0.5095674807883015\n",
      "train loss:0.8540933145415158\n",
      "train loss:0.6059391515413177\n",
      "train loss:0.4196633183608385\n",
      "train loss:0.6126151586912271\n",
      "train loss:0.6190351554658757\n",
      "train loss:0.682590004600151\n",
      "train loss:0.6264260867446012\n",
      "train loss:0.7363405146595927\n",
      "train loss:0.4756560052989056\n",
      "train loss:0.7615529572768612\n",
      "train loss:0.5576271428294965\n",
      "train loss:0.5535321043806887\n",
      "train loss:0.6805571902216443\n",
      "train loss:0.7107271392359648\n",
      "train loss:0.5047614666792459\n",
      "train loss:0.5472358590001405\n",
      "train loss:0.5591413238684126\n",
      "train loss:0.6579168919326334\n",
      "train loss:0.6852315391326665\n",
      "train loss:0.6638584047218697\n",
      "train loss:0.5298458639391754\n",
      "train loss:0.613454926971247\n",
      "train loss:0.5438251119551923\n",
      "train loss:0.6799261746275655\n",
      "train loss:0.6183730008773215\n",
      "train loss:0.8853652098178809\n",
      "train loss:0.539140876981814\n",
      "train loss:0.5380849001940032\n",
      "train loss:0.4555145640231755\n",
      "train loss:0.5161434847855508\n",
      "train loss:0.40999979601480163\n",
      "train loss:0.7259524766795525\n",
      "train loss:0.6085992774347319\n",
      "train loss:0.4087543775002816\n",
      "train loss:0.9279762746178596\n",
      "train loss:0.8585869905690549\n",
      "train loss:0.5872151970593327\n",
      "train loss:0.5194081852122248\n",
      "train loss:0.7418861325865639\n",
      "train loss:0.5591197238106498\n",
      "train loss:0.7526402563609429\n",
      "train loss:0.4979362258949281\n",
      "train loss:0.5013933396669733\n",
      "train loss:0.5405771937086611\n",
      "train loss:0.46061869302441083\n",
      "train loss:0.5255802847740012\n",
      "train loss:0.41235488460052305\n",
      "train loss:0.3060636777298833\n",
      "train loss:0.39526845896835466\n",
      "train loss:0.4947052797430628\n",
      "train loss:0.6911014676803772\n",
      "train loss:0.44543076753803873\n",
      "train loss:0.7419466116459688\n",
      "train loss:0.8786146056093376\n",
      "train loss:0.729814106710087\n",
      "train loss:0.4740742270205985\n",
      "train loss:0.35403186236167933\n",
      "train loss:0.38422995151137473\n",
      "train loss:0.7594173905252741\n",
      "train loss:0.8318375463936226\n",
      "train loss:0.49776784377144434\n",
      "train loss:0.705507573543749\n",
      "train loss:0.7277451469219215\n",
      "train loss:0.6168061736190126\n",
      "train loss:0.6964523762364929\n",
      "train loss:0.6027194453883953\n",
      "train loss:0.6750645701930231\n",
      "train loss:0.5605336840497388\n",
      "train loss:0.5952282198138023\n",
      "train loss:0.543617038362318\n",
      "train loss:0.5440852705118526\n",
      "train loss:0.5760749443978546\n",
      "train loss:0.6186047414396944\n",
      "train loss:0.7273995563739392\n",
      "train loss:0.6750060503219624\n",
      "train loss:0.6127303786876868\n",
      "train loss:0.6063603608368304\n",
      "train loss:0.6936250184144844\n",
      "train loss:0.6114866475170245\n",
      "train loss:0.5920191228218545\n",
      "train loss:0.5156764536999743\n",
      "train loss:0.815395242096655\n",
      "train loss:0.7657424774993639\n",
      "train loss:0.6806691210100577\n",
      "train loss:0.47014599928599266\n",
      "train loss:0.45156700857994087\n",
      "train loss:0.55353895010161\n",
      "train loss:0.7652639582470686\n",
      "train loss:0.5127944793081838\n",
      "train loss:0.49605938291912655\n",
      "train loss:0.4975743813666055\n",
      "train loss:0.5053879462612889\n",
      "train loss:0.8450713035326702\n",
      "train loss:0.7353857508183026\n",
      "train loss:0.5510887750840594\n",
      "train loss:0.9548758913791371\n",
      "train loss:0.5081417855115178\n",
      "train loss:0.6134383275549629\n",
      "train loss:0.704652552612049\n",
      "train loss:0.6124607247430989\n",
      "train loss:0.607025503040801\n",
      "train loss:0.6837869862410967\n",
      "train loss:0.734778884247453\n",
      "train loss:0.6186593087708862\n",
      "train loss:0.7777556770097704\n",
      "train loss:0.6736546023088092\n",
      "train loss:0.5445474626468982\n",
      "train loss:0.4985994058586387\n",
      "train loss:0.6690232910897549\n",
      "train loss:0.577281258262064\n",
      "train loss:0.6606198511437222\n",
      "train loss:0.5198206112425023\n",
      "train loss:0.6170729777164242\n",
      "train loss:0.5578300521765616\n",
      "train loss:0.7416285480591436\n",
      "train loss:0.5440466218866824\n",
      "train loss:0.6184299849196429\n",
      "train loss:0.4562550585345388\n",
      "train loss:0.7851186829593637\n",
      "train loss:0.5116093488117132\n",
      "train loss:0.3945465894693856\n",
      "train loss:0.5910810512598882\n",
      "train loss:0.504613139179602\n",
      "train loss:0.9565883784123386\n",
      "train loss:0.4837729126174667\n",
      "train loss:0.486412218344204\n",
      "train loss:0.6217804356970134\n",
      "train loss:0.38630599145027794\n",
      "train loss:0.9100083405288728\n",
      "train loss:0.6214362386814904\n",
      "train loss:0.4357636966270685\n",
      "train loss:0.5415212043565412\n",
      "train loss:0.5058035354816064\n",
      "train loss:0.7547767353274153\n",
      "train loss:0.5969080868042742\n",
      "train loss:0.6005941906513439\n",
      "train loss:0.5305258659239239\n",
      "train loss:0.6317049505652866\n",
      "train loss:0.6863288910109705\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6161223675252859\n",
      "train loss:0.6196167795390087\n",
      "train loss:0.7204234672946526\n",
      "train loss:0.6123929545233986\n",
      "train loss:0.7390731862778968\n",
      "train loss:0.5705155590343064\n",
      "train loss:0.49665134841313197\n",
      "train loss:0.6896839517340703\n",
      "train loss:0.7200634559623306\n",
      "train loss:0.6131351354507861\n",
      "train loss:0.7315300300225193\n",
      "train loss:0.5536476177629132\n",
      "train loss:0.5005714611807541\n",
      "train loss:0.5404532823409285\n",
      "train loss:0.6276401716943927\n",
      "train loss:0.7468805223104692\n",
      "train loss:0.536669537780579\n",
      "train loss:0.42925508121567174\n",
      "train loss:0.6184906948260785\n",
      "train loss:0.7057074403087453\n",
      "train loss:0.510178159211646\n",
      "train loss:0.7375406407440547\n",
      "train loss:0.6075295984522124\n",
      "train loss:0.5215523249840988\n",
      "train loss:0.5887843289654178\n",
      "train loss:0.5099872651192495\n",
      "train loss:0.7248182673582454\n",
      "train loss:0.8025151398998271\n",
      "train loss:0.5815408870627543\n",
      "train loss:0.518462685874207\n",
      "train loss:0.6875763832776789\n",
      "train loss:0.6073840110904414\n",
      "train loss:0.6134861215836795\n",
      "train loss:0.6278430359472024\n",
      "train loss:0.7492054761979725\n",
      "train loss:0.5384558779480868\n",
      "train loss:0.47688124718446756\n",
      "train loss:0.6977930048689039\n",
      "train loss:0.6173398357779037\n",
      "train loss:0.6756133812153317\n",
      "train loss:0.48319520068901295\n",
      "train loss:0.5412004443802048\n",
      "train loss:0.62049792376354\n",
      "train loss:0.6792196071138198\n",
      "train loss:0.43574353851933373\n",
      "train loss:0.6935465154070987\n",
      "train loss:0.38796587342165334\n",
      "train loss:0.2980326455395405\n",
      "train loss:0.7400498633104959\n",
      "train loss:0.3786811978058605\n",
      "train loss:0.6305734090361086\n",
      "train loss:0.4560921924399466\n",
      "train loss:0.49430738346066516\n",
      "train loss:0.6416301445562487\n",
      "train loss:0.8059029639444487\n",
      "train loss:0.7147901542780737\n",
      "train loss:0.40051829649842824\n",
      "train loss:0.23659518565355647\n",
      "train loss:0.5201715316400054\n",
      "train loss:0.6404518414377823\n",
      "train loss:0.38909585259134616\n",
      "train loss:0.7511438206217982\n",
      "train loss:0.5067934939059108\n",
      "train loss:0.7309820753738097\n",
      "train loss:0.740631440250587\n",
      "train loss:0.6104547171261671\n",
      "train loss:0.4975030507245396\n",
      "train loss:0.57880850705983\n",
      "train loss:0.6832790132809687\n",
      "train loss:0.5659041547641953\n",
      "train loss:0.4810450101268292\n",
      "train loss:0.38961276384234944\n",
      "train loss:0.6077557437067249\n",
      "train loss:0.5968559119205348\n",
      "train loss:0.569052507172908\n",
      "train loss:0.38676697404744054\n",
      "train loss:0.6142395060046504\n",
      "train loss:0.2694622183647967\n",
      "train loss:0.5270321675954499\n",
      "train loss:0.34307902124450734\n",
      "train loss:0.9498553282673493\n",
      "train loss:0.6602311618639732\n",
      "train loss:0.34538786180615494\n",
      "train loss:0.8022254244788106\n",
      "train loss:0.8690459056862855\n",
      "train loss:0.6960115320542856\n",
      "train loss:0.6975327176184974\n",
      "train loss:0.5316124083766686\n",
      "train loss:0.5356073044265005\n",
      "train loss:0.5528462838945437\n",
      "train loss:0.5389898687652126\n",
      "train loss:0.7022583276205481\n",
      "train loss:0.6149317531518321\n",
      "train loss:0.6151894789825957\n",
      "train loss:0.5597881344548299\n",
      "train loss:0.5564796758229085\n",
      "train loss:0.5598581351377538\n",
      "train loss:0.5606755187828448\n",
      "train loss:0.5221867959339794\n",
      "train loss:0.5091288253085384\n",
      "train loss:0.5312412705834924\n",
      "train loss:0.8364793458750089\n",
      "train loss:0.6982898062089006\n",
      "train loss:0.4707200222421532\n",
      "train loss:0.5187274467491998\n",
      "train loss:0.7487632515766416\n",
      "train loss:0.5143300996281248\n",
      "train loss:0.7282339062052376\n",
      "train loss:0.77842073793141\n",
      "train loss:0.6703835621454062\n",
      "train loss:0.6165164171791432\n",
      "train loss:0.5608189297314874\n",
      "train loss:0.8174016274302837\n",
      "train loss:0.5042725004007005\n",
      "train loss:0.6631360271012511\n",
      "train loss:0.6208531103633456\n",
      "train loss:0.5712684899139404\n",
      "train loss:0.6288484250550846\n",
      "train loss:0.5696261714125016\n",
      "train loss:0.6217138306753619\n",
      "train loss:0.5425724541514849\n",
      "train loss:0.6789329644404256\n",
      "train loss:0.4370244560827722\n",
      "train loss:0.7550104385497738\n",
      "train loss:0.7130487172233726\n",
      "train loss:0.7697909044123071\n",
      "train loss:0.46708223021432127\n",
      "train loss:0.683189210215612\n",
      "train loss:0.6265531493004021\n",
      "train loss:0.6259554626906094\n",
      "train loss:0.5975631194179953\n",
      "train loss:0.6704673686270782\n",
      "train loss:0.5349695392181383\n",
      "train loss:0.461983564726271\n",
      "train loss:0.5313446794237381\n",
      "train loss:0.8025510919099886\n",
      "train loss:0.6036689362027877\n",
      "train loss:0.6078467580730147\n",
      "train loss:0.6988613718572958\n",
      "train loss:0.7771216809535031\n",
      "train loss:0.6578893196091088\n",
      "train loss:0.5572217924434558\n",
      "train loss:0.48634573778949586\n",
      "train loss:0.5376167360613683\n",
      "train loss:0.538462641937366\n",
      "train loss:0.6143977504249882\n",
      "train loss:0.42208157203075514\n",
      "train loss:0.7047818519344216\n",
      "train loss:0.5362743112251895\n",
      "train loss:0.6157294277296532\n",
      "train loss:0.5207749596180797\n",
      "train loss:0.9126149848742102\n",
      "train loss:0.6030239209245681\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5685877930926093\n",
      "train loss:0.7060386738665642\n",
      "train loss:0.7002572993939898\n",
      "train loss:0.5997271947182438\n",
      "train loss:0.709520698692593\n",
      "train loss:0.652460183483642\n",
      "train loss:0.5477215963379451\n",
      "train loss:0.6183525889935986\n",
      "train loss:0.41511070565672037\n",
      "train loss:0.5279969476313383\n",
      "train loss:0.5369380561133394\n",
      "train loss:0.5967399774611455\n",
      "train loss:0.6822095467885061\n",
      "train loss:0.6003697011868755\n",
      "train loss:0.3176846305302013\n",
      "train loss:0.6009505833521673\n",
      "train loss:0.7000032906745397\n",
      "train loss:0.38473518225706577\n",
      "train loss:0.7800826092865039\n",
      "train loss:0.7092094018042866\n",
      "train loss:0.922813517650497\n",
      "train loss:0.589744260152233\n",
      "train loss:0.44845035162379493\n",
      "train loss:0.8630085486591321\n",
      "train loss:0.6187481048695067\n",
      "train loss:0.6795838675918472\n",
      "train loss:0.5497470465619252\n",
      "train loss:0.7820372665944036\n",
      "train loss:0.5598437224499103\n",
      "train loss:0.5579912176284192\n",
      "train loss:0.6548696262264263\n",
      "train loss:0.631856803954006\n",
      "train loss:0.632119142176943\n",
      "train loss:0.6334662756943752\n",
      "train loss:0.622150690442385\n",
      "train loss:0.6733997449637856\n",
      "train loss:0.6040610751064924\n",
      "train loss:0.5095550754346332\n",
      "train loss:0.7723991996588601\n",
      "train loss:0.6965315442398995\n",
      "train loss:0.6161686411109702\n",
      "train loss:0.48924947007656455\n",
      "train loss:0.5416563653410454\n",
      "train loss:0.5987718242158107\n",
      "train loss:0.6815177138762495\n",
      "train loss:0.666672000153935\n",
      "train loss:0.5189225303017454\n",
      "train loss:0.41471928822972226\n",
      "train loss:0.6169628335363033\n",
      "train loss:0.8088103058721595\n",
      "train loss:0.6649184672058925\n",
      "train loss:0.9759248288480276\n",
      "train loss:0.5301430773641631\n",
      "train loss:0.761546049324987\n",
      "train loss:0.8033157819617257\n",
      "train loss:0.6643164412375626\n",
      "train loss:0.5538896584960592\n",
      "train loss:0.6615179885624252\n",
      "train loss:0.6209048893042289\n",
      "train loss:0.6187576302982883\n",
      "train loss:0.5839990101307448\n",
      "train loss:0.6179833057650819\n",
      "train loss:0.5747287205316256\n",
      "train loss:0.6708606888494433\n",
      "train loss:0.6307337314630612\n",
      "train loss:0.6097885648714636\n",
      "train loss:0.6376437762304705\n",
      "train loss:0.6538378269814771\n",
      "train loss:0.5603319701457588\n",
      "train loss:0.6068461567670351\n",
      "train loss:0.5541354926331806\n",
      "train loss:0.4829307694800372\n",
      "train loss:0.6829263969785643\n",
      "train loss:0.5852759168883985\n",
      "train loss:0.6529203753071272\n",
      "train loss:0.5774154116606681\n",
      "train loss:0.46101216359168395\n",
      "train loss:0.4402957831347174\n",
      "train loss:0.6152522427902173\n",
      "train loss:0.7397973634295465\n",
      "train loss:0.581707799522921\n",
      "train loss:0.6211753754114717\n",
      "train loss:0.5061022708334804\n",
      "train loss:0.8049327517037899\n",
      "train loss:0.7213914209368943\n",
      "train loss:0.799865493553481\n",
      "train loss:0.7152391472171068\n",
      "train loss:0.49177363128011126\n",
      "train loss:0.5264641970980912\n",
      "train loss:0.5491777904009668\n",
      "train loss:0.6178014744985403\n",
      "train loss:0.6901265108971117\n",
      "train loss:0.4826602564873933\n",
      "train loss:0.4933745540731084\n",
      "train loss:0.3925635808640656\n",
      "train loss:0.7864260564204346\n",
      "train loss:0.5237412698938767\n",
      "train loss:0.7768978390712091\n",
      "train loss:0.6341082501812546\n",
      "train loss:0.6138025939718157\n",
      "train loss:0.683671436124077\n",
      "train loss:0.6789880100697083\n",
      "train loss:0.440157954651351\n",
      "train loss:0.6208223899881211\n",
      "train loss:0.5406875245069349\n",
      "train loss:0.43687140855820267\n",
      "train loss:0.38150656670361804\n",
      "train loss:0.7361947739866035\n",
      "train loss:0.6798392662269638\n",
      "train loss:0.6076177999843881\n",
      "train loss:0.6312116404049712\n",
      "train loss:0.2607474536881977\n",
      "train loss:0.6128156492466175\n",
      "train loss:0.4875647746607969\n",
      "train loss:0.8898090266531031\n",
      "train loss:0.7163168953971166\n",
      "train loss:0.5313840296482746\n",
      "train loss:0.6041101897423513\n",
      "train loss:0.7044717562087568\n",
      "train loss:0.6951317185524513\n",
      "train loss:0.4519502600407722\n",
      "train loss:0.8291327862782356\n",
      "train loss:0.5963854612002494\n",
      "train loss:0.5534885824774161\n",
      "train loss:0.7433289731846969\n",
      "train loss:0.5645752517714799\n",
      "train loss:0.6656645760303848\n",
      "train loss:0.5634898806015289\n",
      "train loss:0.616845831034339\n",
      "train loss:0.6386274172314432\n",
      "train loss:0.6543126247354838\n",
      "train loss:0.49232483856493825\n",
      "train loss:0.4325570193729794\n",
      "train loss:0.5146302743968904\n",
      "train loss:0.5002980101420655\n",
      "train loss:0.42651308158881474\n",
      "train loss:0.6002318041278935\n",
      "train loss:0.5144043852787592\n",
      "train loss:0.5039541381989557\n",
      "train loss:0.7555970887225383\n",
      "train loss:0.516612481080441\n",
      "train loss:0.3735711920815102\n",
      "train loss:0.5111058608276193\n",
      "train loss:0.8222730822297633\n",
      "train loss:0.8072835837772097\n",
      "train loss:0.3593216820878412\n",
      "train loss:0.5118778799763708\n",
      "train loss:0.5852782436013342\n",
      "train loss:0.7938081978696665\n",
      "train loss:0.6984474765757734\n",
      "train loss:0.6872808746773529\n",
      "train loss:0.6176099701657097\n",
      "train loss:0.7178006435503104\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6642122672344433\n",
      "train loss:0.6236342275805952\n",
      "train loss:0.7055256567617868\n",
      "train loss:0.628275316631856\n",
      "train loss:0.5828901045853703\n",
      "train loss:0.6610391012811637\n",
      "train loss:0.7973071165065866\n",
      "train loss:0.6136539455420296\n",
      "train loss:0.5823075102768038\n",
      "train loss:0.6480444240292064\n",
      "train loss:0.6792877639949333\n",
      "train loss:0.6042410150410269\n",
      "train loss:0.603296159555458\n",
      "train loss:0.645144767328043\n",
      "train loss:0.6753503336660479\n",
      "train loss:0.6512873157667451\n",
      "train loss:0.6534026018522047\n",
      "train loss:0.5586390213132902\n",
      "train loss:0.7563341088142612\n",
      "train loss:0.575376667602755\n",
      "train loss:0.523962740193889\n",
      "train loss:0.7606669443512105\n",
      "train loss:0.505319349798393\n",
      "train loss:0.4785890909728206\n",
      "train loss:0.5103667470743541\n",
      "train loss:0.691130975947744\n",
      "train loss:0.7075130661360882\n",
      "train loss:0.526416461613558\n",
      "train loss:0.3812291316014226\n",
      "train loss:0.7370404827680933\n",
      "train loss:1.0491185804129806\n",
      "train loss:0.777995037113095\n",
      "train loss:0.6109286603949726\n",
      "train loss:0.5058920816325936\n",
      "train loss:0.6848652836976936\n",
      "train loss:0.4581022968988564\n",
      "train loss:0.7170947138579713\n",
      "train loss:0.6241563475308055\n",
      "train loss:0.5614379996760992\n",
      "train loss:0.5457912201358813\n",
      "train loss:0.7426183939438247\n",
      "train loss:0.5378065871224053\n",
      "train loss:0.39599406881409405\n",
      "train loss:0.4415435884169761\n",
      "train loss:0.48194296146996657\n",
      "train loss:0.8129145009139298\n",
      "train loss:0.5216303529082577\n",
      "train loss:0.6005177866584682\n",
      "train loss:0.7463681726225502\n",
      "train loss:0.6868679409617702\n",
      "train loss:0.5004992410482452\n",
      "train loss:0.485556866613632\n",
      "train loss:0.5779816212387743\n",
      "train loss:0.6893380303873967\n",
      "train loss:0.6883353595647804\n",
      "train loss:0.7807643042420592\n",
      "train loss:0.4172876392777374\n",
      "train loss:0.5028955462673463\n",
      "train loss:0.8017895486337651\n",
      "train loss:0.6088665739971997\n",
      "train loss:0.5957085930547238\n",
      "train loss:0.5111145838712735\n",
      "train loss:0.5963715024912806\n",
      "train loss:0.5976698906420909\n",
      "train loss:0.6953223318263702\n",
      "train loss:0.5714237437520546\n",
      "train loss:0.7474265342482571\n",
      "train loss:0.7710959835173063\n",
      "train loss:0.6054084143577115\n",
      "train loss:0.5395302705817605\n",
      "train loss:0.5877189375681812\n",
      "train loss:0.5661512645076532\n",
      "train loss:0.6616215329095041\n",
      "train loss:0.5337683768164181\n",
      "train loss:0.6056056956096592\n",
      "train loss:0.5758910522082739\n",
      "train loss:0.6507022708657662\n",
      "train loss:0.5280164566944987\n",
      "train loss:0.4328902323831222\n",
      "train loss:0.4017917156705155\n",
      "train loss:0.3998152619297998\n",
      "train loss:0.22743488424379404\n",
      "train loss:0.6232846419285408\n",
      "train loss:0.6500997239301106\n",
      "train loss:0.5718934294139801\n",
      "train loss:0.1650371845828132\n",
      "train loss:0.49488038726605577\n",
      "train loss:0.7022944406885989\n",
      "train loss:0.7514685976227977\n",
      "train loss:0.6341998407640979\n",
      "train loss:0.17384189230550895\n",
      "train loss:0.6139644637996244\n",
      "train loss:0.7090257649667892\n",
      "train loss:0.49633469208463527\n",
      "train loss:0.6297604247669052\n",
      "train loss:0.3984492192472125\n",
      "train loss:0.6296731375394506\n",
      "train loss:0.5417593030361816\n",
      "train loss:0.31392794024171383\n",
      "train loss:0.6990295666362685\n",
      "train loss:0.5542823443576266\n",
      "train loss:0.6986331689799729\n",
      "train loss:0.601461265506815\n",
      "train loss:0.6087460519960277\n",
      "train loss:0.6211064965358841\n",
      "train loss:0.7894281278210754\n",
      "train loss:0.48279457601195286\n",
      "train loss:0.7024684539892718\n",
      "train loss:0.734226478546751\n",
      "train loss:0.4852073355077824\n",
      "train loss:0.5901294407528546\n",
      "train loss:0.6082023842297557\n",
      "train loss:0.5532679524808355\n",
      "train loss:0.6396679587539353\n",
      "train loss:0.7026575856619643\n",
      "train loss:0.42587891513433174\n",
      "train loss:0.5899031266395962\n",
      "train loss:0.3963564317577887\n",
      "train loss:0.6621326799720513\n",
      "train loss:0.5317213503961571\n",
      "train loss:0.6382567893609548\n",
      "train loss:0.7660629116611526\n",
      "train loss:0.38294930406767613\n",
      "train loss:0.5307989383094667\n",
      "train loss:0.5942936553967251\n",
      "train loss:0.4425237215813449\n",
      "train loss:0.5244579835616069\n",
      "train loss:0.4987168023750289\n",
      "train loss:0.6402838197168705\n",
      "train loss:0.7283275955732906\n",
      "train loss:0.8337990113921491\n",
      "train loss:0.3828468491138759\n",
      "train loss:0.5234299636848546\n",
      "train loss:0.8106201528422519\n",
      "train loss:0.5049466083735745\n",
      "train loss:0.6842963779066905\n",
      "train loss:0.4376231522672124\n",
      "train loss:0.602526909085147\n",
      "train loss:0.5273349914656912\n",
      "train loss:0.7198650736749928\n",
      "train loss:0.5123641042264129\n",
      "train loss:0.592928764817904\n",
      "train loss:0.5166755004275011\n",
      "train loss:0.5296924483236263\n",
      "train loss:0.5184450437985105\n",
      "train loss:0.41940802131714505\n",
      "train loss:0.7548469162790167\n",
      "train loss:0.37135535080128246\n",
      "train loss:0.6127783857182518\n",
      "train loss:1.0164304473029975\n",
      "train loss:0.3889205962906014\n",
      "train loss:0.5803618636372124\n",
      "train loss:0.6127863548809496\n",
      "=== epoch:5, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.4464470350601159\n",
      "train loss:0.28590991999235443\n",
      "train loss:0.752488630004119\n",
      "train loss:0.5010811774799855\n",
      "train loss:0.4778936463452639\n",
      "train loss:0.23963229943430292\n",
      "train loss:0.5238471483300936\n",
      "train loss:0.3728984255031934\n",
      "train loss:0.43320313477451355\n",
      "train loss:0.9326217181265466\n",
      "train loss:0.38640767535559245\n",
      "train loss:1.0180764131673206\n",
      "train loss:0.6030853689202467\n",
      "train loss:0.750112236381886\n",
      "train loss:0.29999261374764813\n",
      "train loss:0.6170750768426927\n",
      "train loss:0.5331105702898798\n",
      "train loss:0.7380487126442871\n",
      "train loss:0.7189473239820382\n",
      "train loss:0.4555246443275207\n",
      "train loss:0.7006832781950081\n",
      "train loss:0.5370700419452866\n",
      "train loss:0.4242703572201309\n",
      "train loss:0.6132666417078976\n",
      "train loss:0.5327082737399821\n",
      "train loss:0.5361778269240205\n",
      "train loss:0.5245802358452861\n",
      "train loss:0.42462344592472856\n",
      "train loss:0.573196687214284\n",
      "train loss:0.534068977678034\n",
      "train loss:0.5125831876488613\n",
      "train loss:0.8373832538924537\n",
      "train loss:0.7926355064438221\n",
      "train loss:0.4959065872216007\n",
      "train loss:0.6259586823232091\n",
      "train loss:0.5940960769119779\n",
      "train loss:0.48105280165158126\n",
      "train loss:0.4785082334911899\n",
      "train loss:0.5282124401203039\n",
      "train loss:0.8311316438417968\n",
      "train loss:0.5047499879109502\n",
      "train loss:0.7560379773640926\n",
      "train loss:0.5882095853663059\n",
      "train loss:0.6264906898544879\n",
      "train loss:0.5880408917555477\n",
      "train loss:0.8473611962248183\n",
      "train loss:0.5784493387100111\n",
      "train loss:0.5772656922814179\n",
      "train loss:0.6896231753749651\n",
      "train loss:0.6264484791830048\n",
      "train loss:0.6395918291583792\n",
      "train loss:0.4950232690966049\n",
      "train loss:0.592302226626878\n",
      "train loss:0.5919880570036665\n",
      "train loss:0.6054801617502013\n",
      "train loss:0.5900471668817381\n",
      "train loss:0.5479150500572978\n",
      "train loss:0.7685461519614818\n",
      "train loss:0.6054425792427371\n",
      "train loss:0.6775305964861084\n",
      "train loss:0.5583646478204299\n",
      "train loss:0.5475034818840698\n",
      "train loss:0.8052175588522885\n",
      "train loss:0.4453968071733331\n",
      "train loss:0.6271977893302235\n",
      "train loss:0.6744672979352173\n",
      "train loss:0.9191723685788808\n",
      "train loss:0.6159592794003307\n",
      "train loss:0.41928537955509937\n",
      "train loss:0.4997154858277618\n",
      "train loss:0.5818567260566125\n",
      "train loss:0.719916918263823\n",
      "train loss:0.6091103105466458\n",
      "train loss:0.6051691553127009\n",
      "train loss:0.7121034200996791\n",
      "train loss:0.7038354621243903\n",
      "train loss:0.6382850533911865\n",
      "train loss:0.6802469561717426\n",
      "train loss:0.39908438433940385\n",
      "train loss:0.6502870212261003\n",
      "train loss:0.6018013671334187\n",
      "train loss:0.6125302678481745\n",
      "train loss:0.6912711967247006\n",
      "train loss:0.5031972004749172\n",
      "train loss:0.46850011960227994\n",
      "train loss:0.4253471264306176\n",
      "train loss:0.7008390435942958\n",
      "train loss:0.48122923733380374\n",
      "train loss:0.665742267028358\n",
      "train loss:0.5222542995818803\n",
      "train loss:0.41276533914751\n",
      "train loss:0.6919901887017137\n",
      "train loss:0.6403359802529933\n",
      "train loss:0.4422586582530429\n",
      "train loss:0.6176544031538763\n",
      "train loss:0.5025444845021867\n",
      "train loss:0.6789642343059071\n",
      "train loss:0.3798998094972955\n",
      "train loss:0.40136070427137555\n",
      "train loss:0.6839871407096949\n",
      "train loss:0.5532205982986006\n",
      "train loss:0.528312072995585\n",
      "train loss:0.3783967215327583\n",
      "train loss:0.5113400944453959\n",
      "train loss:0.6418769092625212\n",
      "train loss:0.4599925615594067\n",
      "train loss:0.6030256943003978\n",
      "train loss:0.6213584336497695\n",
      "train loss:0.5136079217679577\n",
      "train loss:0.7478693137423618\n",
      "train loss:0.8100655689527885\n",
      "train loss:0.709597895533438\n",
      "train loss:0.5435749457664271\n",
      "train loss:0.5645524414986325\n",
      "train loss:0.45113888635141575\n",
      "train loss:0.5791385788442258\n",
      "train loss:0.5464425958416511\n",
      "train loss:0.5334109708363661\n",
      "train loss:0.4494484863180678\n",
      "train loss:0.5269380805318122\n",
      "train loss:0.46608566903661275\n",
      "train loss:0.48886096958844405\n",
      "train loss:0.5371067836665183\n",
      "train loss:0.5404366265649846\n",
      "train loss:0.37739252451187605\n",
      "train loss:0.8321050712311073\n",
      "train loss:0.9632533681898463\n",
      "train loss:0.5782229416429135\n",
      "train loss:0.4949885707770675\n",
      "train loss:0.6349145105613208\n",
      "train loss:0.3870248098148452\n",
      "train loss:0.6287498355884498\n",
      "train loss:0.5974050120641735\n",
      "train loss:0.5632314618198657\n",
      "train loss:0.7668044137717074\n",
      "train loss:0.5869460815104539\n",
      "train loss:0.7218738370683049\n",
      "train loss:0.3547483634989389\n",
      "train loss:0.599387332501166\n",
      "train loss:0.6820587054438402\n",
      "train loss:0.4712519026407924\n",
      "train loss:0.6610797003774381\n",
      "train loss:0.7988243848013715\n",
      "train loss:0.638023307559094\n",
      "train loss:0.4695767531576792\n",
      "train loss:0.7161286235199709\n",
      "train loss:0.6454378643715744\n",
      "train loss:0.6753150457375517\n",
      "train loss:0.4616484938078571\n",
      "train loss:0.5468899526880813\n",
      "train loss:0.6266575273180854\n",
      "train loss:0.805852419633147\n",
      "train loss:0.8689691415506557\n",
      "=== epoch:6, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.4460339873560085\n",
      "train loss:0.6925046174186792\n",
      "train loss:0.6939504083715494\n",
      "train loss:0.4793938803998552\n",
      "train loss:0.616486967687446\n",
      "train loss:0.6725938441965665\n",
      "train loss:0.5310788126863702\n",
      "train loss:0.6449230305393661\n",
      "train loss:0.5979746014055854\n",
      "train loss:0.5393377539546278\n",
      "train loss:0.5930080103056314\n",
      "train loss:0.663490254694733\n",
      "train loss:0.49503365137084226\n",
      "train loss:0.5120788403463677\n",
      "train loss:0.6984487244338271\n",
      "train loss:0.5602625959882365\n",
      "train loss:0.66108700539664\n",
      "train loss:0.48063068581196083\n",
      "train loss:0.5571358036684275\n",
      "train loss:0.4276469064769525\n",
      "train loss:0.6417018846647151\n",
      "train loss:0.8104386178331371\n",
      "train loss:0.8175614155656247\n",
      "train loss:0.5147236678548957\n",
      "train loss:0.5917206437997601\n",
      "train loss:0.5289677119188253\n",
      "train loss:0.5644928218559678\n",
      "train loss:0.5399179392866886\n",
      "train loss:0.7105100696204207\n",
      "train loss:0.6897994084290869\n",
      "train loss:0.5987750155457718\n",
      "train loss:0.6191054481120538\n",
      "train loss:0.6696472539782147\n",
      "train loss:0.715000817611151\n",
      "train loss:0.5465221241416142\n",
      "train loss:0.4953059737402552\n",
      "train loss:0.7026476184715602\n",
      "train loss:0.5766811882852116\n",
      "train loss:0.6098417380074447\n",
      "train loss:0.5536538456880459\n",
      "train loss:0.5349061328606661\n",
      "train loss:0.6506088987092535\n",
      "train loss:0.6329044301573649\n",
      "train loss:0.5523907362362274\n",
      "train loss:0.5579319790337129\n",
      "train loss:0.6247355951860488\n",
      "train loss:0.7362715980010939\n",
      "train loss:0.7554375164963509\n",
      "train loss:0.4507584904213079\n",
      "train loss:0.5992738515320741\n",
      "train loss:0.5259635577455621\n",
      "train loss:0.6211007453304098\n",
      "train loss:0.9263794102495494\n",
      "train loss:0.5175400536392062\n",
      "train loss:0.8834893230244747\n",
      "train loss:0.7162242166728925\n",
      "train loss:0.6327969106115228\n",
      "train loss:0.691508239172032\n",
      "train loss:0.6848006042231358\n",
      "train loss:0.6861714272926459\n",
      "train loss:0.638749279421196\n",
      "train loss:0.6643580632961751\n",
      "train loss:0.5998533527894618\n",
      "train loss:0.6858956754451937\n",
      "train loss:0.6429898552172599\n",
      "train loss:0.653183410233451\n",
      "train loss:0.6631614778106829\n",
      "train loss:0.6427757178511123\n",
      "train loss:0.5763754561968828\n",
      "train loss:0.586676556032106\n",
      "train loss:0.6085606377171657\n",
      "train loss:0.6694390971085634\n",
      "train loss:0.69345077836852\n",
      "train loss:0.4948976101624659\n",
      "train loss:0.6675131136981836\n",
      "train loss:0.554857015780917\n",
      "train loss:0.6723954677689272\n",
      "train loss:0.582393964382707\n",
      "train loss:0.5442588224761459\n",
      "train loss:0.7159536713372767\n",
      "train loss:0.7508930475374477\n",
      "train loss:0.6121385846767609\n",
      "train loss:0.7284276512733321\n",
      "train loss:0.5038495191199986\n",
      "train loss:0.6606347711156195\n",
      "train loss:0.6425835753879255\n",
      "train loss:0.5058788129075519\n",
      "train loss:0.518843523608828\n",
      "train loss:0.5107874224767626\n",
      "train loss:0.5066263379198515\n",
      "train loss:0.49101888597592797\n",
      "train loss:0.7147222449057783\n",
      "train loss:0.519593982517376\n",
      "train loss:0.4934174485241664\n",
      "train loss:0.6908051598307392\n",
      "train loss:0.5525573373797151\n",
      "train loss:0.48953718128900847\n",
      "train loss:0.44458254278247616\n",
      "train loss:0.38337735087401265\n",
      "train loss:0.5972946763574412\n",
      "train loss:0.613899137411354\n",
      "train loss:0.8504139810179773\n",
      "train loss:0.5099649922398948\n",
      "train loss:0.6461904069097398\n",
      "train loss:0.32680362253106493\n",
      "train loss:0.3770331180166576\n",
      "train loss:0.838481559438082\n",
      "train loss:0.4602805462876914\n",
      "train loss:0.511680560753976\n",
      "train loss:0.7801216158853677\n",
      "train loss:0.42608983828815206\n",
      "train loss:0.5603913085477814\n",
      "train loss:0.5024019549783121\n",
      "train loss:0.7169880328417868\n",
      "train loss:0.4219590556267206\n",
      "train loss:0.6381323505537746\n",
      "train loss:0.2800203256573238\n",
      "train loss:0.798648360385111\n",
      "train loss:0.7166850696859528\n",
      "train loss:0.6918462067749046\n",
      "train loss:0.5799192704366374\n",
      "train loss:0.4670850804874159\n",
      "train loss:0.542365838461998\n",
      "train loss:0.675293197381384\n",
      "train loss:0.4236297611627012\n",
      "train loss:0.862813123780068\n",
      "train loss:0.6526367261653758\n",
      "train loss:0.5739831931281215\n",
      "train loss:0.527336048258992\n",
      "train loss:0.5176525245460982\n",
      "train loss:0.6643508340846644\n",
      "train loss:0.6633300136058324\n",
      "train loss:0.46366270114713587\n",
      "train loss:0.46925892304872596\n",
      "train loss:0.5706921579369403\n",
      "train loss:0.504536120536649\n",
      "train loss:0.4834197568821077\n",
      "train loss:0.6081390196191552\n",
      "train loss:0.6628850751832125\n",
      "train loss:0.7633060080192247\n",
      "train loss:0.5274763171858347\n",
      "train loss:0.7005961456055114\n",
      "train loss:0.6200446643591595\n",
      "train loss:0.4076637312524894\n",
      "train loss:0.5874366333507941\n",
      "train loss:0.8959093952102768\n",
      "train loss:0.8376649910051862\n",
      "train loss:0.39027116363855585\n",
      "train loss:0.5064427545918082\n",
      "train loss:0.3449805852338282\n",
      "train loss:0.5395417943649518\n",
      "train loss:0.5003297251685598\n",
      "train loss:0.7548604337101705\n",
      "=== epoch:7, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5331637072427282\n",
      "train loss:0.503949612689989\n",
      "train loss:0.35347757488550385\n",
      "train loss:0.3565402165975789\n",
      "train loss:0.498455970526377\n",
      "train loss:0.5689549779611681\n",
      "train loss:0.5201645688620896\n",
      "train loss:0.38676617690534876\n",
      "train loss:0.675290783677114\n",
      "train loss:0.4452405950044505\n",
      "train loss:1.1484269771400137\n",
      "train loss:0.6895876416983009\n",
      "train loss:0.4713999965588502\n",
      "train loss:0.40723224718102174\n",
      "train loss:0.39779421677605387\n",
      "train loss:0.6486332250488006\n",
      "train loss:0.43292133995050264\n",
      "train loss:0.36020379044312345\n",
      "train loss:0.781961870284188\n",
      "train loss:0.5006929211012514\n",
      "train loss:0.5376286845406065\n",
      "train loss:0.6256081040396306\n",
      "train loss:0.4593198335340718\n",
      "train loss:0.6195007594590434\n",
      "train loss:0.6600506870165475\n",
      "train loss:0.4965855759205109\n",
      "train loss:0.33470578602033185\n",
      "train loss:0.5699960008231234\n",
      "train loss:0.37540428334868625\n",
      "train loss:0.5498126497449642\n",
      "train loss:0.6301423047119151\n",
      "train loss:0.6162063649824931\n",
      "train loss:0.6335010852273661\n",
      "train loss:0.6000886671176717\n",
      "train loss:0.3703727150019473\n",
      "train loss:0.5801407999767021\n",
      "train loss:0.5514083281919145\n",
      "train loss:0.6210159700582685\n",
      "train loss:0.32826075697104773\n",
      "train loss:0.44815689501995226\n",
      "train loss:0.603592468234736\n",
      "train loss:0.6428084722126954\n",
      "train loss:0.4081857075594268\n",
      "train loss:0.40244727207514164\n",
      "train loss:0.3787260822539017\n",
      "train loss:0.5361158616764347\n",
      "train loss:0.6796059440880842\n",
      "train loss:0.7724774185735013\n",
      "train loss:0.6057019917903234\n",
      "train loss:0.7340714908489525\n",
      "train loss:0.74069349530798\n",
      "train loss:0.41269902623368343\n",
      "train loss:0.5809481525994731\n",
      "train loss:0.5920608672082445\n",
      "train loss:0.44115537353617673\n",
      "train loss:0.4989593574532897\n",
      "train loss:0.4987329513063396\n",
      "train loss:0.4611924603321512\n",
      "train loss:0.5283497956602097\n",
      "train loss:0.5926736261457378\n",
      "train loss:0.6349599722529992\n",
      "train loss:0.38244950433761743\n",
      "train loss:0.505883528586706\n",
      "train loss:0.35994281875040873\n",
      "train loss:0.2467912510132071\n",
      "train loss:0.7893763082157819\n",
      "train loss:0.2920200754873673\n",
      "train loss:0.4487883228812571\n",
      "train loss:0.9426406038528267\n",
      "train loss:0.34378894510126295\n",
      "train loss:0.5883169084610774\n",
      "train loss:0.5683884734561916\n",
      "train loss:0.8008685993051555\n",
      "train loss:0.7318245782963771\n",
      "train loss:0.49128991756612284\n",
      "train loss:0.5687720346701222\n",
      "train loss:0.51997029080203\n",
      "train loss:0.7218290445146333\n",
      "train loss:0.49307568299175636\n",
      "train loss:0.761782052447265\n",
      "train loss:0.6785763297788538\n",
      "train loss:0.5793422330075131\n",
      "train loss:0.4168640121789129\n",
      "train loss:0.4746855934916049\n",
      "train loss:0.6210265537326055\n",
      "train loss:0.46618601730123876\n",
      "train loss:0.5973197956536161\n",
      "train loss:0.4793437076845894\n",
      "train loss:0.49563216084079276\n",
      "train loss:0.7164006075795142\n",
      "train loss:0.6939351485868355\n",
      "train loss:0.6036809186998091\n",
      "train loss:0.5592911386514491\n",
      "train loss:0.5934941153353325\n",
      "train loss:0.5162655144756118\n",
      "train loss:0.8866864598323719\n",
      "train loss:0.424773678508439\n",
      "train loss:0.5644872713188331\n",
      "train loss:0.8675684346656098\n",
      "train loss:0.6646112775038084\n",
      "train loss:0.7010570780623021\n",
      "train loss:0.5120839522997749\n",
      "train loss:0.4701275381479447\n",
      "train loss:0.6191583734965501\n",
      "train loss:0.5484976327107403\n",
      "train loss:0.6607495551130186\n",
      "train loss:0.587857837997322\n",
      "train loss:0.3458862985939395\n",
      "train loss:0.4043146335569621\n",
      "train loss:0.466246877467975\n",
      "train loss:0.5504999043141471\n",
      "train loss:0.6763238483700056\n",
      "train loss:0.6806631996717698\n",
      "train loss:0.5069400604846699\n",
      "train loss:0.5226274641046185\n",
      "train loss:0.3512329578048049\n",
      "train loss:0.32534119645292436\n",
      "train loss:0.6302087100001812\n",
      "train loss:0.4190059870993257\n",
      "train loss:0.8367393010266133\n",
      "train loss:0.6277404579554255\n",
      "train loss:0.6409412063900174\n",
      "train loss:0.7144090470580661\n",
      "train loss:0.5523455180938723\n",
      "train loss:0.49636663345199894\n",
      "train loss:0.488023378057608\n",
      "train loss:0.5263412980770205\n",
      "train loss:0.4957197677367434\n",
      "train loss:0.6853121449190969\n",
      "train loss:0.401803263610904\n",
      "train loss:0.5359451575511046\n",
      "train loss:0.7440582833098038\n",
      "train loss:0.6173855570391277\n",
      "train loss:0.4298335942354422\n",
      "train loss:0.6589338839785612\n",
      "train loss:0.4576052410512193\n",
      "train loss:0.763193566751607\n",
      "train loss:0.5084110168823758\n",
      "train loss:0.4334991041944497\n",
      "train loss:0.5956288921052446\n",
      "train loss:0.6821289137698836\n",
      "train loss:0.49250693633277265\n",
      "train loss:0.5060925585933489\n",
      "train loss:0.5379692104074444\n",
      "train loss:0.421352208082565\n",
      "train loss:0.4938794105236406\n",
      "train loss:0.5953648896676401\n",
      "train loss:0.5411133744767656\n",
      "train loss:0.49801023483309387\n",
      "train loss:0.5343111189286376\n",
      "train loss:0.7820802143621601\n",
      "train loss:0.37984393217839063\n",
      "train loss:0.6320876082553881\n",
      "=== epoch:8, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.8386543126986249\n",
      "train loss:0.5487339814582285\n",
      "train loss:0.5312945015109709\n",
      "train loss:0.5586119634376268\n",
      "train loss:0.44915539790234316\n",
      "train loss:0.5349140520809373\n",
      "train loss:0.6153247036611964\n",
      "train loss:0.44054224235315054\n",
      "train loss:0.7812689386386611\n",
      "train loss:0.3332767018065966\n",
      "train loss:0.518086434496227\n",
      "train loss:0.5057084199925337\n",
      "train loss:0.5292881313622807\n",
      "train loss:0.3887587487904505\n",
      "train loss:0.8102598530884461\n",
      "train loss:0.59230972046378\n",
      "train loss:0.610684235048282\n",
      "train loss:0.6070034713918738\n",
      "train loss:0.7591499796316652\n",
      "train loss:0.5779152319336627\n",
      "train loss:0.7038156626348602\n",
      "train loss:0.5275757190377541\n",
      "train loss:0.6171834481191184\n",
      "train loss:0.5205838009815096\n",
      "train loss:0.6352365378663494\n",
      "train loss:0.5844345168697453\n",
      "train loss:0.6505742786307483\n",
      "train loss:0.6607638849143435\n",
      "train loss:0.6575509612388304\n",
      "train loss:0.5556134698990377\n",
      "train loss:0.6218658760908258\n",
      "train loss:0.7749022046009878\n",
      "train loss:0.46826614966807234\n",
      "train loss:0.6041126343271785\n",
      "train loss:0.66277797618031\n",
      "train loss:0.629192465157508\n",
      "train loss:0.5741574074577619\n",
      "train loss:0.7013853330894181\n",
      "train loss:0.7075221418889093\n",
      "train loss:0.6128290935230527\n",
      "train loss:0.5740532450635969\n",
      "train loss:0.3793252624083853\n",
      "train loss:0.5751891736688765\n",
      "train loss:0.6686896770027972\n",
      "train loss:0.47435333631153576\n",
      "train loss:0.4828937828959816\n",
      "train loss:0.48954548381505003\n",
      "train loss:0.29570436527328103\n",
      "train loss:0.5566970636393636\n",
      "train loss:0.7728746889398057\n",
      "train loss:0.6148934872764661\n",
      "train loss:0.6639931001972676\n",
      "train loss:0.7046614060873682\n",
      "train loss:0.7128555610398324\n",
      "train loss:0.5550668374208069\n",
      "train loss:0.5646670611839495\n",
      "train loss:0.6031335812967991\n",
      "train loss:0.5053353299784482\n",
      "train loss:0.6006139300378974\n",
      "train loss:0.6292069747958909\n",
      "train loss:0.6777114645937325\n",
      "train loss:0.6833638265363792\n",
      "train loss:0.5387928617317679\n",
      "train loss:0.5364549992420282\n",
      "train loss:0.6046417954277786\n",
      "train loss:0.5674247297721188\n",
      "train loss:0.6268817828398809\n",
      "train loss:0.6393490657583216\n",
      "train loss:0.4869773557036982\n",
      "train loss:0.500778134742976\n",
      "train loss:0.615558507037699\n",
      "train loss:0.40977364038497555\n",
      "train loss:0.5203574924760599\n",
      "train loss:0.7856212666426483\n",
      "train loss:0.623486316863763\n",
      "train loss:0.41878351059230867\n",
      "train loss:0.5769129603755498\n",
      "train loss:0.5289241389368963\n",
      "train loss:0.8056905507628629\n",
      "train loss:0.5383377931749294\n",
      "train loss:0.5716298889455768\n",
      "train loss:0.5098070394377893\n",
      "train loss:0.8516279040514434\n",
      "train loss:0.3691126071526047\n",
      "train loss:0.33765924077269827\n",
      "train loss:0.48625544303815715\n",
      "train loss:0.591125845138515\n",
      "train loss:0.7401913557733714\n",
      "train loss:0.29911922503459426\n",
      "train loss:0.33675300834710387\n",
      "train loss:0.68984149037432\n",
      "train loss:0.1974992713752089\n",
      "train loss:0.513243985501002\n",
      "train loss:0.6063241471389369\n",
      "train loss:0.6259151155150353\n",
      "train loss:0.6549301571865546\n",
      "train loss:0.44815650479839625\n",
      "train loss:0.7369377613142408\n",
      "train loss:0.7674224238503609\n",
      "train loss:0.71725584692051\n",
      "train loss:0.33914945658208745\n",
      "train loss:0.5605206988401459\n",
      "train loss:0.49748431329678694\n",
      "train loss:0.6100972577603495\n",
      "train loss:0.5611324541696516\n",
      "train loss:0.616910198386052\n",
      "train loss:0.6332864890291863\n",
      "train loss:0.5830293956162177\n",
      "train loss:0.46988430564855116\n",
      "train loss:0.6100220032911838\n",
      "train loss:0.5999829897541136\n",
      "train loss:0.5212048248009273\n",
      "train loss:0.7480744874513686\n",
      "train loss:0.5253149813866231\n",
      "train loss:0.6454708144383743\n",
      "train loss:0.49626103006779765\n",
      "train loss:0.4795468758804663\n",
      "train loss:0.539783987938443\n",
      "train loss:0.7718638016650241\n",
      "train loss:0.3701578096541566\n",
      "train loss:0.55162449015421\n",
      "train loss:0.5356567565766357\n",
      "train loss:0.5423605560266193\n",
      "train loss:0.3991347465496594\n",
      "train loss:0.5523004381162611\n",
      "train loss:0.5198178824206197\n",
      "train loss:0.3904155223508597\n",
      "train loss:0.48858810858671015\n",
      "train loss:0.6105514767925737\n",
      "train loss:0.6991974334710858\n",
      "train loss:0.7963390536473336\n",
      "train loss:0.5701338679038558\n",
      "train loss:0.5638676256337946\n",
      "train loss:0.6248171204555841\n",
      "train loss:0.628883401940308\n",
      "train loss:0.49222188665440314\n",
      "train loss:0.7905030404886102\n",
      "train loss:0.49226573958246067\n",
      "train loss:0.5314421308504182\n",
      "train loss:0.5648754413236932\n",
      "train loss:0.6306264541218\n",
      "train loss:0.5523889168554283\n",
      "train loss:0.6066972460537422\n",
      "train loss:0.5508891043210212\n",
      "train loss:0.4498087741981636\n",
      "train loss:0.5822658348988594\n",
      "train loss:0.7005577517373016\n",
      "train loss:0.666007720530253\n",
      "train loss:0.4549907606219503\n",
      "train loss:0.5457479415138438\n",
      "train loss:0.7196219336889829\n",
      "train loss:0.8665413825992928\n",
      "train loss:0.45443272890591035\n",
      "=== epoch:9, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.8535808106681207\n",
      "train loss:0.42763046002137123\n",
      "train loss:0.4471057727610207\n",
      "train loss:0.6041057067541247\n",
      "train loss:0.5800588339014332\n",
      "train loss:0.6070085382152819\n",
      "train loss:0.5717769348496309\n",
      "train loss:0.6249730526842393\n",
      "train loss:0.562476049398535\n",
      "train loss:0.4500381796619112\n",
      "train loss:0.3166611818061042\n",
      "train loss:0.8122229914190005\n",
      "train loss:0.47271401193200663\n",
      "train loss:0.6976121521816376\n",
      "train loss:0.5983768435786898\n",
      "train loss:0.5919459532907535\n",
      "train loss:0.3756167844684063\n",
      "train loss:0.5276168521508524\n",
      "train loss:0.6969174538472492\n",
      "train loss:0.5478422164424519\n",
      "train loss:0.9163889318665049\n",
      "train loss:0.6167315531937808\n",
      "train loss:0.7682672010505984\n",
      "train loss:0.44132941186980085\n",
      "train loss:0.5290157791067143\n",
      "train loss:0.6828500326286507\n",
      "train loss:0.7348205944720925\n",
      "train loss:0.5832898829439457\n",
      "train loss:0.42660595593351947\n",
      "train loss:0.66230515824582\n",
      "train loss:0.6881322234359022\n",
      "train loss:0.6421181879546927\n",
      "train loss:0.611041496552766\n",
      "train loss:0.4062710967062241\n",
      "train loss:0.5468469223947142\n",
      "train loss:0.5422946718413928\n",
      "train loss:0.6913821547974981\n",
      "train loss:0.5340834472395515\n",
      "train loss:0.4540196208952347\n",
      "train loss:0.6602082374041347\n",
      "train loss:0.3735924936461575\n",
      "train loss:0.6945542142603556\n",
      "train loss:0.6070251357425906\n",
      "train loss:0.5143986404766082\n",
      "train loss:0.5157782134464759\n",
      "train loss:0.5989019789527559\n",
      "train loss:0.5579337599113872\n",
      "train loss:0.7321833919599142\n",
      "train loss:0.5618415752840163\n",
      "train loss:0.8059606443537645\n",
      "train loss:0.7525429845388402\n",
      "train loss:0.7766226775091812\n",
      "train loss:0.34109658543479826\n",
      "train loss:0.5073143689275079\n",
      "train loss:0.4504966725302163\n",
      "train loss:0.7144741693296959\n",
      "train loss:0.3486427758288378\n",
      "train loss:0.5228638570343087\n",
      "train loss:0.46511719354566994\n",
      "train loss:0.6368633963803502\n",
      "train loss:0.5651739361634507\n",
      "train loss:0.6921124028669727\n",
      "train loss:0.6945533347594208\n",
      "train loss:0.5475634258947494\n",
      "train loss:0.5853219229569853\n",
      "train loss:0.6530578938955411\n",
      "train loss:0.46771171313272647\n",
      "train loss:0.5846618970568747\n",
      "train loss:0.7094779536451543\n",
      "train loss:0.5215191781429398\n",
      "train loss:0.6082391841299296\n",
      "train loss:0.40012338266470193\n",
      "train loss:0.678429956631373\n",
      "train loss:0.6546227965759777\n",
      "train loss:0.5672173197276827\n",
      "train loss:0.6022374506867412\n",
      "train loss:0.5766502953997473\n",
      "train loss:0.6711695975249654\n",
      "train loss:0.47004729439611587\n",
      "train loss:0.6062540259499014\n",
      "train loss:0.6055660107939004\n",
      "train loss:0.6267983837571642\n",
      "train loss:0.6014380266155583\n",
      "train loss:0.6612829897626766\n",
      "train loss:0.6401913704569105\n",
      "train loss:0.6031208928427322\n",
      "train loss:0.39171395507032525\n",
      "train loss:0.5151963347727186\n",
      "train loss:0.42721391217353855\n",
      "train loss:0.4880298871931771\n",
      "train loss:0.45990452290348455\n",
      "train loss:0.32265530620563815\n",
      "train loss:0.5885440775219763\n",
      "train loss:0.9593165989239161\n",
      "train loss:0.3946382327628283\n",
      "train loss:0.273658684357579\n",
      "train loss:0.6640615672287864\n",
      "train loss:0.5968636537588075\n",
      "train loss:0.5566087933379453\n",
      "train loss:0.7690747409434977\n",
      "train loss:0.7208631235084912\n",
      "train loss:0.3216105015235021\n",
      "train loss:0.4852673112432725\n",
      "train loss:0.718625911315707\n",
      "train loss:0.7365093875101077\n",
      "train loss:0.5798482469828687\n",
      "train loss:0.6470938666917878\n",
      "train loss:0.6417003351986269\n",
      "train loss:0.3912020771456638\n",
      "train loss:0.7470429552166706\n",
      "train loss:0.7391512040251171\n",
      "train loss:0.5471020961230597\n",
      "train loss:0.7629955840266185\n",
      "train loss:0.480860449233668\n",
      "train loss:0.5187788599136558\n",
      "train loss:0.5899122670458224\n",
      "train loss:0.41937855223414966\n",
      "train loss:0.6527130483319306\n",
      "train loss:0.5261073528892266\n",
      "train loss:0.5944665600487772\n",
      "train loss:0.6172849414900754\n",
      "train loss:0.5393359906397103\n",
      "train loss:0.3429439539397124\n",
      "train loss:0.5972857496963664\n",
      "train loss:0.655625143530088\n",
      "train loss:0.6455151395734469\n",
      "train loss:0.5545704819127252\n",
      "train loss:0.5007387113561907\n",
      "train loss:0.4483300338006095\n",
      "train loss:0.7932773512881326\n",
      "train loss:0.8591404586573275\n",
      "train loss:0.5623621436700279\n",
      "train loss:0.6066833781283867\n",
      "train loss:0.6146225259103055\n",
      "train loss:0.4564847134917893\n",
      "train loss:0.5280886072536831\n",
      "train loss:0.4792688112971688\n",
      "train loss:0.5772057226008827\n",
      "train loss:0.6136719653507284\n",
      "train loss:0.40562530447631107\n",
      "train loss:0.39945856609494995\n",
      "train loss:0.6062131786833636\n",
      "train loss:0.609348559149557\n",
      "train loss:0.39709187061706225\n",
      "train loss:0.33664217824311904\n",
      "train loss:0.46756746536651256\n",
      "train loss:0.6420561295865204\n",
      "train loss:0.7573347378973587\n",
      "train loss:0.14364330337756312\n",
      "train loss:0.508429643601455\n",
      "train loss:0.5094323083031347\n",
      "train loss:1.010271743756759\n",
      "train loss:0.2499104506583012\n",
      "=== epoch:10, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7666012006409743\n",
      "train loss:0.5419018526921704\n",
      "train loss:0.3949852401061106\n",
      "train loss:0.49577295232039464\n",
      "train loss:0.44886262442413194\n",
      "train loss:0.4611206913515794\n",
      "train loss:0.3487313965283162\n",
      "train loss:0.6553150386440962\n",
      "train loss:0.5078560724227114\n",
      "train loss:0.4951964864724472\n",
      "train loss:0.3334855937306218\n",
      "train loss:0.5989280477325617\n",
      "train loss:0.45156464326283663\n",
      "train loss:0.40318255850255386\n",
      "train loss:0.5977146836340296\n",
      "train loss:0.5734396201144707\n",
      "train loss:0.8306490075961721\n",
      "train loss:0.7120267171627812\n",
      "train loss:0.516635782689531\n",
      "train loss:0.6890270491446312\n",
      "train loss:0.7682248235235536\n",
      "train loss:0.5907772918215615\n",
      "train loss:0.48433549558445643\n",
      "train loss:0.5455019632358236\n",
      "train loss:0.6711009106779026\n",
      "train loss:0.48513062513437066\n",
      "train loss:0.5170136874102608\n",
      "train loss:0.7585780384344052\n",
      "train loss:0.664216226294847\n",
      "train loss:0.4991442928005558\n",
      "train loss:0.531326555034921\n",
      "train loss:0.5218027055766141\n",
      "train loss:0.6539259947733954\n",
      "train loss:0.5503760564393654\n",
      "train loss:0.550106642755698\n",
      "train loss:0.45724766200202505\n",
      "train loss:0.4965873302530205\n",
      "train loss:0.6611740207122005\n",
      "train loss:0.6912495591141716\n",
      "train loss:0.7882989802240412\n",
      "train loss:0.6772584905774974\n",
      "train loss:0.5228082304823803\n",
      "train loss:0.8023523496522916\n",
      "train loss:0.5889893120684011\n",
      "train loss:0.4074898985599199\n",
      "train loss:0.7193220145189377\n",
      "train loss:0.5515094390097404\n",
      "train loss:0.44455143232805405\n",
      "train loss:0.4837413960785059\n",
      "train loss:0.5633452653777056\n",
      "train loss:0.4812174723442647\n",
      "train loss:0.5255178704078456\n",
      "train loss:0.3286047476928918\n",
      "train loss:0.5247912602430196\n",
      "train loss:0.596960502162482\n",
      "train loss:0.5447840099465899\n",
      "train loss:0.6079346371298293\n",
      "train loss:0.6032623275223215\n",
      "train loss:0.8150872482451659\n",
      "train loss:0.661203776373676\n",
      "train loss:0.5562403124053801\n",
      "train loss:0.5578646248795402\n",
      "train loss:0.5244668178834413\n",
      "train loss:0.4264534548784731\n",
      "train loss:0.7963064879878827\n",
      "train loss:0.5886720744961846\n",
      "train loss:0.663258398679464\n",
      "train loss:0.5035158268047428\n",
      "train loss:0.5141983544295416\n",
      "train loss:0.5419175571618595\n",
      "train loss:0.4436150360690947\n",
      "train loss:0.7036063287830044\n",
      "train loss:0.3987915735831093\n",
      "train loss:0.502887555055172\n",
      "train loss:0.702414801488159\n",
      "train loss:0.5185606319871218\n",
      "train loss:0.37607313131398973\n",
      "train loss:0.4889295270319359\n",
      "train loss:0.686683105851285\n",
      "train loss:0.5829085977385253\n",
      "train loss:0.6842616605754083\n",
      "train loss:0.48425947845213646\n",
      "train loss:0.5894726746251795\n",
      "train loss:0.4922802519560337\n",
      "train loss:0.5083691655102073\n",
      "train loss:0.2728699727268062\n",
      "train loss:0.7122666120763481\n",
      "train loss:0.47973424707209633\n",
      "train loss:0.5651332411382197\n",
      "train loss:0.6294825596167775\n",
      "train loss:0.3518670495810219\n",
      "train loss:0.813430001053035\n",
      "train loss:0.629704476830925\n",
      "train loss:0.4803720199482114\n",
      "train loss:0.4444121915679481\n",
      "train loss:0.5220599445302654\n",
      "train loss:0.6167759960582699\n",
      "train loss:0.4080785154041243\n",
      "train loss:0.6774125356963729\n",
      "train loss:0.5799909162689392\n",
      "train loss:0.5135035412020462\n",
      "train loss:0.6094161415019533\n",
      "train loss:0.5420795158667786\n",
      "train loss:0.6767077505021549\n",
      "train loss:0.6097013159450326\n",
      "train loss:0.40745366140065953\n",
      "train loss:0.6030728863779476\n",
      "train loss:0.4083053544317551\n",
      "train loss:0.4923713677324352\n",
      "train loss:0.5073592109856356\n",
      "train loss:0.5066478516762672\n",
      "train loss:0.6531335018978727\n",
      "train loss:0.49330642464144986\n",
      "train loss:0.4098272662325323\n",
      "train loss:0.5445164758848754\n",
      "train loss:0.7389537412217759\n",
      "train loss:0.48524853453601835\n",
      "train loss:0.32106185365929163\n",
      "train loss:0.22372721636042234\n",
      "train loss:0.6516354687683372\n",
      "train loss:0.5146741836190636\n",
      "train loss:0.6561899319692543\n",
      "train loss:0.49565294203252364\n",
      "train loss:0.3585758570759324\n",
      "train loss:0.44315009243456027\n",
      "train loss:0.5797532005861361\n",
      "train loss:0.46064684713741916\n",
      "train loss:0.36153710370180214\n",
      "train loss:0.7337880904814299\n",
      "train loss:0.6639751326796102\n",
      "train loss:0.7332645631015206\n",
      "train loss:0.6099863165238171\n",
      "train loss:0.6353872486119995\n",
      "train loss:0.5154926250911452\n",
      "train loss:0.6863813664219984\n",
      "train loss:0.5210557827743684\n",
      "train loss:0.43077185447738736\n",
      "train loss:0.5408138676435764\n",
      "train loss:0.46799582817836693\n",
      "train loss:0.6579734604389735\n",
      "train loss:0.5009619317675403\n",
      "train loss:0.6406513328151971\n",
      "train loss:0.681688101242157\n",
      "train loss:0.45711760990524936\n",
      "train loss:0.4011306796076656\n",
      "train loss:0.6564633369046575\n",
      "train loss:0.6128843314529822\n",
      "train loss:0.5290988457244415\n",
      "train loss:0.6308643566784125\n",
      "train loss:0.47167223723290475\n",
      "train loss:0.4299554796979244\n",
      "train loss:0.3839625729975253\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5372549019607843\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 70, 'filter_size': 7, 'pad': 1, 'stride': 1},\n",
    "                        hidden_size=100, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "89781c77-dceb-4ab5-952a-281a0c14a597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.690724834628417\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6813511576643965\n",
      "train loss:0.6916391971363883\n",
      "train loss:0.6827638528886916\n",
      "train loss:0.6926381177956659\n",
      "train loss:0.6541170062484747\n",
      "train loss:0.642494572055041\n",
      "train loss:0.5795511526229744\n",
      "train loss:0.6094598725301135\n",
      "train loss:0.6311812602660193\n",
      "train loss:0.6402066602603147\n",
      "train loss:0.49951082079951875\n",
      "train loss:0.6547229844724136\n",
      "train loss:0.3642587029936785\n",
      "train loss:0.4703559048691076\n",
      "train loss:0.5139455775923983\n",
      "train loss:0.16471286982786434\n",
      "train loss:0.3010474754900809\n",
      "train loss:0.7101819707373213\n",
      "train loss:0.8337331251287281\n",
      "train loss:0.7579549962684904\n",
      "train loss:0.655452271511401\n",
      "train loss:0.7924620552291932\n",
      "train loss:0.7375785312270187\n",
      "train loss:0.6549787801761748\n",
      "train loss:0.6846505957496742\n",
      "train loss:0.6128711808401452\n",
      "train loss:0.6673712012774222\n",
      "train loss:0.6466645907658956\n",
      "train loss:0.6407687554852607\n",
      "train loss:0.6804002829568278\n",
      "train loss:0.6775624128173056\n",
      "train loss:0.6475191522237729\n",
      "train loss:0.6478462803619544\n",
      "train loss:0.6763237195209352\n",
      "train loss:0.6922714002331996\n",
      "train loss:0.629591565865575\n",
      "train loss:0.6764653225639934\n",
      "train loss:0.6306395009281041\n",
      "train loss:0.6499620279777366\n",
      "train loss:0.6683887304930927\n",
      "train loss:0.6475004101132411\n",
      "train loss:0.678088576159249\n",
      "train loss:0.6843736964716959\n",
      "train loss:0.6835776486886906\n",
      "train loss:0.5921115766661472\n",
      "train loss:0.6213686867126288\n",
      "train loss:0.6794215435990552\n",
      "train loss:0.6356780148613186\n",
      "train loss:0.6448932283279007\n",
      "train loss:0.6086198491306832\n",
      "train loss:0.4712554143165374\n",
      "train loss:0.5974140745598151\n",
      "train loss:0.439299822204668\n",
      "train loss:0.5254272639896667\n",
      "train loss:0.5281960133990848\n",
      "train loss:0.6144321256151327\n",
      "train loss:0.8192082416440616\n",
      "train loss:0.7987518615839996\n",
      "train loss:0.5967550797762909\n",
      "train loss:0.6039358977982914\n",
      "train loss:0.3957932896485107\n",
      "train loss:0.5074022869364818\n",
      "train loss:0.7790466884319213\n",
      "train loss:0.4097869970156002\n",
      "train loss:0.5197251033255916\n",
      "train loss:0.31303197817466083\n",
      "train loss:0.4960781239894117\n",
      "train loss:0.49120953922004845\n",
      "train loss:0.38579687463577805\n",
      "train loss:0.8718930428699716\n",
      "train loss:1.0023151696719796\n",
      "train loss:0.8976401054431971\n",
      "train loss:0.5935588871197685\n",
      "train loss:0.7593777893219874\n",
      "train loss:0.47283600750296584\n",
      "train loss:0.6265856078905234\n",
      "train loss:0.5507958923640153\n",
      "train loss:0.7012171022419904\n",
      "train loss:0.5881397066222127\n",
      "train loss:0.5779130150715054\n",
      "train loss:0.596419988606151\n",
      "train loss:0.49152141250605486\n",
      "train loss:0.5576345023878158\n",
      "train loss:0.5616935938680779\n",
      "train loss:0.6525713025746641\n",
      "train loss:0.468194537011897\n",
      "train loss:0.849694238132361\n",
      "train loss:0.5312884231883925\n",
      "train loss:0.5231776728167882\n",
      "train loss:0.5194740226732284\n",
      "train loss:0.7341391994137881\n",
      "train loss:0.6031607840960775\n",
      "train loss:0.36631659237097364\n",
      "train loss:0.8201022016460685\n",
      "train loss:0.7378942891294094\n",
      "train loss:0.6258018074058571\n",
      "train loss:0.41775681524482466\n",
      "train loss:0.6008497329406388\n",
      "train loss:0.5036887677014334\n",
      "train loss:0.7216552131977184\n",
      "train loss:0.6013933439647214\n",
      "train loss:0.7360739852944895\n",
      "train loss:0.8878404600164904\n",
      "train loss:0.5195791659943806\n",
      "train loss:0.5537323690667547\n",
      "train loss:0.666233548134125\n",
      "train loss:0.4859457407895982\n",
      "train loss:0.6591358786853052\n",
      "train loss:0.5522763668365582\n",
      "train loss:0.7300525338866025\n",
      "train loss:0.6217343864888714\n",
      "train loss:0.539182609787527\n",
      "train loss:0.6105474496515387\n",
      "train loss:0.6478434188518067\n",
      "train loss:0.6001365130096323\n",
      "train loss:0.7074566337720432\n",
      "train loss:0.7307180161141578\n",
      "train loss:0.6219306842079841\n",
      "train loss:0.550920655795743\n",
      "train loss:0.6080101566698612\n",
      "train loss:0.5941292406255358\n",
      "train loss:0.77395530213671\n",
      "train loss:0.5676198275964107\n",
      "train loss:0.6068510370822654\n",
      "train loss:0.6319571331302277\n",
      "train loss:0.6236408131832525\n",
      "train loss:0.5592517361854454\n",
      "train loss:0.5408088957075514\n",
      "train loss:0.5013052091641088\n",
      "train loss:0.6879210649237929\n",
      "train loss:0.5276974455962655\n",
      "train loss:0.6974875073853799\n",
      "train loss:0.5865133336947782\n",
      "train loss:0.37832390217783973\n",
      "train loss:0.7269775262270296\n",
      "train loss:0.6379335141709099\n",
      "train loss:0.4758243501738713\n",
      "train loss:0.824248543573147\n",
      "train loss:0.6173658500662146\n",
      "train loss:0.5118540817918473\n",
      "train loss:0.5830191579378964\n",
      "train loss:0.4884559459089995\n",
      "train loss:0.6823159612113344\n",
      "train loss:0.4206081284741073\n",
      "train loss:0.5905670068705883\n",
      "train loss:0.6719485017229399\n",
      "train loss:0.7211118798125628\n",
      "train loss:0.6860070339785103\n",
      "train loss:0.764899325878203\n",
      "train loss:0.5310457025945774\n",
      "train loss:0.5152037390162386\n",
      "train loss:0.37591623583663625\n",
      "train loss:0.44375761135887914\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.48322428709276455\n",
      "train loss:0.8088412481812348\n",
      "train loss:0.9863345280418159\n",
      "train loss:0.743860777953414\n",
      "train loss:0.5404255303959183\n",
      "train loss:0.671835817154438\n",
      "train loss:0.46961491877253436\n",
      "train loss:0.5370562036828279\n",
      "train loss:0.6598623406674121\n",
      "train loss:0.45594098779532777\n",
      "train loss:0.548119471862145\n",
      "train loss:0.7098205196689663\n",
      "train loss:0.427551959658656\n",
      "train loss:0.5148627436496115\n",
      "train loss:0.5797668686631512\n",
      "train loss:0.8526976606760657\n",
      "train loss:0.6172988024703263\n",
      "train loss:0.6096115819319785\n",
      "train loss:0.45239958998966545\n",
      "train loss:0.880990548411179\n",
      "train loss:0.6222158849786148\n",
      "train loss:0.4442446959974301\n",
      "train loss:0.918519218317553\n",
      "train loss:0.5897487505567182\n",
      "train loss:0.7113379196750695\n",
      "train loss:0.526388657512545\n",
      "train loss:0.5472592896192727\n",
      "train loss:0.6866901106075043\n",
      "train loss:0.4961824134181165\n",
      "train loss:0.6473111789880679\n",
      "train loss:0.6769164237845763\n",
      "train loss:0.7096491423765732\n",
      "train loss:0.5215595899412124\n",
      "train loss:0.6623843106326242\n",
      "train loss:0.5700465011223061\n",
      "train loss:0.4593457053439173\n",
      "train loss:0.5584918313259529\n",
      "train loss:0.6768769215808857\n",
      "train loss:0.6180283748703281\n",
      "train loss:0.6326068791868162\n",
      "train loss:0.6087319340559987\n",
      "train loss:0.5237293078955184\n",
      "train loss:0.8772476185908793\n",
      "train loss:0.4045842312637159\n",
      "train loss:0.5405975805927724\n",
      "train loss:0.7115680416047854\n",
      "train loss:0.6383677525486847\n",
      "train loss:0.654165637957201\n",
      "train loss:0.49283922688574944\n",
      "train loss:0.5756446560089798\n",
      "train loss:0.5186160882806977\n",
      "train loss:0.617565403351949\n",
      "train loss:0.4645074289062173\n",
      "train loss:0.5288537121219484\n",
      "train loss:0.6599145638244279\n",
      "train loss:0.6949087938607144\n",
      "train loss:0.6405985166505178\n",
      "train loss:0.3857783503147675\n",
      "train loss:0.6195312827768684\n",
      "train loss:0.7545408146509305\n",
      "train loss:0.8144291696933508\n",
      "train loss:0.5277402731202382\n",
      "train loss:0.5662618450312366\n",
      "train loss:0.6406618259549706\n",
      "train loss:0.5946592904779966\n",
      "train loss:0.4468343902418413\n",
      "train loss:0.6830875244267582\n",
      "train loss:0.6453606741713374\n",
      "train loss:0.5927731288984441\n",
      "train loss:0.5541691287310198\n",
      "train loss:0.7427613445428551\n",
      "train loss:0.5426069361479133\n",
      "train loss:0.5983055715940072\n",
      "train loss:0.5865093910878793\n",
      "train loss:0.7034620261862687\n",
      "train loss:0.5219070113700858\n",
      "train loss:0.7933648721192395\n",
      "train loss:0.6004670781275937\n",
      "train loss:0.5384707270043977\n",
      "train loss:0.5093218984912239\n",
      "train loss:0.5605465138154453\n",
      "train loss:0.7515743812419794\n",
      "train loss:0.625502950513249\n",
      "train loss:0.633832521913926\n",
      "train loss:0.5291397936128541\n",
      "train loss:0.6785544124209065\n",
      "train loss:0.5826317812467285\n",
      "train loss:0.684212846824382\n",
      "train loss:0.7966267136526332\n",
      "train loss:0.6081261910262673\n",
      "train loss:0.3984488639546249\n",
      "train loss:0.607718298387558\n",
      "train loss:0.6799876209786816\n",
      "train loss:0.5398018382926704\n",
      "train loss:0.5393887539363704\n",
      "train loss:0.41137527147804603\n",
      "train loss:0.5298019244570547\n",
      "train loss:0.5228072706323756\n",
      "train loss:0.8085773473525786\n",
      "train loss:0.7661421722437031\n",
      "train loss:0.4864357005513146\n",
      "train loss:0.5057671959961008\n",
      "train loss:0.6260200145721169\n",
      "train loss:0.5503405468628009\n",
      "train loss:0.5811767297868905\n",
      "train loss:0.6812693172045255\n",
      "train loss:0.5011570808609342\n",
      "train loss:0.6831743776063514\n",
      "train loss:0.6154785066320912\n",
      "train loss:0.6977872310358227\n",
      "train loss:0.6978103493405163\n",
      "train loss:0.5867151550730265\n",
      "train loss:0.6901873648213165\n",
      "train loss:0.5746946698867331\n",
      "train loss:0.466340419569596\n",
      "train loss:0.6109614663852274\n",
      "train loss:0.6123189594212076\n",
      "train loss:0.5519972174823343\n",
      "train loss:0.6532463968709135\n",
      "train loss:0.5578971816813392\n",
      "train loss:0.46599093988696466\n",
      "train loss:0.5311296481296361\n",
      "train loss:0.7676898023173173\n",
      "train loss:0.6733407285800774\n",
      "train loss:0.48966551503265654\n",
      "train loss:0.6786665098643021\n",
      "train loss:0.41144990688567573\n",
      "train loss:0.504655620748741\n",
      "train loss:0.8291567972815315\n",
      "train loss:0.5227413435069077\n",
      "train loss:0.610002210853768\n",
      "train loss:0.5317584300543154\n",
      "train loss:0.5681757797945269\n",
      "train loss:0.6271340682764237\n",
      "train loss:0.6068464704414429\n",
      "train loss:0.47131284288802544\n",
      "train loss:0.4097757319107944\n",
      "train loss:0.5127088269992932\n",
      "train loss:0.7256257411487043\n",
      "train loss:0.7333843718409708\n",
      "train loss:0.48328028549798485\n",
      "train loss:0.9014048027515971\n",
      "train loss:0.9685146560887153\n",
      "train loss:0.4909752567633183\n",
      "train loss:0.799281130826239\n",
      "train loss:0.6315173595375317\n",
      "train loss:0.6196514851249444\n",
      "train loss:0.6302014449197801\n",
      "train loss:0.6394703778366942\n",
      "train loss:0.6871066169414528\n",
      "train loss:0.6770093738114356\n",
      "train loss:0.6014728047342439\n",
      "train loss:0.6559945083430354\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.699311596724887\n",
      "train loss:0.5921824440098591\n",
      "train loss:0.557283176877754\n",
      "train loss:0.627784178442293\n",
      "train loss:0.7068187681626623\n",
      "train loss:0.579081715378552\n",
      "train loss:0.5289252064693317\n",
      "train loss:0.7338992899882789\n",
      "train loss:0.5520592235087726\n",
      "train loss:0.67208417284736\n",
      "train loss:0.764299313979796\n",
      "train loss:0.4734021192662567\n",
      "train loss:0.6016097305121317\n",
      "train loss:0.7219821157627472\n",
      "train loss:0.6031125808856963\n",
      "train loss:0.5014088769725112\n",
      "train loss:0.6287830500531915\n",
      "train loss:0.6041066659410691\n",
      "train loss:0.7148303669831648\n",
      "train loss:0.38141478304561527\n",
      "train loss:0.6371821117993947\n",
      "train loss:0.5251559965290823\n",
      "train loss:0.3842516467022409\n",
      "train loss:0.3627652972248198\n",
      "train loss:0.519649679343351\n",
      "train loss:0.3641083632693595\n",
      "train loss:0.36174540360278107\n",
      "train loss:0.6042552584101276\n",
      "train loss:0.6979223345844925\n",
      "train loss:0.6845405092401864\n",
      "train loss:0.6402741831685687\n",
      "train loss:0.5281120890101747\n",
      "train loss:0.639691073562616\n",
      "train loss:0.5318836184847898\n",
      "train loss:0.8542551861280294\n",
      "train loss:0.5622086412119683\n",
      "train loss:0.670214025377777\n",
      "train loss:0.7215794084898821\n",
      "train loss:0.5324421189473603\n",
      "train loss:0.6241078640170483\n",
      "train loss:0.58088834917802\n",
      "train loss:0.7268611318813338\n",
      "train loss:0.5917311697534645\n",
      "train loss:0.5969391475703918\n",
      "train loss:0.546564591745569\n",
      "train loss:0.6186918836469695\n",
      "train loss:0.6603097320389384\n",
      "train loss:0.5640639183539282\n",
      "train loss:0.5976781334917949\n",
      "train loss:0.5508443836367386\n",
      "train loss:0.6297779022164027\n",
      "train loss:0.6034470374362065\n",
      "train loss:0.8090449018211852\n",
      "train loss:0.7446705688037782\n",
      "train loss:0.4729427236908285\n",
      "train loss:0.5337312016903287\n",
      "train loss:0.7937378368963453\n",
      "train loss:0.3633965171558664\n",
      "train loss:0.3430556397265471\n",
      "train loss:0.6005408463256009\n",
      "train loss:0.46331018380081596\n",
      "train loss:0.623362528000587\n",
      "train loss:0.488934060894166\n",
      "train loss:0.4912073609254299\n",
      "train loss:0.8507074499067284\n",
      "train loss:0.7421883879668696\n",
      "train loss:0.5741296058761047\n",
      "train loss:0.7179657440811756\n",
      "train loss:0.41139997112843607\n",
      "train loss:0.40499256729978744\n",
      "train loss:0.6856823056568965\n",
      "train loss:0.4135879141340988\n",
      "train loss:0.6658382698098516\n",
      "train loss:0.5338895869253495\n",
      "train loss:0.6642488268961875\n",
      "train loss:0.9332152932391524\n",
      "train loss:0.6356797714316751\n",
      "train loss:0.47213050231314624\n",
      "train loss:0.6004052146356085\n",
      "train loss:0.5491106684238709\n",
      "train loss:0.5286561759265305\n",
      "train loss:0.5923314682569529\n",
      "train loss:0.5149968035661104\n",
      "train loss:0.580496488001565\n",
      "train loss:0.5937762735482932\n",
      "train loss:0.5661391876441284\n",
      "train loss:0.696277629682125\n",
      "train loss:0.6593310930562664\n",
      "train loss:0.4477634640347466\n",
      "train loss:0.6465126658371044\n",
      "train loss:0.9099612441171281\n",
      "train loss:0.60048897024869\n",
      "train loss:0.46207990040564645\n",
      "train loss:0.6122035603614696\n",
      "train loss:0.593780645713854\n",
      "train loss:0.6623960886373117\n",
      "train loss:0.7528585696558028\n",
      "train loss:0.47797741711337227\n",
      "train loss:0.7484597035265276\n",
      "train loss:0.5497046566668564\n",
      "train loss:0.46420939588029075\n",
      "train loss:0.5678704795913025\n",
      "train loss:0.6628359311824565\n",
      "train loss:0.5866158822283782\n",
      "train loss:0.5709160737951271\n",
      "train loss:0.45433858342459255\n",
      "train loss:0.42186821180040096\n",
      "train loss:0.38736447163728877\n",
      "train loss:0.7400608402473424\n",
      "train loss:0.5521959644365365\n",
      "train loss:0.9185757074246365\n",
      "train loss:0.5046891509419704\n",
      "train loss:0.511480481400503\n",
      "train loss:0.6302415822151766\n",
      "train loss:0.6291590879794005\n",
      "train loss:0.3951929346648541\n",
      "train loss:0.5989340204334428\n",
      "train loss:0.5282888709479648\n",
      "train loss:0.8932670873837523\n",
      "train loss:0.6309911902336485\n",
      "train loss:0.6094972176630549\n",
      "train loss:0.41766660083330753\n",
      "train loss:0.5940153166450772\n",
      "train loss:0.6174555888248704\n",
      "train loss:0.8529791553757924\n",
      "train loss:0.5531258121635171\n",
      "train loss:0.6160072885884464\n",
      "train loss:0.6389037676562187\n",
      "train loss:0.5435065719919646\n",
      "train loss:0.6261397718492681\n",
      "train loss:0.7847970584018266\n",
      "train loss:0.6541068112648135\n",
      "train loss:0.6020472396817145\n",
      "train loss:0.6252987631972344\n",
      "train loss:0.5824896364708726\n",
      "train loss:0.577797645520167\n",
      "train loss:0.568794546287374\n",
      "train loss:0.7260752165515517\n",
      "train loss:0.6265318775271742\n",
      "train loss:0.5533231732359744\n",
      "train loss:0.6602669745935008\n",
      "train loss:0.46669909925048403\n",
      "train loss:0.5682918096348608\n",
      "train loss:0.5177790007406334\n",
      "train loss:0.36861264173626085\n",
      "train loss:0.4754798530330578\n",
      "train loss:0.6809428639774467\n",
      "train loss:0.45764592051678993\n",
      "train loss:0.46693443412031793\n",
      "train loss:0.5070085676248116\n",
      "train loss:1.0288944996936966\n",
      "train loss:0.46687546002551894\n",
      "train loss:1.0812921231494796\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7350616931599138\n",
      "train loss:0.7639414212240309\n",
      "train loss:0.7072035487610762\n",
      "train loss:0.6317967734272342\n",
      "train loss:0.6781204347946319\n",
      "train loss:0.6072900120682903\n",
      "train loss:0.5898426438209626\n",
      "train loss:0.6901059013515256\n",
      "train loss:0.5261000312600608\n",
      "train loss:0.647768261822568\n",
      "train loss:0.5624330386299873\n",
      "train loss:0.6976075540804031\n",
      "train loss:0.6882433067058134\n",
      "train loss:0.6712351360683819\n",
      "train loss:0.6882459103960323\n",
      "train loss:0.5844379036981433\n",
      "train loss:0.7553496972592227\n",
      "train loss:0.6062008018884273\n",
      "train loss:0.5957562016738056\n",
      "train loss:0.5268946316714909\n",
      "train loss:0.6275245048790428\n",
      "train loss:0.5763528296386188\n",
      "train loss:0.5971829572611271\n",
      "train loss:0.2914245978583648\n",
      "train loss:0.5081943669590776\n",
      "train loss:0.8076757093683081\n",
      "train loss:0.9102054493629208\n",
      "train loss:0.8314362399599787\n",
      "train loss:0.7688836719525953\n",
      "train loss:0.5684472168233025\n",
      "train loss:0.7460528247138519\n",
      "train loss:0.5134712337617564\n",
      "train loss:0.5618264959898452\n",
      "train loss:0.6661026280422224\n",
      "train loss:0.5304455394857853\n",
      "train loss:0.6447881348627854\n",
      "train loss:0.45125123013954777\n",
      "train loss:0.6264232834479706\n",
      "train loss:0.7051891726556678\n",
      "train loss:0.6257465989192252\n",
      "train loss:0.6427644732922457\n",
      "train loss:0.6411741963588187\n",
      "train loss:0.5847991568778512\n",
      "train loss:0.6259096189878047\n",
      "train loss:0.5594614794557653\n",
      "train loss:0.5536364741837205\n",
      "train loss:0.587232818587447\n",
      "train loss:0.5700059858512109\n",
      "train loss:0.5348496874248336\n",
      "train loss:0.6551475556217173\n",
      "train loss:0.45303766441010207\n",
      "train loss:0.7181966483671672\n",
      "train loss:0.7331604548264564\n",
      "train loss:0.6886809769857634\n",
      "train loss:0.5880070130794606\n",
      "train loss:0.6930534162244996\n",
      "train loss:0.6176314888193715\n",
      "train loss:0.6105284510352029\n",
      "train loss:0.590217106405459\n",
      "train loss:0.520607324410238\n",
      "train loss:0.6026796204993612\n",
      "train loss:0.47432500143427764\n",
      "train loss:0.5974514234298929\n",
      "train loss:0.733932949102359\n",
      "train loss:0.7368442388906851\n",
      "train loss:0.6733316620941455\n",
      "train loss:0.5558042620632575\n",
      "train loss:0.7324526701415266\n",
      "train loss:0.5655852169287389\n",
      "train loss:0.5942042097893254\n",
      "train loss:0.48894533502321896\n",
      "train loss:0.6210856830081166\n",
      "train loss:0.5645844177388251\n",
      "train loss:0.4313187748099036\n",
      "train loss:0.5015660243133596\n",
      "train loss:0.6671389051279584\n",
      "train loss:0.5011887927857429\n",
      "train loss:0.7775449954738126\n",
      "train loss:0.772987701204502\n",
      "train loss:0.4569446114686434\n",
      "train loss:0.5306172606842302\n",
      "train loss:0.5201953382087197\n",
      "train loss:0.5949612402505177\n",
      "train loss:0.5431721849951585\n",
      "train loss:0.698727962367426\n",
      "train loss:0.4066688568265063\n",
      "train loss:0.7650857239187975\n",
      "train loss:0.4308154787140763\n",
      "train loss:0.37077065580473734\n",
      "train loss:0.7513239712748497\n",
      "train loss:0.47791981691354674\n",
      "train loss:0.734243103439577\n",
      "train loss:0.4887506087810702\n",
      "train loss:0.4287775426747668\n",
      "train loss:0.48043487220909115\n",
      "train loss:0.6563297699574527\n",
      "train loss:0.5006228811681516\n",
      "train loss:0.6829811580040834\n",
      "train loss:0.34552286885933087\n",
      "train loss:0.4908365445937174\n",
      "train loss:0.5881887431253288\n",
      "train loss:0.43169470010633537\n",
      "train loss:0.538879410768905\n",
      "train loss:0.778032754881167\n",
      "train loss:0.5468110468614542\n",
      "train loss:0.7671787131302205\n",
      "train loss:0.7542780408813776\n",
      "train loss:0.5436164973290737\n",
      "train loss:0.7653535913351762\n",
      "train loss:0.41831889828200275\n",
      "train loss:0.526390491400307\n",
      "train loss:0.7751047615862705\n",
      "train loss:0.6024995054128013\n",
      "train loss:0.6941883550671977\n",
      "train loss:0.5714453074409491\n",
      "train loss:0.6317261059983886\n",
      "train loss:0.4230855321479946\n",
      "train loss:0.5873576281853106\n",
      "train loss:0.5007317794493102\n",
      "train loss:0.6694675617015473\n",
      "train loss:0.5094201298243626\n",
      "train loss:0.6319199387977223\n",
      "train loss:0.7632228793629044\n",
      "train loss:0.5633883929788607\n",
      "train loss:0.5462247103189318\n",
      "train loss:0.45177664104956233\n",
      "train loss:0.5776001432977473\n",
      "train loss:0.9164465504943685\n",
      "train loss:0.5870993267336561\n",
      "train loss:0.5533995825052493\n",
      "train loss:0.591736066991049\n",
      "train loss:0.8591874941347063\n",
      "train loss:0.5716442155183105\n",
      "train loss:0.4317912639080611\n",
      "train loss:0.5038430220864413\n",
      "train loss:0.7961527636802925\n",
      "train loss:0.5436272031288325\n",
      "train loss:0.7127782899541362\n",
      "train loss:0.7150316423829024\n",
      "train loss:0.4873786692855754\n",
      "train loss:0.697892091637856\n",
      "train loss:0.6854816274141813\n",
      "train loss:0.6791666840312783\n",
      "train loss:0.46731659061626657\n",
      "train loss:0.5483192164493883\n",
      "train loss:0.490097185331393\n",
      "train loss:0.47082465901451814\n",
      "train loss:0.40848398057539537\n",
      "train loss:0.39360516479804175\n",
      "train loss:0.832092453913836\n",
      "train loss:0.7273833311582674\n",
      "train loss:0.7973672391976738\n",
      "train loss:0.6488604406547511\n",
      "=== epoch:5, train acc:0.73, test acc:0.69 ===\n",
      "train loss:0.6778799712095377\n",
      "train loss:0.5968449008153174\n",
      "train loss:0.7974386110144382\n",
      "train loss:0.5148092305079763\n",
      "train loss:0.8089235288451265\n",
      "train loss:0.5259804977753066\n",
      "train loss:0.5493034391470465\n",
      "train loss:0.7088156584220693\n",
      "train loss:0.5772088591394575\n",
      "train loss:0.5976270840042051\n",
      "train loss:0.726214687120379\n",
      "train loss:0.6448473367096985\n",
      "train loss:0.5672145322135209\n",
      "train loss:0.6471629029487789\n",
      "train loss:0.5644353166001619\n",
      "train loss:0.6858834181910267\n",
      "train loss:0.437033623097676\n",
      "train loss:0.48891856038277826\n",
      "train loss:0.5576018571056073\n",
      "train loss:0.6311813134552724\n",
      "train loss:0.672389301206577\n",
      "train loss:0.6215227394036628\n",
      "train loss:0.694972154478844\n",
      "train loss:0.5882154545302143\n",
      "train loss:0.4376489459445156\n",
      "train loss:0.7971948460999196\n",
      "train loss:0.6815381321780254\n",
      "train loss:0.5858623719252116\n",
      "train loss:0.5822206224567609\n",
      "train loss:0.5220198530709482\n",
      "train loss:0.5382982301059156\n",
      "train loss:0.4997906794589932\n",
      "train loss:0.37515446235065114\n",
      "train loss:0.5639302789778798\n",
      "train loss:0.6249151288863484\n",
      "train loss:0.7236930024134202\n",
      "train loss:0.5120806844047081\n",
      "train loss:0.8245793462885901\n",
      "train loss:0.641376950116409\n",
      "train loss:0.6030409088670091\n",
      "train loss:0.583629171240814\n",
      "train loss:0.46387656051105247\n",
      "train loss:0.6632845316524583\n",
      "train loss:0.7390850295689717\n",
      "train loss:0.6643415532308417\n",
      "train loss:0.5894988880371054\n",
      "train loss:0.6352087584180256\n",
      "train loss:0.606609928550152\n",
      "train loss:0.5535785272787933\n",
      "train loss:0.5914135956813921\n",
      "train loss:0.6216961762955626\n",
      "train loss:0.5972721239836696\n",
      "train loss:0.5527475803021086\n",
      "train loss:0.6035306916124858\n",
      "train loss:0.5182719928170704\n",
      "train loss:0.4606038787181702\n",
      "train loss:0.5242543076586725\n",
      "train loss:0.8394398811918936\n",
      "train loss:0.4585197773191725\n",
      "train loss:0.5375486326772949\n",
      "train loss:0.3928995725790848\n",
      "train loss:0.46387622718745253\n",
      "train loss:0.30098494194463826\n",
      "train loss:0.9538673323603485\n",
      "train loss:0.5132070541487599\n",
      "train loss:0.7493640722348238\n",
      "train loss:0.711225216003047\n",
      "train loss:0.6204412898304245\n",
      "train loss:0.49983636412522403\n",
      "train loss:0.7774817259995528\n",
      "train loss:0.5477565376768295\n",
      "train loss:0.7424876446348546\n",
      "train loss:0.5828651211376327\n",
      "train loss:0.5942268228317085\n",
      "train loss:0.6447879749614861\n",
      "train loss:0.6133170438562429\n",
      "train loss:0.6993486335489256\n",
      "train loss:0.4805167099465278\n",
      "train loss:0.6811049249006796\n",
      "train loss:0.6143943426736181\n",
      "train loss:0.7122560996702767\n",
      "train loss:0.6945695208647624\n",
      "train loss:0.6268792824257863\n",
      "train loss:0.7366012008513285\n",
      "train loss:0.6763927986694557\n",
      "train loss:0.5870482969289507\n",
      "train loss:0.6559235725818187\n",
      "train loss:0.5566145282886417\n",
      "train loss:0.5404230112943169\n",
      "train loss:0.5620158025798304\n",
      "train loss:0.5535698157753972\n",
      "train loss:0.5338530509931485\n",
      "train loss:0.6570066530913594\n",
      "train loss:0.5058744367184631\n",
      "train loss:0.8156204492853709\n",
      "train loss:0.608972323867178\n",
      "train loss:0.5111092581270889\n",
      "train loss:0.6555047405806745\n",
      "train loss:0.5264101318152832\n",
      "train loss:0.6572083542317093\n",
      "train loss:0.43737374670853857\n",
      "train loss:0.5869281934437298\n",
      "train loss:0.7587751280380598\n",
      "train loss:0.5065107236461472\n",
      "train loss:0.6791827816205906\n",
      "train loss:0.6364688579299309\n",
      "train loss:0.4461798836261652\n",
      "train loss:0.694374517812536\n",
      "train loss:0.41707627751484866\n",
      "train loss:0.3413935461027079\n",
      "train loss:0.4161071913068528\n",
      "train loss:0.5065562826167607\n",
      "train loss:0.6036205606511618\n",
      "train loss:0.7259432617769529\n",
      "train loss:0.8418866622575875\n",
      "train loss:0.4016738589313535\n",
      "train loss:0.9746954359607745\n",
      "train loss:0.5349226715974323\n",
      "train loss:0.5999647697468331\n",
      "train loss:0.6396010532635541\n",
      "train loss:0.6133376743637262\n",
      "train loss:0.7339926116821356\n",
      "train loss:0.6640755013857881\n",
      "train loss:0.6877474287093306\n",
      "train loss:0.6157174125036432\n",
      "train loss:0.5185534560289728\n",
      "train loss:0.5352453437141645\n",
      "train loss:0.6761140390940559\n",
      "train loss:0.5800174872612655\n",
      "train loss:0.69426708549524\n",
      "train loss:0.8406879923775945\n",
      "train loss:0.6256571031019993\n",
      "train loss:0.5536386654063036\n",
      "train loss:0.573331870863443\n",
      "train loss:0.5601076465458312\n",
      "train loss:0.5336107093063833\n",
      "train loss:0.47796211919554443\n",
      "train loss:0.6323434900212217\n",
      "train loss:0.48100213284631677\n",
      "train loss:0.6099947851983964\n",
      "train loss:0.5176092383656925\n",
      "train loss:0.6017757839827147\n",
      "train loss:0.7089367919673152\n",
      "train loss:0.6786079168108798\n",
      "train loss:0.5686626689666728\n",
      "train loss:0.4332868453404898\n",
      "train loss:0.6433045588283861\n",
      "train loss:0.8239241239533897\n",
      "train loss:0.7051335692132942\n",
      "train loss:0.6507071687672163\n",
      "train loss:0.46378885178055657\n",
      "train loss:0.5444277174726504\n",
      "train loss:0.6964114694755301\n",
      "=== epoch:6, train acc:0.73, test acc:0.69 ===\n",
      "train loss:0.5632383011961395\n",
      "train loss:0.4091296482246398\n",
      "train loss:0.6942341245254557\n",
      "train loss:0.5373758709175996\n",
      "train loss:0.6560508268116448\n",
      "train loss:0.6850093543187468\n",
      "train loss:0.39311482559103944\n",
      "train loss:0.5275177589343987\n",
      "train loss:0.537973208786174\n",
      "train loss:0.5381498919125762\n",
      "train loss:0.6406330323543331\n",
      "train loss:0.6523444894729176\n",
      "train loss:0.5958941064249973\n",
      "train loss:0.5078995667016871\n",
      "train loss:0.5725489309825045\n",
      "train loss:0.4704733267652899\n",
      "train loss:0.7800814900891363\n",
      "train loss:0.4061546215298975\n",
      "train loss:0.5254958575989757\n",
      "train loss:0.6064000612540181\n",
      "train loss:0.5498507288928997\n",
      "train loss:0.6898351741903777\n",
      "train loss:0.6277706138080692\n",
      "train loss:0.45876666801407107\n",
      "train loss:0.6199858715475876\n",
      "train loss:0.5533969541257183\n",
      "train loss:0.7075280031366116\n",
      "train loss:0.81832583507134\n",
      "train loss:0.7593353504873771\n",
      "train loss:0.6212602350883767\n",
      "train loss:0.7395272024928359\n",
      "train loss:0.7426994756761314\n",
      "train loss:0.5652626055300229\n",
      "train loss:0.5566678156947092\n",
      "train loss:0.567627387820898\n",
      "train loss:0.5468950298642168\n",
      "train loss:0.5601776920894335\n",
      "train loss:0.5935261397407877\n",
      "train loss:0.6316588933471783\n",
      "train loss:0.5430967311806951\n",
      "train loss:0.4567286449312597\n",
      "train loss:0.6615695538462452\n",
      "train loss:0.483232332229152\n",
      "train loss:0.4994100900752083\n",
      "train loss:0.37961005509046963\n",
      "train loss:0.32820616047053525\n",
      "train loss:0.744615365782161\n",
      "train loss:0.11441094873999147\n",
      "train loss:0.33984181944213004\n",
      "train loss:0.8870615390261831\n",
      "train loss:0.5029863439024913\n",
      "train loss:0.34659914284780585\n",
      "train loss:0.33435757598449845\n",
      "train loss:0.5399535935340731\n",
      "train loss:0.7773594325279805\n",
      "train loss:0.6902526418248092\n",
      "train loss:0.4890412364366147\n",
      "train loss:0.5895070629071516\n",
      "train loss:0.7079772058555226\n",
      "train loss:0.4618955117674849\n",
      "train loss:0.40758018312458394\n",
      "train loss:0.7093119501061164\n",
      "train loss:0.5686070443042246\n",
      "train loss:0.41212333265692036\n",
      "train loss:0.6469647740016068\n",
      "train loss:0.603393735584481\n",
      "train loss:0.5982495018760056\n",
      "train loss:0.6724495286680383\n",
      "train loss:0.6760022534056483\n",
      "train loss:0.6463014770867183\n",
      "train loss:0.5191495480691657\n",
      "train loss:0.6747914756289465\n",
      "train loss:0.5156162197612353\n",
      "train loss:0.5100874748572031\n",
      "train loss:0.5303995079058818\n",
      "train loss:0.4239205158341705\n",
      "train loss:0.5802130961155696\n",
      "train loss:0.4603284949799195\n",
      "train loss:0.8750487124091337\n",
      "train loss:0.44792225079213066\n",
      "train loss:0.2634719763902958\n",
      "train loss:0.8120048818607797\n",
      "train loss:0.24195800334784762\n",
      "train loss:0.4161284640595679\n",
      "train loss:0.3256895447435072\n",
      "train loss:0.5240430558490491\n",
      "train loss:0.5973042108152912\n",
      "train loss:0.8173452657111643\n",
      "train loss:0.3593883452329658\n",
      "train loss:0.3838748754959564\n",
      "train loss:0.5887202539445415\n",
      "train loss:0.4136224251513257\n",
      "train loss:0.49278456030232726\n",
      "train loss:0.15812348423153516\n",
      "train loss:0.5136260011624287\n",
      "train loss:0.9915226977651999\n",
      "train loss:0.38405436259523407\n",
      "train loss:0.41171504901926603\n",
      "train loss:0.33195145486022315\n",
      "train loss:0.8789635089747915\n",
      "train loss:0.5135502077019748\n",
      "train loss:0.4518946842950342\n",
      "train loss:0.43935741895838987\n",
      "train loss:0.6247703694949831\n",
      "train loss:0.45906530493590597\n",
      "train loss:0.6582327348279635\n",
      "train loss:0.5024366094448433\n",
      "train loss:0.5841403095328115\n",
      "train loss:0.5088344131871179\n",
      "train loss:0.6720464346263835\n",
      "train loss:0.6996960223036155\n",
      "train loss:0.6512900549474672\n",
      "train loss:0.48098060710777507\n",
      "train loss:0.7114880708405666\n",
      "train loss:0.5470722220246456\n",
      "train loss:0.45585969531598175\n",
      "train loss:0.475733468776963\n",
      "train loss:0.5786412999974458\n",
      "train loss:0.6481397214678802\n",
      "train loss:0.4673007206223894\n",
      "train loss:0.3423767957278361\n",
      "train loss:0.5472111466308329\n",
      "train loss:0.6354573361394846\n",
      "train loss:0.553288304702322\n",
      "train loss:0.568338843391764\n",
      "train loss:0.5174934536492402\n",
      "train loss:0.7451597049524962\n",
      "train loss:0.39011063983223804\n",
      "train loss:0.8075062202882263\n",
      "train loss:0.36153065961479497\n",
      "train loss:0.5151749537126669\n",
      "train loss:0.4936680594483088\n",
      "train loss:0.5407845426194408\n",
      "train loss:0.6096693119547205\n",
      "train loss:0.6542522368276593\n",
      "train loss:0.4815915538297403\n",
      "train loss:0.38671058262500013\n",
      "train loss:0.37207632158179926\n",
      "train loss:0.44153322509087\n",
      "train loss:0.43682477210751197\n",
      "train loss:0.7794200609281189\n",
      "train loss:0.13859990388240764\n",
      "train loss:0.6011934272857603\n",
      "train loss:0.6495828099018205\n",
      "train loss:0.35450815456451557\n",
      "train loss:0.43904882255286803\n",
      "train loss:0.47313337739495304\n",
      "train loss:0.7772232037424558\n",
      "train loss:0.36105244368344314\n",
      "train loss:0.4902550263558842\n",
      "train loss:0.6391376573053051\n",
      "train loss:0.4148547347714996\n",
      "train loss:0.3566055507879514\n",
      "=== epoch:7, train acc:0.76, test acc:0.69 ===\n",
      "train loss:0.3443796453554091\n",
      "train loss:0.5182435814309521\n",
      "train loss:0.3937700601752472\n",
      "train loss:0.6299886459223333\n",
      "train loss:0.5233282318321165\n",
      "train loss:0.890157524348829\n",
      "train loss:0.5052869335192979\n",
      "train loss:0.5668400290163813\n",
      "train loss:0.7331402809358435\n",
      "train loss:0.4382996800181386\n",
      "train loss:0.6761191862788474\n",
      "train loss:0.5635173838750118\n",
      "train loss:0.6463576794022537\n",
      "train loss:0.4682831277094942\n",
      "train loss:0.653350035448237\n",
      "train loss:0.7495700106194855\n",
      "train loss:0.5677046412413936\n",
      "train loss:0.45903719812856786\n",
      "train loss:0.6845512150086319\n",
      "train loss:0.5838035026044658\n",
      "train loss:0.5377524682220651\n",
      "train loss:0.5096117881837081\n",
      "train loss:0.6288312587047578\n",
      "train loss:0.3150332322039916\n",
      "train loss:0.5068076658106127\n",
      "train loss:0.6028130855935762\n",
      "train loss:0.7240001472145109\n",
      "train loss:0.6378187290885908\n",
      "train loss:0.6688938802475368\n",
      "train loss:0.6214456220626295\n",
      "train loss:0.42944434914170787\n",
      "train loss:0.4283935025252217\n",
      "train loss:0.4961154471629806\n",
      "train loss:0.2269501711070812\n",
      "train loss:0.6318333583340345\n",
      "train loss:0.39427754962975115\n",
      "train loss:0.609981735902447\n",
      "train loss:0.6790270835082636\n",
      "train loss:0.3092657872053128\n",
      "train loss:0.44978426495031815\n",
      "train loss:0.40076904683332293\n",
      "train loss:0.7932804371770728\n",
      "train loss:0.22941405241944746\n",
      "train loss:0.7170040422706752\n",
      "train loss:0.45066835951755946\n",
      "train loss:0.5027933335425294\n",
      "train loss:0.6707699292670374\n",
      "train loss:0.3640247993702309\n",
      "train loss:0.39944282666224795\n",
      "train loss:0.48544119692837995\n",
      "train loss:0.7824937218458794\n",
      "train loss:0.5107344794710879\n",
      "train loss:0.6406991295545107\n",
      "train loss:0.5715424704767226\n",
      "train loss:0.45481789269621775\n",
      "train loss:0.6039228034088351\n",
      "train loss:0.5165349251342344\n",
      "train loss:0.35273704243863463\n",
      "train loss:0.5508198240339368\n",
      "train loss:0.45986704819858604\n",
      "train loss:0.42516470635402903\n",
      "train loss:0.5037117883912497\n",
      "train loss:0.6701924355765155\n",
      "train loss:0.7878090553108518\n",
      "train loss:0.4178197528491455\n",
      "train loss:0.475988131007053\n",
      "train loss:0.5615590439743278\n",
      "train loss:0.5078649553927289\n",
      "train loss:0.675605782598362\n",
      "train loss:0.6287397886141212\n",
      "train loss:0.5070535159898355\n",
      "train loss:0.37580641838176465\n",
      "train loss:0.6711581246022363\n",
      "train loss:0.855565195380785\n",
      "train loss:0.7892618321096413\n",
      "train loss:0.4464357523470068\n",
      "train loss:0.6899356121564238\n",
      "train loss:0.6142558114756584\n",
      "train loss:0.4308367294790555\n",
      "train loss:0.4743568804873579\n",
      "train loss:0.3244627951259733\n",
      "train loss:0.5225048593028596\n",
      "train loss:0.5670144908322337\n",
      "train loss:0.41144812800755365\n",
      "train loss:0.6128977714494124\n",
      "train loss:0.7876670626055022\n",
      "train loss:0.5072839957601898\n",
      "train loss:0.4343806939972049\n",
      "train loss:0.6257663269181217\n",
      "train loss:0.3948105814004899\n",
      "train loss:0.35280186244276107\n",
      "train loss:0.4718753114378397\n",
      "train loss:0.6439191012054069\n",
      "train loss:0.6178728540530682\n",
      "train loss:0.7645776867549146\n",
      "train loss:0.6581912332913916\n",
      "train loss:0.6726978840273877\n",
      "train loss:0.3497626770006562\n",
      "train loss:0.49700297829197826\n",
      "train loss:0.4491230371406606\n",
      "train loss:0.6025917248727797\n",
      "train loss:0.4134232176142971\n",
      "train loss:0.4906685090778864\n",
      "train loss:0.5692165038178316\n",
      "train loss:0.44185109461657734\n",
      "train loss:0.41577631322455455\n",
      "train loss:0.5898292871899941\n",
      "train loss:0.49440890673302934\n",
      "train loss:0.3475850179955885\n",
      "train loss:0.44663290930198346\n",
      "train loss:0.6278516547461239\n",
      "train loss:0.3561971395679663\n",
      "train loss:0.5142492811608828\n",
      "train loss:0.26557047965011604\n",
      "train loss:0.31495724472879927\n",
      "train loss:0.7191082068633183\n",
      "train loss:0.4829236559926617\n",
      "train loss:0.9864593976219158\n",
      "train loss:0.9169071140859101\n",
      "train loss:0.7547849461965328\n",
      "train loss:0.47375319354587714\n",
      "train loss:0.6386351466195764\n",
      "train loss:0.6440772852540129\n",
      "train loss:0.5097972049531707\n",
      "train loss:0.5623513679259179\n",
      "train loss:0.5032000218591798\n",
      "train loss:0.5136189238988766\n",
      "train loss:0.5760033544110784\n",
      "train loss:0.539162688466041\n",
      "train loss:0.5736763491883765\n",
      "train loss:0.5468762844874623\n",
      "train loss:0.5156657454657843\n",
      "train loss:0.7229722941116822\n",
      "train loss:0.7255277718380562\n",
      "train loss:0.5879839307088377\n",
      "train loss:0.5380283783103903\n",
      "train loss:0.48664661841555645\n",
      "train loss:0.3069757377655597\n",
      "train loss:0.54907428690069\n",
      "train loss:0.4727540930622968\n",
      "train loss:0.4632447978066394\n",
      "train loss:0.32768869106756754\n",
      "train loss:0.7653824293618732\n",
      "train loss:0.4403672735457039\n",
      "train loss:0.7761886021123152\n",
      "train loss:0.5580678070973567\n",
      "train loss:0.35759914811277965\n",
      "train loss:0.644943152079833\n",
      "train loss:0.5021244751341112\n",
      "train loss:0.459403292607817\n",
      "train loss:0.6519040422265929\n",
      "train loss:0.4770984579185603\n",
      "train loss:0.4907149345288583\n",
      "=== epoch:8, train acc:0.75, test acc:0.68 ===\n",
      "train loss:0.6981248710382668\n",
      "train loss:0.42842268039612624\n",
      "train loss:0.6445056514240948\n",
      "train loss:0.5183094875194052\n",
      "train loss:0.5674863948337945\n",
      "train loss:0.41370479205803345\n",
      "train loss:0.39390037756182894\n",
      "train loss:0.5394907746923111\n",
      "train loss:0.5730003104038943\n",
      "train loss:0.608522648648588\n",
      "train loss:0.4888114362777368\n",
      "train loss:0.3658980797572101\n",
      "train loss:0.6556163150450645\n",
      "train loss:0.5726664735999044\n",
      "train loss:0.5878206292430914\n",
      "train loss:0.5446970166823507\n",
      "train loss:0.5578817765057155\n",
      "train loss:0.3342894212034564\n",
      "train loss:0.5933688697409629\n",
      "train loss:0.6637068794859704\n",
      "train loss:0.6792241961710324\n",
      "train loss:0.4348175443460837\n",
      "train loss:0.590345691697262\n",
      "train loss:0.6051104686527634\n",
      "train loss:0.5814783965645648\n",
      "train loss:0.40600660735666755\n",
      "train loss:0.5087178775176742\n",
      "train loss:0.622107419550372\n",
      "train loss:0.6275798476413225\n",
      "train loss:0.44144955746105274\n",
      "train loss:0.6887296780114456\n",
      "train loss:0.5840373912055732\n",
      "train loss:0.3550359123568949\n",
      "train loss:0.7909937576373659\n",
      "train loss:0.728743206295313\n",
      "train loss:0.5157077626047506\n",
      "train loss:0.5854283636927016\n",
      "train loss:0.5915442164600445\n",
      "train loss:0.6584931933515114\n",
      "train loss:0.5527211896683375\n",
      "train loss:0.6591577186729217\n",
      "train loss:0.582627781184416\n",
      "train loss:0.6519998428122794\n",
      "train loss:0.654902225335173\n",
      "train loss:0.6984631764913406\n",
      "train loss:0.6279880900330126\n",
      "train loss:0.6617931365196883\n",
      "train loss:0.6805224918517846\n",
      "train loss:0.620601670714516\n",
      "train loss:0.5875375142836164\n",
      "train loss:0.737418495096542\n",
      "train loss:0.5768401501884088\n",
      "train loss:0.4926188926700939\n",
      "train loss:0.6496016082814722\n",
      "train loss:0.4110327537606914\n",
      "train loss:0.5822674339349556\n",
      "train loss:0.5191311552674156\n",
      "train loss:0.3821886262901991\n",
      "train loss:0.38279862059571246\n",
      "train loss:0.6088438390468871\n",
      "train loss:0.48793087696199394\n",
      "train loss:0.7403625760604697\n",
      "train loss:0.6197954171264191\n",
      "train loss:0.48512396112387457\n",
      "train loss:0.4543292027515083\n",
      "train loss:0.5244492367041634\n",
      "train loss:0.5606561219307038\n",
      "train loss:0.4778179210875623\n",
      "train loss:0.5458668204904567\n",
      "train loss:0.48729631318561645\n",
      "train loss:0.4311348783881094\n",
      "train loss:0.31141430509994433\n",
      "train loss:0.6083786663279411\n",
      "train loss:0.5629373948183531\n",
      "train loss:0.42251920452569475\n",
      "train loss:0.4714293167061604\n",
      "train loss:0.32674268647548976\n",
      "train loss:0.8637457038262226\n",
      "train loss:0.933897409602731\n",
      "train loss:0.5444570391886514\n",
      "train loss:0.9869781261836928\n",
      "train loss:0.7644659071033884\n",
      "train loss:0.6625649861904885\n",
      "train loss:0.5972119744647078\n",
      "train loss:0.6343042526605209\n",
      "train loss:0.6015441762276803\n",
      "train loss:0.5987278372934355\n",
      "train loss:0.6335407031410579\n",
      "train loss:0.5953642450597247\n",
      "train loss:0.7072520847775519\n",
      "train loss:0.6623981625285346\n",
      "train loss:0.6139443667796399\n",
      "train loss:0.5732386583612209\n",
      "train loss:0.6719283649932095\n",
      "train loss:0.43790986813024435\n",
      "train loss:0.6991195840849922\n",
      "train loss:0.7234315932971398\n",
      "train loss:0.6954120806157896\n",
      "train loss:0.7077698428331547\n",
      "train loss:0.6400329305493135\n",
      "train loss:0.5803604485770063\n",
      "train loss:0.6487312138803277\n",
      "train loss:0.41042003562306323\n",
      "train loss:0.5571697169962654\n",
      "train loss:0.5380119260426539\n",
      "train loss:0.5436644848469263\n",
      "train loss:0.6491595592334454\n",
      "train loss:0.6553991399479757\n",
      "train loss:0.5975851262846011\n",
      "train loss:0.3587515069706377\n",
      "train loss:0.4271302174935457\n",
      "train loss:0.4357365461756152\n",
      "train loss:0.23141433678132586\n",
      "train loss:0.3637919399494391\n",
      "train loss:0.3358385205073664\n",
      "train loss:0.5365561915708457\n",
      "train loss:1.355743160868386\n",
      "train loss:0.8756123394735008\n",
      "train loss:0.9497667523100919\n",
      "train loss:0.6303537688842498\n",
      "train loss:0.6954721881568725\n",
      "train loss:0.7345828720929317\n",
      "train loss:0.6165302870345776\n",
      "train loss:0.4610939476184335\n",
      "train loss:0.6826510517308263\n",
      "train loss:0.6869652383515965\n",
      "train loss:0.5672370865645893\n",
      "train loss:0.594985792123061\n",
      "train loss:0.48495872385623195\n",
      "train loss:0.6040333898973236\n",
      "train loss:0.5902870092980778\n",
      "train loss:0.5811785734462676\n",
      "train loss:0.6362188528297904\n",
      "train loss:0.6076837098472103\n",
      "train loss:0.5381438605883433\n",
      "train loss:0.4481535419447022\n",
      "train loss:0.5407800466234315\n",
      "train loss:0.6444020830603501\n",
      "train loss:0.5223851210452772\n",
      "train loss:0.7525987485739769\n",
      "train loss:0.5671631055367925\n",
      "train loss:0.4799602316465049\n",
      "train loss:0.6741057671525578\n",
      "train loss:0.5499504731861157\n",
      "train loss:0.6441021193057825\n",
      "train loss:0.6306447697310573\n",
      "train loss:0.5232337312380257\n",
      "train loss:0.5965251093593561\n",
      "train loss:0.7737817630098169\n",
      "train loss:0.5333054390704142\n",
      "train loss:0.57043602037052\n",
      "train loss:0.6771001328004261\n",
      "train loss:0.7565541966981078\n",
      "=== epoch:9, train acc:0.76, test acc:0.69 ===\n",
      "train loss:0.5872019318358569\n",
      "train loss:0.4484218237217849\n",
      "train loss:0.4995460212473569\n",
      "train loss:0.47218346654379467\n",
      "train loss:0.47466219982829705\n",
      "train loss:0.6090902769739212\n",
      "train loss:0.3246132484073684\n",
      "train loss:0.545731078077915\n",
      "train loss:0.5982591384731706\n",
      "train loss:0.3110206994172978\n",
      "train loss:0.5460285429995828\n",
      "train loss:0.4323318626261088\n",
      "train loss:0.39563781256950287\n",
      "train loss:0.4669002552501934\n",
      "train loss:0.5458106197894353\n",
      "train loss:0.7226478975691988\n",
      "train loss:0.6351605217176247\n",
      "train loss:0.4297128418986816\n",
      "train loss:0.3699851923977049\n",
      "train loss:0.828586769244685\n",
      "train loss:0.6051643196427541\n",
      "train loss:0.5572800289624983\n",
      "train loss:0.4695521129455161\n",
      "train loss:0.33678457153692487\n",
      "train loss:0.42953892832024404\n",
      "train loss:0.4375334088990487\n",
      "train loss:0.5512214317418833\n",
      "train loss:0.5838462350938546\n",
      "train loss:0.3801998809989577\n",
      "train loss:0.44168598984486385\n",
      "train loss:0.6034710874446009\n",
      "train loss:0.3884423616554787\n",
      "train loss:0.4537440393615261\n",
      "train loss:0.3648469943024476\n",
      "train loss:0.4959133376254015\n",
      "train loss:0.5322608395598694\n",
      "train loss:0.3088131109934423\n",
      "train loss:0.4423778959114735\n",
      "train loss:0.5733023297739744\n",
      "train loss:0.3345959270186623\n",
      "train loss:0.4617291591865529\n",
      "train loss:0.38188951481664823\n",
      "train loss:0.7924983332705512\n",
      "train loss:0.5356372307072681\n",
      "train loss:0.33165735942119134\n",
      "train loss:0.5019833938047434\n",
      "train loss:0.5128424174026104\n",
      "train loss:0.6014544412050983\n",
      "train loss:0.6859808922832504\n",
      "train loss:0.9187872410743452\n",
      "train loss:0.6809177868164522\n",
      "train loss:0.5816381773644046\n",
      "train loss:0.4585534481128318\n",
      "train loss:0.6649378353848071\n",
      "train loss:0.5427746191079346\n",
      "train loss:0.6008722924213075\n",
      "train loss:0.48346626795870434\n",
      "train loss:0.6616099583764188\n",
      "train loss:0.5447237559605056\n",
      "train loss:0.6251966155755576\n",
      "train loss:0.6308044295799347\n",
      "train loss:0.5153631262808606\n",
      "train loss:0.607432191911754\n",
      "train loss:0.5800952213944102\n",
      "train loss:0.5719940775318403\n",
      "train loss:0.5367790935727927\n",
      "train loss:0.47696457378751556\n",
      "train loss:0.6289116898189512\n",
      "train loss:0.2866549293845076\n",
      "train loss:0.6520093896966375\n",
      "train loss:0.6213285574927807\n",
      "train loss:0.5474921960593283\n",
      "train loss:0.5553912099588968\n",
      "train loss:0.8026522972277135\n",
      "train loss:0.3927016612073206\n",
      "train loss:0.8231608090933854\n",
      "train loss:0.4264248505009608\n",
      "train loss:0.5213325307561469\n",
      "train loss:0.6510034292221321\n",
      "train loss:0.5272616836485621\n",
      "train loss:0.5140469703955345\n",
      "train loss:0.5730500361382899\n",
      "train loss:0.3317940961481125\n",
      "train loss:0.5694233462354317\n",
      "train loss:0.6830502503463113\n",
      "train loss:0.4846183821684414\n",
      "train loss:0.49179216860896824\n",
      "train loss:0.4984975311430634\n",
      "train loss:0.6296427872470554\n",
      "train loss:0.7861685856809031\n",
      "train loss:0.4327949319030261\n",
      "train loss:0.5366547384010071\n",
      "train loss:0.42907808484439675\n",
      "train loss:0.600070449007258\n",
      "train loss:0.4736015471159879\n",
      "train loss:0.44186375497665786\n",
      "train loss:0.573337129680995\n",
      "train loss:0.5522389408974391\n",
      "train loss:0.5304491611612929\n",
      "train loss:0.6199339621973584\n",
      "train loss:0.4720940478802477\n",
      "train loss:0.5773360750033216\n",
      "train loss:0.30329760101362735\n",
      "train loss:0.44232040401574213\n",
      "train loss:0.7623511890035986\n",
      "train loss:0.6237340723963155\n",
      "train loss:0.4915192918885888\n",
      "train loss:0.5662504364826454\n",
      "train loss:0.4937730679089819\n",
      "train loss:0.4297822072178258\n",
      "train loss:0.4716187455491562\n",
      "train loss:0.7091370516880406\n",
      "train loss:0.6518142710164402\n",
      "train loss:0.5655528372579253\n",
      "train loss:0.6706922984767345\n",
      "train loss:0.5518982323404499\n",
      "train loss:0.6031878777015188\n",
      "train loss:0.6074883898249664\n",
      "train loss:0.38147880756574054\n",
      "train loss:0.6036610856584842\n",
      "train loss:0.5065263662714345\n",
      "train loss:0.43229175760062655\n",
      "train loss:0.5544345691370924\n",
      "train loss:0.8518615907703403\n",
      "train loss:0.6501584418602746\n",
      "train loss:0.5154097115482487\n",
      "train loss:0.528553702226669\n",
      "train loss:0.3598718125585538\n",
      "train loss:0.6075024692856242\n",
      "train loss:0.7045274576472482\n",
      "train loss:0.6964141133373665\n",
      "train loss:0.6080458133917958\n",
      "train loss:0.39894715342421516\n",
      "train loss:0.5135404719597948\n",
      "train loss:0.4301001466368234\n",
      "train loss:0.8334831527967597\n",
      "train loss:0.5327126225164115\n",
      "train loss:0.6473522588098031\n",
      "train loss:0.6136547733193902\n",
      "train loss:0.482452856526966\n",
      "train loss:0.7597663971360819\n",
      "train loss:0.5263162225764584\n",
      "train loss:0.46739772104216976\n",
      "train loss:0.7663712384840065\n",
      "train loss:0.7422232366212187\n",
      "train loss:0.49253616926999866\n",
      "train loss:0.45606122011355754\n",
      "train loss:0.5043749276158935\n",
      "train loss:0.5840199874133519\n",
      "train loss:0.4804678978750966\n",
      "train loss:0.6394120383246198\n",
      "train loss:0.4895182048801292\n",
      "train loss:0.5818010182552315\n",
      "=== epoch:10, train acc:0.74, test acc:0.69 ===\n",
      "train loss:0.7665882706946194\n",
      "train loss:0.8488571830378466\n",
      "train loss:0.583047735596209\n",
      "train loss:0.4974010268510708\n",
      "train loss:0.6111262431992429\n",
      "train loss:0.5725804383993114\n",
      "train loss:0.3999767005418899\n",
      "train loss:0.6847329252146279\n",
      "train loss:0.6712745192423093\n",
      "train loss:0.5107950890684508\n",
      "train loss:0.6277203591981384\n",
      "train loss:0.5172138605416129\n",
      "train loss:0.5471148460365197\n",
      "train loss:0.5525024834547635\n",
      "train loss:0.522823019051413\n",
      "train loss:0.5933236255400448\n",
      "train loss:0.4162361093905528\n",
      "train loss:0.6853542841483631\n",
      "train loss:0.5449190849663138\n",
      "train loss:0.4869855914047506\n",
      "train loss:0.570898579788422\n",
      "train loss:0.5570188249224021\n",
      "train loss:0.520575368243134\n",
      "train loss:0.4446725183861771\n",
      "train loss:0.4162290374617893\n",
      "train loss:0.58166104272354\n",
      "train loss:0.5224863226121481\n",
      "train loss:0.3399943588435651\n",
      "train loss:0.36161975351554465\n",
      "train loss:0.6752083543788364\n",
      "train loss:0.49041276541429824\n",
      "train loss:0.443833834941845\n",
      "train loss:0.5009444922797902\n",
      "train loss:0.6202640816652132\n",
      "train loss:0.725149769489921\n",
      "train loss:0.39740620542817984\n",
      "train loss:0.36889800710183107\n",
      "train loss:0.41543380967524063\n",
      "train loss:0.2213585459218408\n",
      "train loss:0.696273325782695\n",
      "train loss:0.6304033186506516\n",
      "train loss:0.23586180914099977\n",
      "train loss:0.7177616477440922\n",
      "train loss:0.5663330715098066\n",
      "train loss:0.3335684606376309\n",
      "train loss:0.3643944232260881\n",
      "train loss:0.472536519880321\n",
      "train loss:0.47604030597346386\n",
      "train loss:0.5401237234427197\n",
      "train loss:0.46201468130205753\n",
      "train loss:0.7039813143357159\n",
      "train loss:0.4747244180389075\n",
      "train loss:0.5956906486916559\n",
      "train loss:0.46701249571530445\n",
      "train loss:0.5032970086746184\n",
      "train loss:0.5387555865264182\n",
      "train loss:0.3696630228517422\n",
      "train loss:0.6714774299637201\n",
      "train loss:0.4954087029852925\n",
      "train loss:0.7192892702867131\n",
      "train loss:0.5582821157602142\n",
      "train loss:0.8131561879493479\n",
      "train loss:0.631743907896537\n",
      "train loss:0.5128141629625906\n",
      "train loss:0.5529136084375198\n",
      "train loss:0.49238603390718855\n",
      "train loss:0.6608729218629141\n",
      "train loss:0.44997699835650706\n",
      "train loss:0.6197074932903044\n",
      "train loss:0.5451970206741323\n",
      "train loss:0.5105078566693766\n",
      "train loss:0.30713099378945496\n",
      "train loss:0.6651262725528888\n",
      "train loss:0.4930860272948639\n",
      "train loss:0.46508173244928874\n",
      "train loss:0.6421267398620649\n",
      "train loss:0.36186863345518133\n",
      "train loss:0.30636522523408644\n",
      "train loss:0.6535118408218066\n",
      "train loss:0.64357206142394\n",
      "train loss:0.7204582863899156\n",
      "train loss:0.566184910890758\n",
      "train loss:0.5903848530614566\n",
      "train loss:0.6729141240114326\n",
      "train loss:0.5757117701399579\n",
      "train loss:0.561905178274141\n",
      "train loss:0.51566500192551\n",
      "train loss:0.4188234707666118\n",
      "train loss:0.4680859097041722\n",
      "train loss:0.4249622782665471\n",
      "train loss:0.6045723120177471\n",
      "train loss:0.4478004677022221\n",
      "train loss:0.5202364827450151\n",
      "train loss:0.673213844994401\n",
      "train loss:0.6412405368548457\n",
      "train loss:0.5560751216673134\n",
      "train loss:0.6361571414667819\n",
      "train loss:0.43667080695202376\n",
      "train loss:0.3813994574111937\n",
      "train loss:0.6186912526274029\n",
      "train loss:0.9134698426423544\n",
      "train loss:0.3934081323359768\n",
      "train loss:0.6860907542593454\n",
      "train loss:0.56563507659223\n",
      "train loss:0.6098885162468589\n",
      "train loss:0.5543271281049951\n",
      "train loss:0.6964522400360931\n",
      "train loss:0.4638169535207014\n",
      "train loss:0.6611528456850899\n",
      "train loss:0.5933281352365913\n",
      "train loss:0.37529379414866015\n",
      "train loss:0.5515388996529993\n",
      "train loss:0.5581147466584209\n",
      "train loss:0.4468958657766927\n",
      "train loss:0.749094598444991\n",
      "train loss:0.5337184759069344\n",
      "train loss:0.46017781295171056\n",
      "train loss:0.574417779809798\n",
      "train loss:0.4927934772204072\n",
      "train loss:0.6010371983411892\n",
      "train loss:0.38248833071920696\n",
      "train loss:0.564867149993576\n",
      "train loss:0.546437776003567\n",
      "train loss:0.38378243921653155\n",
      "train loss:0.5097729142067517\n",
      "train loss:0.48742416726836224\n",
      "train loss:0.6314175613295996\n",
      "train loss:0.4403439282065169\n",
      "train loss:0.7763815663979442\n",
      "train loss:0.3972422190476366\n",
      "train loss:0.37970633227872475\n",
      "train loss:0.6176535973228106\n",
      "train loss:0.5274807125116676\n",
      "train loss:0.39244163114337943\n",
      "train loss:0.502426007705856\n",
      "train loss:0.6004254512776563\n",
      "train loss:0.2578851541131033\n",
      "train loss:0.631712427644047\n",
      "train loss:0.5732974296696233\n",
      "train loss:0.4611684625133585\n",
      "train loss:0.5448832404014403\n",
      "train loss:0.2722201227937241\n",
      "train loss:0.5158143521103065\n",
      "train loss:0.5062220133658181\n",
      "train loss:0.49930159624268794\n",
      "train loss:0.5070618935055726\n",
      "train loss:0.4538293403267978\n",
      "train loss:0.380921476743194\n",
      "train loss:0.23829464941547243\n",
      "train loss:0.8224224508673771\n",
      "train loss:0.6241576053227126\n",
      "train loss:0.23028346538161734\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5529411764705883\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 90, 'filter_size': 7, 'pad': 2, 'stride': 1},\n",
    "                        hidden_size=100, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c25f596d-3e6a-43ff-8cb1-16691973626b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6876137153397166\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6912227628752132\n",
      "train loss:0.6828700642460732\n",
      "train loss:0.6642679949775651\n",
      "train loss:0.6486703809547707\n",
      "train loss:0.6687435170504311\n",
      "train loss:0.6402918417060333\n",
      "train loss:0.6383884123370734\n",
      "train loss:0.5086445813759763\n",
      "train loss:0.5287137310753158\n",
      "train loss:0.6093183444210533\n",
      "train loss:0.8087701538918262\n",
      "train loss:0.7091399039993552\n",
      "train loss:0.6496924660890607\n",
      "train loss:0.6948435299658592\n",
      "train loss:0.6474852820167182\n",
      "train loss:0.5725119594280739\n",
      "train loss:0.63301807890932\n",
      "train loss:0.6788207397984704\n",
      "train loss:0.6393056245670975\n",
      "train loss:0.6537788164413525\n",
      "train loss:0.6469591582495606\n",
      "train loss:0.6187662114853665\n",
      "train loss:0.6458464202296453\n",
      "train loss:0.6459383025534139\n",
      "train loss:0.6344724961167358\n",
      "train loss:0.5951933783524972\n",
      "train loss:0.6593364258080768\n",
      "train loss:0.5665102927965929\n",
      "train loss:0.5171616684627466\n",
      "train loss:0.6898635274917723\n",
      "train loss:0.6877656670634783\n",
      "train loss:0.5850556049773068\n",
      "train loss:0.6478650175518637\n",
      "train loss:0.6407015966422626\n",
      "train loss:0.7186048257475014\n",
      "train loss:0.7094996012242175\n",
      "train loss:0.5146901424670082\n",
      "train loss:0.8074660614265126\n",
      "train loss:0.44558233415058207\n",
      "train loss:0.6394933463259636\n",
      "train loss:0.7528806683186728\n",
      "train loss:0.6049382506075789\n",
      "train loss:0.6083513360746157\n",
      "train loss:0.5425277273283405\n",
      "train loss:0.6791939601947583\n",
      "train loss:0.665020597956373\n",
      "train loss:0.507648236725229\n",
      "train loss:0.6459378051661102\n",
      "train loss:0.618722201046894\n",
      "train loss:0.6072824312828642\n",
      "train loss:0.5797048425622133\n",
      "train loss:0.4919070387906889\n",
      "train loss:0.5941048413434302\n",
      "train loss:0.6597329447505572\n",
      "train loss:0.5948310224603068\n",
      "train loss:0.5263643117945784\n",
      "train loss:0.7653238117927267\n",
      "train loss:0.591213818542955\n",
      "train loss:0.713552490991919\n",
      "train loss:0.637316061935578\n",
      "train loss:0.43243213720331264\n",
      "train loss:0.7004196128807257\n",
      "train loss:0.6322820935424208\n",
      "train loss:0.6143473394994777\n",
      "train loss:0.7223399894284228\n",
      "train loss:0.5288312451719926\n",
      "train loss:0.6799624394177152\n",
      "train loss:0.6428276417554986\n",
      "train loss:0.5113590636330628\n",
      "train loss:0.5674645076779256\n",
      "train loss:0.7417735424229719\n",
      "train loss:0.609663146891432\n",
      "train loss:0.6550204121641434\n",
      "train loss:0.5690149969273113\n",
      "train loss:0.6383109085733993\n",
      "train loss:0.6534096823119799\n",
      "train loss:0.6683022811845211\n",
      "train loss:0.5816691143921753\n",
      "train loss:0.5891693568480643\n",
      "train loss:0.6750709277079754\n",
      "train loss:0.6439107958835132\n",
      "train loss:0.45038757759247156\n",
      "train loss:0.610206120136042\n",
      "train loss:0.5153009822752559\n",
      "train loss:0.5665681353638796\n",
      "train loss:0.38542019592900545\n",
      "train loss:0.5993945127304225\n",
      "train loss:0.74230004206872\n",
      "train loss:0.9209025409770394\n",
      "train loss:0.44947156661715215\n",
      "train loss:0.6628138480417077\n",
      "train loss:0.500623230816714\n",
      "train loss:0.4778967896170627\n",
      "train loss:0.6565975536060761\n",
      "train loss:0.6437410116511286\n",
      "train loss:0.6449217869762826\n",
      "train loss:0.6712121545021185\n",
      "train loss:0.5473378133861225\n",
      "train loss:0.6048341237532097\n",
      "train loss:0.4746090900126535\n",
      "train loss:0.686491815988599\n",
      "train loss:0.5274767151106687\n",
      "train loss:0.5688392487392377\n",
      "train loss:0.6892350980152553\n",
      "train loss:0.5655166678347564\n",
      "train loss:0.580485165449384\n",
      "train loss:0.41182301045121045\n",
      "train loss:0.5160427238769066\n",
      "train loss:0.6899361269277224\n",
      "train loss:0.5595954113480246\n",
      "train loss:0.7588385481619456\n",
      "train loss:0.5807172466147443\n",
      "train loss:0.49524036667575305\n",
      "train loss:0.5217291282944801\n",
      "train loss:0.49489580918711684\n",
      "train loss:0.6114782483619147\n",
      "train loss:0.5362194761482033\n",
      "train loss:0.615050993923148\n",
      "train loss:0.4620163296331287\n",
      "train loss:0.4894423284384132\n",
      "train loss:0.48234216011348385\n",
      "train loss:0.5888236593961834\n",
      "train loss:0.6646050789383032\n",
      "train loss:0.3888128977904729\n",
      "train loss:0.5865121103548939\n",
      "train loss:0.4776221885253101\n",
      "train loss:0.7321787883853953\n",
      "train loss:0.5546287234016972\n",
      "train loss:0.5811397473968468\n",
      "train loss:0.6236235317582122\n",
      "train loss:0.4428961254399503\n",
      "train loss:0.5257339321215948\n",
      "train loss:0.4285811658795895\n",
      "train loss:0.4673446553862853\n",
      "train loss:0.5361797930341\n",
      "train loss:0.42846911603810584\n",
      "train loss:0.5238944616829595\n",
      "train loss:0.5836063785504548\n",
      "train loss:0.6882895440769741\n",
      "train loss:0.6658268795041197\n",
      "train loss:0.5029473479279339\n",
      "train loss:0.4898899954073917\n",
      "train loss:0.5442276129361852\n",
      "train loss:0.8182638629320186\n",
      "train loss:0.43520537436848467\n",
      "train loss:0.6817369701975633\n",
      "train loss:0.643192266836745\n",
      "train loss:0.6719043872115884\n",
      "train loss:0.5529126282316635\n",
      "train loss:0.5036136472212946\n",
      "train loss:0.60322273532839\n",
      "train loss:0.5652147326322259\n",
      "train loss:0.5094411400583614\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5591203077452462\n",
      "train loss:0.6386392053689798\n",
      "train loss:0.6705273228689842\n",
      "train loss:0.6663607939153042\n",
      "train loss:0.5158602355791005\n",
      "train loss:0.46333540159846054\n",
      "train loss:0.6231056250667878\n",
      "train loss:0.620775842279073\n",
      "train loss:0.5679161741296447\n",
      "train loss:0.782173286574311\n",
      "train loss:0.5656843554511352\n",
      "train loss:0.4847206380179756\n",
      "train loss:0.6218066982602135\n",
      "train loss:0.28534271088232693\n",
      "train loss:0.6300389869582743\n",
      "train loss:0.49092289458805327\n",
      "train loss:0.7350774368514703\n",
      "train loss:0.5741644193278679\n",
      "train loss:0.6030110382190601\n",
      "train loss:0.5566562105350713\n",
      "train loss:0.4907965718231506\n",
      "train loss:0.5387765441188501\n",
      "train loss:0.592127925499425\n",
      "train loss:0.6176977834025673\n",
      "train loss:0.6990428915150579\n",
      "train loss:0.712036188810296\n",
      "train loss:0.609304950815801\n",
      "train loss:0.6234325128262681\n",
      "train loss:0.5801234243172925\n",
      "train loss:0.6962921834033102\n",
      "train loss:0.5519754881348132\n",
      "train loss:0.5852384946596901\n",
      "train loss:0.6109674714445362\n",
      "train loss:0.6335609206564512\n",
      "train loss:0.48786965606046645\n",
      "train loss:0.5895787404794088\n",
      "train loss:0.6010144774497227\n",
      "train loss:0.5081151162776936\n",
      "train loss:0.6677532275725112\n",
      "train loss:0.45983055096287623\n",
      "train loss:0.5671084632685519\n",
      "train loss:0.7373433002856521\n",
      "train loss:0.566165399711869\n",
      "train loss:0.7557014212864717\n",
      "train loss:0.6476037735688783\n",
      "train loss:0.6078344631531348\n",
      "train loss:0.6147198687096683\n",
      "train loss:0.6422081512691038\n",
      "train loss:0.7179134072324173\n",
      "train loss:0.605272459858821\n",
      "train loss:0.6068754809072336\n",
      "train loss:0.7001373782690382\n",
      "train loss:0.6936926592265837\n",
      "train loss:0.6108997090317576\n",
      "train loss:0.6605355090832516\n",
      "train loss:0.6041922313189652\n",
      "train loss:0.6291770062871299\n",
      "train loss:0.7016338765009216\n",
      "train loss:0.6278108948391373\n",
      "train loss:0.6127652537599915\n",
      "train loss:0.6318309925906508\n",
      "train loss:0.5652576928986685\n",
      "train loss:0.6636147828286425\n",
      "train loss:0.5412516176913228\n",
      "train loss:0.3939921078858246\n",
      "train loss:0.7588809963552869\n",
      "train loss:0.5841251571377121\n",
      "train loss:0.6763535691069771\n",
      "train loss:0.3625065919571793\n",
      "train loss:0.2828142191526047\n",
      "train loss:0.5727992348330442\n",
      "train loss:0.49151658750818406\n",
      "train loss:0.6857179594992734\n",
      "train loss:0.6730775677124711\n",
      "train loss:0.9794220566315788\n",
      "train loss:0.739026935925142\n",
      "train loss:0.6227469805175518\n",
      "train loss:0.6876677536439847\n",
      "train loss:0.5582095795341677\n",
      "train loss:0.5575903287877958\n",
      "train loss:0.6190688739882377\n",
      "train loss:0.6629389544018799\n",
      "train loss:0.6291142014181874\n",
      "train loss:0.613743587821461\n",
      "train loss:0.6145934440254184\n",
      "train loss:0.6475490284290013\n",
      "train loss:0.573817020354509\n",
      "train loss:0.6254940138349562\n",
      "train loss:0.5336783850040072\n",
      "train loss:0.5427693708012213\n",
      "train loss:0.596881812808608\n",
      "train loss:0.6437884202769228\n",
      "train loss:0.6770502435441134\n",
      "train loss:0.5497323485178971\n",
      "train loss:0.5360165833626247\n",
      "train loss:0.5516172467006714\n",
      "train loss:0.7840056924040587\n",
      "train loss:0.4558843703082953\n",
      "train loss:0.6218269976334192\n",
      "train loss:0.5349348941574082\n",
      "train loss:0.6414949894370687\n",
      "train loss:0.5591925683670833\n",
      "train loss:0.637997464606071\n",
      "train loss:0.5829215808861158\n",
      "train loss:0.7430801432985322\n",
      "train loss:0.5489210002911531\n",
      "train loss:0.6069921635933484\n",
      "train loss:0.5520912869414972\n",
      "train loss:0.5867246918963257\n",
      "train loss:0.5668764988764791\n",
      "train loss:0.5701644292690228\n",
      "train loss:0.48346753032254497\n",
      "train loss:0.5169609407772701\n",
      "train loss:0.4877985839446053\n",
      "train loss:0.7043442971109146\n",
      "train loss:0.4783206671283062\n",
      "train loss:0.45728316561615845\n",
      "train loss:0.49606693493259124\n",
      "train loss:0.7230319333516325\n",
      "train loss:0.7555815477650774\n",
      "train loss:0.5780138200801122\n",
      "train loss:0.746758381741024\n",
      "train loss:0.7055411227117385\n",
      "train loss:0.5478546285088134\n",
      "train loss:0.4921556842137932\n",
      "train loss:0.6688706503203969\n",
      "train loss:0.6434408141838343\n",
      "train loss:0.7593451126601451\n",
      "train loss:0.5375237949600649\n",
      "train loss:0.5841759770362573\n",
      "train loss:0.569488332198058\n",
      "train loss:0.7002914538188201\n",
      "train loss:0.5838867837631765\n",
      "train loss:0.6260643271249877\n",
      "train loss:0.6036995648873524\n",
      "train loss:0.5990509145468609\n",
      "train loss:0.6131753299727601\n",
      "train loss:0.657820555393785\n",
      "train loss:0.4579762337626668\n",
      "train loss:0.6128443404662816\n",
      "train loss:0.6315039708054413\n",
      "train loss:0.5389683668754642\n",
      "train loss:0.42670486945336333\n",
      "train loss:0.3819068114364899\n",
      "train loss:0.7616633686577259\n",
      "train loss:0.8310241231703644\n",
      "train loss:0.7022815318782841\n",
      "train loss:0.551219117148285\n",
      "train loss:0.5533845957811608\n",
      "train loss:0.7537057510757956\n",
      "train loss:0.5909759309820062\n",
      "train loss:0.6828814296303649\n",
      "train loss:0.5769715367244582\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.7303085547256123\n",
      "train loss:0.5332526884638098\n",
      "train loss:0.5946985818008497\n",
      "train loss:0.670249282775133\n",
      "train loss:0.5792702737433847\n",
      "train loss:0.5885643066913251\n",
      "train loss:0.5601375362024207\n",
      "train loss:0.6204825536398328\n",
      "train loss:0.5926103626636825\n",
      "train loss:0.6459539746627875\n",
      "train loss:0.5584988381591605\n",
      "train loss:0.6832710856758318\n",
      "train loss:0.5651953264656527\n",
      "train loss:0.5127487534948756\n",
      "train loss:0.48166122709542625\n",
      "train loss:0.5067778283420346\n",
      "train loss:0.550761980677959\n",
      "train loss:0.538563600675561\n",
      "train loss:0.7451500942522967\n",
      "train loss:0.8136102396039646\n",
      "train loss:0.5282937639471262\n",
      "train loss:0.5261087603571188\n",
      "train loss:0.7578176132124019\n",
      "train loss:0.5216423438133149\n",
      "train loss:0.606068450581925\n",
      "train loss:0.5551065947128191\n",
      "train loss:0.5541804602127376\n",
      "train loss:0.6932716019231796\n",
      "train loss:0.7458590537805622\n",
      "train loss:0.6154152434733806\n",
      "train loss:0.6547401932396826\n",
      "train loss:0.497805089189462\n",
      "train loss:0.6302863302357642\n",
      "train loss:0.5408645367909409\n",
      "train loss:0.5291326194733912\n",
      "train loss:0.5650466010988924\n",
      "train loss:0.6111571545848103\n",
      "train loss:0.4790054701610741\n",
      "train loss:0.4938245231550547\n",
      "train loss:0.6512212348575492\n",
      "train loss:0.5492538721105465\n",
      "train loss:0.6558100987081484\n",
      "train loss:0.6290240515520652\n",
      "train loss:0.466414569867103\n",
      "train loss:0.529430535910188\n",
      "train loss:0.5535028347663671\n",
      "train loss:0.4316030607003256\n",
      "train loss:0.52103636002526\n",
      "train loss:0.5930577322752023\n",
      "train loss:0.5711248032744459\n",
      "train loss:0.6005495912869547\n",
      "train loss:0.5044322944158929\n",
      "train loss:0.6371745481967114\n",
      "train loss:0.5718972367791989\n",
      "train loss:0.5073371517407402\n",
      "train loss:0.5653509751927625\n",
      "train loss:0.6525865618984048\n",
      "train loss:0.6221970808283397\n",
      "train loss:0.6246049540314473\n",
      "train loss:0.6781140308444923\n",
      "train loss:0.5457111971433307\n",
      "train loss:0.5768322283058881\n",
      "train loss:0.5969725287086322\n",
      "train loss:0.475314780837067\n",
      "train loss:0.536977987783085\n",
      "train loss:0.7210246487409238\n",
      "train loss:0.5815794509020293\n",
      "train loss:0.593570206578609\n",
      "train loss:0.6688446680695475\n",
      "train loss:0.5943670237975442\n",
      "train loss:0.588999713826156\n",
      "train loss:0.5107068345120273\n",
      "train loss:0.4935727636369663\n",
      "train loss:0.7370638143295019\n",
      "train loss:0.5787624538214411\n",
      "train loss:0.5876099628291668\n",
      "train loss:0.6865586400721091\n",
      "train loss:0.6771934541297958\n",
      "train loss:0.5210310635514578\n",
      "train loss:0.5257807877670521\n",
      "train loss:0.5490936022233708\n",
      "train loss:0.4578861458863739\n",
      "train loss:0.653939755337453\n",
      "train loss:0.6767846351915947\n",
      "train loss:0.5900804252251947\n",
      "train loss:0.5173873388074715\n",
      "train loss:0.6718292508946939\n",
      "train loss:0.6112509048453346\n",
      "train loss:0.5517124507423524\n",
      "train loss:0.5754178995185317\n",
      "train loss:0.6434362383255628\n",
      "train loss:0.5260500810438911\n",
      "train loss:0.6274393599616732\n",
      "train loss:0.6349848944130606\n",
      "train loss:0.6892843478372128\n",
      "train loss:0.5443033537387645\n",
      "train loss:0.7226536841072128\n",
      "train loss:0.48940145492713805\n",
      "train loss:0.49667671039559086\n",
      "train loss:0.6921074266180908\n",
      "train loss:0.6640392439161191\n",
      "train loss:0.8313271511828135\n",
      "train loss:0.48892889149583496\n",
      "train loss:0.6526255814744328\n",
      "train loss:0.5159058744223917\n",
      "train loss:0.654219590795698\n",
      "train loss:0.5901304679565383\n",
      "train loss:0.5396992871962639\n",
      "train loss:0.6332565666912566\n",
      "train loss:0.5959571301249226\n",
      "train loss:0.48064803005163237\n",
      "train loss:0.6436171747594448\n",
      "train loss:0.44838018362967996\n",
      "train loss:0.5133388789699697\n",
      "train loss:0.737011588048029\n",
      "train loss:0.7318802984881476\n",
      "train loss:0.48293475555057624\n",
      "train loss:0.7417601563465291\n",
      "train loss:0.6413363399958338\n",
      "train loss:0.7381033040150664\n",
      "train loss:0.6048215675127997\n",
      "train loss:0.455637874040226\n",
      "train loss:0.5588018203374808\n",
      "train loss:0.5637063144409677\n",
      "train loss:0.5081519939620949\n",
      "train loss:0.5328240539623254\n",
      "train loss:0.5595903521347081\n",
      "train loss:0.6463381210303563\n",
      "train loss:0.47543914272135596\n",
      "train loss:0.6146347145021169\n",
      "train loss:0.561900283334173\n",
      "train loss:0.6834085526142681\n",
      "train loss:0.45922446132619843\n",
      "train loss:0.6889132303542586\n",
      "train loss:0.5683230733552687\n",
      "train loss:0.3903723527688004\n",
      "train loss:0.5630625181426463\n",
      "train loss:0.7483409204880731\n",
      "train loss:0.5695943820729572\n",
      "train loss:0.8096682837897895\n",
      "train loss:0.5439979833655948\n",
      "train loss:0.5618001267726105\n",
      "train loss:0.5476145387952007\n",
      "train loss:0.6243450108169719\n",
      "train loss:0.6146359630149585\n",
      "train loss:0.6121440934749783\n",
      "train loss:0.5551985256590916\n",
      "train loss:0.5130716108029592\n",
      "train loss:0.6171320483800088\n",
      "train loss:0.45668330990399253\n",
      "train loss:0.616226803595149\n",
      "train loss:0.5746220779004811\n",
      "train loss:0.6070148615333782\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6526677036009781\n",
      "train loss:0.6517541148101207\n",
      "train loss:0.5641680985876278\n",
      "train loss:0.6221171186259951\n",
      "train loss:0.6554431107557018\n",
      "train loss:0.5311141529561096\n",
      "train loss:0.6031906665961644\n",
      "train loss:0.7572882089379613\n",
      "train loss:0.5776628613517408\n",
      "train loss:0.47722677615925424\n",
      "train loss:0.6187160734514973\n",
      "train loss:0.5079385744112206\n",
      "train loss:0.6537243852289335\n",
      "train loss:0.6724173267511911\n",
      "train loss:0.696633187661872\n",
      "train loss:0.6008435090260973\n",
      "train loss:0.6236427816693595\n",
      "train loss:0.5876765309640538\n",
      "train loss:0.5272699527659237\n",
      "train loss:0.6518398225463837\n",
      "train loss:0.49355719201126275\n",
      "train loss:0.5493762464594052\n",
      "train loss:0.6586961593785334\n",
      "train loss:0.6499092658471006\n",
      "train loss:0.47594762097696347\n",
      "train loss:0.5549959688807755\n",
      "train loss:0.5074367242132942\n",
      "train loss:0.673003149260972\n",
      "train loss:0.5417212086424809\n",
      "train loss:0.7702604382724606\n",
      "train loss:0.4027211974433048\n",
      "train loss:0.5114794992690717\n",
      "train loss:0.4022614114917711\n",
      "train loss:0.656430631804412\n",
      "train loss:0.6076792872621486\n",
      "train loss:0.5720972445435804\n",
      "train loss:0.4966178699858892\n",
      "train loss:0.5757332604131298\n",
      "train loss:0.5406204929911331\n",
      "train loss:0.44645090865572856\n",
      "train loss:0.5691765569950638\n",
      "train loss:0.47201704040775194\n",
      "train loss:0.7188172326652136\n",
      "train loss:0.3895517140043165\n",
      "train loss:0.3532711445582797\n",
      "train loss:0.7712655010648639\n",
      "train loss:0.5989284837662654\n",
      "train loss:0.529468727167106\n",
      "train loss:0.5734312482423229\n",
      "train loss:0.6827360786025464\n",
      "train loss:0.5670297389564767\n",
      "train loss:0.6221592130340095\n",
      "train loss:0.6179281387894526\n",
      "train loss:0.6550885604170583\n",
      "train loss:0.5765239417743679\n",
      "train loss:0.5964587286424952\n",
      "train loss:0.6772886210409537\n",
      "train loss:0.5537082188124882\n",
      "train loss:0.6984679016504474\n",
      "train loss:0.5505800171289501\n",
      "train loss:0.6376100512800159\n",
      "train loss:0.6830494696948513\n",
      "train loss:0.5229908090009074\n",
      "train loss:0.5937585098808188\n",
      "train loss:0.6011501688741719\n",
      "train loss:0.5750765781805256\n",
      "train loss:0.476501274182465\n",
      "train loss:0.5684033378489034\n",
      "train loss:0.41536757955510806\n",
      "train loss:0.6371569276392703\n",
      "train loss:0.598106384907516\n",
      "train loss:0.7199782201918457\n",
      "train loss:0.5147069529269969\n",
      "train loss:0.7798530024436456\n",
      "train loss:0.5400974553520873\n",
      "train loss:0.793184133215022\n",
      "train loss:0.4154562488834572\n",
      "train loss:0.4932422685032812\n",
      "train loss:0.8079776565156582\n",
      "train loss:0.5997198852172977\n",
      "train loss:0.5826348305585178\n",
      "train loss:0.4615954051166865\n",
      "train loss:0.6898741597116668\n",
      "train loss:0.4642933010913867\n",
      "train loss:0.5438111055292956\n",
      "train loss:0.5833981121341886\n",
      "train loss:0.6917211192976125\n",
      "train loss:0.47328430859380405\n",
      "train loss:0.6398918709620427\n",
      "train loss:0.6523743154809098\n",
      "train loss:0.5186059106828251\n",
      "train loss:0.5632624164372713\n",
      "train loss:0.40795247048871974\n",
      "train loss:0.5512204435019789\n",
      "train loss:0.4459081228148628\n",
      "train loss:0.5507155669095011\n",
      "train loss:0.5485630821818805\n",
      "train loss:0.5180128310684123\n",
      "train loss:0.47431038785131563\n",
      "train loss:0.6344576922375691\n",
      "train loss:0.6831147757144544\n",
      "train loss:0.7744942769584131\n",
      "train loss:0.5418703451996845\n",
      "train loss:0.5788327015923577\n",
      "train loss:0.607364193509204\n",
      "train loss:0.6331819129220609\n",
      "train loss:0.655261130465712\n",
      "train loss:0.5173670322490825\n",
      "train loss:0.6424500128769651\n",
      "train loss:0.5622079416767571\n",
      "train loss:0.5467423273963475\n",
      "train loss:0.5468684296154546\n",
      "train loss:0.4404005601626927\n",
      "train loss:0.41471932604822026\n",
      "train loss:0.651139480011617\n",
      "train loss:0.6987211448707796\n",
      "train loss:0.5421015881455264\n",
      "train loss:0.5777303936883345\n",
      "train loss:0.4979028699976021\n",
      "train loss:0.6445529249126993\n",
      "train loss:0.4845398883138343\n",
      "train loss:0.6367939227645506\n",
      "train loss:0.528202530179303\n",
      "train loss:0.6152354953707755\n",
      "train loss:0.5101419777739935\n",
      "train loss:0.8474465887428027\n",
      "train loss:0.5315991085286059\n",
      "train loss:0.6364490448345166\n",
      "train loss:0.46689990580278395\n",
      "train loss:0.587236910979506\n",
      "train loss:0.6674211434551973\n",
      "train loss:0.6092220090346013\n",
      "train loss:0.4823069755790611\n",
      "train loss:0.5707323175752121\n",
      "train loss:0.5774704170487042\n",
      "train loss:0.5210921140371227\n",
      "train loss:0.5548910269870715\n",
      "train loss:0.5323949345582554\n",
      "train loss:0.49334972465392707\n",
      "train loss:0.7094483134901999\n",
      "train loss:0.3942544240277982\n",
      "train loss:0.38540433066482926\n",
      "train loss:0.7171701443099991\n",
      "train loss:0.5310815879148361\n",
      "train loss:0.5845914306490075\n",
      "train loss:0.6575386153951726\n",
      "train loss:0.7667945556855716\n",
      "train loss:0.5520540963815431\n",
      "train loss:0.6240288372946207\n",
      "train loss:0.607484137202091\n",
      "train loss:0.6419193163364965\n",
      "train loss:0.6561953882598475\n",
      "train loss:0.662797438551608\n",
      "=== epoch:5, train acc:0.7, test acc:0.69 ===\n",
      "train loss:0.5815975087363816\n",
      "train loss:0.5198464275909493\n",
      "train loss:0.6158442872293681\n",
      "train loss:0.5558497864605109\n",
      "train loss:0.62255150850618\n",
      "train loss:0.5458771853816629\n",
      "train loss:0.5593484680710049\n",
      "train loss:0.5421174537988633\n",
      "train loss:0.6335874906571295\n",
      "train loss:0.5263344959222874\n",
      "train loss:0.5982635349019705\n",
      "train loss:0.615283509643606\n",
      "train loss:0.4882319314023274\n",
      "train loss:0.4569198245216863\n",
      "train loss:0.6705333992899463\n",
      "train loss:0.6612579597552448\n",
      "train loss:0.5642901181844016\n",
      "train loss:0.4701517650713124\n",
      "train loss:0.5727270205942385\n",
      "train loss:0.5852302397868224\n",
      "train loss:0.5294958417257001\n",
      "train loss:0.594175065619592\n",
      "train loss:0.473799838208241\n",
      "train loss:0.5512300800852715\n",
      "train loss:0.7719221713913872\n",
      "train loss:0.41618055648583213\n",
      "train loss:0.5419969002044545\n",
      "train loss:0.6258731578918619\n",
      "train loss:0.5646081695128675\n",
      "train loss:0.5829202704718103\n",
      "train loss:0.5061344995003452\n",
      "train loss:0.4394687439436266\n",
      "train loss:0.7372891038744493\n",
      "train loss:0.6331748493237683\n",
      "train loss:0.5393709729941674\n",
      "train loss:0.5489110542806122\n",
      "train loss:0.6118896794376936\n",
      "train loss:0.5598792814329082\n",
      "train loss:0.49655962501860207\n",
      "train loss:0.4290008659611682\n",
      "train loss:0.5045593687737779\n",
      "train loss:0.6169846075405678\n",
      "train loss:0.5792487037054088\n",
      "train loss:0.4473808391785784\n",
      "train loss:0.515997318438267\n",
      "train loss:0.5850755368135984\n",
      "train loss:0.4254440883871512\n",
      "train loss:0.45289657152561763\n",
      "train loss:0.25118287503615433\n",
      "train loss:0.454962849471171\n",
      "train loss:0.6392953274446433\n",
      "train loss:0.6451049760932208\n",
      "train loss:0.40733990447234464\n",
      "train loss:0.3905224016116503\n",
      "train loss:0.557101435407308\n",
      "train loss:0.3873094369792714\n",
      "train loss:0.6751952869884069\n",
      "train loss:0.2908209237706929\n",
      "train loss:0.5268187453842876\n",
      "train loss:0.6253245311366729\n",
      "train loss:0.6218931647417346\n",
      "train loss:0.7260030564549635\n",
      "train loss:0.5624641747462535\n",
      "train loss:0.5126480328477483\n",
      "train loss:0.5146803289248861\n",
      "train loss:0.5959259390746221\n",
      "train loss:0.6467673456074117\n",
      "train loss:0.5141513199953902\n",
      "train loss:0.4793297497357899\n",
      "train loss:0.5609191280895295\n",
      "train loss:0.573211213613556\n",
      "train loss:0.6076572181524604\n",
      "train loss:0.5982570415503339\n",
      "train loss:0.2920105307572947\n",
      "train loss:0.7004227179163547\n",
      "train loss:0.4958936620065332\n",
      "train loss:0.67322264759892\n",
      "train loss:0.6426436190247992\n",
      "train loss:0.5765289413700417\n",
      "train loss:0.5317243987410502\n",
      "train loss:0.538483097200267\n",
      "train loss:0.4601431531270263\n",
      "train loss:0.6714630349143894\n",
      "train loss:0.4839569627189138\n",
      "train loss:0.624229775902766\n",
      "train loss:0.5089363152269994\n",
      "train loss:0.5840379002736349\n",
      "train loss:0.6953103717673231\n",
      "train loss:0.6965697976414748\n",
      "train loss:0.5485861128638168\n",
      "train loss:0.6059421503248962\n",
      "train loss:0.6095902223975396\n",
      "train loss:0.42094644349545146\n",
      "train loss:0.561807216045502\n",
      "train loss:0.5305228240762123\n",
      "train loss:0.47728740937168845\n",
      "train loss:0.33192016148436815\n",
      "train loss:0.4074627288346379\n",
      "train loss:0.651251003453106\n",
      "train loss:0.6847892600112593\n",
      "train loss:0.5386153047458011\n",
      "train loss:0.2994811552189348\n",
      "train loss:0.8080858096153802\n",
      "train loss:0.49022729680138655\n",
      "train loss:0.5671367976490782\n",
      "train loss:0.5002920212383788\n",
      "train loss:0.4020378008253835\n",
      "train loss:0.5977893214272034\n",
      "train loss:0.4364107139023393\n",
      "train loss:0.6187319425488889\n",
      "train loss:0.5276456847967608\n",
      "train loss:0.5169207961962039\n",
      "train loss:0.583249388109471\n",
      "train loss:0.6058664340233755\n",
      "train loss:0.551742040381702\n",
      "train loss:0.3954002692945592\n",
      "train loss:0.4289771260535794\n",
      "train loss:0.5359560725861033\n",
      "train loss:0.5332854989086818\n",
      "train loss:0.4137404017517877\n",
      "train loss:0.6157538632922085\n",
      "train loss:0.4127692526208452\n",
      "train loss:0.34441464733905214\n",
      "train loss:0.5963057774407194\n",
      "train loss:0.5286088867355311\n",
      "train loss:0.5398602671456373\n",
      "train loss:0.5916395728822126\n",
      "train loss:0.4223227257544477\n",
      "train loss:0.459885789959771\n",
      "train loss:0.4438651473631977\n",
      "train loss:0.42355204979418215\n",
      "train loss:0.5388340451557664\n",
      "train loss:0.6387959848079955\n",
      "train loss:0.3235798298986576\n",
      "train loss:0.5494765403073053\n",
      "train loss:0.49137530963361326\n",
      "train loss:0.6655095832325848\n",
      "train loss:0.6352867622076859\n",
      "train loss:0.4887050610223252\n",
      "train loss:0.4150991115666436\n",
      "train loss:0.4880613336745581\n",
      "train loss:0.42686166234522\n",
      "train loss:0.45377890335779103\n",
      "train loss:0.6373910442257026\n",
      "train loss:0.4104326838638458\n",
      "train loss:0.7096966448703207\n",
      "train loss:0.5430795338834343\n",
      "train loss:0.6221610192273527\n",
      "train loss:0.38181213017378113\n",
      "train loss:0.7212365741168226\n",
      "train loss:0.5352175650392365\n",
      "train loss:0.5362708011192245\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 90, 'filter_size': 7, 'pad': 2, 'stride': 1},\n",
    "                        hidden_size=100, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=20,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "428f0c4f-dbd0-43b2-94a1-e4e47f746a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6908975949154785\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6804822134633867\n",
      "train loss:0.6912576687046675\n",
      "train loss:0.6498729891198651\n",
      "train loss:0.6564544589905207\n",
      "train loss:0.5792550264734586\n",
      "train loss:0.647571055260105\n",
      "train loss:0.6212280116954986\n",
      "train loss:0.5064970445830964\n",
      "train loss:0.5873141468020416\n",
      "train loss:0.5648542118503058\n",
      "train loss:0.5379519474213648\n",
      "train loss:0.568322518143697\n",
      "train loss:0.7023306017564976\n",
      "train loss:0.6534746375278221\n",
      "train loss:0.6206482977299459\n",
      "train loss:0.6229257343059852\n",
      "train loss:0.6002851730169947\n",
      "train loss:0.6443507979331751\n",
      "train loss:0.6098505119278913\n",
      "train loss:0.636514421079235\n",
      "train loss:0.569863723119603\n",
      "train loss:0.6021746159681862\n",
      "train loss:0.6120112881166172\n",
      "train loss:0.5335159887858033\n",
      "train loss:0.6875591615366663\n",
      "train loss:0.6146944606065403\n",
      "train loss:0.6873682444180301\n",
      "train loss:0.5157449043964344\n",
      "train loss:0.558037264851179\n",
      "train loss:0.5980252727643225\n",
      "train loss:0.5270616122574191\n",
      "train loss:0.6875713943792924\n",
      "train loss:0.6241306012275761\n",
      "train loss:0.6985398220441768\n",
      "train loss:0.5108207445530645\n",
      "train loss:0.625267888746391\n",
      "train loss:0.5313140856276598\n",
      "train loss:0.6352837741820615\n",
      "train loss:0.5629153589357867\n",
      "train loss:0.5582988259019345\n",
      "train loss:0.6305526442507455\n",
      "train loss:0.6066498867263116\n",
      "train loss:0.5464358068791035\n",
      "train loss:0.6317286722240282\n",
      "train loss:0.6124964759854239\n",
      "train loss:0.705015837804267\n",
      "train loss:0.6430685845864507\n",
      "train loss:0.5935215483017325\n",
      "train loss:0.5883903538476892\n",
      "train loss:0.49109649664329574\n",
      "train loss:0.5530484086915065\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5908671340756528\n",
      "train loss:0.6424262876289405\n",
      "train loss:0.6122286287580996\n",
      "train loss:0.4983288586129741\n",
      "train loss:0.6721860920364141\n",
      "train loss:0.6794009380114688\n",
      "train loss:0.6132848566545828\n",
      "train loss:0.5844423515455536\n",
      "train loss:0.5611138739893455\n",
      "train loss:0.6371731122190349\n",
      "train loss:0.5743251802085023\n",
      "train loss:0.6225915990402693\n",
      "train loss:0.6175912429636454\n",
      "train loss:0.6261765550625038\n",
      "train loss:0.5687222222300103\n",
      "train loss:0.5695828409298974\n",
      "train loss:0.7646362351069558\n",
      "train loss:0.6774567347460682\n",
      "train loss:0.7835451454324732\n",
      "train loss:0.5908040060325273\n",
      "train loss:0.606336119084943\n",
      "train loss:0.6453614276497001\n",
      "train loss:0.5812901377438723\n",
      "train loss:0.6672728063695449\n",
      "train loss:0.5700722962650975\n",
      "train loss:0.5385021007922168\n",
      "train loss:0.5850313304621618\n",
      "train loss:0.7274792863779385\n",
      "train loss:0.6623458701848718\n",
      "train loss:0.5852116402328716\n",
      "train loss:0.6109045830173978\n",
      "train loss:0.7322777891272739\n",
      "train loss:0.5520711988406934\n",
      "train loss:0.5944076866105058\n",
      "train loss:0.5449726068570262\n",
      "train loss:0.5200633502002752\n",
      "train loss:0.5274599540377244\n",
      "train loss:0.6134100825943335\n",
      "train loss:0.7288936780557533\n",
      "train loss:0.6741567053118648\n",
      "train loss:0.541666397305988\n",
      "train loss:0.6056108922667239\n",
      "train loss:0.5955255641685103\n",
      "train loss:0.6278360768454978\n",
      "train loss:0.6657543445314745\n",
      "train loss:0.5628694172376515\n",
      "train loss:0.7047384540619684\n",
      "train loss:0.5905453520059514\n",
      "train loss:0.5525723801921577\n",
      "train loss:0.632507739971775\n",
      "train loss:0.5614723033498661\n",
      "=== epoch:3, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6465742725356436\n",
      "train loss:0.5234671895686522\n",
      "train loss:0.4158428599643839\n",
      "train loss:0.6669962738931853\n",
      "train loss:0.5358242578717896\n",
      "train loss:0.48556295703870245\n",
      "train loss:0.5890146694099371\n",
      "train loss:0.7097339669849406\n",
      "train loss:0.5373483866743727\n",
      "train loss:0.6472940436883454\n",
      "train loss:0.6461895005581629\n",
      "train loss:0.6660825605840217\n",
      "train loss:0.5320536709043499\n",
      "train loss:0.5713605369554572\n",
      "train loss:0.6061787273572806\n",
      "train loss:0.5572047997463376\n",
      "train loss:0.6958735524814086\n",
      "train loss:0.5428592093527012\n",
      "train loss:0.5621392721410668\n",
      "train loss:0.7080376082745489\n",
      "train loss:0.5766816462221555\n",
      "train loss:0.5782233011264349\n",
      "train loss:0.5871544607889339\n",
      "train loss:0.4627519899629697\n",
      "train loss:0.6985140957493933\n",
      "train loss:0.5796729467405644\n",
      "train loss:0.6559531495835611\n",
      "train loss:0.6462789354889252\n",
      "train loss:0.613207220270683\n",
      "train loss:0.7152083894037091\n",
      "train loss:0.609839672522407\n",
      "train loss:0.6430013410990029\n",
      "train loss:0.6939679815892001\n",
      "train loss:0.5848704779102039\n",
      "train loss:0.6569762664066563\n",
      "train loss:0.6342078526112872\n",
      "train loss:0.5935300799741404\n",
      "train loss:0.6156893400815551\n",
      "train loss:0.5218497561269683\n",
      "train loss:0.5369790756252348\n",
      "train loss:0.5504087356941765\n",
      "train loss:0.5955240317578181\n",
      "train loss:0.6867587656503384\n",
      "train loss:0.6504995916126572\n",
      "train loss:0.6537544706349139\n",
      "train loss:0.6600888762735245\n",
      "train loss:0.6030109405014166\n",
      "train loss:0.5745196376834442\n",
      "train loss:0.5820671729456635\n",
      "train loss:0.43831236279440194\n",
      "train loss:0.6637992674152733\n",
      "=== epoch:4, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6093130919879025\n",
      "train loss:0.4787310387666032\n",
      "train loss:0.6695542845678334\n",
      "train loss:0.6556881370581719\n",
      "train loss:0.5226340355344247\n",
      "train loss:0.4876719021720034\n",
      "train loss:0.5133567234908614\n",
      "train loss:0.5821701492572162\n",
      "train loss:0.7767706370546538\n",
      "train loss:0.6023901710515319\n",
      "train loss:0.5534180750189547\n",
      "train loss:0.5328579893057124\n",
      "train loss:0.6304080898786067\n",
      "train loss:0.5265537204328512\n",
      "train loss:0.764847479347525\n",
      "train loss:0.6129895405359013\n",
      "train loss:0.5713354148931192\n",
      "train loss:0.6350117227891139\n",
      "train loss:0.6036100117904807\n",
      "train loss:0.6172425325280747\n",
      "train loss:0.535336851532564\n",
      "train loss:0.580072969927652\n",
      "train loss:0.48008407462843944\n",
      "train loss:0.5455772976889304\n",
      "train loss:0.6463520251956495\n",
      "train loss:0.5427677902254863\n",
      "train loss:0.6434956081289755\n",
      "train loss:0.5178862147913098\n",
      "train loss:0.6028735909576793\n",
      "train loss:0.6298448112633078\n",
      "train loss:0.43344105198135574\n",
      "train loss:0.6345998649814975\n",
      "train loss:0.5332523859545213\n",
      "train loss:0.3703939547331173\n",
      "train loss:0.6707405005801814\n",
      "train loss:0.5718185695319595\n",
      "train loss:0.5462702026851354\n",
      "train loss:0.5360222343278126\n",
      "train loss:0.6287379678705202\n",
      "train loss:0.5874139036462488\n",
      "train loss:0.507711692252196\n",
      "train loss:0.6522306678035363\n",
      "train loss:0.6336198462405388\n",
      "train loss:0.65694550443727\n",
      "train loss:0.6693791538187107\n",
      "train loss:0.5498178385528579\n",
      "train loss:0.6415195385696999\n",
      "train loss:0.5458763232368082\n",
      "train loss:0.6380526492948675\n",
      "train loss:0.5409806013454521\n",
      "train loss:0.5608704758084855\n",
      "=== epoch:5, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5230330958532513\n",
      "train loss:0.5186261060354636\n",
      "train loss:0.6392821041291761\n",
      "train loss:0.7770739199285117\n",
      "train loss:0.6114666181678869\n",
      "train loss:0.5960884252257771\n",
      "train loss:0.5985015882544035\n",
      "train loss:0.7040590190190035\n",
      "train loss:0.6078628032669453\n",
      "train loss:0.6244919398029308\n",
      "train loss:0.5585160919361924\n",
      "train loss:0.6823175722499681\n",
      "train loss:0.5976377952455347\n",
      "train loss:0.5184449708894939\n",
      "train loss:0.6304096889286511\n",
      "train loss:0.5950325233545309\n",
      "train loss:0.6078067449041114\n",
      "train loss:0.6915610614229554\n",
      "train loss:0.5248714661953324\n",
      "train loss:0.5642446170174188\n",
      "train loss:0.5745401925250481\n",
      "train loss:0.555788329543574\n",
      "train loss:0.637265082983522\n",
      "train loss:0.6080106319402627\n",
      "train loss:0.6032254609784888\n",
      "train loss:0.5614321359583934\n",
      "train loss:0.5144434264141381\n",
      "train loss:0.7527907265005206\n",
      "train loss:0.36579161312249026\n",
      "train loss:0.3999516749560609\n",
      "train loss:0.5330943977498028\n",
      "train loss:0.4658885935547122\n",
      "train loss:0.6778019746923177\n",
      "train loss:0.6911945359459463\n",
      "train loss:0.4062853465165534\n",
      "train loss:0.6269815290958729\n",
      "train loss:0.5627415609821466\n",
      "train loss:0.5097928685749958\n",
      "train loss:0.6809884619058576\n",
      "train loss:0.6136213740619705\n",
      "train loss:0.6526066944302326\n",
      "train loss:0.5857290312786744\n",
      "train loss:0.5143820148258622\n",
      "train loss:0.4566181455266092\n",
      "train loss:0.6571250969502794\n",
      "train loss:0.6958846065485483\n",
      "train loss:0.522865248859224\n",
      "train loss:0.5755901860778552\n",
      "train loss:0.5509035294035041\n",
      "train loss:0.6123382271398815\n",
      "train loss:0.49847412125533985\n",
      "=== epoch:6, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5244307801087509\n",
      "train loss:0.3909946854795036\n",
      "train loss:0.43829960410900853\n",
      "train loss:0.708938926855564\n",
      "train loss:0.7078891058519629\n",
      "train loss:0.6840164063564976\n",
      "train loss:0.47055337602313824\n",
      "train loss:0.6760179478067925\n",
      "train loss:0.4946839404887406\n",
      "train loss:0.5086330811282985\n",
      "train loss:0.6072149571736669\n",
      "train loss:0.7152161698336355\n",
      "train loss:0.597094963370571\n",
      "train loss:0.7531015593223469\n",
      "train loss:0.4890740125563707\n",
      "train loss:0.5614569925526837\n",
      "train loss:0.6046688457533894\n",
      "train loss:0.6873717557334176\n",
      "train loss:0.6320070225323601\n",
      "train loss:0.5332762703649102\n",
      "train loss:0.5383624178209973\n",
      "train loss:0.6138850716641735\n",
      "train loss:0.6203024434348836\n",
      "train loss:0.5287926619731432\n",
      "train loss:0.697256277635138\n",
      "train loss:0.6523333557216933\n",
      "train loss:0.5125066148898126\n",
      "train loss:0.6052803234063393\n",
      "train loss:0.6346494910515569\n",
      "train loss:0.3866274856151263\n",
      "train loss:0.7426388627560324\n",
      "train loss:0.5208956765765218\n",
      "train loss:0.6228869132693533\n",
      "train loss:0.45181479769976995\n",
      "train loss:0.6698088584730127\n",
      "train loss:0.40506658046818766\n",
      "train loss:0.6395010778622184\n",
      "train loss:0.5311917484925838\n",
      "train loss:0.5767800434482715\n",
      "train loss:0.6900381581780739\n",
      "train loss:0.5866157750170119\n",
      "train loss:0.49236601900163296\n",
      "train loss:0.47573835677761334\n",
      "train loss:0.6120164059974703\n",
      "train loss:0.6333008240682889\n",
      "train loss:0.5943087780212065\n",
      "train loss:0.49730828229119745\n",
      "train loss:0.6015974060611722\n",
      "train loss:0.5652421047355316\n",
      "train loss:0.4531022035379809\n",
      "train loss:0.558317754164018\n",
      "=== epoch:7, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5702756908194792\n",
      "train loss:0.6537161768797279\n",
      "train loss:0.6738288008793375\n",
      "train loss:0.4360846694886273\n",
      "train loss:0.5921485255512172\n",
      "train loss:0.6600269331291014\n",
      "train loss:0.6044044559918667\n",
      "train loss:0.6206243623312027\n",
      "train loss:0.5366316463030361\n",
      "train loss:0.6283441717084848\n",
      "train loss:0.537485392377643\n",
      "train loss:0.5611578740956895\n",
      "train loss:0.6279385953057397\n",
      "train loss:0.4975871376784356\n",
      "train loss:0.5550200926053462\n",
      "train loss:0.5976270592783215\n",
      "train loss:0.7619093362645886\n",
      "train loss:0.5507090243277234\n",
      "train loss:0.4801847968316084\n",
      "train loss:0.6898786559836794\n",
      "train loss:0.5349993496148462\n",
      "train loss:0.7134136979790212\n",
      "train loss:0.5480163886800408\n",
      "train loss:0.4596135909407764\n",
      "train loss:0.6305932092413786\n",
      "train loss:0.5224207018418766\n",
      "train loss:0.5301206658142896\n",
      "train loss:0.6031479941061964\n",
      "train loss:0.489381699492983\n",
      "train loss:0.44140148860615086\n",
      "train loss:0.5876390913158137\n",
      "train loss:0.5690900362091088\n",
      "train loss:0.6759321743105282\n",
      "train loss:0.6217263877352939\n",
      "train loss:0.5773963783270405\n",
      "train loss:0.457442553257722\n",
      "train loss:0.5752635689184232\n",
      "train loss:0.5500156077039529\n",
      "train loss:0.45266585687814886\n",
      "train loss:0.5070065545881023\n",
      "train loss:0.5829262743013012\n",
      "train loss:0.4423654057597319\n",
      "train loss:0.5136797456898399\n",
      "train loss:0.5488061506410201\n",
      "train loss:0.6053960558045812\n",
      "train loss:0.6079158324571817\n",
      "train loss:0.4466893144836074\n",
      "train loss:0.5095239261008665\n",
      "train loss:0.5474013852283012\n",
      "train loss:0.5492011199042435\n",
      "train loss:0.5189863853735743\n",
      "=== epoch:8, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5167174019522974\n",
      "train loss:0.6425317720855505\n",
      "train loss:0.6112924881567927\n",
      "train loss:0.5543987949349929\n",
      "train loss:0.5779839879629936\n",
      "train loss:0.5053129792769697\n",
      "train loss:0.6007356750904572\n",
      "train loss:0.6294347379929495\n",
      "train loss:0.4361102536769005\n",
      "train loss:0.5213331275092437\n",
      "train loss:0.5066214742011665\n",
      "train loss:0.4536726659669048\n",
      "train loss:0.6197755037840559\n",
      "train loss:0.6200466859270186\n",
      "train loss:0.7265921053016132\n",
      "train loss:0.53657307749037\n",
      "train loss:0.590956546037169\n",
      "train loss:0.5763974736570197\n",
      "train loss:0.6045289872571954\n",
      "train loss:0.5133671010678358\n",
      "train loss:0.5999480353633827\n",
      "train loss:0.5095252812659081\n",
      "train loss:0.6038408650089349\n",
      "train loss:0.6120793914546829\n",
      "train loss:0.5312661629628387\n",
      "train loss:0.6081131375450859\n",
      "train loss:0.511227335230376\n",
      "train loss:0.5384263590037949\n",
      "train loss:0.5162276230503207\n",
      "train loss:0.5290538882193251\n",
      "train loss:0.6500724798209387\n",
      "train loss:0.5825391931011954\n",
      "train loss:0.6464606959605789\n",
      "train loss:0.4153696092482794\n",
      "train loss:0.4636840103558357\n",
      "train loss:0.48936248340919636\n",
      "train loss:0.5311117207402898\n",
      "train loss:0.6536107035317424\n",
      "train loss:0.5604782009424821\n",
      "train loss:0.6208576533324567\n",
      "train loss:0.5760259232219702\n",
      "train loss:0.574898034937038\n",
      "train loss:0.5139570411733007\n",
      "train loss:0.4692335242591645\n",
      "train loss:0.6223796835680002\n",
      "train loss:0.6385906585437081\n",
      "train loss:0.5649464693380074\n",
      "train loss:0.5590667733970339\n",
      "train loss:0.6398726913222156\n",
      "train loss:0.5913746608078346\n",
      "train loss:0.5165793282944064\n",
      "=== epoch:9, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6071457649938984\n",
      "train loss:0.4439804689212313\n",
      "train loss:0.4745827853722691\n",
      "train loss:0.6474487281419212\n",
      "train loss:0.47476563986046566\n",
      "train loss:0.46390978299795216\n",
      "train loss:0.48718654899531366\n",
      "train loss:0.49635348168671645\n",
      "train loss:0.6876479788163153\n",
      "train loss:0.48898838497815017\n",
      "train loss:0.6052596360129775\n",
      "train loss:0.6069841472407448\n",
      "train loss:0.440050146884096\n",
      "train loss:0.6318189699037826\n",
      "train loss:0.6310713594097289\n",
      "train loss:0.5148561401767628\n",
      "train loss:0.6967361604174153\n",
      "train loss:0.5639851434990142\n",
      "train loss:0.5568340122083141\n",
      "train loss:0.6385037636068913\n",
      "train loss:0.5752481032201575\n",
      "train loss:0.5752655115338303\n",
      "train loss:0.5628831267498481\n",
      "train loss:0.5812230529971499\n",
      "train loss:0.514457757112661\n",
      "train loss:0.6430665342851457\n",
      "train loss:0.4450358535896565\n",
      "train loss:0.6251930536163354\n",
      "train loss:0.46852943455960483\n",
      "train loss:0.5583244791915116\n",
      "train loss:0.6858128641483521\n",
      "train loss:0.32716569245731747\n",
      "train loss:0.5691055033160526\n",
      "train loss:0.49465394638861254\n",
      "train loss:0.48611425406215436\n",
      "train loss:0.38777981083336566\n",
      "train loss:0.40659580888195773\n",
      "train loss:0.5872834375674219\n",
      "train loss:0.6591185617385038\n",
      "train loss:0.5430594048723665\n",
      "train loss:0.4860689043304315\n",
      "train loss:0.6691913069848053\n",
      "train loss:0.4959112949984361\n",
      "train loss:0.640208078673879\n",
      "train loss:0.6232831523581611\n",
      "train loss:0.5332102841694805\n",
      "train loss:0.5393217794354845\n",
      "train loss:0.5522792661446733\n",
      "train loss:0.4759805734761661\n",
      "train loss:0.45948763239062884\n",
      "train loss:0.651661256111861\n",
      "=== epoch:10, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6919096908691793\n",
      "train loss:0.6188967505916096\n",
      "train loss:0.6592859964923738\n",
      "train loss:0.5065917961419578\n",
      "train loss:0.6122454081719706\n",
      "train loss:0.5723661877819082\n",
      "train loss:0.5548031943036105\n",
      "train loss:0.545479332013889\n",
      "train loss:0.5670929484556759\n",
      "train loss:0.7210762716491624\n",
      "train loss:0.46250874719972873\n",
      "train loss:0.646933967328293\n",
      "train loss:0.5139047704612404\n",
      "train loss:0.5213509336184657\n",
      "train loss:0.5966986293244106\n",
      "train loss:0.6107980252835936\n",
      "train loss:0.5547469092850412\n",
      "train loss:0.6921062547380734\n",
      "train loss:0.5997151453934585\n",
      "train loss:0.4239128371961382\n",
      "train loss:0.3190955885192771\n",
      "train loss:0.3880279841482891\n",
      "train loss:0.6953513384153643\n",
      "train loss:0.46141697916005897\n",
      "train loss:0.6196776760570952\n",
      "train loss:0.5470146957928753\n",
      "train loss:0.6349406527557222\n",
      "train loss:0.5595737189772713\n",
      "train loss:0.4760822393121906\n",
      "train loss:0.5392279285477423\n",
      "train loss:0.5349681915842517\n",
      "train loss:0.4343137795395674\n",
      "train loss:0.5908180178178437\n",
      "train loss:0.5708318554252673\n",
      "train loss:0.6729934063214192\n",
      "train loss:0.5964956462132129\n",
      "train loss:0.4912888793840701\n",
      "train loss:0.4502859764148163\n",
      "train loss:0.7242925536074113\n",
      "train loss:0.5215283662952539\n",
      "train loss:0.6680886530380513\n",
      "train loss:0.5830408473471047\n",
      "train loss:0.46272158172039934\n",
      "train loss:0.6193104200767628\n",
      "train loss:0.5797259599430042\n",
      "train loss:0.4733815202932105\n",
      "train loss:0.5548581573248375\n",
      "train loss:0.5123166246901969\n",
      "train loss:0.5221441684558797\n",
      "train loss:0.48220899786807947\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5490196078431373\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 90, 'filter_size': 7, 'pad': 2, 'stride': 1},\n",
    "                        hidden_size=100, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=30,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "946eb8fe-a248-4d92-b3c9-671c37c1bc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6920259580716391\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6850742721949759\n",
      "train loss:0.6756710430512316\n",
      "train loss:0.6738333050009475\n",
      "train loss:0.633265821688911\n",
      "train loss:0.632111282281851\n",
      "train loss:0.6437952087280968\n",
      "train loss:0.6508215428583093\n",
      "train loss:0.6449044547123213\n",
      "train loss:0.5652086390602428\n",
      "train loss:0.5881463884658892\n",
      "train loss:0.6214469009075817\n",
      "train loss:0.72938118632608\n",
      "train loss:0.6554481901378093\n",
      "train loss:0.5291937833280212\n",
      "train loss:0.5966621600933955\n",
      "train loss:0.6432911438454855\n",
      "train loss:0.618392097900574\n",
      "train loss:0.6626039953202938\n",
      "train loss:0.5662300384307233\n",
      "train loss:0.6585118006189637\n",
      "train loss:0.583396869222167\n",
      "train loss:0.5488877779082136\n",
      "train loss:0.547093966613876\n",
      "train loss:0.5021096524363559\n",
      "train loss:0.528456205728997\n",
      "train loss:0.5581901750365185\n",
      "train loss:0.6557462567924219\n",
      "train loss:0.4910582539621938\n",
      "train loss:0.5913588331515325\n",
      "train loss:0.6984977126650251\n",
      "train loss:0.4724306551631927\n",
      "train loss:0.5257892267882138\n",
      "train loss:0.5939028179396761\n",
      "train loss:0.5411279233512923\n",
      "train loss:0.6425896059895765\n",
      "train loss:0.6873336790439403\n",
      "train loss:0.5489664162793098\n",
      "train loss:0.5073590628563334\n",
      "train loss:0.634425969676772\n",
      "train loss:0.6207088347510634\n",
      "train loss:0.5663987019318124\n",
      "train loss:0.6958828727713632\n",
      "train loss:0.5483723828901981\n",
      "train loss:0.6660816730692716\n",
      "train loss:0.49240538642693155\n",
      "train loss:0.6469961782925646\n",
      "train loss:0.5310130159834235\n",
      "train loss:0.6184154743540279\n",
      "train loss:0.5740109732865083\n",
      "train loss:0.6220570154984384\n",
      "train loss:0.5625307040904701\n",
      "train loss:0.5017647879660435\n",
      "train loss:0.6960145388893736\n",
      "train loss:0.5692507919843962\n",
      "train loss:0.7334058807573005\n",
      "train loss:0.6342793278231262\n",
      "train loss:0.5603446807241029\n",
      "train loss:0.5431686610584834\n",
      "train loss:0.6647394804533977\n",
      "train loss:0.5781538992343223\n",
      "train loss:0.6703877680200602\n",
      "train loss:0.6276626450713649\n",
      "train loss:0.564175097071\n",
      "train loss:0.6063286224495087\n",
      "train loss:0.636316002996387\n",
      "train loss:0.6271123333598954\n",
      "train loss:0.5563781598094953\n",
      "train loss:0.5772940342434869\n",
      "train loss:0.47784494081880363\n",
      "train loss:0.5853957715307685\n",
      "train loss:0.5634358760089422\n",
      "train loss:0.562778253412703\n",
      "train loss:0.5532445871894897\n",
      "train loss:0.6020547293582996\n",
      "train loss:0.7603690833339641\n",
      "train loss:0.5111165295954068\n",
      "train loss:0.5638820839135521\n",
      "train loss:0.47456141184562617\n",
      "train loss:0.6867526537749964\n",
      "train loss:0.5525711075284148\n",
      "train loss:0.6692147591119009\n",
      "train loss:0.606100172323842\n",
      "train loss:0.6080436502479541\n",
      "train loss:0.6751583263988918\n",
      "train loss:0.555006756254471\n",
      "train loss:0.5969496641316153\n",
      "train loss:0.6764676880778475\n",
      "train loss:0.631695527286649\n",
      "train loss:0.5742732040069692\n",
      "train loss:0.5188016169266746\n",
      "train loss:0.5055775030404203\n",
      "train loss:0.6725899231120034\n",
      "train loss:0.4575837026371534\n",
      "train loss:0.38976892505103333\n",
      "train loss:0.7400092686307582\n",
      "train loss:0.577970340748376\n",
      "train loss:0.627492993469575\n",
      "train loss:0.7126697421512926\n",
      "train loss:0.5759705165552025\n",
      "train loss:0.7095183761800896\n",
      "train loss:0.5889735431243122\n",
      "train loss:0.6509059469004462\n",
      "train loss:0.6613142929204006\n",
      "train loss:0.6689697037998261\n",
      "train loss:0.6200648930771353\n",
      "train loss:0.6321113934770335\n",
      "train loss:0.6667266443362662\n",
      "train loss:0.6499619123360911\n",
      "train loss:0.6451375724883974\n",
      "train loss:0.6085968937996932\n",
      "train loss:0.6488250452009503\n",
      "train loss:0.6301133009552405\n",
      "train loss:0.5441032888926042\n",
      "train loss:0.6620197961967167\n",
      "train loss:0.5612703484723316\n",
      "train loss:0.6068419221900131\n",
      "train loss:0.6379663648991731\n",
      "train loss:0.3894344128813903\n",
      "train loss:0.7920606752765487\n",
      "train loss:0.6095728648809985\n",
      "train loss:0.6103107107079462\n",
      "train loss:0.5545317352268265\n",
      "train loss:0.493158581405695\n",
      "train loss:0.6438775874864383\n",
      "train loss:0.5682478673270517\n",
      "train loss:0.6269592358814977\n",
      "train loss:0.5884006086286975\n",
      "train loss:0.5787467566212572\n",
      "train loss:0.6141558777126821\n",
      "train loss:0.6555543638753327\n",
      "train loss:0.6106557625743319\n",
      "train loss:0.7113678468506847\n",
      "train loss:0.674602377604948\n",
      "train loss:0.6046926232847423\n",
      "train loss:0.6433307828176679\n",
      "train loss:0.6187841678869648\n",
      "train loss:0.5901437381964353\n",
      "train loss:0.576337868105715\n",
      "train loss:0.6117010281798771\n",
      "train loss:0.6275434826259217\n",
      "train loss:0.6822048349318498\n",
      "train loss:0.5767645706079056\n",
      "train loss:0.5801039156862545\n",
      "train loss:0.6035056285353042\n",
      "train loss:0.5681423701376527\n",
      "train loss:0.5827678736072203\n",
      "train loss:0.544007301228506\n",
      "train loss:0.5342435093302672\n",
      "train loss:0.6144601430611834\n",
      "train loss:0.6203256805770379\n",
      "train loss:0.669135387921744\n",
      "train loss:0.6970484406551362\n",
      "train loss:0.628647691436282\n",
      "=== epoch:2, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.5638702460149012\n",
      "train loss:0.649116787370664\n",
      "train loss:0.6023158597142714\n",
      "train loss:0.6737959674440893\n",
      "train loss:0.6607058885887455\n",
      "train loss:0.5730562296619234\n",
      "train loss:0.6487882690734725\n",
      "train loss:0.537357216303287\n",
      "train loss:0.6473772696426907\n",
      "train loss:0.6797277970794197\n",
      "train loss:0.5072128006111714\n",
      "train loss:0.5491459574173414\n",
      "train loss:0.584624524995555\n",
      "train loss:0.47229347551462286\n",
      "train loss:0.7623069295058404\n",
      "train loss:0.6086497000258593\n",
      "train loss:0.6663988943107372\n",
      "train loss:0.48071870504123676\n",
      "train loss:0.5602132544095164\n",
      "train loss:0.6032383139400652\n",
      "train loss:0.5357753892279605\n",
      "train loss:0.5855591837906123\n",
      "train loss:0.6378232239607049\n",
      "train loss:0.5554179956999354\n",
      "train loss:0.4838592878457978\n",
      "train loss:0.6038627769027254\n",
      "train loss:0.561006979396246\n",
      "train loss:0.6484238970464278\n",
      "train loss:0.5752285076664807\n",
      "train loss:0.6026252782435503\n",
      "train loss:0.5737415919259943\n",
      "train loss:0.47842605145210326\n",
      "train loss:0.6584560729683469\n",
      "train loss:0.5168783673757849\n",
      "train loss:0.6351501614425034\n",
      "train loss:0.5687506795921845\n",
      "train loss:0.582405380115604\n",
      "train loss:0.6438744736661512\n",
      "train loss:0.694267704255211\n",
      "train loss:0.7082607080523526\n",
      "train loss:0.6013480457809605\n",
      "train loss:0.6644903166954947\n",
      "train loss:0.5929988948396188\n",
      "train loss:0.7069986854769936\n",
      "train loss:0.6212357573450561\n",
      "train loss:0.6406330784996752\n",
      "train loss:0.609968646284559\n",
      "train loss:0.6132695692832989\n",
      "train loss:0.5556009539387381\n",
      "train loss:0.5847397373526786\n",
      "train loss:0.6522146824064003\n",
      "train loss:0.6614772665532799\n",
      "train loss:0.5193924766725768\n",
      "train loss:0.6681391375250259\n",
      "train loss:0.47473419827061736\n",
      "train loss:0.644532810441006\n",
      "train loss:0.6470272287927615\n",
      "train loss:0.6757081166003959\n",
      "train loss:0.5590380133353126\n",
      "train loss:0.5989693323578557\n",
      "train loss:0.5815664848737481\n",
      "train loss:0.48175299706557545\n",
      "train loss:0.7037531602899592\n",
      "train loss:0.6164901673529835\n",
      "train loss:0.6538796438031748\n",
      "train loss:0.6680790795160136\n",
      "train loss:0.5768673882727562\n",
      "train loss:0.6315394711080702\n",
      "train loss:0.5668931610336128\n",
      "train loss:0.534617436697624\n",
      "train loss:0.5663586938787266\n",
      "train loss:0.6443348668565311\n",
      "train loss:0.537163418268794\n",
      "train loss:0.6591472203331457\n",
      "train loss:0.6359358909055324\n",
      "train loss:0.5374515392343137\n",
      "train loss:0.5839000943465631\n",
      "train loss:0.48761442685785655\n",
      "train loss:0.6215218171421512\n",
      "train loss:0.6191666237428567\n",
      "train loss:0.616579488505632\n",
      "train loss:0.5466258099729\n",
      "train loss:0.5965193727935759\n",
      "train loss:0.5548253800162979\n",
      "train loss:0.5961608337718171\n",
      "train loss:0.5924417949070507\n",
      "train loss:0.7728566492518159\n",
      "train loss:0.6219701784666019\n",
      "train loss:0.6072385287405123\n",
      "train loss:0.6162795008063051\n",
      "train loss:0.5911964366734689\n",
      "train loss:0.577333710892187\n",
      "train loss:0.6771976746525412\n",
      "train loss:0.6572712883565026\n",
      "train loss:0.5855708560769564\n",
      "train loss:0.48727686837709827\n",
      "train loss:0.642066471970355\n",
      "train loss:0.49643124909189995\n",
      "train loss:0.5680972211314959\n",
      "train loss:0.4713332911161519\n",
      "train loss:0.4747872594888839\n",
      "train loss:0.5622213212370412\n",
      "train loss:0.606142491659524\n",
      "train loss:0.5555567162408204\n",
      "train loss:0.585911121523922\n",
      "train loss:0.6226799715860013\n",
      "train loss:0.6241501609587632\n",
      "train loss:0.5414364263279973\n",
      "train loss:0.6177082982132424\n",
      "train loss:0.5666289933965031\n",
      "train loss:0.573810429535549\n",
      "train loss:0.5930045886585649\n",
      "train loss:0.566766970694856\n",
      "train loss:0.4971267315242523\n",
      "train loss:0.6365147662876237\n",
      "train loss:0.6146451969217807\n",
      "train loss:0.5096602040169931\n",
      "train loss:0.6083078869443883\n",
      "train loss:0.6051253548190892\n",
      "train loss:0.6024633433649683\n",
      "train loss:0.6045479438381982\n",
      "train loss:0.677673801397544\n",
      "train loss:0.6129325979912522\n",
      "train loss:0.5972694983337894\n",
      "train loss:0.6083669569368682\n",
      "train loss:0.5265472892009113\n",
      "train loss:0.51390176075436\n",
      "train loss:0.5529744271886801\n",
      "train loss:0.5564442610800518\n",
      "train loss:0.4184951277035169\n",
      "train loss:0.6507400044920859\n",
      "train loss:0.7598874403459097\n",
      "train loss:0.41602577004973734\n",
      "train loss:0.5066936178403596\n",
      "train loss:0.6726209952676988\n",
      "train loss:0.6633143029768396\n",
      "train loss:0.5570489731276699\n",
      "train loss:0.7192807263213129\n",
      "train loss:0.5845676323947028\n",
      "train loss:0.5696413187679152\n",
      "train loss:0.5747144223425277\n",
      "train loss:0.5641402654786187\n",
      "train loss:0.5127471468688641\n",
      "train loss:0.6759090267206862\n",
      "train loss:0.5542868985192416\n",
      "train loss:0.5227120987447192\n",
      "train loss:0.4098147421787942\n",
      "train loss:0.6765317598089332\n",
      "train loss:0.6359099721300167\n",
      "train loss:0.5859670134161175\n",
      "train loss:0.46798088131376847\n",
      "train loss:0.43633879794537445\n",
      "train loss:0.5848333532112806\n",
      "=== epoch:3, train acc:0.75, test acc:0.68 ===\n",
      "train loss:0.5422509082554064\n",
      "train loss:0.5053614940702549\n",
      "train loss:0.6834267207231867\n",
      "train loss:0.5840057524117722\n",
      "train loss:0.5751769049322417\n",
      "train loss:0.5774176730869396\n",
      "train loss:0.5360221024552219\n",
      "train loss:0.5390336828865713\n",
      "train loss:0.5396917040459495\n",
      "train loss:0.5765936536192957\n",
      "train loss:0.5494681044120379\n",
      "train loss:0.5997075799181207\n",
      "train loss:0.5078048065730492\n",
      "train loss:0.49622398462562173\n",
      "train loss:0.5865177922153688\n",
      "train loss:0.5351185548863662\n",
      "train loss:0.6261438231781256\n",
      "train loss:0.530315280521364\n",
      "train loss:0.7146736176471384\n",
      "train loss:0.5032505713495515\n",
      "train loss:0.4957511570554005\n",
      "train loss:0.5615585901218807\n",
      "train loss:0.5307180681873926\n",
      "train loss:0.6075180835121525\n",
      "train loss:0.6303205102673096\n",
      "train loss:0.607180466632042\n",
      "train loss:0.5485600216350641\n",
      "train loss:0.481341997805559\n",
      "train loss:0.6617985686888053\n",
      "train loss:0.6059583697002585\n",
      "train loss:0.5615483163006363\n",
      "train loss:0.45614543449847444\n",
      "train loss:0.5775574757490621\n",
      "train loss:0.6399336027770424\n",
      "train loss:0.5839258101877826\n",
      "train loss:0.5031769742868178\n",
      "train loss:0.5867340785031823\n",
      "train loss:0.5343980488283904\n",
      "train loss:0.5263711901439432\n",
      "train loss:0.5385769121556496\n",
      "train loss:0.5275172805875611\n",
      "train loss:0.6162228991136456\n",
      "train loss:0.4874171572942836\n",
      "train loss:0.6110177343106616\n",
      "train loss:0.6215574744938754\n",
      "train loss:0.6333098797491654\n",
      "train loss:0.5752390820324754\n",
      "train loss:0.5817042360064173\n",
      "train loss:0.5987054916163592\n",
      "train loss:0.5910337525171002\n",
      "train loss:0.6351923452121037\n",
      "train loss:0.5768318353836356\n",
      "train loss:0.47500787548552675\n",
      "train loss:0.6242208375083165\n",
      "train loss:0.580234279003165\n",
      "train loss:0.5408497761865928\n",
      "train loss:0.5722263936882923\n",
      "train loss:0.4610781643959463\n",
      "train loss:0.5865740623688884\n",
      "train loss:0.46462741398314533\n",
      "train loss:0.572879756397381\n",
      "train loss:0.5275949835715603\n",
      "train loss:0.6522638359757422\n",
      "train loss:0.5124696999763481\n",
      "train loss:0.5411700689317593\n",
      "train loss:0.5819676407216642\n",
      "train loss:0.40986172718359637\n",
      "train loss:0.5068047790550859\n",
      "train loss:0.540337401214871\n",
      "train loss:0.504910148625705\n",
      "train loss:0.49417419972375576\n",
      "train loss:0.3938439165679354\n",
      "train loss:0.655413072641099\n",
      "train loss:0.6691617313805762\n",
      "train loss:0.5839585078913379\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5411764705882353\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 90, 'filter_size': 7, 'pad': 2, 'stride': 1},\n",
    "                        hidden_size=100, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=40,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88362a59-7973-4584-a68f-e4ec51a911f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.6890298638239041\n",
      "=== epoch:1, train acc:0.72, test acc:0.69 ===\n",
      "train loss:0.6818436123032023\n",
      "train loss:0.687442309088182\n",
      "train loss:0.6378299325596456\n",
      "train loss:0.6191798902766569\n",
      "train loss:0.5989097442583547\n",
      "train loss:0.6663217866518888\n",
      "train loss:0.627200812326288\n",
      "train loss:0.6561204118852924\n",
      "train loss:0.43844264565803387\n",
      "train loss:0.58997557546556\n",
      "train loss:0.5290666576254316\n",
      "train loss:0.49962655647046217\n",
      "train loss:0.5935340911407487\n",
      "train loss:0.5292443071079856\n",
      "train loss:0.6340000210650631\n",
      "train loss:0.58717179455823\n",
      "train loss:0.6450234029028314\n",
      "train loss:0.5458536703708351\n",
      "train loss:0.6995484326139138\n",
      "train loss:0.6634681402508968\n",
      "train loss:0.699664246423847\n",
      "train loss:0.6234475191805268\n",
      "train loss:0.6268275651278657\n",
      "train loss:0.645172314831444\n",
      "train loss:0.637964011251021\n",
      "train loss:0.6039102091638939\n",
      "train loss:0.608066543373919\n",
      "train loss:0.4943499327090054\n",
      "train loss:0.5406971445084189\n",
      "train loss:0.692542336392918\n",
      "train loss:0.5453812559924297\n",
      "train loss:0.6179807263828399\n",
      "train loss:0.5591408859315907\n",
      "train loss:0.6972691712252209\n",
      "train loss:0.6451658941158717\n",
      "train loss:0.5446740864545955\n",
      "train loss:0.6174964884270675\n",
      "train loss:0.60816435920464\n",
      "train loss:0.6657173463275575\n",
      "train loss:0.6054229363555653\n",
      "train loss:0.6676334102628597\n",
      "train loss:0.6209613302103199\n",
      "train loss:0.6407792839387999\n",
      "train loss:0.6457492910599129\n",
      "train loss:0.6508296577863608\n",
      "train loss:0.653491843030866\n",
      "train loss:0.6433550868770849\n",
      "train loss:0.5548139553779834\n",
      "train loss:0.6355275368570354\n",
      "train loss:0.6036752397431205\n",
      "train loss:0.5915306942176991\n",
      "train loss:0.5992090107898962\n",
      "train loss:0.544423103967702\n",
      "train loss:0.6163798800350986\n",
      "train loss:0.642497412599162\n",
      "train loss:0.5808075536699363\n",
      "train loss:0.578284631299691\n",
      "train loss:0.6303092873166372\n",
      "train loss:0.4728717215316437\n",
      "train loss:0.78654681999894\n",
      "train loss:0.6419706460720214\n",
      "train loss:0.6447772205745013\n",
      "train loss:0.6230174334314622\n",
      "train loss:0.5854101533260104\n",
      "train loss:0.6136900067003235\n",
      "train loss:0.6192674348354288\n",
      "train loss:0.6365998778594567\n",
      "train loss:0.6614501087536127\n",
      "train loss:0.6021877324724381\n",
      "train loss:0.6175770008116823\n",
      "train loss:0.711984086788254\n",
      "train loss:0.5995943575736814\n",
      "train loss:0.5951165357743826\n",
      "train loss:0.610044634752021\n",
      "train loss:0.45229567615154326\n",
      "train loss:0.6208823481016676\n",
      "train loss:0.6225211324923252\n",
      "train loss:0.6484986524265953\n",
      "train loss:0.5677334471721938\n",
      "train loss:0.5141208544129008\n",
      "train loss:0.6384871253224068\n",
      "train loss:0.5872839085929324\n",
      "train loss:0.6051537244561622\n",
      "train loss:0.6021229310423082\n",
      "train loss:0.5476510853375918\n",
      "train loss:0.6107300992134248\n",
      "train loss:0.5922843526267805\n",
      "train loss:0.48662550441380154\n",
      "train loss:0.5290054053343585\n",
      "train loss:0.6011068998330871\n",
      "train loss:0.6414740668977368\n",
      "train loss:0.5604708973796826\n",
      "train loss:0.5122467300198363\n",
      "train loss:0.6685354326880979\n",
      "train loss:0.5043181454334267\n",
      "train loss:0.5188462794360277\n",
      "train loss:0.6016499655642538\n",
      "train loss:0.6124109883574875\n",
      "train loss:0.6289648629378278\n",
      "train loss:0.5774836379164034\n",
      "train loss:0.5885445668527898\n",
      "train loss:0.564323296401185\n",
      "train loss:0.5561400037871621\n",
      "train loss:0.6006846552651115\n",
      "train loss:0.6843084523555005\n",
      "train loss:0.6983987318484709\n",
      "train loss:0.5548024239974432\n",
      "train loss:0.582291203526824\n",
      "train loss:0.5444570953087361\n",
      "train loss:0.5888520555788618\n",
      "train loss:0.6206027176856598\n",
      "train loss:0.6427327863386487\n",
      "train loss:0.5723693072598043\n",
      "train loss:0.5307786214986577\n",
      "train loss:0.5852877352975837\n",
      "train loss:0.6254561197222592\n",
      "train loss:0.5758144976349345\n",
      "train loss:0.64031697120102\n",
      "train loss:0.6185438831541078\n",
      "train loss:0.5810202525200424\n",
      "train loss:0.6996978118894226\n",
      "train loss:0.5844861907017455\n",
      "train loss:0.6695258593237386\n",
      "train loss:0.6060889395979427\n",
      "train loss:0.5861826048245591\n",
      "train loss:0.6198073711377267\n",
      "train loss:0.6176752181520834\n",
      "train loss:0.5320683601767391\n",
      "train loss:0.5160865376503149\n",
      "train loss:0.575366646400404\n",
      "train loss:0.5709954853895295\n",
      "train loss:0.6041020926527251\n",
      "train loss:0.5669166476468862\n",
      "train loss:0.5027473943161274\n",
      "train loss:0.575252236022606\n",
      "train loss:0.6976561867667143\n",
      "train loss:0.5188337070578023\n",
      "train loss:0.49223964353971533\n",
      "train loss:0.6998767523652603\n",
      "train loss:0.5688446265234874\n",
      "train loss:0.6204031021988605\n",
      "train loss:0.6065933963224579\n",
      "train loss:0.6526578028711431\n",
      "train loss:0.6597906852707079\n",
      "train loss:0.6618490888424472\n",
      "train loss:0.628215281583153\n",
      "train loss:0.6330121587011116\n",
      "train loss:0.5609817194058182\n",
      "train loss:0.6205350981950535\n",
      "train loss:0.64939622538229\n",
      "train loss:0.5792143616304719\n",
      "train loss:0.5402508070525767\n",
      "train loss:0.5865605155050067\n",
      "train loss:0.6379881376404595\n",
      "train loss:0.635048733729302\n",
      "train loss:0.6721836893575106\n",
      "train loss:0.6674204859795019\n",
      "train loss:0.625151265750398\n",
      "train loss:0.5911111906756947\n",
      "train loss:0.5901123596408588\n",
      "train loss:0.6400497404148274\n",
      "train loss:0.575907345922433\n",
      "train loss:0.6179039956257876\n",
      "train loss:0.6756840499125245\n",
      "train loss:0.5851644606877923\n",
      "train loss:0.582897589252417\n",
      "train loss:0.6727852478956428\n",
      "train loss:0.5630476539934502\n",
      "train loss:0.6614891563272346\n",
      "train loss:0.6289961079276992\n",
      "train loss:0.6478528534350023\n",
      "train loss:0.6003251605926665\n",
      "train loss:0.5144520396801692\n",
      "train loss:0.5944796871991026\n",
      "train loss:0.6734127590014589\n",
      "train loss:0.5327372679449174\n",
      "train loss:0.5897815242848773\n",
      "train loss:0.5907426890602036\n",
      "train loss:0.6340771225921383\n",
      "train loss:0.5341891558130344\n",
      "train loss:0.44760656463791454\n",
      "train loss:0.5534487790039632\n",
      "train loss:0.5750783086023258\n",
      "train loss:0.6873682832921297\n",
      "train loss:0.5595474701786454\n",
      "train loss:0.6569689008491642\n",
      "train loss:0.5992545326036586\n",
      "train loss:0.5243234056158328\n",
      "train loss:0.5787498028004229\n",
      "train loss:0.6605773762903783\n",
      "train loss:0.6003428233549492\n",
      "train loss:0.5527998317953842\n",
      "train loss:0.638403815138755\n",
      "train loss:0.5691678653901332\n",
      "train loss:0.6100526847086322\n",
      "train loss:0.5345672093144352\n",
      "train loss:0.6609260983361489\n",
      "train loss:0.5044085004221972\n",
      "train loss:0.6115980239341353\n",
      "train loss:0.46057534563975416\n",
      "train loss:0.6074605138758822\n",
      "train loss:0.5276242206056303\n",
      "train loss:0.511241461594921\n",
      "train loss:0.5743162997616259\n",
      "train loss:0.6311814405936057\n",
      "train loss:0.5541525158351428\n",
      "train loss:0.5540418435430775\n",
      "train loss:0.4892940512171785\n",
      "train loss:0.6302298366084036\n",
      "train loss:0.6055901013238508\n",
      "train loss:0.49314060258388404\n",
      "train loss:0.5814851661527968\n",
      "train loss:0.5277878347148597\n",
      "train loss:0.5145016291652835\n",
      "train loss:0.48116279614648916\n",
      "train loss:0.5070031876839536\n",
      "train loss:0.45871461446649237\n",
      "train loss:0.6383746528396997\n",
      "train loss:0.5467779959285373\n",
      "train loss:0.6635240920221075\n",
      "train loss:0.7361876496563354\n",
      "train loss:0.5428305498686876\n",
      "train loss:0.6440540643265201\n",
      "train loss:0.6633210244412746\n",
      "train loss:0.6432223551849028\n",
      "train loss:0.6210825694505396\n",
      "train loss:0.5962321809323071\n",
      "train loss:0.6241775842855397\n",
      "train loss:0.6085472836779112\n",
      "train loss:0.6270857082719108\n",
      "train loss:0.6023842468305259\n",
      "train loss:0.548020195669747\n",
      "train loss:0.6509732230601631\n",
      "train loss:0.591079517143583\n",
      "train loss:0.5287409141065094\n",
      "train loss:0.804372287002586\n",
      "train loss:0.5293635866912603\n",
      "train loss:0.5918580929766659\n",
      "train loss:0.5293559298376238\n",
      "train loss:0.5367557938866074\n",
      "train loss:0.45862322525422294\n",
      "train loss:0.607959021251324\n",
      "train loss:0.4955571469795709\n",
      "train loss:0.5311488993270725\n",
      "train loss:0.7160771381331975\n",
      "train loss:0.5931335765821544\n",
      "train loss:0.6178322899851739\n",
      "train loss:0.527357955712226\n",
      "train loss:0.6203660076737473\n",
      "train loss:0.5055355072047182\n",
      "train loss:0.5549655226589727\n",
      "train loss:0.5550741289933593\n",
      "train loss:0.5544932911422484\n",
      "train loss:0.5629478545771478\n",
      "train loss:0.6473875539035987\n",
      "train loss:0.6020134899762212\n",
      "train loss:0.5708663484666681\n",
      "train loss:0.5767486010635978\n",
      "train loss:0.5943293694291354\n",
      "train loss:0.6755536808439938\n",
      "train loss:0.4943606632211114\n",
      "train loss:0.5297053074597382\n",
      "train loss:0.5815260767166701\n",
      "train loss:0.603528235066075\n",
      "train loss:0.5160565515897599\n",
      "train loss:0.5813373065361598\n",
      "train loss:0.6740421241413058\n",
      "train loss:0.5187723898658959\n",
      "train loss:0.5527409627141379\n",
      "train loss:0.6397372581586414\n",
      "train loss:0.6792056547807183\n",
      "train loss:0.5881411717599797\n",
      "train loss:0.5672033501414282\n",
      "train loss:0.5798016154626683\n",
      "train loss:0.6066113747673602\n",
      "train loss:0.5867945396965296\n",
      "train loss:0.5372158899485797\n",
      "train loss:0.5067415287650788\n",
      "train loss:0.5505840556721026\n",
      "train loss:0.45815603649872294\n",
      "train loss:0.6215671850753912\n",
      "train loss:0.6192089479864856\n",
      "train loss:0.5032263596932107\n",
      "train loss:0.5848423687258495\n",
      "train loss:0.7356689977472713\n",
      "train loss:0.5471735045954916\n",
      "train loss:0.5107073780931838\n",
      "train loss:0.6026693218011417\n",
      "train loss:0.5468530450166519\n",
      "train loss:0.5884702672847744\n",
      "train loss:0.6781721116946674\n",
      "train loss:0.6143615984132342\n",
      "train loss:0.600313854409259\n",
      "train loss:0.5875028668031248\n",
      "train loss:0.5993273225463489\n",
      "train loss:0.5911931184204433\n",
      "train loss:0.5828985456793271\n",
      "train loss:0.6296010974389816\n",
      "train loss:0.6431957743678954\n",
      "train loss:0.5963652849019332\n",
      "train loss:0.588908686172469\n",
      "train loss:0.5640582582571646\n",
      "train loss:0.5892042458484199\n",
      "train loss:0.4746241100965197\n",
      "train loss:0.5552213375585615\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5490196078431373\n"
     ]
    }
   ],
   "source": [
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 90, 'filter_size': 7, 'pad': 2, 'stride': 1},\n",
    "                        hidden_size=100, output_size=2, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=50,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36229d1f-f879-4ec2-9d6b-a62b90eb2d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
