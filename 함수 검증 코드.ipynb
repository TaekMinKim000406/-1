{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f7f74a-6ee5-4cbc-bd9b-724ab2422e61",
   "metadata": {},
   "source": [
    "교재의 코드를 가져와서 사용하고, 수정하면서 테스트 한 코드들입니다.\n",
    "수정 과정에서 일부 동작하지 않는 코드들도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b974e809-84fd-4889-be83-ec541f47a464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(current_dir)\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "image_folder_path = './G1020/Images/'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87032dee-9a8a-4fe6-9b6f-3fe5a0d98c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Hue images shape: (1020, 1, 28, 28)\n",
      "Labels shape: (1020,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('G1020.csv')\n",
    "image_files = df['imageID'].tolist()  # 이미지 파일 이름이 있는 열 이름\n",
    "labels = df['binaryLabels'].values  # 레이블이 있는 열 이름\n",
    "\n",
    "# 이미지 크기 설정\n",
    "target_size = (28, 28)\n",
    "\n",
    "# cv2로 이미지 읽고 전처리\n",
    "def load_and_extract_hue(image_path, target_size):\n",
    "    image = cv2.imread(image_path)  # 이미지 읽기 (기본 BGR 형식)\n",
    "    if image is None:\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return None\n",
    "    image = cv2.resize(image, target_size)  # 이미지 크기 조정\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)  # BGR을 HSV로 변환\n",
    "    hue_channel = hsv_image[:, :, 0]  # Hue 채널만 추출\n",
    "    hue_channel = hue_channel / 180.0  # Hue 값 정규화 (0~1 범위로 스케일링, Hue 범위는 0-179)\n",
    "    return hue_channel\n",
    "\n",
    "# 모든 이미지를 불러와서 리스트에 저장\n",
    "images = [load_and_extract_hue(image_folder_path + img_path, target_size) for img_path in image_files]\n",
    "images = np.array([img for img in images if img is not None])  # None 값 제거\n",
    "\n",
    "# 차원 추가하여 (1020, 1, 128, 128) 형태로 변환\n",
    "images = images[:, np.newaxis, :, :]\n",
    "\n",
    "print(f\"Processed Hue images shape: {images.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1efdcc55-91d4-43bb-9d9e-98a9095118b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.298728745272224\n",
      "=== epoch:1, train acc:0.71, test acc:0.65 ===\n",
      "train loss:2.2915672513341816\n",
      "train loss:2.278192103247589\n",
      "train loss:2.2611860287323093\n",
      "train loss:2.2288854519747305\n",
      "train loss:2.2021287573809127\n",
      "train loss:2.1320700536686896\n",
      "train loss:2.0693445921974463\n",
      "train loss:1.9693877910185258\n",
      "train loss:1.857416198242364\n",
      "train loss:1.726076208850762\n",
      "train loss:1.5490495904720016\n",
      "train loss:1.421714440860163\n",
      "train loss:1.1211308254016175\n",
      "train loss:1.132373071517696\n",
      "train loss:0.9153614355164637\n",
      "train loss:0.6540820043891454\n",
      "train loss:0.5305928676705182\n",
      "train loss:0.6673070916095627\n",
      "train loss:0.6826430451703822\n",
      "train loss:0.2117629012697873\n",
      "train loss:0.1147160907361311\n",
      "train loss:1.1995831342767007\n",
      "train loss:0.9977086903322581\n",
      "train loss:0.8064557628032339\n",
      "train loss:1.1979887524747976\n",
      "train loss:0.32632353421200316\n",
      "train loss:0.33527168362406956\n",
      "train loss:0.37286250112090313\n",
      "train loss:0.5474583312672959\n",
      "train loss:0.49378366604410945\n",
      "train loss:0.3879277411280187\n",
      "train loss:0.7163686906019266\n",
      "train loss:0.6327546101595782\n",
      "train loss:0.4479525523464779\n",
      "train loss:0.5229567637524802\n",
      "train loss:0.5157869603108425\n",
      "train loss:0.49718935098377115\n",
      "train loss:0.6176861681886783\n",
      "train loss:0.610854004284975\n",
      "train loss:0.9173457397989564\n",
      "train loss:0.4229643549588169\n",
      "train loss:0.5979716515530837\n",
      "train loss:0.4311899932629036\n",
      "train loss:0.5381804588711719\n",
      "train loss:0.7234500380688159\n",
      "train loss:0.5144834420612824\n",
      "train loss:0.5071537506395406\n",
      "train loss:0.8287890436492423\n",
      "train loss:0.5865873174027257\n",
      "train loss:0.6022560034946176\n",
      "train loss:0.5387173298209462\n",
      "train loss:0.5880921207330468\n",
      "train loss:0.328117223672915\n",
      "train loss:0.624264575534136\n",
      "train loss:0.4864379218999061\n",
      "train loss:0.6359819533236206\n",
      "train loss:0.7547463602153576\n",
      "train loss:0.6504676319647225\n",
      "train loss:0.26496348521749347\n",
      "train loss:0.5132796161201483\n",
      "train loss:1.0072263041759595\n",
      "train loss:0.702736545584892\n",
      "train loss:0.6031653936973055\n",
      "train loss:0.6040497944448544\n",
      "train loss:0.5257973880389161\n",
      "train loss:0.47263794520492936\n",
      "train loss:0.5263615338647072\n",
      "train loss:0.4307063972743374\n",
      "train loss:0.5124198261008991\n",
      "train loss:0.5322990253402495\n",
      "train loss:0.5137778889902053\n",
      "train loss:0.7955903930769922\n",
      "train loss:0.627124088653284\n",
      "train loss:0.6749957782752427\n",
      "train loss:0.7602354824880636\n",
      "train loss:0.6435906451315049\n",
      "train loss:0.7293237786189897\n",
      "train loss:0.6536312527877318\n",
      "train loss:0.5505820067810724\n",
      "train loss:0.5804792344389456\n",
      "train loss:0.6193506022555368\n",
      "train loss:0.7055919772680788\n",
      "train loss:0.5931374546129687\n",
      "train loss:0.6718503160492312\n",
      "train loss:0.68385320495633\n",
      "train loss:0.5913303256292415\n",
      "train loss:0.5719374972332283\n",
      "train loss:0.6190305501791976\n",
      "train loss:0.6133537860737992\n",
      "train loss:0.6722180861010727\n",
      "train loss:0.6093669230009737\n",
      "train loss:0.48363690841139134\n",
      "train loss:0.8340865100171582\n",
      "train loss:0.5229481105611915\n",
      "train loss:0.9170645880584696\n",
      "train loss:0.6928082006476445\n",
      "train loss:0.5368560874392645\n",
      "train loss:0.6833502406625295\n",
      "train loss:0.8686677745405689\n",
      "train loss:0.6188460192360153\n",
      "train loss:0.5919133846270579\n",
      "train loss:0.6472545473532254\n",
      "train loss:0.5640438117630524\n",
      "train loss:0.6737605572311065\n",
      "train loss:0.6578650621478245\n",
      "train loss:0.6635887947830441\n",
      "train loss:0.6783441399569888\n",
      "train loss:0.7890566661645481\n",
      "train loss:0.5582114836715935\n",
      "train loss:0.5445193670606828\n",
      "train loss:0.5333323922934918\n",
      "train loss:0.6886683470418451\n",
      "train loss:0.5439006558769428\n",
      "train loss:0.6225933857480428\n",
      "train loss:0.7884146376307936\n",
      "train loss:0.4213955552072358\n",
      "train loss:0.6226326510499754\n",
      "train loss:0.5137887422894285\n",
      "train loss:0.47655620676773636\n",
      "train loss:0.3760262128768262\n",
      "train loss:0.3767162466485242\n",
      "train loss:0.49741741156929714\n",
      "train loss:0.6277997108880753\n",
      "train loss:0.4823256252200965\n",
      "train loss:0.4823091069948246\n",
      "train loss:0.5156798958476304\n",
      "train loss:0.6449694311585391\n",
      "train loss:0.1651177921973575\n",
      "train loss:0.3005230426691866\n",
      "train loss:1.134972175106369\n",
      "train loss:0.35645115280273837\n",
      "train loss:0.6389293550471564\n",
      "train loss:0.7450865475391365\n",
      "train loss:0.7773670991659685\n",
      "train loss:0.5222919158636221\n",
      "train loss:0.7001526505564102\n",
      "train loss:0.6824898869486125\n",
      "train loss:0.5338268506596362\n",
      "train loss:0.5756026581654785\n",
      "train loss:0.5700514365134376\n",
      "train loss:0.6206721416505342\n",
      "train loss:0.5165260865476122\n",
      "train loss:0.48274954816958965\n",
      "train loss:0.5530657158246032\n",
      "train loss:0.5444222021451649\n",
      "train loss:0.2827035158006138\n",
      "train loss:0.6269875378577888\n",
      "train loss:0.8059606961664952\n",
      "train loss:0.5151492211614694\n",
      "train loss:0.7503845943195522\n",
      "train loss:0.6304304499533\n",
      "train loss:0.6531873553685477\n",
      "train loss:0.6421775853772824\n",
      "train loss:0.7777321855943946\n",
      "train loss:0.511633189920699\n",
      "train loss:0.3918613595884686\n",
      "train loss:0.5857204866958566\n",
      "train loss:0.6309410761439911\n",
      "train loss:0.4157609991557071\n",
      "train loss:0.32894818675388404\n",
      "train loss:0.7217077330961749\n",
      "train loss:0.6501730235830191\n",
      "train loss:0.5276997769825202\n",
      "train loss:0.4003687381498133\n",
      "train loss:0.5204719204737114\n",
      "train loss:0.5006341948175267\n",
      "train loss:0.3921683081905474\n",
      "train loss:0.7361088426013342\n",
      "train loss:0.6167296171122071\n",
      "train loss:0.9885927157691492\n",
      "train loss:0.5997674133990955\n",
      "train loss:0.5986870495318671\n",
      "train loss:0.29257079166978295\n",
      "train loss:0.6477081260078129\n",
      "train loss:0.5474356448534032\n",
      "train loss:0.6977383628184408\n",
      "train loss:0.6224239343949083\n",
      "train loss:0.5950357515050475\n",
      "train loss:0.6089172891228884\n",
      "train loss:0.534968018004157\n",
      "train loss:0.6928096534699676\n",
      "train loss:0.689548289659647\n",
      "train loss:0.519952110002589\n",
      "train loss:0.36551606098778117\n",
      "train loss:0.4229128757345636\n",
      "train loss:0.8216300501930887\n",
      "train loss:0.5354643523740494\n",
      "train loss:0.9598008474267636\n",
      "train loss:0.7154780629244817\n",
      "train loss:0.7735545505659024\n",
      "train loss:0.46989382166122456\n",
      "train loss:0.6076763209257735\n",
      "train loss:0.662060398262416\n",
      "train loss:0.4761818977048583\n",
      "train loss:0.5465645471112167\n",
      "train loss:0.4659695514627405\n",
      "train loss:0.5258021698352937\n",
      "train loss:0.6021059187154181\n",
      "train loss:0.5293616131029638\n",
      "train loss:0.7445269065821416\n",
      "train loss:0.7220661457837374\n",
      "train loss:0.6221444802697171\n",
      "train loss:0.6566705011626255\n",
      "train loss:0.3923524143849936\n",
      "train loss:0.5153813416911052\n",
      "train loss:0.7104725275545926\n",
      "train loss:0.7277835860318126\n",
      "train loss:0.29466652139829824\n",
      "train loss:0.6510423164406614\n",
      "train loss:0.3966216500766742\n",
      "train loss:0.6379376142046943\n",
      "train loss:0.7011419172818145\n",
      "train loss:0.397108584032909\n",
      "train loss:0.39465228673930347\n",
      "train loss:0.7537031239810087\n",
      "train loss:0.6948770834149705\n",
      "train loss:0.6278088770623442\n",
      "train loss:0.5120373675130226\n",
      "train loss:0.38516546739036206\n",
      "train loss:0.709307278701633\n",
      "train loss:0.6156665327409303\n",
      "train loss:0.36111319210196535\n",
      "train loss:0.8952896978241318\n",
      "train loss:0.5177317596071491\n",
      "train loss:0.6867898301210346\n",
      "train loss:0.5117160287366918\n",
      "train loss:0.526651257348517\n",
      "train loss:0.4239483336664053\n",
      "train loss:0.8098060714637322\n",
      "train loss:0.5298823606193908\n",
      "train loss:0.42464983853981736\n",
      "train loss:0.7167363800981412\n",
      "train loss:0.4171735097615622\n",
      "train loss:0.6393598196499388\n",
      "train loss:0.7190429151619238\n",
      "train loss:0.5288124942511522\n",
      "train loss:0.7291286468658249\n",
      "train loss:0.6094539034395311\n",
      "train loss:0.4315389854435877\n",
      "train loss:0.8070723302948617\n",
      "train loss:0.5258180262975791\n",
      "train loss:0.5052500255197149\n",
      "train loss:0.5211651286375347\n",
      "train loss:0.4224497225432298\n",
      "train loss:0.404972865738907\n",
      "train loss:0.5951627645695899\n",
      "train loss:0.3826801560629787\n",
      "train loss:0.7802225407936509\n",
      "train loss:0.6304191126708207\n",
      "train loss:0.7499464369066551\n",
      "train loss:0.5751312485103675\n",
      "train loss:0.6244621250192581\n",
      "train loss:0.5008493736284182\n",
      "train loss:0.8481611300669408\n",
      "train loss:0.6961192121987757\n",
      "train loss:0.599689444823779\n",
      "train loss:0.6837996418371863\n",
      "train loss:0.6055147281727704\n",
      "train loss:0.6374538492885284\n",
      "train loss:0.7527132873780654\n",
      "train loss:0.6341375041418107\n",
      "train loss:0.5230440182864655\n",
      "train loss:0.641850553897424\n",
      "train loss:0.6416810663556834\n",
      "train loss:0.717128894282648\n",
      "train loss:0.7110455308558594\n",
      "train loss:0.7234329762300735\n",
      "train loss:0.5992325125376406\n",
      "train loss:0.6346067659106354\n",
      "train loss:0.6811684046927701\n",
      "train loss:0.6271259322019003\n",
      "train loss:0.6758153120047632\n",
      "train loss:0.6180158632395575\n",
      "train loss:0.5589426586308005\n",
      "train loss:0.7540547587909165\n",
      "train loss:0.6034252006094651\n",
      "train loss:0.6964560404334541\n",
      "train loss:0.6277069650120535\n",
      "train loss:0.6791489793583039\n",
      "train loss:0.667120999677525\n",
      "train loss:0.47898372379272514\n",
      "train loss:0.6091660958948069\n",
      "train loss:0.6994126447657616\n",
      "train loss:0.7569993846267218\n",
      "train loss:0.5375932073130356\n",
      "train loss:0.4437900766972633\n",
      "train loss:0.5229241421627704\n",
      "train loss:0.594822343791078\n",
      "train loss:0.9037156928445107\n",
      "train loss:0.686243396798049\n",
      "train loss:0.5263802542619174\n",
      "train loss:0.43062059665203584\n",
      "train loss:0.4083450477888113\n",
      "train loss:0.5032786957079181\n",
      "train loss:0.5246340868682438\n",
      "train loss:0.7361192792279351\n",
      "train loss:0.645721176749922\n",
      "train loss:0.25846259324561605\n",
      "train loss:0.7564365910992487\n",
      "train loss:0.6273547727317419\n",
      "train loss:0.7458367374682147\n",
      "train loss:0.7112630748196865\n",
      "train loss:0.6020471385992913\n",
      "train loss:0.6411762020481511\n",
      "train loss:0.5545169571064673\n",
      "train loss:0.5253880932868383\n",
      "train loss:0.6268250591389816\n",
      "train loss:0.6032897014036498\n",
      "train loss:0.536879609983356\n",
      "train loss:0.7700473395647431\n",
      "train loss:0.6150475565228677\n",
      "train loss:0.46788704587155605\n",
      "train loss:0.4500360947499356\n",
      "train loss:0.7861713291496238\n",
      "train loss:0.9488851624177705\n",
      "train loss:0.6090881752756186\n",
      "train loss:0.789995143868134\n",
      "train loss:0.6848254358474546\n",
      "train loss:0.638090303584424\n",
      "train loss:0.5233990456703056\n",
      "train loss:0.6047473662152764\n",
      "train loss:0.5920414022069502\n",
      "train loss:0.5106177158222222\n",
      "train loss:0.7525826941490954\n",
      "train loss:0.5377272653697112\n",
      "train loss:0.36789413942030225\n",
      "train loss:0.5255644146897913\n",
      "train loss:0.27188134608132697\n",
      "train loss:0.6134406485612661\n",
      "train loss:0.4849903061464393\n",
      "train loss:0.6472620614363835\n",
      "train loss:0.6484165076275137\n",
      "train loss:0.818510754061912\n",
      "train loss:0.49768731775751174\n",
      "train loss:0.6782496235848623\n",
      "train loss:0.35411418602782074\n",
      "train loss:0.5128972459124117\n",
      "train loss:0.207908198686172\n",
      "train loss:0.3277572098337348\n",
      "train loss:0.8125920040707728\n",
      "train loss:0.7842225168855238\n",
      "train loss:0.5093982468090469\n",
      "train loss:0.34603692724847923\n",
      "train loss:0.3800198099725651\n",
      "train loss:0.3858256122273217\n",
      "train loss:0.4839729092926287\n",
      "train loss:0.5108710994895814\n",
      "train loss:0.3447394691173366\n",
      "train loss:0.5212860978382061\n",
      "train loss:0.5231264448510651\n",
      "train loss:0.5030885777279123\n",
      "train loss:0.8931163553830948\n",
      "train loss:0.481708988289269\n",
      "train loss:0.7510672511117017\n",
      "train loss:0.7445578971661068\n",
      "train loss:0.49888710500912115\n",
      "train loss:0.524659818251697\n",
      "train loss:0.8913198955211928\n",
      "train loss:0.5298013541867255\n",
      "train loss:0.6095152511070647\n",
      "train loss:0.46041121023720405\n",
      "train loss:0.745241235986222\n",
      "train loss:0.8854235391395322\n",
      "train loss:0.5868629890776\n",
      "train loss:0.6884800185932505\n",
      "train loss:0.5884675980198207\n",
      "train loss:0.6184666641636702\n",
      "train loss:0.6703182235110411\n",
      "train loss:0.5460083648203797\n",
      "train loss:0.6291693303162904\n",
      "train loss:0.6744884467062645\n",
      "train loss:0.7166914974366486\n",
      "train loss:0.6312702378003076\n",
      "train loss:0.5811504260055143\n",
      "train loss:0.5681719103966251\n",
      "train loss:0.6198496197697958\n",
      "train loss:0.6815206377139674\n",
      "train loss:0.5375268638950219\n",
      "train loss:0.4526503673088106\n",
      "train loss:0.9535865275870595\n",
      "train loss:0.4283603473494928\n",
      "train loss:0.8528669356545805\n",
      "train loss:0.6941345594224376\n",
      "train loss:0.5283049844319032\n",
      "train loss:0.6253087097919043\n",
      "train loss:0.6337776481641582\n",
      "train loss:0.5307324671173073\n",
      "train loss:0.6065659519511066\n",
      "train loss:0.6271670084621545\n",
      "train loss:0.4346765365336792\n",
      "train loss:0.6027475745906813\n",
      "train loss:0.5207301287451798\n",
      "train loss:0.5149798621340806\n",
      "train loss:0.9981060172833278\n",
      "train loss:0.6323668010004109\n",
      "train loss:0.5168455204299993\n",
      "train loss:0.5192423817254711\n",
      "train loss:0.7386394245352588\n",
      "train loss:0.4369592393320971\n",
      "train loss:0.4159622007300953\n",
      "train loss:0.7790225560015601\n",
      "train loss:0.42694136270110505\n",
      "train loss:0.31065341010698505\n",
      "train loss:0.7021510842778363\n",
      "train loss:0.502345831871167\n",
      "train loss:0.6237646413317661\n",
      "train loss:0.6543056739358949\n",
      "train loss:0.5024351763569909\n",
      "train loss:0.8366091319886749\n",
      "train loss:0.39638935059320474\n",
      "train loss:0.6105415129750676\n",
      "train loss:0.8026444426454761\n",
      "train loss:0.40987586306262375\n",
      "train loss:0.7244008342887498\n",
      "train loss:0.6151855385579623\n",
      "train loss:0.5174384923637302\n",
      "train loss:0.5265412007725674\n",
      "train loss:0.42803221532431196\n",
      "train loss:0.6009145993445303\n",
      "train loss:0.7253405699229709\n",
      "train loss:0.6223035552959055\n",
      "train loss:0.7161435168853786\n",
      "train loss:0.7856604636452648\n",
      "train loss:0.6935279399805684\n",
      "train loss:0.815402484630791\n",
      "train loss:0.4956754005715366\n",
      "train loss:0.677407347839419\n",
      "train loss:0.6700243331594793\n",
      "train loss:0.6780172145954003\n",
      "train loss:0.5423645277416655\n",
      "train loss:0.679110397795365\n",
      "train loss:0.6805275613281603\n",
      "train loss:0.5905310359863205\n",
      "train loss:0.5899377825097609\n",
      "train loss:0.6206072053426939\n",
      "train loss:0.5166588130998454\n",
      "train loss:0.5533973762866293\n",
      "train loss:0.5735174746611953\n",
      "train loss:0.4377984175117902\n",
      "train loss:0.3149451163016752\n",
      "train loss:0.6253293419669944\n",
      "train loss:0.5054676495933019\n",
      "train loss:0.6349355609801746\n",
      "train loss:0.20153895727712295\n",
      "train loss:0.5236910782853311\n",
      "train loss:0.664985446434668\n",
      "train loss:0.14828950626179221\n",
      "train loss:0.8746825830908291\n",
      "train loss:1.1016927571419302\n",
      "train loss:0.8518298340729894\n",
      "train loss:0.35298549390289724\n",
      "train loss:0.9142626732292785\n",
      "train loss:0.38462234154837377\n",
      "train loss:0.5995350995560108\n",
      "train loss:0.601329235380717\n",
      "train loss:0.7101178259106206\n",
      "train loss:0.7015738379412152\n",
      "train loss:0.5486979019014047\n",
      "train loss:0.7604404305926353\n",
      "train loss:0.5580558747439228\n",
      "train loss:0.6868791770643493\n",
      "train loss:0.5015931945991187\n",
      "train loss:0.5662864158544664\n",
      "train loss:0.7388935209718173\n",
      "train loss:0.4496885174232233\n",
      "train loss:0.7472833130919679\n",
      "train loss:0.6195373053407106\n",
      "train loss:0.49871709994878327\n",
      "train loss:0.5502650844430275\n",
      "train loss:0.4808669815233215\n",
      "train loss:0.6983683088793072\n",
      "train loss:0.6980243958206586\n",
      "train loss:0.43850464950755336\n",
      "train loss:0.4283396207945026\n",
      "train loss:0.42183617595612466\n",
      "train loss:0.40754973355194873\n",
      "train loss:0.767023551098745\n",
      "train loss:0.6629238394882846\n",
      "train loss:0.7345703481685493\n",
      "train loss:0.38015713264414386\n",
      "train loss:0.6366864344947927\n",
      "train loss:0.6240170986021918\n",
      "train loss:0.4949789348319683\n",
      "train loss:0.6386440091432951\n",
      "train loss:0.5077049306537732\n",
      "train loss:0.515545335202721\n",
      "train loss:0.6251310013268903\n",
      "train loss:0.3990603614573079\n",
      "train loss:0.6291978483090755\n",
      "train loss:0.6117994197641906\n",
      "train loss:0.6039606569206105\n",
      "train loss:0.7316064949325349\n",
      "train loss:0.4872423021114898\n",
      "train loss:0.7005177708267823\n",
      "train loss:0.7002117379337838\n",
      "train loss:0.7062470047593017\n",
      "train loss:0.5217000376546926\n",
      "train loss:0.5323717660952938\n",
      "train loss:0.6124364748529059\n",
      "train loss:0.6238564175057754\n",
      "train loss:0.45492529456034614\n",
      "train loss:0.9036107831475494\n",
      "train loss:0.6900713975356426\n",
      "train loss:0.7873633340943803\n",
      "train loss:0.5646654503772606\n",
      "train loss:0.7268950006102723\n",
      "train loss:0.6773925729406688\n",
      "train loss:0.6274979837674138\n",
      "train loss:0.6745343779880117\n",
      "train loss:0.7159343653324381\n",
      "train loss:0.605381512035137\n",
      "train loss:0.6479258356942034\n",
      "train loss:0.6350250221861482\n",
      "train loss:0.5901441730647751\n",
      "train loss:0.6357283689168759\n",
      "train loss:0.5423047523640775\n",
      "train loss:0.6762616143547837\n",
      "train loss:0.46127025984403724\n",
      "train loss:0.5517246499512728\n",
      "train loss:0.6909572148307503\n",
      "train loss:0.4611380988540018\n",
      "train loss:0.7162695779366866\n",
      "train loss:0.5129932677772395\n",
      "train loss:0.9001951419914818\n",
      "train loss:0.6944551313213339\n",
      "train loss:0.31425897872954994\n",
      "train loss:0.5100134680499743\n",
      "train loss:0.6103432607226014\n",
      "train loss:0.5004264724078713\n",
      "train loss:0.72040302129569\n",
      "train loss:0.8412324533380721\n",
      "train loss:0.6909556355346278\n",
      "train loss:0.4103555282064633\n",
      "train loss:0.7442559716082254\n",
      "train loss:0.7993816243661241\n",
      "train loss:0.6450360970443343\n",
      "train loss:0.6983005551159882\n",
      "train loss:0.5278989184000477\n",
      "train loss:0.5410137825949985\n",
      "train loss:0.608804321804291\n",
      "train loss:0.6064435506582024\n",
      "train loss:0.6188780707238755\n",
      "train loss:0.616259977726654\n",
      "train loss:0.4668624500593504\n",
      "train loss:0.6159668297820322\n",
      "train loss:0.4716325828056024\n",
      "train loss:0.6000182373743718\n",
      "train loss:0.6213530970018334\n",
      "train loss:0.6029798001259674\n",
      "train loss:0.5158360176931924\n",
      "train loss:0.6046514073909186\n",
      "train loss:0.5244116247719368\n",
      "train loss:0.6065357712848911\n",
      "train loss:0.5145403113247914\n",
      "train loss:0.5998866272728065\n",
      "train loss:0.509327027786675\n",
      "train loss:0.7248296421016628\n",
      "train loss:0.49195245978006075\n",
      "train loss:0.49842934592142313\n",
      "train loss:0.9945430631996383\n",
      "train loss:0.6235300822800179\n",
      "train loss:0.7347953203069395\n",
      "train loss:0.7125071938697809\n",
      "train loss:0.5373531911275493\n",
      "train loss:0.6118105414777228\n",
      "train loss:0.618234532494135\n",
      "train loss:0.604982160200845\n",
      "train loss:0.5518904501129502\n",
      "train loss:0.6080054391890768\n",
      "train loss:0.6203268577105767\n",
      "train loss:0.3998536593814407\n",
      "train loss:0.6826660988099081\n",
      "train loss:0.6648328560030345\n",
      "train loss:0.6759704478675321\n",
      "train loss:0.6251757505137319\n",
      "train loss:0.6823006449578605\n",
      "train loss:0.4618084152785881\n",
      "train loss:0.4755654784968337\n",
      "train loss:0.5341930353506965\n",
      "train loss:0.5103671573739079\n",
      "train loss:0.5250802904265898\n",
      "train loss:0.7937831621730125\n",
      "train loss:0.4144612014369876\n",
      "train loss:0.8017347099239689\n",
      "train loss:0.8177045506312014\n",
      "train loss:0.6425838807805119\n",
      "train loss:0.4144846736087871\n",
      "train loss:0.7105714694000648\n",
      "train loss:0.7061793833842105\n",
      "train loss:0.6098809024266865\n",
      "train loss:0.5128811999582377\n",
      "train loss:0.7808851197304574\n",
      "train loss:0.6194337988604104\n",
      "train loss:0.6864266206930766\n",
      "train loss:0.5393957420871246\n",
      "train loss:0.5315487875683357\n",
      "train loss:0.6865098319840275\n",
      "train loss:0.6062652587909306\n",
      "train loss:0.5350609993900524\n",
      "train loss:0.6026963330665894\n",
      "train loss:0.6174819419441454\n",
      "train loss:0.6054049257903727\n",
      "train loss:0.4561702769621829\n",
      "train loss:0.6972836846317515\n",
      "train loss:0.532616850155229\n",
      "train loss:0.527815478016045\n",
      "train loss:0.7092037753332046\n",
      "train loss:0.9490282243389544\n",
      "train loss:0.7858055895580094\n",
      "train loss:0.4682757766496077\n",
      "train loss:0.452095762173932\n",
      "train loss:0.536820278341483\n",
      "train loss:0.6090810275385649\n",
      "train loss:0.5388440131068717\n",
      "train loss:0.6002078418256185\n",
      "train loss:0.7900535695800213\n",
      "train loss:0.5182566344637998\n",
      "train loss:0.619826670845754\n",
      "train loss:0.5258302625598191\n",
      "train loss:0.6215354795438449\n",
      "train loss:0.4380814500990226\n",
      "train loss:0.42042625246769677\n",
      "train loss:0.5130552685866616\n",
      "train loss:0.7444469066970687\n",
      "train loss:0.7285156267104405\n",
      "train loss:0.6127635473163595\n",
      "train loss:0.4147757384829389\n",
      "train loss:0.39125308211843746\n",
      "train loss:0.6420112696408826\n",
      "train loss:0.7258154635306561\n",
      "train loss:0.6231505886943978\n",
      "train loss:0.6293635898828615\n",
      "train loss:0.6198891538378815\n",
      "train loss:0.7262483619248077\n",
      "train loss:0.6148469111500015\n",
      "train loss:0.5183454747564358\n",
      "train loss:0.6188682870473791\n",
      "train loss:0.5240542456081492\n",
      "train loss:0.6889218405518504\n",
      "train loss:0.3374052369824875\n",
      "train loss:0.7855787079884377\n",
      "train loss:0.5214798865309581\n",
      "train loss:0.6018710945580543\n",
      "train loss:0.6064911155908963\n",
      "train loss:0.6039025971958334\n",
      "train loss:0.6115515201491929\n",
      "train loss:0.33786266600810355\n",
      "train loss:0.6099634238599176\n",
      "train loss:0.606158632839399\n",
      "train loss:0.7996586358175743\n",
      "train loss:0.5988431223474902\n",
      "train loss:0.6148146477977772\n",
      "train loss:0.424229991620604\n",
      "train loss:0.7041114248215139\n",
      "train loss:0.4233421670162093\n",
      "train loss:0.616891499032038\n",
      "train loss:0.6991353030299992\n",
      "train loss:0.7055159112631243\n",
      "train loss:0.5107884066842664\n",
      "train loss:0.5098047880604597\n",
      "train loss:0.7865632536431336\n",
      "train loss:0.4206224514046669\n",
      "train loss:0.4983308599997761\n",
      "train loss:0.6060321310355737\n",
      "train loss:0.4310566088399822\n",
      "train loss:0.5062384588550708\n",
      "train loss:0.49547773509351567\n",
      "train loss:0.7377792775509169\n",
      "train loss:0.5142414055587731\n",
      "train loss:0.6248684333098631\n",
      "train loss:0.38093871722126765\n",
      "train loss:0.9518690452218237\n",
      "train loss:0.6151946441520305\n",
      "train loss:0.2967530166506468\n",
      "train loss:0.5171964344374012\n",
      "train loss:0.5010093515158811\n",
      "train loss:0.4977377892445329\n",
      "train loss:0.49375385859021614\n",
      "train loss:0.38496742657910404\n",
      "train loss:0.38675022608793297\n",
      "train loss:0.38841782141939085\n",
      "train loss:0.9206843190357162\n",
      "train loss:1.0236564225668805\n",
      "train loss:0.6278057879832676\n",
      "train loss:0.5157785082957729\n",
      "train loss:0.26781273141906575\n",
      "train loss:0.6232857438024896\n",
      "train loss:0.5013362364613361\n",
      "train loss:0.6262878119871371\n",
      "train loss:0.5192543967975356\n",
      "train loss:0.8294283526982806\n",
      "train loss:0.6208045214627055\n",
      "train loss:0.5192893194437913\n",
      "train loss:0.6921682271169891\n",
      "train loss:0.34921691117116527\n",
      "train loss:0.5304800688680654\n",
      "train loss:0.6035022705827976\n",
      "train loss:0.4351662037125343\n",
      "train loss:0.42848045619417724\n",
      "train loss:0.4054404374299855\n",
      "train loss:0.7270131227093455\n",
      "train loss:0.6202031249107645\n",
      "train loss:0.7314410612246494\n",
      "train loss:0.8224803001813308\n",
      "train loss:0.6170233070375519\n",
      "train loss:0.5125176148357656\n",
      "train loss:0.8060693294272661\n",
      "train loss:0.5227265415534879\n",
      "train loss:0.5254647403410688\n",
      "train loss:0.7872607363823982\n",
      "train loss:0.5321670432300631\n",
      "train loss:0.5383675711497385\n",
      "train loss:0.4406674010660975\n",
      "train loss:0.6852279826123806\n",
      "train loss:0.6183869788065082\n",
      "train loss:0.6977136757118692\n",
      "train loss:0.6035538700974664\n",
      "train loss:0.6816370778509758\n",
      "train loss:0.681511231089281\n",
      "train loss:0.7537512413717979\n",
      "train loss:0.5464281558037685\n",
      "train loss:0.5363123087484387\n",
      "train loss:0.6144666922476943\n",
      "train loss:0.4673032452073338\n",
      "train loss:0.7562379052135675\n",
      "train loss:0.6046898525326619\n",
      "train loss:0.39071600203016493\n",
      "train loss:0.44708121864575856\n",
      "train loss:1.020973967293478\n",
      "train loss:0.6930099496768165\n",
      "train loss:0.6094509849185616\n",
      "train loss:0.763161672767174\n",
      "train loss:0.5310406019074574\n",
      "train loss:0.6134971633814065\n",
      "train loss:0.5404711967158936\n",
      "train loss:0.5240926597136878\n",
      "train loss:0.4545000857513764\n",
      "train loss:0.5944848781104782\n",
      "train loss:0.5296032083296366\n",
      "train loss:0.4232226987287741\n",
      "train loss:0.30701453804883594\n",
      "train loss:0.3892719801586687\n",
      "train loss:0.49901896481876173\n",
      "train loss:0.5126172596254749\n",
      "train loss:0.6500027379961661\n",
      "train loss:0.1837682107022765\n",
      "train loss:0.6849183569498407\n",
      "train loss:0.9966465955342498\n",
      "train loss:0.6951137787943276\n",
      "train loss:0.6547940846656888\n",
      "train loss:0.6311368615766952\n",
      "train loss:0.7539841023266896\n",
      "train loss:0.6259543726121167\n",
      "train loss:0.39589389547970705\n",
      "train loss:0.5027277450403542\n",
      "train loss:0.4076288036364244\n",
      "train loss:0.5016628666122507\n",
      "train loss:0.7125615233833876\n",
      "train loss:0.5128077614752261\n",
      "train loss:0.8000172896545801\n",
      "train loss:0.5081539890995586\n",
      "train loss:0.5173344088864986\n",
      "train loss:0.5256402790871906\n",
      "train loss:0.6031627993693688\n",
      "train loss:0.517472946459017\n",
      "train loss:0.8674104196581544\n",
      "train loss:0.7744548502438252\n",
      "train loss:0.6125129298504102\n",
      "train loss:0.6890401058957834\n",
      "train loss:0.6827660964375488\n",
      "train loss:0.6779861619500936\n",
      "train loss:0.5522166864563679\n",
      "train loss:0.735659740906301\n",
      "train loss:0.6816758153069122\n",
      "train loss:0.5069282148739721\n",
      "train loss:0.6760978396730309\n",
      "train loss:0.5109575186010689\n",
      "train loss:0.6157148177959463\n",
      "train loss:0.6164638615301203\n",
      "train loss:0.6702270661338322\n",
      "train loss:0.4916433259685949\n",
      "train loss:0.48177039237807967\n",
      "train loss:0.5430968957423213\n",
      "train loss:0.6159924228226178\n",
      "train loss:0.4446993866432581\n",
      "train loss:0.6192314057249001\n",
      "train loss:0.5993916207077657\n",
      "train loss:0.4089819902290028\n",
      "train loss:0.5068917718127386\n",
      "train loss:0.8613204563469727\n",
      "train loss:0.76865425304535\n",
      "train loss:0.6219459951096411\n",
      "train loss:0.944462255072725\n",
      "train loss:0.5087828211940641\n",
      "train loss:0.634451030977009\n",
      "train loss:0.4104001015141529\n",
      "train loss:0.8046379971113347\n",
      "train loss:0.7838966456072216\n",
      "train loss:0.6945334578765909\n",
      "train loss:0.4598117602014648\n",
      "train loss:0.6069876492191334\n",
      "train loss:0.5443890459195567\n",
      "train loss:0.46214986051219925\n",
      "train loss:0.541021143893067\n",
      "train loss:0.6172274945568915\n",
      "train loss:0.523603374568146\n",
      "train loss:0.5364886534559137\n",
      "train loss:0.5285966768957935\n",
      "train loss:0.6224793557487641\n",
      "train loss:0.6189942930246135\n",
      "train loss:0.7963687689583495\n",
      "train loss:0.5203187182872808\n",
      "train loss:0.6052937393573619\n",
      "train loss:0.5268529317977341\n",
      "train loss:0.5313449129919399\n",
      "train loss:0.6072272284270179\n",
      "train loss:0.6051895743285656\n",
      "train loss:0.6074427438308859\n",
      "train loss:0.6137950142541935\n",
      "train loss:0.5114905328667888\n",
      "train loss:0.7112868295638433\n",
      "train loss:0.6111921319075551\n",
      "train loss:0.40274834550990357\n",
      "train loss:0.39057357184490754\n",
      "train loss:0.2823078684999639\n",
      "train loss:0.3755703551295679\n",
      "train loss:0.6234976779707206\n",
      "train loss:0.3485972335408026\n",
      "train loss:0.3335079520718094\n",
      "train loss:0.5187906159570634\n",
      "train loss:0.16215319633408445\n",
      "train loss:0.8640359072727973\n",
      "train loss:0.8552172826207294\n",
      "train loss:0.5227846909093418\n",
      "train loss:0.33977757415850623\n",
      "train loss:0.6545287064839576\n",
      "train loss:0.4902816260289236\n",
      "train loss:0.8101480205039865\n",
      "train loss:0.9057981454672562\n",
      "train loss:0.6349494987998596\n",
      "train loss:0.6138659211959344\n",
      "train loss:0.5084670445329416\n",
      "train loss:0.8631463680705306\n",
      "train loss:0.6858485497317586\n",
      "train loss:0.6195642920284735\n",
      "train loss:0.6727643071258598\n",
      "train loss:0.731679719420418\n",
      "train loss:0.6283003162533892\n",
      "train loss:0.6831228866801536\n",
      "train loss:0.5568872095509263\n",
      "train loss:0.5987023503684394\n",
      "train loss:0.6370907548404064\n",
      "train loss:0.5595976301306375\n",
      "train loss:0.599118239315823\n",
      "train loss:0.5866091282559158\n",
      "train loss:0.6349564013505893\n",
      "train loss:0.6240158656771789\n",
      "train loss:0.5776120305444881\n",
      "train loss:0.6179705267882698\n",
      "train loss:0.5530833641467183\n",
      "train loss:0.608856813636139\n",
      "train loss:0.47716708923960083\n",
      "train loss:0.4630850755185354\n",
      "train loss:0.8315729644882696\n",
      "train loss:0.5413716475508592\n",
      "train loss:0.6148404847128275\n",
      "train loss:0.6857246404732711\n",
      "train loss:0.4196685004075179\n",
      "train loss:0.2790888153265649\n",
      "train loss:0.6361667477542902\n",
      "train loss:0.8482501313895401\n",
      "train loss:0.6237347750792875\n",
      "train loss:0.25378110077222427\n",
      "train loss:0.5223097930539332\n",
      "train loss:0.4897748881731935\n",
      "train loss:0.3454722852713165\n",
      "train loss:0.5350947756627027\n",
      "train loss:0.7824116858250847\n",
      "train loss:0.3406464839462634\n",
      "train loss:0.6815136962720876\n",
      "train loss:0.659873329536471\n",
      "train loss:0.9200533251495588\n",
      "train loss:0.8669773170931144\n",
      "train loss:0.6058713046039006\n",
      "train loss:0.401143799861127\n",
      "train loss:0.5008754464773219\n",
      "train loss:0.704292629218855\n",
      "train loss:0.7904242994157434\n",
      "train loss:0.5336684064826138\n",
      "train loss:0.5394355440370833\n",
      "train loss:0.5455958375424761\n",
      "train loss:0.5350904574865931\n",
      "train loss:0.5471062806702003\n",
      "train loss:0.537282761360565\n",
      "train loss:0.5381349889642058\n",
      "train loss:0.46476029237101074\n",
      "train loss:0.8263928042930255\n",
      "train loss:0.7784720198618549\n",
      "train loss:0.6086080284897706\n",
      "train loss:0.7653213594861745\n",
      "train loss:0.8259165256041298\n",
      "train loss:0.41175628805513753\n",
      "train loss:0.5489986986869418\n",
      "train loss:0.6084769204172692\n",
      "train loss:0.6210577464230305\n",
      "train loss:0.546300097754975\n",
      "train loss:0.6034928171937517\n",
      "train loss:0.7619129003322715\n",
      "train loss:0.6166843154085091\n",
      "train loss:0.5314988833083591\n",
      "train loss:0.531800890330031\n",
      "train loss:0.7593622534242561\n",
      "train loss:0.8985304837487472\n",
      "train loss:0.4732156505519606\n",
      "train loss:0.47077628455377624\n",
      "train loss:0.614567986650284\n",
      "train loss:0.8319953975611337\n",
      "train loss:0.6807560167745335\n",
      "train loss:0.46271649037281615\n",
      "train loss:0.6068355251599464\n",
      "train loss:0.6746576772236303\n",
      "train loss:0.760015786827567\n",
      "train loss:0.6161965871685495\n",
      "train loss:0.6074422500927147\n",
      "train loss:0.47174700419116844\n",
      "train loss:0.46141220592966814\n",
      "train loss:0.6197143531229297\n",
      "train loss:0.6842723475025837\n",
      "train loss:0.6993563172213884\n",
      "train loss:0.44191833901101135\n",
      "train loss:0.6888049513933759\n",
      "train loss:0.5337078310725413\n",
      "train loss:0.8697533575882239\n",
      "train loss:0.5276188642488615\n",
      "train loss:0.8664164321348995\n",
      "train loss:0.7762368830678232\n",
      "train loss:0.6098388263362235\n",
      "train loss:0.6149518721791247\n",
      "train loss:0.472668611739794\n",
      "train loss:0.6093368451618596\n",
      "train loss:0.4663945516138163\n",
      "train loss:0.47399666945632396\n",
      "train loss:0.6058273060486887\n",
      "train loss:0.525276570158284\n",
      "train loss:0.44515340984792634\n",
      "train loss:0.6248319156295448\n",
      "train loss:0.5253492884702156\n",
      "train loss:0.5103202637839624\n",
      "train loss:0.5020359900282585\n",
      "train loss:0.39184056868753564\n",
      "train loss:0.6199880333936705\n",
      "train loss:0.76222397056988\n",
      "train loss:0.4884243136334536\n",
      "train loss:0.6196454421973441\n",
      "train loss:0.6322855038985614\n",
      "train loss:0.6457130894387837\n",
      "train loss:0.742398929893951\n",
      "train loss:0.52476888670676\n",
      "train loss:0.8344911261083926\n",
      "train loss:0.8460333550512236\n",
      "train loss:0.5188653018030005\n",
      "train loss:0.809076198892407\n",
      "train loss:0.6773924361577659\n",
      "train loss:0.5340959865475228\n",
      "train loss:0.7465744062062021\n",
      "train loss:0.6213219164226944\n",
      "train loss:0.4934868637647716\n",
      "train loss:0.5628003464716632\n",
      "train loss:0.673163551430816\n",
      "train loss:0.564910672135528\n",
      "train loss:0.5093143625421273\n",
      "train loss:0.4986511782321609\n",
      "train loss:0.6813137493423607\n",
      "train loss:0.6812213193579442\n",
      "train loss:0.5509500356721743\n",
      "train loss:0.5431801191277554\n",
      "train loss:0.47341346401097734\n",
      "train loss:0.4682726277476747\n",
      "train loss:0.5262977833569438\n",
      "train loss:0.8622075458845175\n",
      "train loss:0.7058384225674212\n",
      "train loss:0.6974026144659489\n",
      "train loss:0.6034745888420419\n",
      "train loss:0.5127760294812636\n",
      "train loss:0.5208818164939255\n",
      "train loss:0.5152542142812292\n",
      "train loss:0.5238307238912279\n",
      "train loss:0.7005352791552555\n",
      "train loss:0.7195563137289254\n",
      "train loss:0.6271304907939513\n",
      "train loss:0.7121260180927199\n",
      "train loss:0.6172188985733148\n",
      "train loss:0.6038309418914124\n",
      "train loss:0.6064666056403646\n",
      "train loss:0.5238241452781716\n",
      "train loss:0.7927833325471263\n",
      "train loss:0.7842494005338823\n",
      "train loss:0.7668234226651724\n",
      "train loss:0.612219983226024\n",
      "train loss:0.6900726233618216\n",
      "train loss:0.6118535375206642\n",
      "train loss:0.8043812809169184\n",
      "train loss:0.43911042703419023\n",
      "train loss:0.6186424089378899\n",
      "train loss:0.6766194159198976\n",
      "train loss:0.6757501636507255\n",
      "train loss:0.7867101995204928\n",
      "train loss:0.5188233454462179\n",
      "train loss:0.6243042474365101\n",
      "train loss:0.5217251897196411\n",
      "train loss:0.568277218857462\n",
      "train loss:0.5665933573091323\n",
      "train loss:0.5028970777351474\n",
      "train loss:0.4897226120132898\n",
      "train loss:0.6799440191626013\n",
      "train loss:0.5483407541788048\n",
      "train loss:0.37286982091190624\n",
      "train loss:0.5300507855427157\n",
      "train loss:0.512704757771554\n",
      "train loss:0.6122324656367764\n",
      "train loss:1.0238745305433385\n",
      "train loss:0.7258795813721508\n",
      "train loss:0.42089306596631226\n",
      "train loss:0.5267255319957844\n",
      "train loss:0.7189720895630963\n",
      "train loss:0.2899821402826025\n",
      "train loss:0.6202431625343514\n",
      "train loss:0.6180623610638152\n",
      "train loss:0.5124943353253826\n",
      "train loss:0.6127678434952653\n",
      "train loss:0.5069184587857493\n",
      "train loss:0.49181254652347023\n",
      "train loss:0.2512401321905418\n",
      "train loss:0.5055960354987625\n",
      "train loss:0.7595565667567695\n",
      "train loss:0.4963861295989556\n",
      "train loss:0.626874626292323\n",
      "train loss:0.6494337056701658\n",
      "train loss:0.2338248846240394\n",
      "train loss:0.3678291943436546\n",
      "train loss:0.647038058266866\n",
      "train loss:0.21852605409668815\n",
      "train loss:0.6518997057993557\n",
      "train loss:0.48983215379348755\n",
      "train loss:0.5143688447598639\n",
      "train loss:0.6522327955605773\n",
      "train loss:0.7819393451029657\n",
      "train loss:0.508233633852324\n",
      "train loss:1.0175702575176504\n",
      "train loss:0.7353467863138228\n",
      "train loss:0.5118088423166149\n",
      "train loss:0.7231606622802269\n",
      "train loss:0.4261026499055959\n",
      "train loss:0.6147077828224679\n",
      "train loss:0.7724025504081639\n",
      "train loss:0.45733333115433644\n",
      "train loss:0.6846572598867705\n",
      "train loss:0.6857346326541498\n",
      "train loss:0.6158757582283547\n",
      "train loss:0.4890611169481457\n",
      "train loss:0.6733746960444883\n",
      "train loss:0.5593563651184864\n",
      "train loss:0.6732439420129273\n",
      "train loss:0.7332654666313732\n",
      "train loss:0.6841550053552405\n",
      "train loss:0.5047804756490691\n",
      "train loss:0.4947874276297578\n",
      "train loss:0.552840307287528\n",
      "train loss:0.6202247597872305\n",
      "train loss:0.5476760728511259\n",
      "train loss:0.6771403776522088\n",
      "train loss:0.469654531364184\n",
      "train loss:0.89930145055466\n",
      "train loss:0.5384445380934741\n",
      "train loss:0.533785254792633\n",
      "train loss:0.6804785341815165\n",
      "train loss:0.5266657693025082\n",
      "train loss:0.5285886651744016\n",
      "train loss:0.5336226840530325\n",
      "train loss:0.4267811396723508\n",
      "train loss:0.5206898147806587\n",
      "train loss:0.2970916615369838\n",
      "train loss:0.49582582454747215\n",
      "train loss:0.7692122970724331\n",
      "train loss:0.6304676316053885\n",
      "train loss:0.7557513572125304\n",
      "train loss:0.49591611083855947\n",
      "train loss:0.7515002994122388\n",
      "train loss:0.507086225328462\n",
      "train loss:0.49523564616932714\n",
      "train loss:0.5044304945372996\n",
      "train loss:0.9672718684173726\n",
      "train loss:0.6200255960214719\n",
      "train loss:0.5000711743444815\n",
      "train loss:0.7190377858492452\n",
      "train loss:0.6130412927968518\n",
      "train loss:0.717096351347779\n",
      "train loss:0.5274745466586204\n",
      "train loss:0.5279482737652489\n",
      "train loss:0.5235877135051936\n",
      "train loss:0.5382730688551682\n",
      "train loss:0.3535823127219248\n",
      "train loss:0.6132779899349994\n",
      "train loss:0.5257594079341149\n",
      "train loss:0.606132142238581\n",
      "train loss:0.6082841634885326\n",
      "train loss:0.6019176854533029\n",
      "train loss:0.7889379685386804\n",
      "train loss:0.6183602764927852\n",
      "train loss:0.5155181641497807\n",
      "train loss:0.6071033057037825\n",
      "train loss:0.7879494303083977\n",
      "train loss:0.5198389068763019\n",
      "train loss:0.6162086629712276\n",
      "train loss:0.3328002861089268\n",
      "train loss:0.5175554451896416\n",
      "train loss:0.5063763331856259\n",
      "train loss:0.4127588777012523\n",
      "train loss:0.830269040042757\n",
      "train loss:0.608818663898864\n",
      "train loss:0.7194089174224436\n",
      "train loss:0.6066213757133994\n",
      "train loss:0.5153420732303913\n",
      "train loss:0.6076580048292747\n",
      "train loss:0.7176672604863827\n",
      "train loss:0.3079014025907098\n",
      "train loss:0.29308633828738173\n",
      "train loss:0.39772608672688553\n",
      "train loss:0.5035235504375573\n",
      "train loss:0.6296493445471162\n",
      "train loss:0.509276065346945\n",
      "train loss:0.7801839584557354\n",
      "train loss:0.3671772756269492\n",
      "train loss:0.6252695968223655\n",
      "train loss:0.6204342163388966\n",
      "train loss:0.5129769134985052\n",
      "train loss:0.3757736932656438\n",
      "train loss:0.6338151686483353\n",
      "train loss:0.9996672090163129\n",
      "train loss:0.7307285004346951\n",
      "train loss:0.5034117199500201\n",
      "train loss:0.6118036462638184\n",
      "train loss:0.5038434199722661\n",
      "train loss:0.5216454675759543\n",
      "train loss:0.8107051388610772\n",
      "train loss:0.4297194203379826\n",
      "train loss:0.6077483043071423\n",
      "train loss:0.521939103887816\n",
      "train loss:0.43455614342150695\n",
      "train loss:0.7030539595072998\n",
      "train loss:0.5200110459256868\n",
      "train loss:0.5240066216336594\n",
      "train loss:0.703277008958644\n",
      "train loss:0.6060891036656237\n",
      "train loss:0.4316728980352975\n",
      "train loss:0.5146873059442787\n",
      "train loss:0.8676700633684277\n",
      "train loss:0.5251942033346841\n",
      "train loss:0.6059295002762797\n",
      "train loss:0.5178566436221025\n",
      "train loss:0.6186793399915875\n",
      "train loss:0.517129055281955\n",
      "train loss:0.6023275221554857\n",
      "train loss:0.5180939751672319\n",
      "train loss:0.30732170068113274\n",
      "train loss:0.621699923450254\n",
      "train loss:0.6188424994483739\n",
      "train loss:0.4998299084360453\n",
      "train loss:0.7303114470739707\n",
      "train loss:0.8494190436175456\n",
      "train loss:0.7132473682518841\n",
      "train loss:0.5229903268057604\n",
      "train loss:0.5091413259920776\n",
      "train loss:0.8155963290526792\n",
      "train loss:0.6014483949887643\n",
      "train loss:0.5246090104143553\n",
      "train loss:0.6143242298836513\n",
      "train loss:0.5285868423295473\n",
      "train loss:0.7789880093777066\n",
      "train loss:0.6920861267409257\n",
      "train loss:0.451014544503997\n",
      "train loss:0.7666083449339737\n",
      "train loss:0.45950939376461764\n",
      "train loss:0.6840208530294297\n",
      "train loss:0.46204742317921754\n",
      "train loss:0.53620808278722\n",
      "train loss:0.3751188489778022\n",
      "train loss:0.5254251457741241\n",
      "train loss:0.6078178039376773\n",
      "train loss:0.6997975662548469\n",
      "train loss:0.4163014113768839\n",
      "train loss:0.7086818527205001\n",
      "train loss:0.6121625132449402\n",
      "train loss:0.6016324010699531\n",
      "train loss:0.5115548071184959\n",
      "train loss:0.5077222772273485\n",
      "train loss:0.614507316334769\n",
      "train loss:0.28211003828591624\n",
      "train loss:0.847706897340528\n",
      "train loss:0.5123534349502078\n",
      "train loss:0.6216058104623373\n",
      "train loss:0.5069223836883039\n",
      "train loss:0.6161554366484159\n",
      "train loss:0.500201946232777\n",
      "train loss:0.3831257278165121\n",
      "train loss:0.4987658766723671\n",
      "train loss:0.4989805255047008\n",
      "train loss:0.368487722467512\n",
      "train loss:0.8901213314697468\n",
      "train loss:0.49528216548219317\n",
      "train loss:0.8706010430954165\n",
      "train loss:0.2533418912307833\n",
      "train loss:0.7693530878186274\n",
      "train loss:0.8528963040430518\n",
      "train loss:0.6141932968629412\n",
      "train loss:0.5030088551377113\n",
      "train loss:0.608101002089196\n",
      "train loss:0.6144034529168877\n",
      "train loss:0.5188125903163457\n",
      "train loss:0.6887754462032171\n",
      "train loss:0.43523849446490825\n",
      "train loss:0.612470432754604\n",
      "train loss:0.43659559716053764\n",
      "train loss:0.526129075805983\n",
      "train loss:0.7838775705267228\n",
      "train loss:0.526080839754522\n",
      "train loss:0.3426404690428387\n",
      "train loss:0.6160040357235487\n",
      "train loss:0.7045089457808641\n",
      "train loss:0.7096110878813444\n",
      "train loss:0.518310855493646\n",
      "train loss:0.6100774151318438\n",
      "train loss:0.5189731497224704\n",
      "train loss:0.7959256369171723\n",
      "train loss:0.424844744781715\n",
      "train loss:0.5126606460384173\n",
      "train loss:0.803949106225556\n",
      "train loss:0.7993394406030686\n",
      "train loss:0.42488720418608905\n",
      "train loss:0.6114994473202917\n",
      "train loss:0.7067443751258512\n",
      "train loss:0.6925513851331285\n",
      "train loss:0.6940594691781117\n",
      "train loss:0.4471139015775523\n",
      "train loss:0.6107912199613157\n",
      "train loss:0.6125151814911782\n",
      "train loss:0.6088926391280662\n",
      "train loss:0.6165988883961957\n",
      "train loss:0.4521434807995669\n",
      "train loss:0.5309255408909251\n",
      "train loss:0.6912694347020952\n",
      "train loss:0.5256903629524831\n",
      "train loss:0.7831808481279054\n",
      "train loss:0.5271238564381198\n",
      "train loss:0.6983188381404615\n",
      "train loss:0.6888893552184837\n",
      "train loss:0.6952839971696427\n",
      "train loss:0.6181531310858389\n",
      "train loss:0.6906006536196003\n",
      "train loss:0.45539701991962483\n",
      "train loss:0.4538789720805033\n",
      "train loss:0.5297918884205715\n",
      "train loss:0.6074803284108068\n",
      "train loss:0.6117146997563545\n",
      "train loss:0.7819469642377561\n",
      "train loss:0.5189442161010237\n",
      "train loss:0.7792412948907328\n",
      "train loss:0.35248083132104824\n",
      "train loss:0.7009256935551694\n",
      "train loss:0.6085225369232761\n",
      "train loss:0.5204437807661236\n",
      "train loss:0.6126810668054931\n",
      "train loss:0.6993434391918726\n",
      "train loss:0.5168584454211363\n",
      "train loss:0.5186819548942834\n",
      "train loss:0.7018592712965901\n",
      "train loss:0.3230445640634014\n",
      "train loss:0.6133887832904072\n",
      "train loss:0.6146467716646199\n",
      "train loss:0.7191012754043923\n",
      "train loss:0.6068150031400027\n",
      "train loss:0.4027831436667471\n",
      "train loss:0.9171871476325633\n",
      "train loss:0.5150450724162112\n",
      "train loss:0.6109771563038713\n",
      "train loss:0.5162643095340476\n",
      "train loss:0.3152005308996432\n",
      "train loss:0.517179295272144\n",
      "train loss:0.6184628126617303\n",
      "train loss:0.29182268302737313\n",
      "train loss:0.6169556242944579\n",
      "train loss:0.6107440679316658\n",
      "train loss:0.6237226715318421\n",
      "train loss:0.7308187117104779\n",
      "train loss:0.5046420039112081\n",
      "train loss:0.8477028084579658\n",
      "train loss:0.6009704787556804\n",
      "train loss:0.7265315539578288\n",
      "train loss:0.7156693497861552\n",
      "train loss:0.5154062255912288\n",
      "train loss:0.7044502448569128\n",
      "train loss:0.7002723081783063\n",
      "train loss:0.6958540523662222\n",
      "train loss:0.5309412663244317\n",
      "train loss:0.5336774892919054\n",
      "train loss:0.5366586139532433\n",
      "train loss:0.6151288269940066\n",
      "train loss:0.4688193260354684\n",
      "train loss:0.6125157400856066\n",
      "train loss:0.6073194711067784\n",
      "train loss:0.531771385622564\n",
      "train loss:0.533155329950189\n",
      "train loss:0.6909749633569321\n",
      "train loss:0.6098542039828097\n",
      "train loss:0.7743357397824286\n",
      "train loss:0.7730161206972717\n",
      "train loss:0.6076864810542913\n",
      "train loss:0.6176325464230377\n",
      "train loss:0.5356164743646235\n",
      "train loss:0.5304706198166037\n",
      "train loss:0.6107438837946753\n",
      "train loss:0.36905105851823905\n",
      "train loss:0.6103777291965208\n",
      "train loss:0.7879539166692536\n",
      "train loss:0.6083542852980464\n",
      "train loss:0.6065202265435221\n",
      "train loss:0.4332006916850176\n",
      "train loss:0.5184973059765584\n",
      "train loss:0.887114363852052\n",
      "train loss:0.6147635550387082\n",
      "train loss:0.4269864093558633\n",
      "train loss:0.6998101939310242\n",
      "train loss:0.7901367699662176\n",
      "train loss:0.4275047496948027\n",
      "train loss:0.518868513074919\n",
      "train loss:0.6013207444284815\n",
      "train loss:0.6970186355468807\n",
      "train loss:0.5241807032702025\n",
      "train loss:0.4231020797904891\n",
      "train loss:0.5068542570125634\n",
      "train loss:0.5058685523534284\n",
      "train loss:0.6198320590613718\n",
      "train loss:0.40009685206072215\n",
      "train loss:0.6176871023135779\n",
      "train loss:0.38735322263214317\n",
      "train loss:0.388586973235888\n",
      "train loss:0.6272514475628712\n",
      "train loss:0.6299133731478318\n",
      "train loss:0.7558148795252655\n",
      "train loss:0.4982306248375756\n",
      "train loss:1.1168589044787052\n",
      "train loss:0.7313035279955502\n",
      "train loss:0.6102506127274928\n",
      "train loss:0.608682669980635\n",
      "train loss:0.6109894661228373\n",
      "train loss:0.4259393582317602\n",
      "train loss:0.5210619006747206\n",
      "train loss:0.945573949217291\n",
      "train loss:0.5304262650386713\n",
      "train loss:0.5347920012936173\n",
      "train loss:0.6851015092996532\n",
      "train loss:0.6804439944120407\n",
      "train loss:0.6123078059303488\n",
      "train loss:0.6807624066109275\n",
      "train loss:0.6117969105054354\n",
      "train loss:0.6143428229374726\n",
      "train loss:0.554466419355508\n",
      "train loss:0.6141271411376411\n",
      "train loss:0.676205428629814\n",
      "train loss:0.6730013249489306\n",
      "train loss:0.42725692368658147\n",
      "train loss:0.7369970183620315\n",
      "train loss:0.6841990155072011\n",
      "train loss:0.549042865013661\n",
      "train loss:0.6800878366508971\n",
      "train loss:0.6791620709633199\n",
      "train loss:0.6744902116682586\n",
      "train loss:0.6164005227774175\n",
      "train loss:0.549352430144417\n",
      "train loss:0.5468283538688774\n",
      "train loss:0.6796337727365157\n",
      "train loss:0.6175835319705885\n",
      "train loss:0.6091197816278306\n",
      "train loss:0.6138185968811765\n",
      "train loss:0.45995132515895476\n",
      "train loss:0.5285097196412233\n",
      "train loss:0.4405403199579473\n",
      "train loss:0.5997468522824627\n",
      "train loss:0.4220239410133866\n",
      "train loss:0.5124029127920899\n",
      "train loss:0.40093791856355265\n",
      "train loss:0.6087744701791955\n",
      "train loss:0.6317421097756546\n",
      "train loss:0.6232882234735493\n",
      "train loss:0.5070591549239152\n",
      "train loss:0.7675411813836038\n",
      "train loss:0.6210088716151978\n",
      "train loss:0.5042096193970618\n",
      "train loss:0.7457254571872298\n",
      "train loss:0.7457346441351339\n",
      "train loss:0.38401451515553076\n",
      "train loss:1.0668130833419336\n",
      "train loss:0.5099666383125396\n",
      "train loss:0.6080614573587231\n",
      "train loss:0.5175462319532413\n",
      "train loss:0.6121680036714517\n",
      "train loss:0.6043019203656991\n",
      "train loss:0.5195834244685393\n",
      "train loss:0.43798684059622567\n",
      "train loss:0.5209215482639079\n",
      "train loss:0.6982431129662556\n",
      "train loss:0.7640889852153825\n",
      "train loss:0.6112862787217365\n",
      "train loss:0.6874668275581226\n",
      "train loss:0.4536446469503169\n",
      "train loss:0.6914480678054162\n",
      "train loss:0.6035096645212372\n",
      "train loss:0.6898822994885477\n",
      "train loss:0.6124906092279774\n",
      "train loss:0.6837369706969729\n",
      "train loss:0.5374532401085468\n",
      "train loss:0.6819974576083538\n",
      "train loss:0.6040356146537463\n",
      "train loss:0.6095433938131608\n",
      "train loss:0.5446692742529333\n",
      "train loss:0.6811062433162517\n",
      "train loss:0.5312474238948207\n",
      "train loss:0.7519852307928243\n",
      "train loss:0.5396989917804469\n",
      "train loss:0.7595721836831141\n",
      "train loss:0.46200918793320483\n",
      "train loss:0.38171456635538875\n",
      "train loss:0.6128158203914779\n",
      "train loss:0.6176112276038973\n",
      "train loss:0.5316652216357092\n",
      "train loss:0.5200571605458698\n",
      "train loss:0.5956984727299923\n",
      "train loss:0.41189788009413497\n",
      "train loss:0.6053452397162632\n",
      "train loss:0.8125754225302823\n",
      "train loss:0.5027179938014241\n",
      "train loss:0.6313126200202708\n",
      "train loss:0.5051228248875091\n",
      "train loss:0.7289786711773724\n",
      "train loss:0.6248323329661006\n",
      "train loss:0.3970919651624746\n",
      "train loss:0.5086020292973206\n",
      "train loss:0.9594827924898602\n",
      "train loss:0.7238047794200617\n",
      "train loss:0.520188612631425\n",
      "train loss:0.705213246712806\n",
      "train loss:0.42364360815090246\n",
      "train loss:0.6173544506277644\n",
      "train loss:0.9622606744229285\n",
      "train loss:0.5268484176042966\n",
      "train loss:0.4511071289822346\n",
      "train loss:0.5286620920088045\n",
      "train loss:0.6088708994937155\n",
      "train loss:0.6075646218892501\n",
      "train loss:0.6100718569704708\n",
      "train loss:0.5378001967393693\n",
      "train loss:0.8427303145883472\n",
      "train loss:0.527652744321028\n",
      "train loss:0.6926479958795483\n",
      "train loss:0.533101103186649\n",
      "train loss:0.4654534414129465\n",
      "train loss:0.8377792134575435\n",
      "train loss:0.6872620949134695\n",
      "train loss:0.617354930521625\n",
      "train loss:0.5390029979740553\n",
      "train loss:0.5398054698169521\n",
      "train loss:0.5360262492003225\n",
      "train loss:0.45058137052028313\n",
      "train loss:0.6826213782684768\n",
      "train loss:0.7760421427597957\n",
      "train loss:0.45025406227681086\n",
      "train loss:0.6109049037853367\n",
      "train loss:0.7808893217475089\n",
      "train loss:0.5256850018691345\n",
      "train loss:0.7035605461615804\n",
      "train loss:0.5231372606766407\n",
      "train loss:0.3413742041033702\n",
      "train loss:0.7800817758559491\n",
      "train loss:0.42383615324786916\n",
      "train loss:0.6113739984892927\n",
      "train loss:0.7955254764241643\n",
      "train loss:0.5977728602479152\n",
      "train loss:0.6113472821002649\n",
      "train loss:0.5265149119259861\n",
      "train loss:0.6089802754488971\n",
      "train loss:0.4103689853410303\n",
      "train loss:0.7150586699575407\n",
      "train loss:0.9101875646720015\n",
      "train loss:0.40889967866889726\n",
      "train loss:0.5170330372977956\n",
      "train loss:0.31873048149726396\n",
      "train loss:0.7145581364754251\n",
      "train loss:0.4083422047991913\n",
      "train loss:0.5113903679174214\n",
      "train loss:0.2782714902000068\n",
      "train loss:0.8596341549059504\n",
      "train loss:0.49917250519109546\n",
      "train loss:0.7298612078853102\n",
      "train loss:0.5024936114465327\n",
      "train loss:0.49825335908714596\n",
      "train loss:0.3827793016639436\n",
      "train loss:0.37512958100975613\n",
      "train loss:0.8813100238301941\n",
      "train loss:0.6196612875986466\n",
      "train loss:0.4843948103237228\n",
      "train loss:0.9739322257422336\n",
      "train loss:0.7262711086545327\n",
      "train loss:0.5110228515445862\n",
      "train loss:0.814904929267201\n",
      "train loss:0.41785362579169316\n",
      "train loss:0.510600405669196\n",
      "train loss:0.6130950380972161\n",
      "train loss:0.3372128398307682\n",
      "train loss:0.5205066610455902\n",
      "train loss:0.707478280976217\n",
      "train loss:0.7940293277942377\n",
      "train loss:0.5207483954001344\n",
      "train loss:0.6219370822418437\n",
      "train loss:0.6049897133007037\n",
      "train loss:0.44442137956936023\n",
      "train loss:0.6151700542453487\n",
      "train loss:0.6118483300162121\n",
      "train loss:0.6887272034675017\n",
      "train loss:0.8614322482238116\n",
      "train loss:0.6139728662696241\n",
      "train loss:0.6060945492813936\n",
      "train loss:0.5318929791303902\n",
      "train loss:0.6101326507015853\n",
      "train loss:0.6808834675532863\n",
      "train loss:0.7569234804272399\n",
      "train loss:0.5361196185387856\n",
      "train loss:0.6787071056097392\n",
      "train loss:0.6124580963714396\n",
      "train loss:0.6124778960231422\n",
      "train loss:0.7485770913696959\n",
      "train loss:0.5469993573634186\n",
      "train loss:0.5429079725883049\n",
      "train loss:0.5380199868933557\n",
      "train loss:0.6099206711670296\n",
      "train loss:0.5403803480616729\n",
      "train loss:0.4557862742207973\n",
      "train loss:0.6027370115302391\n",
      "train loss:0.5313423237134577\n",
      "train loss:0.4336042926143093\n",
      "train loss:0.6906914225804451\n",
      "train loss:0.5179183060499801\n",
      "train loss:0.7068289027993193\n",
      "train loss:0.5097559480218741\n",
      "train loss:0.5060665848349184\n",
      "train loss:0.4991887421533002\n",
      "train loss:0.49893201119373937\n",
      "train loss:0.6195900860693583\n",
      "train loss:0.6051915289501864\n",
      "train loss:0.48862180493306023\n",
      "train loss:0.5048612085435366\n",
      "train loss:0.7700239828190697\n",
      "train loss:0.4994448812660888\n",
      "train loss:0.6165636448234648\n",
      "train loss:0.7209758650593346\n",
      "train loss:0.3621307395879004\n",
      "train loss:0.3823919380323698\n",
      "train loss:0.8723956004831482\n",
      "train loss:0.621203597971516\n",
      "train loss:0.5036747520059848\n",
      "train loss:0.831845150914799\n",
      "train loss:0.6195735292512443\n",
      "train loss:0.7111741386564011\n",
      "train loss:0.4153520484009685\n",
      "train loss:0.4276894017315378\n",
      "train loss:0.6979482397932397\n",
      "train loss:0.5182688334327655\n",
      "train loss:0.5957319582451992\n",
      "train loss:0.5211829751999876\n",
      "train loss:0.6041275587822321\n",
      "train loss:0.6987020528533964\n",
      "train loss:0.6221392331957055\n",
      "train loss:0.606579153384786\n",
      "train loss:0.7854302215131466\n",
      "train loss:0.6990152381224815\n",
      "train loss:0.762898109823839\n",
      "train loss:0.5442437715123043\n",
      "train loss:0.5361848773942135\n",
      "train loss:0.609845628628347\n",
      "train loss:0.5341832440522298\n",
      "train loss:0.687717278757046\n",
      "train loss:0.7476628349883729\n",
      "train loss:0.6130697727019835\n",
      "train loss:0.738073837054736\n",
      "train loss:0.5526224755941256\n",
      "train loss:0.6738724549741315\n",
      "train loss:0.7332554331783744\n",
      "train loss:0.6726062062357983\n",
      "train loss:0.5017692785227497\n",
      "train loss:0.6084859965972507\n",
      "train loss:0.6754673273667099\n",
      "train loss:0.5526988678949294\n",
      "train loss:0.48719732037481195\n",
      "train loss:0.7407870356389294\n",
      "train loss:0.5450277766065236\n",
      "train loss:0.6061532116150642\n",
      "train loss:0.5391663732975351\n",
      "train loss:0.475297073326853\n",
      "train loss:0.6109806604122551\n",
      "train loss:0.4368885027431464\n",
      "train loss:0.6899322023941623\n",
      "train loss:0.5284634030250707\n",
      "train loss:0.6994611792275156\n",
      "train loss:0.6187686463485005\n",
      "train loss:0.7097907461356596\n",
      "train loss:0.5996440155956806\n",
      "train loss:0.3099027070654462\n",
      "train loss:0.8284663774774874\n",
      "train loss:0.5109314890444533\n",
      "train loss:0.8245171802991577\n",
      "train loss:0.3014970157692472\n",
      "train loss:0.827230144159088\n",
      "train loss:0.7188936246568491\n",
      "train loss:0.6123327155206206\n",
      "train loss:0.6967040951513448\n",
      "train loss:0.613332544390049\n",
      "train loss:0.7925319758014153\n",
      "train loss:0.5145137780290965\n",
      "train loss:0.6984771607965474\n",
      "train loss:0.6127631741270851\n",
      "train loss:0.5468087526208258\n",
      "train loss:0.7580997982426511\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6617647058823529\n"
     ]
    }
   ],
   "source": [
    "from mymethod.neural_network import *\n",
    "from common.trainer import Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a700c9c5-c1bc-4ddc-93af-554b078238c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet7Layer:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = int((input_size - filter_size + 2 * filter_pad) / filter_stride + 1)\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b3'] = np.zeros(hidden_size)        \n",
    "        self.params['W4'] = weight_init_std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b4'] = np.zeros(hidden_size)\n",
    "        self.params['W5'] = weight_init_std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b5'] = np.zeros(hidden_size)\n",
    "        self.params['W6'] = weight_init_std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b6'] = np.zeros(hidden_size)\n",
    "        self.params['W7'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b7'] = np.zeros(output_size)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.layers['Relu3'] = Relu()  # 추가된 ReLU 활성화 함수\n",
    "        self.layers['Affine3'] = Affine(self.params['W4'], self.params['b4'])\n",
    "        self.layers['Relu4'] = Relu()  # 추가된 ReLU 활성화 함\n",
    "        self.layers['Affine4'] = Affine(self.params['W5'], self.params['b5'])\n",
    "        self.layers['Relu5'] = Relu()  # 추가된 ReLU 활성화 함\n",
    "        self.layers['Affine5'] = Affine(self.params['W6'], self.params['b6'])\n",
    "        self.layers['Relu6'] = Relu()  # 추가된 ReLU 활성화 함\n",
    "        self.layers['Affine6'] = Affine(self.params['W7'], self.params['b7'])\n",
    "\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3,4,5,6,7):\n",
    "        # for idx in (1, 2, 3,4,5):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        grads['W4'], grads['b4'] = self.layers['Affine3'].dW, self.layers['Affine3'].db\n",
    "        grads['W5'], grads['b5'] = self.layers['Affine4'].dW, self.layers['Affine4'].db\n",
    "        grads['W6'], grads['b6'] = self.layers['Affine5'].dW, self.layers['Affine5'].db\n",
    "        grads['W7'], grads['b7'] = self.layers['Affine6'].dW, self.layers['Affine6'].db\n",
    "\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70db4091-d55d-47d4-b733-0087a8d7a282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3008114774041903\n",
      "=== epoch:1, train acc:0.7, test acc:0.65 ===\n",
      "train loss:2.2988904549713927\n",
      "train loss:2.2969470171466773\n",
      "train loss:2.2948219251030877\n",
      "train loss:2.292427563758855\n",
      "train loss:2.2904356052459898\n",
      "train loss:2.2877042553616165\n",
      "train loss:2.2843028112514205\n",
      "train loss:2.2819834593741843\n",
      "train loss:2.278728535281702\n",
      "train loss:2.274476995767912\n",
      "train loss:2.270079530444556\n",
      "train loss:2.2656456872486475\n",
      "train loss:2.26201989154609\n",
      "train loss:2.25684617735912\n",
      "train loss:2.2511047784432554\n",
      "train loss:2.2447073577084025\n",
      "train loss:2.236492987613217\n",
      "train loss:2.2270199809842626\n",
      "train loss:2.2186166993680247\n",
      "train loss:2.208667449523956\n",
      "train loss:2.194595446136657\n",
      "train loss:2.1722494322061574\n",
      "train loss:2.1495205757108704\n",
      "train loss:2.121939319740249\n",
      "train loss:2.060846425053188\n",
      "train loss:2.018914764356996\n",
      "train loss:1.8913538246663317\n",
      "train loss:1.7272542997486073\n",
      "train loss:1.5710305268491693\n",
      "train loss:1.255144951366407\n",
      "train loss:0.989104062369163\n",
      "train loss:0.7055317576696544\n",
      "train loss:0.7996796193763189\n",
      "train loss:0.36525979587904556\n",
      "train loss:0.5154161330059216\n",
      "train loss:0.3473016882500269\n",
      "train loss:1.4953358356659427\n",
      "train loss:0.6387213193154009\n",
      "train loss:1.2973236806874833\n",
      "train loss:0.749991397218781\n",
      "train loss:0.6382356959499595\n",
      "train loss:0.802373596524272\n",
      "train loss:0.5097012284491022\n",
      "train loss:0.6453460483332322\n",
      "train loss:0.6787040314151458\n",
      "train loss:0.687425590158049\n",
      "train loss:0.7030163906371824\n",
      "train loss:0.7066569668014983\n",
      "train loss:0.6985519205604971\n",
      "train loss:0.6872549810267148\n",
      "train loss:0.685348239814858\n",
      "train loss:0.6988076770362188\n",
      "train loss:0.678576357102385\n",
      "train loss:0.633052407009554\n",
      "train loss:0.6410600135222799\n",
      "train loss:0.6422308192314402\n",
      "train loss:0.5791525738990992\n",
      "train loss:0.5814981465651473\n",
      "train loss:0.6805558403144472\n",
      "train loss:0.5463278551290068\n",
      "train loss:0.7121087994628392\n",
      "train loss:0.5096257451746073\n",
      "train loss:0.6162087710249944\n",
      "train loss:0.8119446672567348\n",
      "train loss:0.40688969183735413\n",
      "train loss:0.6359409322110778\n",
      "train loss:0.6112349290514738\n",
      "train loss:0.26870158205507505\n",
      "train loss:0.754314745250845\n",
      "train loss:0.3909307520075206\n",
      "train loss:0.625141754649772\n",
      "train loss:0.46808575219344395\n",
      "train loss:0.5104513690260852\n",
      "train loss:1.009790953201361\n",
      "train loss:1.103588781035165\n",
      "train loss:0.49967207967504484\n",
      "train loss:0.5507590568345402\n",
      "train loss:0.5377660177809047\n",
      "train loss:0.717376368834995\n",
      "train loss:0.4714817167599038\n",
      "train loss:0.7545896854877634\n",
      "train loss:0.6332901840214082\n",
      "train loss:0.6788473302304323\n",
      "train loss:0.7000755060621487\n",
      "train loss:0.5067148020207013\n",
      "train loss:0.6331439158180387\n",
      "train loss:0.6198666130769898\n",
      "train loss:0.5551136102456182\n",
      "train loss:0.6928798591187075\n",
      "train loss:0.8364050975478667\n",
      "train loss:0.7678601175394839\n",
      "train loss:0.5094425216113816\n",
      "train loss:0.699496841582118\n",
      "train loss:0.5455895110333825\n",
      "train loss:0.6801835150256694\n",
      "train loss:0.7590326933832581\n",
      "train loss:0.5455610676299669\n",
      "train loss:0.7461814947306292\n",
      "train loss:0.5587771972514717\n",
      "train loss:0.7253965375406249\n",
      "train loss:0.6173979451109227\n",
      "train loss:0.631977196802682\n",
      "train loss:0.6849607681923071\n",
      "train loss:0.6086611955486234\n",
      "train loss:0.5499334534046281\n",
      "train loss:0.5396436590053061\n",
      "train loss:0.38250105884753793\n",
      "train loss:0.9283318002102441\n",
      "train loss:0.5268855812435079\n",
      "train loss:0.427717014821714\n",
      "train loss:0.7095132146642309\n",
      "train loss:0.4984280451929834\n",
      "train loss:0.7238256356314611\n",
      "train loss:0.8298680171563874\n",
      "train loss:0.43651937318039397\n",
      "train loss:0.7162870246788255\n",
      "train loss:0.7159232746068227\n",
      "train loss:0.5159075483127091\n",
      "train loss:0.7847275959151121\n",
      "train loss:0.3879294162574049\n",
      "train loss:0.5447364198798292\n",
      "train loss:0.6136881225024656\n",
      "train loss:0.5139142212783818\n",
      "train loss:0.7105804435205859\n",
      "train loss:0.7824699234016295\n",
      "train loss:0.527877262561657\n",
      "train loss:0.5328676899425476\n",
      "train loss:0.6108013342868487\n",
      "train loss:0.5177834092989808\n",
      "train loss:0.42356716217528356\n",
      "train loss:0.7070058265627395\n",
      "train loss:0.2985062622720092\n",
      "train loss:0.4955779473755634\n",
      "train loss:0.9923756158967707\n",
      "train loss:0.5185965226838029\n",
      "train loss:0.7131573162952033\n",
      "train loss:0.8144010588520955\n",
      "train loss:0.5175046112020081\n",
      "train loss:0.3234340565417522\n",
      "train loss:0.8102180221312629\n",
      "train loss:0.5206060396708584\n",
      "train loss:0.6007785603467811\n",
      "train loss:0.5801802716786348\n",
      "train loss:0.6897898817382002\n",
      "train loss:0.7551921701894684\n",
      "train loss:0.47617584432645443\n",
      "train loss:0.6075877639516447\n",
      "train loss:0.3796597616411393\n",
      "train loss:0.6251148940874297\n",
      "train loss:0.6980038591750978\n",
      "train loss:0.6948996176794711\n",
      "train loss:0.5255691869233037\n",
      "train loss:0.6916893862176238\n",
      "train loss:0.526216739638981\n",
      "train loss:0.5270549696277089\n",
      "train loss:0.5232284881826279\n",
      "train loss:0.646287347627141\n",
      "train loss:0.723203551399677\n",
      "train loss:0.7233376206631801\n",
      "train loss:0.4096539126895977\n",
      "train loss:0.4195581604059254\n",
      "train loss:0.633088898915939\n",
      "train loss:0.9179168853127939\n",
      "train loss:0.42823492789286066\n",
      "train loss:0.4230630814426772\n",
      "train loss:0.41154211859390755\n",
      "train loss:0.6118178527222504\n",
      "train loss:0.625116152101888\n",
      "train loss:0.6189299960434094\n",
      "train loss:0.6030824728706579\n",
      "train loss:0.4758202657488371\n",
      "train loss:0.494726692534985\n",
      "train loss:0.8775191639556963\n",
      "train loss:0.7256289046841398\n",
      "train loss:0.48611548886007644\n",
      "train loss:0.41383755129816524\n",
      "train loss:0.4312220583752267\n",
      "train loss:0.5039182654580905\n",
      "train loss:0.6255974683030113\n",
      "train loss:0.3687491810655218\n",
      "train loss:0.5074627795302787\n",
      "train loss:0.5057299795986854\n",
      "train loss:0.7431851041851167\n",
      "train loss:0.7751505047112293\n",
      "train loss:0.5122422014991139\n",
      "train loss:0.5176353735534649\n",
      "train loss:0.5153658257656827\n",
      "train loss:0.4825409308017109\n",
      "train loss:0.3900950525898458\n",
      "train loss:0.590005668194253\n",
      "train loss:0.6158819818233836\n",
      "train loss:0.5024625022004251\n",
      "train loss:0.7486309991819591\n",
      "train loss:0.37747490867148686\n",
      "train loss:0.7268515076699569\n",
      "train loss:0.7236172187203814\n",
      "train loss:0.39905107790686956\n",
      "train loss:0.5219407405721358\n",
      "train loss:0.6328089384859295\n",
      "train loss:0.6117055833595437\n",
      "train loss:0.6225211006191617\n",
      "train loss:0.41791975993234703\n",
      "train loss:0.43114278315068316\n",
      "train loss:0.4146565690771615\n",
      "train loss:0.6096015895926337\n",
      "train loss:0.8337662513194957\n",
      "train loss:0.6085015473646076\n",
      "train loss:0.3752519747099154\n",
      "train loss:0.4963784454896111\n",
      "train loss:0.23774653266169765\n",
      "train loss:0.52449812371131\n",
      "train loss:0.38383491823062893\n",
      "train loss:1.0631281248802706\n",
      "train loss:0.8899224265404466\n",
      "train loss:0.7290888473744551\n",
      "train loss:0.42228377446720194\n",
      "train loss:0.5223538098752607\n",
      "train loss:0.5284447797979588\n",
      "train loss:0.7584921033347297\n",
      "train loss:0.6250600768727399\n",
      "train loss:0.6218069591903772\n",
      "train loss:0.6790157374910436\n",
      "train loss:0.5083484272643187\n",
      "train loss:0.6233841584685624\n",
      "train loss:0.5153604730375327\n",
      "train loss:0.6305318308675872\n",
      "train loss:0.6197896099416902\n",
      "train loss:0.6746576554449489\n",
      "train loss:0.8269923052967482\n",
      "train loss:0.4792121205303988\n",
      "train loss:0.6015451919557082\n",
      "train loss:0.5301670355080713\n",
      "train loss:0.6075118826729101\n",
      "train loss:0.43510301706865856\n",
      "train loss:0.7101515261863676\n",
      "train loss:0.39683280338698534\n",
      "train loss:0.8325605121566506\n",
      "train loss:0.7106838699825982\n",
      "train loss:0.7132586524172012\n",
      "train loss:0.5259911572931877\n",
      "train loss:0.5180511251398797\n",
      "train loss:0.6036453006515103\n",
      "train loss:0.311411768042037\n",
      "train loss:0.2741627408490686\n",
      "train loss:0.6556880114332969\n",
      "train loss:0.764759032851523\n",
      "train loss:0.6491308912552338\n",
      "train loss:0.23633302674981432\n",
      "train loss:1.1370182066432677\n",
      "train loss:0.512373105035639\n",
      "train loss:0.4212369607618977\n",
      "train loss:0.6094651319306752\n",
      "train loss:0.5340145724676815\n",
      "train loss:0.6246453939824226\n",
      "train loss:0.5189856051558335\n",
      "train loss:0.43380371313009214\n",
      "train loss:0.8083171790777209\n",
      "train loss:0.3248703025108383\n",
      "train loss:0.7024204835132281\n",
      "train loss:0.8686218834143474\n",
      "train loss:0.6226051856968386\n",
      "train loss:0.44428659270988613\n",
      "train loss:0.5221556563464527\n",
      "train loss:0.6907286458769144\n",
      "train loss:0.7614310571703673\n",
      "train loss:1.017926023807014\n",
      "train loss:0.5629645782839832\n",
      "train loss:0.6227116284261198\n",
      "train loss:0.771934105484412\n",
      "train loss:0.5556762476096927\n",
      "train loss:0.5488737635289018\n",
      "train loss:0.7620361877447013\n",
      "train loss:0.720257900604292\n",
      "train loss:0.5600039318959963\n",
      "train loss:0.5510625863929263\n",
      "train loss:0.6286771349062367\n",
      "train loss:0.6183531426360351\n",
      "train loss:0.5742899161799817\n",
      "train loss:0.6668336999065511\n",
      "train loss:0.6829499022106369\n",
      "train loss:0.543655882309333\n",
      "train loss:0.6121617787860123\n",
      "train loss:0.5154930969263332\n",
      "train loss:0.7938605884548072\n",
      "train loss:0.6198286135099893\n",
      "train loss:0.7895598646573306\n",
      "train loss:0.6988686239839967\n",
      "train loss:0.6092446677891117\n",
      "train loss:0.4480279120719649\n",
      "train loss:0.5138880882218065\n",
      "train loss:0.6141193640535582\n",
      "train loss:0.6135412782196354\n",
      "train loss:0.4319741493361203\n",
      "train loss:0.5134311976377661\n",
      "train loss:0.8274841759270835\n",
      "train loss:0.4938029741185246\n",
      "train loss:0.7160570340478284\n",
      "train loss:0.4939477451124275\n",
      "train loss:0.514632501921595\n",
      "train loss:0.6173904097679792\n",
      "train loss:0.5202030256153074\n",
      "train loss:0.36874050740865033\n",
      "train loss:0.4977352450730536\n",
      "train loss:0.6239383802720513\n",
      "train loss:0.7325004990022135\n",
      "train loss:0.7427521564658541\n",
      "train loss:0.5037806901077511\n",
      "train loss:0.6243544372628185\n",
      "train loss:0.9065289587041103\n",
      "train loss:0.5369828526112774\n",
      "train loss:0.6232849050873486\n",
      "train loss:0.6868272110957516\n",
      "train loss:0.8261184364123884\n",
      "train loss:0.5861224484002563\n",
      "train loss:0.5889614902592278\n",
      "train loss:0.5989726141201708\n",
      "train loss:0.635341216122514\n",
      "train loss:0.5037286717138113\n",
      "train loss:0.6317957561148203\n",
      "train loss:0.7788135862155543\n",
      "train loss:0.6319534080980889\n",
      "train loss:0.5174206250049735\n",
      "train loss:0.6250270022827312\n",
      "train loss:0.4747010913342599\n",
      "train loss:0.6863525515418226\n",
      "train loss:0.919406783937597\n",
      "train loss:0.5409075985621936\n",
      "train loss:0.7750165875427\n",
      "train loss:0.4606156196567725\n",
      "train loss:0.6100078891425302\n",
      "train loss:0.6006097288893623\n",
      "train loss:0.7801701382649486\n",
      "train loss:0.6079298882669579\n",
      "train loss:0.5972847483452447\n",
      "train loss:0.3469013503093347\n",
      "train loss:0.42108917706616567\n",
      "train loss:0.6092736412407843\n",
      "train loss:0.6983491112501116\n",
      "train loss:0.7249079213471058\n",
      "train loss:0.7335244523476725\n",
      "train loss:0.7189995380868728\n",
      "train loss:0.6214890503831808\n",
      "train loss:0.7176063485947342\n",
      "train loss:0.6186291183265997\n",
      "train loss:0.467967093023199\n",
      "train loss:0.6221672043139693\n",
      "train loss:0.6722092055157743\n",
      "train loss:0.6056517790960063\n",
      "train loss:0.6052601078764108\n",
      "train loss:0.540868424007857\n",
      "train loss:0.6770815099669714\n",
      "train loss:0.6212391069951633\n",
      "train loss:0.7431531776591819\n",
      "train loss:0.736454100694157\n",
      "train loss:0.561900104772402\n",
      "train loss:0.552527451946033\n",
      "train loss:0.4753919050581363\n",
      "train loss:0.67923476303697\n",
      "train loss:0.4582074470867695\n",
      "train loss:0.7811598162972828\n",
      "train loss:0.509557289089796\n",
      "train loss:0.6233802346261347\n",
      "train loss:0.72262026305051\n",
      "train loss:0.7169281873175047\n",
      "train loss:0.429059637685944\n",
      "train loss:0.61116399253983\n",
      "train loss:0.4340701262522136\n",
      "train loss:0.5099404785971186\n",
      "train loss:0.5930037493341846\n",
      "train loss:0.6210335710454877\n",
      "train loss:0.5217102870507129\n",
      "train loss:0.9466554854028901\n",
      "train loss:0.5201166370501143\n",
      "train loss:0.8409246888212119\n",
      "train loss:0.627764449366609\n",
      "train loss:0.6096204609408284\n",
      "train loss:0.6791538575485971\n",
      "train loss:0.6719825672719459\n",
      "train loss:0.7734676681717263\n",
      "train loss:0.7192250528481505\n",
      "train loss:0.6482370137249782\n",
      "train loss:0.6820951571158514\n",
      "train loss:0.6540149338224907\n",
      "train loss:0.6611168355854058\n",
      "train loss:0.6006606014566677\n",
      "train loss:0.7082672070887852\n",
      "train loss:0.6289128941076906\n",
      "train loss:0.6778496655647187\n",
      "train loss:0.6753247012802943\n",
      "train loss:0.7114027845170157\n",
      "train loss:0.5697268334578767\n",
      "train loss:0.6829526872951606\n",
      "train loss:0.5893694061728862\n",
      "train loss:0.6734510713058752\n",
      "train loss:0.7298634770271751\n",
      "train loss:0.5045769580091306\n",
      "train loss:0.4039279936378953\n",
      "train loss:0.6867051132560628\n",
      "train loss:0.413182975964754\n",
      "train loss:0.8327416023208128\n",
      "train loss:0.7225527475256329\n",
      "train loss:0.7130547089339323\n",
      "train loss:0.505431821436156\n",
      "train loss:0.6190972590547225\n",
      "train loss:0.6051377517368913\n",
      "train loss:0.6163969816818962\n",
      "train loss:0.49844106564598406\n",
      "train loss:0.7353574209946707\n",
      "train loss:0.7186793581915236\n",
      "train loss:0.5204467626165638\n",
      "train loss:0.6888354118383344\n",
      "train loss:0.7696658232853288\n",
      "train loss:0.6184637705858053\n",
      "train loss:0.6165845874248026\n",
      "train loss:0.6732477349315831\n",
      "train loss:0.7256132917363696\n",
      "train loss:0.6755088998032533\n",
      "train loss:0.6345392137109511\n",
      "train loss:0.5174310635976489\n",
      "train loss:0.5375740697711581\n",
      "train loss:0.7219813654102661\n",
      "train loss:0.5759192825134881\n",
      "train loss:0.6189797832715714\n",
      "train loss:0.7837925405254496\n",
      "train loss:0.5107246821927409\n",
      "train loss:0.6165412783633561\n",
      "train loss:0.5444385603399463\n",
      "train loss:0.36647111493096884\n",
      "train loss:0.32338485942350387\n",
      "train loss:0.8357239099918464\n",
      "train loss:0.4903634027603115\n",
      "train loss:0.6495262276567256\n",
      "train loss:0.50686587683301\n",
      "train loss:0.9273131733658662\n",
      "train loss:0.6156392374803381\n",
      "train loss:0.5017150488268605\n",
      "train loss:0.4843168855939032\n",
      "train loss:0.3875923101220688\n",
      "train loss:0.6119526014748395\n",
      "train loss:0.24296463893942738\n",
      "train loss:0.4964334486024725\n",
      "train loss:0.6258173168406177\n",
      "train loss:0.5031812220405353\n",
      "train loss:0.21696069813414637\n",
      "train loss:0.6735135210867904\n",
      "train loss:0.67342005729847\n",
      "train loss:0.7714690298528601\n",
      "train loss:0.9744845103403229\n",
      "train loss:0.7880823056526781\n",
      "train loss:0.3715014938286395\n",
      "train loss:0.6054746971268218\n",
      "train loss:0.4954067878561584\n",
      "train loss:0.679664069164541\n",
      "train loss:0.5165553359630883\n",
      "train loss:0.6798859028631947\n",
      "train loss:0.511125227944771\n",
      "train loss:0.44003172739079044\n",
      "train loss:0.7489961603000065\n",
      "train loss:0.4715175242692105\n",
      "train loss:0.5409048913004887\n",
      "train loss:0.4384651805755942\n",
      "train loss:0.49799255898183203\n",
      "train loss:0.6191402079956281\n",
      "train loss:0.8519537028590124\n",
      "train loss:0.73897436369321\n",
      "train loss:0.3823979199007708\n",
      "train loss:0.6306007619905089\n",
      "train loss:1.0566447046430338\n",
      "train loss:0.3978858624909862\n",
      "train loss:0.49980453470865394\n",
      "train loss:0.6915962766110284\n",
      "train loss:0.522677494811042\n",
      "train loss:0.5275104041944592\n",
      "train loss:0.43583582819457317\n",
      "train loss:0.5149919974885141\n",
      "train loss:0.7166948548811372\n",
      "train loss:0.5238246717375448\n",
      "train loss:0.5128483078733004\n",
      "train loss:0.5954234117216489\n",
      "train loss:0.6020112825274477\n",
      "train loss:0.5088455046563679\n",
      "train loss:0.5302755769754637\n",
      "train loss:0.8518075970363969\n",
      "train loss:0.3959641169044551\n",
      "train loss:0.5056843742052861\n",
      "train loss:0.5066075150626487\n",
      "train loss:0.49720386608228184\n",
      "train loss:0.6107749637415704\n",
      "train loss:0.8685988953211806\n",
      "train loss:0.6995069441750925\n",
      "train loss:0.30524100815765676\n",
      "train loss:0.715812095817607\n",
      "train loss:0.5071259535119563\n",
      "train loss:0.3150151871511291\n",
      "train loss:0.7091860993710652\n",
      "train loss:0.40290228346206974\n",
      "train loss:0.5022750153592005\n",
      "train loss:0.6205592068909259\n",
      "train loss:0.6239680197372202\n",
      "train loss:0.6189402192058363\n",
      "train loss:0.39470650939864843\n",
      "train loss:0.5089783141754284\n",
      "train loss:0.36365262327004955\n",
      "train loss:0.4938416498320116\n",
      "train loss:0.6545516649533972\n",
      "train loss:0.49780262324549274\n",
      "train loss:0.64174067174426\n",
      "train loss:0.35450366303294967\n",
      "train loss:0.7716858049727202\n",
      "train loss:0.8767394810483697\n",
      "train loss:0.5308224700777292\n",
      "train loss:0.5120524532861346\n",
      "train loss:0.7987053494043824\n",
      "train loss:0.44737979011928236\n",
      "train loss:0.5344634322240605\n",
      "train loss:0.5329877836732658\n",
      "train loss:0.6151383394341766\n",
      "train loss:0.8858698140899535\n",
      "train loss:0.7912565034500358\n",
      "train loss:0.5737438313264566\n",
      "train loss:0.5717685403694326\n",
      "train loss:0.5721942216177397\n",
      "train loss:0.6294218050885816\n",
      "train loss:0.5133206726227575\n",
      "train loss:0.5553923695202382\n",
      "train loss:0.555170184991707\n",
      "train loss:0.382956161812147\n",
      "train loss:0.7693907153147641\n",
      "train loss:0.50999415585308\n",
      "train loss:0.7285045085811344\n",
      "train loss:0.6059760799753883\n",
      "train loss:0.5050079927193429\n",
      "train loss:0.3800218499441708\n",
      "train loss:0.6292027179337435\n",
      "train loss:0.35590877380683905\n",
      "train loss:0.637188431115368\n",
      "train loss:0.18797678178346872\n",
      "train loss:0.5324521846771397\n",
      "train loss:0.6965360147110117\n",
      "train loss:0.6758949918309121\n",
      "train loss:0.49806971726368243\n",
      "train loss:0.8180177099462476\n",
      "train loss:0.6251799520390289\n",
      "train loss:0.7294241180481078\n",
      "train loss:0.4250861871123598\n",
      "train loss:0.6791288056410507\n",
      "train loss:0.603392009620545\n",
      "train loss:0.6820892479592652\n",
      "train loss:0.5638634431073635\n",
      "train loss:0.6781651936729384\n",
      "train loss:0.6330817420867825\n",
      "train loss:0.7714530364703237\n",
      "train loss:0.6325950226582016\n",
      "train loss:0.7647753631490913\n",
      "train loss:0.5621497753580198\n",
      "train loss:0.56132039086075\n",
      "train loss:0.7219682992615917\n",
      "train loss:0.6392665679162055\n",
      "train loss:0.719081433505375\n",
      "train loss:0.5834611302141256\n",
      "train loss:0.7200499911128848\n",
      "train loss:0.5191154049460319\n",
      "train loss:0.5633065382832846\n",
      "train loss:0.4788015632538179\n",
      "train loss:0.684541275282601\n",
      "train loss:0.6157076126671002\n",
      "train loss:0.615240704200748\n",
      "train loss:0.4080171594923822\n",
      "train loss:0.6255222507685957\n",
      "train loss:0.7178527744623532\n",
      "train loss:0.6212095415666019\n",
      "train loss:0.4909490331987256\n",
      "train loss:0.6171320744470317\n",
      "train loss:0.9443237378016921\n",
      "train loss:0.8231969512910636\n",
      "train loss:0.6880797421057017\n",
      "train loss:0.45788117529610206\n",
      "train loss:0.745211280673875\n",
      "train loss:0.4175079135676695\n",
      "train loss:0.4828203506158152\n",
      "train loss:0.4721842754363215\n",
      "train loss:0.6865890909126225\n",
      "train loss:0.5411752444844791\n",
      "train loss:0.6874549078668216\n",
      "train loss:0.37043448060007106\n",
      "train loss:0.5181855693798159\n",
      "train loss:0.7096882357757534\n",
      "train loss:0.5249257498498301\n",
      "train loss:0.6122598345297773\n",
      "train loss:0.8172051507383259\n",
      "train loss:0.7152490855185878\n",
      "train loss:0.39588387549538695\n",
      "train loss:0.7261366630597984\n",
      "train loss:0.5168668585773519\n",
      "train loss:0.9647776977653161\n",
      "train loss:0.6931761650357455\n",
      "train loss:0.7525908203218428\n",
      "train loss:0.6222023736944047\n",
      "train loss:0.5051030008048338\n",
      "train loss:0.4541880790603699\n",
      "train loss:0.7874490394222717\n",
      "train loss:0.5148497378405047\n",
      "train loss:0.6220392795008307\n",
      "train loss:0.684817149522957\n",
      "train loss:0.6291622454670467\n",
      "train loss:0.5541588556436599\n",
      "train loss:0.48603127674187574\n",
      "train loss:0.4033890097066893\n",
      "train loss:0.5348331932610683\n",
      "train loss:0.5110793330138153\n",
      "train loss:0.7174741310116317\n",
      "train loss:0.6065046230302562\n",
      "train loss:0.733453792841612\n",
      "train loss:0.7102671824834921\n",
      "train loss:0.7314561776938767\n",
      "train loss:0.39995234166193155\n",
      "train loss:0.5119282196474513\n",
      "train loss:0.49910432498596213\n",
      "train loss:0.3820529105023881\n",
      "train loss:0.525111603872175\n",
      "train loss:0.4998281357257334\n",
      "train loss:0.49559015663938943\n",
      "train loss:0.6316550211925834\n",
      "train loss:0.7631693372187136\n",
      "train loss:0.8554733045064642\n",
      "train loss:0.5142209595539937\n",
      "train loss:0.884883986435429\n",
      "train loss:0.5246743648121769\n",
      "train loss:0.6786704418101739\n",
      "train loss:0.5539529673761229\n",
      "train loss:0.6119214215456177\n",
      "train loss:0.5567928287171295\n",
      "train loss:0.5078603865416784\n",
      "train loss:0.6119233671109698\n",
      "train loss:0.6201573421842722\n",
      "train loss:0.4963169701439291\n",
      "train loss:0.5502311932091377\n",
      "train loss:0.6197940419294308\n",
      "train loss:0.6883482773597032\n",
      "train loss:0.44023305307539295\n",
      "train loss:0.51505207622217\n",
      "train loss:0.510327256141401\n",
      "train loss:0.9406216456295571\n",
      "train loss:0.5139612566463323\n",
      "train loss:0.8187770159702883\n",
      "train loss:0.8054389534391134\n",
      "train loss:0.6945079699776189\n",
      "train loss:0.5342285340240235\n",
      "train loss:0.6877882369854728\n",
      "train loss:0.6794619284934318\n",
      "train loss:0.7379745357366869\n",
      "train loss:0.624173742521175\n",
      "train loss:0.5806161743452058\n",
      "train loss:0.6297787792585503\n",
      "train loss:0.6758891634952884\n",
      "train loss:0.5825572087364955\n",
      "train loss:0.7690964531377349\n",
      "train loss:0.6241122633669856\n",
      "train loss:0.5831552641961377\n",
      "train loss:0.6831507349393233\n",
      "train loss:0.8172318738988004\n",
      "train loss:0.5820532878671162\n",
      "train loss:0.6793115222981732\n",
      "train loss:0.583213400076618\n",
      "train loss:0.6811248661790006\n",
      "train loss:0.6274147711254608\n",
      "train loss:0.7317937858651653\n",
      "train loss:0.7795231811738487\n",
      "train loss:0.6216311534978398\n",
      "train loss:0.6215615364117613\n",
      "train loss:0.7709582640454467\n",
      "train loss:0.6236524612125353\n",
      "train loss:0.6256130827551208\n",
      "train loss:0.6750372941392822\n",
      "train loss:0.5207484490006377\n",
      "train loss:0.6806373111271105\n",
      "train loss:0.6174946717940399\n",
      "train loss:0.5571130865169814\n",
      "train loss:0.6140596914751957\n",
      "train loss:0.6151808959304714\n",
      "train loss:0.6003409420675683\n",
      "train loss:0.8359390912300089\n",
      "train loss:0.5365453085998155\n",
      "train loss:0.6903003252977061\n",
      "train loss:0.6050350892400187\n",
      "train loss:0.5240462729871617\n",
      "train loss:0.5226499710007606\n",
      "train loss:0.6969742108161748\n",
      "train loss:0.42678574671909775\n",
      "train loss:0.3925703350255665\n",
      "train loss:0.2527594684156769\n",
      "train loss:0.6301075118583453\n",
      "train loss:0.49507920009569173\n",
      "train loss:0.6426892427467241\n",
      "train loss:0.5373212920589697\n",
      "train loss:0.6581500652804205\n",
      "train loss:0.6556804110796196\n",
      "train loss:0.7541762934788607\n",
      "train loss:0.6210071098535421\n",
      "train loss:0.5261946337204375\n",
      "train loss:0.6017344269543531\n",
      "train loss:0.6200727690289626\n",
      "train loss:0.7526488321606528\n",
      "train loss:0.7419774639039195\n",
      "train loss:0.5171341163108345\n",
      "train loss:0.4703764165904098\n",
      "train loss:0.6256530380652452\n",
      "train loss:0.5663266106719914\n",
      "train loss:0.57338978107223\n",
      "train loss:0.5645342516595744\n",
      "train loss:0.4977862500302243\n",
      "train loss:0.6097413200374258\n",
      "train loss:0.6799446939809489\n",
      "train loss:0.4642677747387819\n",
      "train loss:0.3419993882002469\n",
      "train loss:0.5160337014774168\n",
      "train loss:0.38745015379765413\n",
      "train loss:0.5015715363083475\n",
      "train loss:0.663797526880672\n",
      "train loss:0.9766186618067326\n",
      "train loss:0.5045383084385199\n",
      "train loss:1.047425458989039\n",
      "train loss:0.5006785610730455\n",
      "train loss:0.40066512294444373\n",
      "train loss:0.6189731054094018\n",
      "train loss:0.31985941725296674\n",
      "train loss:0.52671139678878\n",
      "train loss:0.7179148851807637\n",
      "train loss:0.42125327891801545\n",
      "train loss:0.41779180661339277\n",
      "train loss:0.7146599453045035\n",
      "train loss:0.7109975325425714\n",
      "train loss:0.6903924374932545\n",
      "train loss:0.6151286317185459\n",
      "train loss:0.6929353008476654\n",
      "train loss:0.6090252432048887\n",
      "train loss:0.43912119760583074\n",
      "train loss:0.4360966360075329\n",
      "train loss:0.6981083537365613\n",
      "train loss:0.4371115581229028\n",
      "train loss:0.5104028329211573\n",
      "train loss:0.8079144021073221\n",
      "train loss:0.5070011818643443\n",
      "train loss:0.6201352669644182\n",
      "train loss:0.5183735654695691\n",
      "train loss:0.7162243159793906\n",
      "train loss:0.6147429283553211\n",
      "train loss:0.4080343981932882\n",
      "train loss:0.39962085862379576\n",
      "train loss:0.7255563470288184\n",
      "train loss:0.7261235738832011\n",
      "train loss:0.5181291067413072\n",
      "train loss:0.39679697109743584\n",
      "train loss:0.5079487988919551\n",
      "train loss:0.5054642532812124\n",
      "train loss:0.739772381024282\n",
      "train loss:0.36594510918756995\n",
      "train loss:0.8495419843019697\n",
      "train loss:1.0234618272435516\n",
      "train loss:0.4193934247804075\n",
      "train loss:0.6120143787150338\n",
      "train loss:0.4415771603261507\n",
      "train loss:0.5296464185051803\n",
      "train loss:0.5271043791436111\n",
      "train loss:0.6975133599837698\n",
      "train loss:0.6148139708088596\n",
      "train loss:0.6079626308028236\n",
      "train loss:0.6906698751717666\n",
      "train loss:0.522508514442783\n",
      "train loss:0.9181988787886655\n",
      "train loss:0.4654215453937166\n",
      "train loss:0.6822633347662409\n",
      "train loss:0.613587421878324\n",
      "train loss:0.6105640356623963\n",
      "train loss:0.6810416964514185\n",
      "train loss:0.541955775083292\n",
      "train loss:0.6853563893752593\n",
      "train loss:0.5421337026290746\n",
      "train loss:0.537718897938042\n",
      "train loss:0.5339703625942362\n",
      "train loss:0.7129042507942863\n",
      "train loss:0.5206010873568319\n",
      "train loss:0.6153468048872277\n",
      "train loss:0.3091973084052042\n",
      "train loss:0.5198748849817469\n",
      "train loss:0.8439067846559494\n",
      "train loss:0.6235065728519904\n",
      "train loss:0.7147223143582762\n",
      "train loss:0.49983206241194866\n",
      "train loss:0.6106139144592622\n",
      "train loss:0.5075220316308983\n",
      "train loss:0.27882512921362335\n",
      "train loss:0.7446079669259806\n",
      "train loss:0.2587661456901875\n",
      "train loss:0.4922188706783272\n",
      "train loss:0.6482953387767084\n",
      "train loss:0.6180870737276962\n",
      "train loss:0.6301173487292385\n",
      "train loss:0.4913966422806557\n",
      "train loss:0.6261074249708587\n",
      "train loss:0.37359963041405586\n",
      "train loss:0.9550958306671106\n",
      "train loss:0.5996470608261808\n",
      "train loss:0.41017898625311605\n",
      "train loss:0.6127458969434378\n",
      "train loss:0.7070344201415488\n",
      "train loss:0.6940228070350765\n",
      "train loss:0.5492778083949645\n",
      "train loss:0.5393753735091222\n",
      "train loss:0.6865404347358359\n",
      "train loss:0.5387362316355996\n",
      "train loss:0.6824065212954986\n",
      "train loss:0.6802426082335071\n",
      "train loss:0.612539979061281\n",
      "train loss:0.48947732817708556\n",
      "train loss:0.6081744723609412\n",
      "train loss:0.6817909039686888\n",
      "train loss:0.686021778160008\n",
      "train loss:0.5417919439456536\n",
      "train loss:0.6842785595211776\n",
      "train loss:0.8865773255203987\n",
      "train loss:0.743246269902639\n",
      "train loss:0.6708854099227108\n",
      "train loss:0.5655470044261317\n",
      "train loss:0.5012938636219488\n",
      "train loss:0.6213504857472801\n",
      "train loss:0.5603269229465588\n",
      "train loss:0.7370090832267078\n",
      "train loss:0.4804428746032626\n",
      "train loss:0.6146115777087984\n",
      "train loss:0.6875527128571421\n",
      "train loss:0.5300365535278756\n",
      "train loss:0.6142186165360275\n",
      "train loss:0.6081902626283945\n",
      "train loss:0.6054139821932896\n",
      "train loss:0.6969894585018092\n",
      "train loss:0.5175906988537877\n",
      "train loss:0.4102852806940064\n",
      "train loss:0.6115192424515723\n",
      "train loss:0.6208086906976928\n",
      "train loss:0.2606890169587616\n",
      "train loss:0.489803262723154\n",
      "train loss:0.5077875272393993\n",
      "train loss:0.48841096340810664\n",
      "train loss:0.8344742379651071\n",
      "train loss:0.49138574808363\n",
      "train loss:0.48897005397025095\n",
      "train loss:1.039304678508407\n",
      "train loss:0.9340289186282321\n",
      "train loss:0.6065412108431241\n",
      "train loss:0.6740590964152005\n",
      "train loss:0.6733970177424287\n",
      "train loss:0.520873073739172\n",
      "train loss:0.6750620447519301\n",
      "train loss:0.6824741002482899\n",
      "train loss:0.6089088582275827\n",
      "train loss:0.6489933654548966\n",
      "train loss:0.6130238280877767\n",
      "train loss:0.6150871736614876\n",
      "train loss:0.65344175058151\n",
      "train loss:0.6518051346944105\n",
      "train loss:0.7145119261260324\n",
      "train loss:0.6389755325286205\n",
      "train loss:0.5909090244889226\n",
      "train loss:0.7174381239718179\n",
      "train loss:0.6246465427616015\n",
      "train loss:0.4556516695750612\n",
      "train loss:0.7918708965383227\n",
      "train loss:0.61396221947381\n",
      "train loss:0.6795279854574254\n",
      "train loss:0.7595713679372988\n",
      "train loss:0.5410334197360994\n",
      "train loss:0.45701684555297895\n",
      "train loss:0.5411555735093356\n",
      "train loss:0.7008382042195204\n",
      "train loss:0.5903548545804667\n",
      "train loss:0.8006779428736011\n",
      "train loss:0.5129647505279602\n",
      "train loss:0.6935117739938695\n",
      "train loss:0.7020537721787476\n",
      "train loss:0.42164142578978303\n",
      "train loss:0.6123281236599214\n",
      "train loss:0.7775911133548858\n",
      "train loss:0.43203417617203616\n",
      "train loss:0.6226087000715848\n",
      "train loss:0.6973629547354523\n",
      "train loss:0.6041536245878529\n",
      "train loss:0.4348338793840368\n",
      "train loss:0.42045707155301004\n",
      "train loss:0.6068747096367276\n",
      "train loss:0.791841293825734\n",
      "train loss:0.4065872436985297\n",
      "train loss:0.5113155977113366\n",
      "train loss:0.39080043347129145\n",
      "train loss:0.504214992469519\n",
      "train loss:0.7658717509665846\n",
      "train loss:0.5076362675433483\n",
      "train loss:0.8712629893961676\n",
      "train loss:0.7432558253149424\n",
      "train loss:0.399370224682727\n",
      "train loss:0.5183349115235596\n",
      "train loss:0.7060273504690053\n",
      "train loss:0.42919559181732725\n",
      "train loss:0.6083805760248069\n",
      "train loss:0.6128969580090586\n",
      "train loss:0.6985160203465307\n",
      "train loss:0.6045309834764799\n",
      "train loss:0.5209279928567635\n",
      "train loss:0.3472782794914006\n",
      "train loss:0.5120276575945228\n",
      "train loss:0.6025016602675756\n",
      "train loss:0.5004067938713428\n",
      "train loss:0.38501763995201355\n",
      "train loss:0.7448259259815802\n",
      "train loss:0.7341441166709275\n",
      "train loss:0.6085400735495488\n",
      "train loss:0.726667342888754\n",
      "train loss:0.7138873916451814\n",
      "train loss:0.6996657180225533\n",
      "train loss:0.5126848140398852\n",
      "train loss:0.7651115366979455\n",
      "train loss:0.670693739730406\n",
      "train loss:0.7488010729427257\n",
      "train loss:0.49928253959958935\n",
      "train loss:0.5604836235982497\n",
      "train loss:0.5664464012119745\n",
      "train loss:0.5057637639835466\n",
      "train loss:0.7272217966958544\n",
      "train loss:0.572573328730613\n",
      "train loss:0.4968302862723311\n",
      "train loss:0.553769741063326\n",
      "train loss:0.5951249303230075\n",
      "train loss:0.6086016390028287\n",
      "train loss:0.4473886930192868\n",
      "train loss:0.6093608606920581\n",
      "train loss:0.5960752901086724\n",
      "train loss:0.40057914061558353\n",
      "train loss:0.5026999456314131\n",
      "train loss:0.7412802948554533\n",
      "train loss:0.8810957218309549\n",
      "train loss:0.7013263044874206\n",
      "train loss:0.6201510882669548\n",
      "train loss:0.6102239407307656\n",
      "train loss:0.5085391778543258\n",
      "train loss:0.5280057102927729\n",
      "train loss:0.3189759488127188\n",
      "train loss:0.6180556182838799\n",
      "train loss:0.7010482172447576\n",
      "train loss:0.6149371962404073\n",
      "train loss:0.7156044668230058\n",
      "train loss:0.4298777242297415\n",
      "train loss:0.8048072314850423\n",
      "train loss:0.5994783828542365\n",
      "train loss:0.5300528742613192\n",
      "train loss:0.5201622337914041\n",
      "train loss:0.3375607546212517\n",
      "train loss:0.5934302953924643\n",
      "train loss:0.5959265735743114\n",
      "train loss:0.5125842811472999\n",
      "train loss:0.7165122819129885\n",
      "train loss:0.3775912968556792\n",
      "train loss:0.8277957575632288\n",
      "train loss:0.7238476305585088\n",
      "train loss:0.398313208469842\n",
      "train loss:0.3805751622188206\n",
      "train loss:0.9236982946151822\n",
      "train loss:0.7079224245904516\n",
      "train loss:0.6786014956936635\n",
      "train loss:0.3331644078126714\n",
      "train loss:0.41573366432258474\n",
      "train loss:0.6184587766584541\n",
      "train loss:0.6064159243509245\n",
      "train loss:0.6075158198607103\n",
      "train loss:0.7791027769050657\n",
      "train loss:0.528949995408163\n",
      "train loss:0.49631408976908514\n",
      "train loss:0.5223077531883114\n",
      "train loss:0.6959914868529925\n",
      "train loss:0.5121021457577212\n",
      "train loss:0.5076872890583486\n",
      "train loss:0.399650463337448\n",
      "train loss:0.8200984977829993\n",
      "train loss:0.7267375242807733\n",
      "train loss:0.711361482990428\n",
      "train loss:0.3240831661653997\n",
      "train loss:0.7048194962779043\n",
      "train loss:0.41466469251697385\n",
      "train loss:0.584164358943151\n",
      "train loss:0.6947249294681423\n",
      "train loss:0.7999877206532249\n",
      "train loss:0.491667784136501\n",
      "train loss:0.7893163497097967\n",
      "train loss:0.6139231216212494\n",
      "train loss:0.6691046295077376\n",
      "train loss:0.8323358111142329\n",
      "train loss:0.5650920495133456\n",
      "train loss:0.5711369552518628\n",
      "train loss:0.6142757127477455\n",
      "train loss:0.6129634994322745\n",
      "train loss:0.7344520929695026\n",
      "train loss:0.5455737612982225\n",
      "train loss:0.6227677628491839\n",
      "train loss:0.553213309524855\n",
      "train loss:0.6239876614190838\n",
      "train loss:0.6138035858562944\n",
      "train loss:0.5117510872801329\n",
      "train loss:0.611552535665051\n",
      "train loss:0.44991269793646493\n",
      "train loss:0.5120273111716145\n",
      "train loss:0.6173523858815158\n",
      "train loss:0.6118186031134504\n",
      "train loss:0.5024372477107928\n",
      "train loss:0.8824611323918763\n",
      "train loss:0.4990699034964976\n",
      "train loss:0.9569160432946078\n",
      "train loss:0.509041157103339\n",
      "train loss:0.6161581301068206\n",
      "train loss:0.8542027408501587\n",
      "train loss:0.45616012505193054\n",
      "train loss:0.5448103917435424\n",
      "train loss:0.6223518486023534\n",
      "train loss:0.673029754265994\n",
      "train loss:0.48649918604751596\n",
      "train loss:0.7411933903967494\n",
      "train loss:0.5562011636319348\n",
      "train loss:0.5502220569555945\n",
      "train loss:0.561557894761197\n",
      "train loss:0.5426293266299129\n",
      "train loss:0.6825814181032477\n",
      "train loss:0.7609575826532766\n",
      "train loss:0.5982585595259249\n",
      "train loss:0.46078926716555185\n",
      "train loss:0.5212444357715144\n",
      "train loss:0.6935054767582608\n",
      "train loss:0.5995668609454763\n",
      "train loss:0.693543453987121\n",
      "train loss:0.4288426123724441\n",
      "train loss:0.708247643645397\n",
      "train loss:0.6066112090428403\n",
      "train loss:0.7074187198644807\n",
      "train loss:0.5137325354690356\n",
      "train loss:0.7126075333248684\n",
      "train loss:0.4215930856785503\n",
      "train loss:0.6131979516081982\n",
      "train loss:0.40135597463098627\n",
      "train loss:0.5127695579389657\n",
      "train loss:0.37728330373530305\n",
      "train loss:0.5011527394723271\n",
      "train loss:0.6178820431923466\n",
      "train loss:0.8774928290441946\n",
      "train loss:0.7667540774802424\n",
      "train loss:0.8117327268559136\n",
      "train loss:0.6096064632438284\n",
      "train loss:0.7729971481209742\n",
      "train loss:0.6016679236580709\n",
      "train loss:0.6649290456426001\n",
      "train loss:0.5578100560516722\n",
      "train loss:0.5684116508170167\n",
      "train loss:0.574943160594547\n",
      "train loss:0.7210987190534832\n",
      "train loss:0.5784731458208878\n",
      "train loss:0.6237199797791174\n",
      "train loss:0.6262345346526327\n",
      "train loss:0.6774129847946195\n",
      "train loss:0.5673584520054231\n",
      "train loss:0.5621696981199085\n",
      "train loss:0.675573491559152\n",
      "train loss:0.7337935989246123\n",
      "train loss:0.4820523312713423\n",
      "train loss:0.6169742061164195\n",
      "train loss:0.613305321329819\n",
      "train loss:0.6119796564265965\n",
      "train loss:0.6908208502200603\n",
      "train loss:0.602634987457043\n",
      "train loss:0.5270661980397306\n",
      "train loss:0.5993504264020424\n",
      "train loss:0.6096196662796578\n",
      "train loss:0.8606800501144907\n",
      "train loss:0.5064533427733299\n",
      "train loss:0.601397386143157\n",
      "train loss:0.5147513084126534\n",
      "train loss:0.7899378919534417\n",
      "train loss:0.6146316985153544\n",
      "train loss:0.5238680920747414\n",
      "train loss:0.502742029379635\n",
      "train loss:0.5167826099434925\n",
      "train loss:0.7029733503335999\n",
      "train loss:0.49046881060477504\n",
      "train loss:0.6085219880869965\n",
      "train loss:0.5988997077216502\n",
      "train loss:0.5162940527683912\n",
      "train loss:0.7178046347997142\n",
      "train loss:0.29738134619454304\n",
      "train loss:0.6166898457295221\n",
      "train loss:0.39019152984437994\n",
      "train loss:0.623610314314338\n",
      "train loss:0.6230008384303083\n",
      "train loss:0.37053760599159485\n",
      "train loss:0.47349671850253416\n",
      "train loss:0.5766095999847662\n",
      "train loss:0.47368094399940563\n",
      "train loss:0.7847173533585663\n",
      "train loss:0.750671546026495\n",
      "train loss:0.34455498176192173\n",
      "train loss:0.4929512327001178\n",
      "train loss:0.4910368459953884\n",
      "train loss:0.6216690625118809\n",
      "train loss:0.8036857879702225\n",
      "train loss:0.483330450139949\n",
      "train loss:0.6330042199770217\n",
      "train loss:0.6068507820393861\n",
      "train loss:0.7014565860755424\n",
      "train loss:0.5856062910151084\n",
      "train loss:0.5972895437524979\n",
      "train loss:0.6079550870329705\n",
      "train loss:0.4581721067656136\n",
      "train loss:0.5326029172836584\n",
      "train loss:0.6263406965138166\n",
      "train loss:0.45793902735960296\n",
      "train loss:0.5281224443325284\n",
      "train loss:0.4361129164193744\n",
      "train loss:0.38473417978511604\n",
      "train loss:0.49675886623658094\n",
      "train loss:0.5890193173225163\n",
      "train loss:0.7580062002704352\n",
      "train loss:0.924134434286169\n",
      "train loss:0.635606173111727\n",
      "train loss:0.5121766544123424\n",
      "train loss:0.7405851254294642\n",
      "train loss:0.5216885495018168\n",
      "train loss:0.6386115483337067\n",
      "train loss:0.339827635136936\n",
      "train loss:0.701634985363227\n",
      "train loss:0.43438224578884554\n",
      "train loss:0.5868437996267911\n",
      "train loss:0.7869496801745003\n",
      "train loss:0.5315793790244203\n",
      "train loss:0.7329622462559501\n",
      "train loss:0.5182294094915064\n",
      "train loss:0.5263371424290003\n",
      "train loss:0.6762587286530377\n",
      "train loss:0.7537277502558768\n",
      "train loss:0.3575360698011686\n",
      "train loss:0.6867051735343266\n",
      "train loss:0.6609860068707416\n",
      "train loss:0.6042368712204419\n",
      "train loss:0.5310396168148793\n",
      "train loss:0.520592054450134\n",
      "train loss:0.7528704142765401\n",
      "train loss:0.7754468489621507\n",
      "train loss:0.6953917097208124\n",
      "train loss:0.4471948452307414\n",
      "train loss:0.6882920106351091\n",
      "train loss:0.6804877002222737\n",
      "train loss:0.6758382499402609\n",
      "train loss:0.5326728209463203\n",
      "train loss:0.5410015824346227\n",
      "train loss:0.7597368359470831\n",
      "train loss:0.766397663388458\n",
      "train loss:0.7200304120185457\n",
      "train loss:0.48890239879531033\n",
      "train loss:0.5326716305430381\n",
      "train loss:0.6748182041607158\n",
      "train loss:0.7599570678218017\n",
      "train loss:0.6263213982522788\n",
      "train loss:0.7541022095578065\n",
      "train loss:0.48403309177710135\n",
      "train loss:0.6082599433788382\n",
      "train loss:0.5573842714002673\n",
      "train loss:0.6945691252307059\n",
      "train loss:0.5345236605972543\n",
      "train loss:0.8950676054493568\n",
      "train loss:0.5258737427487715\n",
      "train loss:0.6219940409608969\n",
      "train loss:0.8194581703574402\n",
      "train loss:0.728169428648752\n",
      "train loss:0.6216594102061433\n",
      "train loss:0.6818154923266375\n",
      "train loss:0.5911306183533924\n",
      "train loss:0.6789716994649504\n",
      "train loss:0.5661665073467829\n",
      "train loss:0.5539668809881756\n",
      "train loss:0.6151838996792471\n",
      "train loss:0.7401742679236964\n",
      "train loss:0.4944514336567587\n",
      "train loss:0.6709207960294418\n",
      "train loss:0.6016682300266877\n",
      "train loss:0.4681113930314515\n",
      "train loss:0.45577309051386383\n",
      "train loss:0.60219148703251\n",
      "train loss:0.5045904311654947\n",
      "train loss:0.2779960171352355\n",
      "train loss:0.5106096487105554\n",
      "train loss:0.7998549272111165\n",
      "train loss:0.7849430357468503\n",
      "train loss:0.8871151105273869\n",
      "train loss:0.7510752765874911\n",
      "train loss:0.7250542582430686\n",
      "train loss:0.42843387381211195\n",
      "train loss:0.5690698768443309\n",
      "train loss:0.5346782337830402\n",
      "train loss:0.45390240275064964\n",
      "train loss:0.8324117787079643\n",
      "train loss:0.5941898607283752\n",
      "train loss:0.5315714006172447\n",
      "train loss:0.45381375329678286\n",
      "train loss:0.4697666110349886\n",
      "train loss:0.7006880609096335\n",
      "train loss:0.5172261496700676\n",
      "train loss:0.5230780470624394\n",
      "train loss:0.6922251196434399\n",
      "train loss:0.7066404750561579\n",
      "train loss:0.7070558888119272\n",
      "train loss:0.614029672667612\n",
      "train loss:0.4177205858760965\n",
      "train loss:0.6644996528415709\n",
      "train loss:0.8570229731527661\n",
      "train loss:0.5184167885924349\n",
      "train loss:0.6331803543628267\n",
      "train loss:0.5976119148988029\n",
      "train loss:0.587525589871399\n",
      "train loss:0.6907921694337246\n",
      "train loss:0.755458592674777\n",
      "train loss:0.6081982204555554\n",
      "train loss:0.5334526208243708\n",
      "train loss:0.87608939539552\n",
      "train loss:0.6145541362048924\n",
      "train loss:0.5321085636105456\n",
      "train loss:0.6082924757299984\n",
      "train loss:0.6911446389877764\n",
      "train loss:0.5486694828561773\n",
      "train loss:0.5603068076078974\n",
      "train loss:0.47136672671677005\n",
      "train loss:0.6207213669081559\n",
      "train loss:0.4562703310257736\n",
      "train loss:0.5204076206452016\n",
      "train loss:0.5189388044715585\n",
      "train loss:0.5083736867939888\n",
      "train loss:0.7419689273128565\n",
      "train loss:0.23819245635008984\n",
      "train loss:0.8978582601180358\n",
      "train loss:0.6120909977202293\n",
      "train loss:0.6197301605887452\n",
      "train loss:0.7490661434975557\n",
      "train loss:0.5889872247142872\n",
      "train loss:0.5691393224639059\n",
      "train loss:0.4910421035760604\n",
      "train loss:0.6004199957508567\n",
      "train loss:0.4152666201696641\n",
      "train loss:0.41757445883014216\n",
      "train loss:0.8062483554398506\n",
      "train loss:0.6135630526252869\n",
      "train loss:0.7021577731567705\n",
      "train loss:0.6058488342023082\n",
      "train loss:0.528277937200715\n",
      "train loss:0.5898026294330372\n",
      "train loss:0.43642961475657566\n",
      "train loss:0.43332007919827936\n",
      "train loss:0.6201518435086688\n",
      "train loss:0.7037104147558627\n",
      "train loss:0.8784010528145713\n",
      "train loss:0.516341322669625\n",
      "train loss:0.7826888436864837\n",
      "train loss:0.7503185853955252\n",
      "train loss:0.45556520897462327\n",
      "train loss:0.5344575207536894\n",
      "train loss:0.5225178281655767\n",
      "train loss:0.5515284357491029\n",
      "train loss:0.840424762534251\n",
      "train loss:0.6094454259078687\n",
      "train loss:0.5370660295239925\n",
      "train loss:0.6019089936525117\n",
      "train loss:0.43629002258619287\n",
      "train loss:0.6824790962440811\n",
      "train loss:0.6063717754828497\n",
      "train loss:0.6102841070532378\n",
      "train loss:0.9291707056725913\n",
      "train loss:0.606148375507528\n",
      "train loss:0.5222283498016868\n",
      "train loss:0.6821975403383128\n",
      "train loss:0.3610455455677152\n",
      "train loss:0.5198290534483772\n",
      "train loss:0.5094864309797169\n",
      "train loss:0.6176625581163622\n",
      "train loss:0.6130373764763412\n",
      "train loss:0.393524065212539\n",
      "train loss:0.5075640335501658\n",
      "train loss:0.5059018036768221\n",
      "train loss:0.7087781425338685\n",
      "train loss:0.5054236957001582\n",
      "train loss:0.7359427902428065\n",
      "train loss:0.645758058354041\n",
      "train loss:0.8952865916405175\n",
      "train loss:0.4393155285256104\n",
      "train loss:0.4090253655814166\n",
      "train loss:0.7207355857026857\n",
      "train loss:0.41541743252135727\n",
      "train loss:0.43088396586340283\n",
      "train loss:0.6041753671164284\n",
      "train loss:0.31362973740886024\n",
      "train loss:0.8036896860913038\n",
      "train loss:0.6074515719359812\n",
      "train loss:0.7250141396166128\n",
      "train loss:0.6027180826209831\n",
      "train loss:0.7720851184912044\n",
      "train loss:0.6114034225303911\n",
      "train loss:0.5030340034221716\n",
      "train loss:0.7622061940370496\n",
      "train loss:0.6028638212515981\n",
      "train loss:0.8298426703684516\n",
      "train loss:0.5386393323612026\n",
      "train loss:0.609837947613388\n",
      "train loss:0.5329998206934748\n",
      "train loss:0.4856240221309715\n",
      "train loss:0.7970385884034045\n",
      "train loss:0.5354946606134575\n",
      "train loss:0.5402090543467465\n",
      "train loss:0.6008367385710175\n",
      "train loss:0.5373764475970179\n",
      "train loss:0.5240722275776628\n",
      "train loss:0.6772893196312313\n",
      "train loss:0.40162055835956834\n",
      "train loss:0.5707426208734865\n",
      "train loss:0.27224273171700614\n",
      "train loss:0.6091352834838024\n",
      "train loss:0.646214085959422\n",
      "train loss:0.8524473259603225\n",
      "train loss:0.34895554969200093\n",
      "train loss:0.9974295013950447\n",
      "train loss:0.6277005347389528\n",
      "train loss:0.7087265438636795\n",
      "train loss:0.5803440481240205\n",
      "train loss:0.7235385251207951\n",
      "train loss:0.6155131874392223\n",
      "train loss:0.535710537805148\n",
      "train loss:0.6300684410417237\n",
      "train loss:0.6244794659319246\n",
      "train loss:0.6364557810780154\n",
      "train loss:0.5451412858719356\n",
      "train loss:0.5645302616675002\n",
      "train loss:0.4284242414919518\n",
      "train loss:0.6711714516277911\n",
      "train loss:0.5640629584350649\n",
      "train loss:0.4489305467797389\n",
      "train loss:0.47346140906629275\n",
      "train loss:0.39971666435138864\n",
      "train loss:0.6210588060423528\n",
      "train loss:0.8275386929313703\n",
      "train loss:0.8698896135758201\n",
      "train loss:0.24839758335044052\n",
      "train loss:0.7608484912365353\n",
      "train loss:0.23955854380032435\n",
      "train loss:0.71371800478243\n",
      "train loss:0.7059576308381477\n",
      "train loss:0.4486559841130351\n",
      "train loss:0.7152325067806239\n",
      "train loss:0.7228674465968457\n",
      "train loss:0.39793889541961486\n",
      "train loss:0.9059235968389462\n",
      "train loss:0.42345670211176467\n",
      "train loss:0.4377103269676903\n",
      "train loss:0.5110568124280079\n",
      "train loss:0.6605675432354441\n",
      "train loss:0.6095013837054808\n",
      "train loss:0.6002707380032389\n",
      "train loss:0.5871316284639619\n",
      "train loss:0.527227727159642\n",
      "train loss:0.44788874664541495\n",
      "train loss:0.7727099017578454\n",
      "train loss:0.6803266491164364\n",
      "train loss:0.764835063883688\n",
      "train loss:0.3423371692599744\n",
      "train loss:0.5300006704437704\n",
      "train loss:0.5877374424414403\n",
      "train loss:0.796414715642917\n",
      "train loss:0.5250806391362717\n",
      "train loss:0.5996272683706034\n",
      "train loss:0.4111649628685413\n",
      "train loss:0.7790058035480285\n",
      "train loss:0.40871637465079697\n",
      "train loss:0.6796070412105527\n",
      "train loss:0.9105674058381382\n",
      "train loss:0.4728019489799026\n",
      "train loss:0.6241948967541098\n",
      "train loss:0.5061998412456272\n",
      "train loss:0.7169101888124656\n",
      "train loss:0.4023491543695033\n",
      "train loss:0.5250164287398199\n",
      "train loss:0.7070938223768208\n",
      "train loss:0.4970332953795652\n",
      "train loss:0.6370307879982111\n",
      "train loss:0.9193214444154671\n",
      "train loss:0.5171719260518215\n",
      "train loss:0.5192776810891037\n",
      "train loss:0.5880569742813718\n",
      "train loss:0.6084988403747192\n",
      "train loss:0.7746306772175091\n",
      "train loss:0.6770201831930265\n",
      "train loss:0.589999448826935\n",
      "train loss:0.5327795130276487\n",
      "train loss:0.44784098021352037\n",
      "train loss:0.5190224773409009\n",
      "train loss:0.4209748771631353\n",
      "train loss:0.516944343114317\n",
      "train loss:0.3991898776156074\n",
      "train loss:0.5184682726402704\n",
      "train loss:0.7653685426705612\n",
      "train loss:0.6058892520426996\n",
      "train loss:0.7601698997620192\n",
      "train loss:0.8444179416919774\n",
      "train loss:0.5127889047498421\n",
      "train loss:0.37990207618559196\n",
      "train loss:0.6860206239193694\n",
      "train loss:0.8449969747790226\n",
      "train loss:0.5988303073662327\n",
      "train loss:0.4555535296837679\n",
      "train loss:0.4550367127446068\n",
      "train loss:0.7378126578914002\n",
      "train loss:0.4639975847883619\n",
      "train loss:0.3855098000931025\n",
      "train loss:0.5381023007231827\n",
      "train loss:0.8362896984911277\n",
      "train loss:0.60554867627428\n",
      "train loss:0.3965867942454127\n",
      "train loss:0.4103392685408954\n",
      "train loss:0.5153694310850756\n",
      "train loss:0.7072368722388253\n",
      "train loss:0.5218116476419585\n",
      "train loss:0.6279923191836156\n",
      "train loss:0.3504556091059244\n",
      "train loss:0.46960340917359494\n",
      "train loss:0.6002022711745891\n",
      "train loss:0.7635312831294045\n",
      "train loss:0.7719057216407277\n",
      "train loss:0.6168576660330805\n",
      "train loss:0.39244983974919023\n",
      "train loss:0.5156668686784481\n",
      "train loss:0.6113379667355943\n",
      "train loss:0.8884698200171204\n",
      "train loss:0.7161980055728849\n",
      "train loss:0.5851777847805788\n",
      "train loss:0.6073603811353737\n",
      "train loss:0.7996239720396285\n",
      "train loss:0.6635593717906975\n",
      "train loss:0.6327795497422726\n",
      "train loss:0.620390911023039\n",
      "train loss:0.4639424715687717\n",
      "train loss:0.7968413515712182\n",
      "train loss:0.6217584962339441\n",
      "train loss:0.46624665604733656\n",
      "train loss:0.6063912353914177\n",
      "train loss:0.5765886671134814\n",
      "train loss:0.4737145535587272\n",
      "train loss:0.3901603339611879\n",
      "train loss:0.6797665808931657\n",
      "train loss:0.6085798818738757\n",
      "train loss:0.4162246575406986\n",
      "train loss:0.9058258487688395\n",
      "train loss:0.5043865895780534\n",
      "train loss:0.737173911802654\n",
      "train loss:0.6995705053351345\n",
      "train loss:0.41858620089584286\n",
      "train loss:0.6944076499340033\n",
      "train loss:0.5031079593273933\n",
      "train loss:0.6260826189507822\n",
      "train loss:0.7914485321911602\n",
      "train loss:0.6027062486876732\n",
      "train loss:0.41579720847199536\n",
      "train loss:0.7189552197042077\n",
      "train loss:0.7602139820855822\n",
      "train loss:0.44303209741979027\n",
      "train loss:0.5146393765109933\n",
      "train loss:0.6011079332497806\n",
      "train loss:0.6531742892068944\n",
      "train loss:0.43447899768083387\n",
      "train loss:0.6466057280160841\n",
      "train loss:0.41403360198400285\n",
      "train loss:0.9544143652165893\n",
      "train loss:0.5059470137617249\n",
      "train loss:0.7766127801638146\n",
      "train loss:0.5647985752385867\n",
      "train loss:0.5374654629064224\n",
      "train loss:0.42308814929179694\n",
      "train loss:0.49807548518846045\n",
      "train loss:0.4871379390599998\n",
      "train loss:0.48167626270541\n",
      "train loss:0.5002756987103857\n",
      "train loss:0.24604656999006602\n",
      "train loss:0.622110702145479\n",
      "train loss:0.9011174593556879\n",
      "train loss:0.7614686857013097\n",
      "train loss:0.6245388255112407\n",
      "train loss:0.8852116540305202\n",
      "train loss:0.5019431648596272\n",
      "train loss:0.7119285847698589\n",
      "train loss:0.3573463747233662\n",
      "train loss:0.6894838271508162\n",
      "train loss:0.6222045925068003\n",
      "train loss:0.6822310193171524\n",
      "train loss:0.6054790101748858\n",
      "train loss:0.49047485932770113\n",
      "train loss:0.5419857776192131\n",
      "train loss:0.4796901406758936\n",
      "train loss:0.7471248491259268\n",
      "train loss:0.7443046553686725\n",
      "train loss:0.7424491444893058\n",
      "train loss:0.4691820147184996\n",
      "train loss:0.8139570446017244\n",
      "train loss:0.7952274719637794\n",
      "train loss:0.5590074247545762\n",
      "train loss:0.7407698438172037\n",
      "train loss:0.7289309250478346\n",
      "train loss:0.5578713159323216\n",
      "train loss:0.5537261409156782\n",
      "train loss:0.7330730362038871\n",
      "train loss:0.5897047306930435\n",
      "train loss:0.6758040784046287\n",
      "train loss:0.6836539736961103\n",
      "train loss:0.5475830017384595\n",
      "train loss:0.4766245363434118\n",
      "train loss:0.6880841329534038\n",
      "train loss:0.5369095501119195\n",
      "train loss:0.36571344981029763\n",
      "train loss:0.5916038936293104\n",
      "train loss:0.7804366718493423\n",
      "train loss:0.5083392852696986\n",
      "train loss:0.5012520651572531\n",
      "train loss:0.6056422855109461\n",
      "train loss:0.5917851632748106\n",
      "train loss:0.48139475785831315\n",
      "train loss:0.35155292378449626\n",
      "train loss:0.6397219437963739\n",
      "train loss:0.7704204867144353\n",
      "train loss:1.0051274882960035\n",
      "train loss:0.6183600324461442\n",
      "train loss:0.6147462345688988\n",
      "train loss:0.5808804062012733\n",
      "train loss:0.537933995964059\n",
      "train loss:0.6169393829033635\n",
      "train loss:0.607098195138603\n",
      "train loss:0.7293256452814909\n",
      "train loss:0.4206322870401669\n",
      "train loss:0.537852097081611\n",
      "train loss:0.6112157656454348\n",
      "train loss:0.533522603324879\n",
      "train loss:0.7404924379878285\n",
      "train loss:0.7414609510403058\n",
      "train loss:0.6755509383561785\n",
      "train loss:0.6683584846578132\n",
      "train loss:0.5375205601328339\n",
      "train loss:0.7191343410214537\n",
      "train loss:0.6690464588639192\n",
      "train loss:0.4826347233312256\n",
      "train loss:0.5476891784304124\n",
      "train loss:0.6068948954321758\n",
      "train loss:0.5316874202749075\n",
      "train loss:0.59773016456106\n",
      "train loss:0.6628270064333152\n",
      "train loss:0.588476512892736\n",
      "train loss:0.4301503201121095\n",
      "train loss:0.574824724509843\n",
      "train loss:0.5212315596978263\n",
      "train loss:0.6950159008974245\n",
      "train loss:0.387004446284424\n",
      "train loss:0.7606480711817223\n",
      "train loss:0.5300186811594637\n",
      "train loss:0.46783483444261725\n",
      "train loss:0.6245694714744789\n",
      "train loss:0.7526888399809379\n",
      "train loss:0.590380664543829\n",
      "train loss:0.8412117622420812\n",
      "train loss:0.5085941191465565\n",
      "train loss:0.5054661826245599\n",
      "train loss:0.4276767285332547\n",
      "train loss:0.6128882738931328\n",
      "train loss:0.5673259926700144\n",
      "train loss:0.6844710872905467\n",
      "train loss:0.49751293503459915\n",
      "train loss:0.32393752219685085\n",
      "train loss:0.4991309335263918\n",
      "train loss:0.8679807593778669\n",
      "train loss:0.4435904700993819\n",
      "train loss:0.6083123490598752\n",
      "train loss:0.3995011724277168\n",
      "train loss:0.5822380375443219\n",
      "train loss:0.4386398818358379\n",
      "train loss:0.47949505860162783\n",
      "train loss:0.5820798283797083\n",
      "train loss:0.5168265833852519\n",
      "train loss:0.2789916342731951\n",
      "train loss:0.7099044994899958\n",
      "train loss:0.47897632578362864\n",
      "train loss:0.3248663243638068\n",
      "train loss:0.32959781868568194\n",
      "train loss:0.8271803475000507\n",
      "train loss:0.9289123966236807\n",
      "train loss:0.3767813350186496\n",
      "train loss:0.579713839164163\n",
      "train loss:0.6038484575705073\n",
      "train loss:0.6815638264759339\n",
      "train loss:0.6683621138820397\n",
      "train loss:0.4381586323995072\n",
      "train loss:0.6486003344543289\n",
      "train loss:0.6921832827991844\n",
      "train loss:0.482940916884713\n",
      "train loss:0.6050520427558161\n",
      "train loss:0.4787587887861286\n",
      "train loss:0.7556873275197544\n",
      "train loss:0.37606476596263716\n",
      "train loss:0.515727216610094\n",
      "train loss:0.4126349156843652\n",
      "train loss:0.6821156517791169\n",
      "train loss:0.42271562336188506\n",
      "train loss:0.3569157787822864\n",
      "train loss:0.9612520207364843\n",
      "train loss:0.6015395644547211\n",
      "train loss:0.8128520650793352\n",
      "train loss:0.6245578286788661\n",
      "train loss:0.3976706130434876\n",
      "train loss:0.57096929900255\n",
      "train loss:0.6533864808789565\n",
      "train loss:0.5686568885866787\n",
      "train loss:0.4954785323407444\n",
      "train loss:0.28786261668094226\n",
      "train loss:0.40344782865069667\n",
      "train loss:0.5592706252585167\n",
      "train loss:0.2280708755165795\n",
      "train loss:0.8051659426732438\n",
      "train loss:0.7782069616834264\n",
      "train loss:0.5785125469578517\n",
      "train loss:0.482634683726466\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6617647058823529\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet7Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "982fa4ed-434b-430c-9093-3117c14546e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet3Layer:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17d41355-006a-43c4-87f4-c014a25642b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2955387843687447\n",
      "=== epoch:1, train acc:0.7, test acc:0.65 ===\n",
      "train loss:2.2810683670432814\n",
      "train loss:2.2593837199460607\n",
      "train loss:2.2197764723097757\n",
      "train loss:2.1478874392318335\n",
      "train loss:2.0701371567095213\n",
      "train loss:1.9480864399550586\n",
      "train loss:1.787726669259166\n",
      "train loss:1.5525014712680365\n",
      "train loss:1.3375529063478897\n",
      "train loss:1.1191912006098188\n",
      "train loss:0.8837140680144815\n",
      "train loss:0.8132085069479578\n",
      "train loss:0.6379004091810669\n",
      "train loss:0.7032713726106979\n",
      "train loss:1.0079225347317413\n",
      "train loss:0.6465900498169226\n",
      "train loss:0.379905652326491\n",
      "train loss:0.19155091173913907\n",
      "train loss:0.8160813285587283\n",
      "train loss:0.7996255301432403\n",
      "train loss:0.8187666091826667\n",
      "train loss:0.5537119595275705\n",
      "train loss:0.5059697083668777\n",
      "train loss:0.6918266952821859\n",
      "train loss:0.6359812340155764\n",
      "train loss:0.5690794769793784\n",
      "train loss:0.5794678193117313\n",
      "train loss:0.4380813741126507\n",
      "train loss:0.7333851611363766\n",
      "train loss:0.4930857716226247\n",
      "train loss:0.5480605872392789\n",
      "train loss:0.6479298895597745\n",
      "train loss:0.7289416906151762\n",
      "train loss:0.7779560620046915\n",
      "train loss:0.8299492065546558\n",
      "train loss:0.6952698409268799\n",
      "train loss:0.6958705934779986\n",
      "train loss:0.7498565656243861\n",
      "train loss:0.7619313485814465\n",
      "train loss:0.7334716499742839\n",
      "train loss:0.6119368255892492\n",
      "train loss:0.7222549106304605\n",
      "train loss:0.5220624530778879\n",
      "train loss:0.6252161471625547\n",
      "train loss:0.5273017586390732\n",
      "train loss:0.9421088840718367\n",
      "train loss:0.6774041463188758\n",
      "train loss:0.9427887997449602\n",
      "train loss:0.38176811376026776\n",
      "train loss:0.37726897040228546\n",
      "train loss:0.3823420353931201\n",
      "train loss:0.6118960897068899\n",
      "train loss:0.6365706737493876\n",
      "train loss:0.5083921749439522\n",
      "train loss:0.49077552883894393\n",
      "train loss:0.6071103841501782\n",
      "train loss:0.7215135065694895\n",
      "train loss:0.7231450170891813\n",
      "train loss:0.6789215684262423\n",
      "train loss:0.6988806996481605\n",
      "train loss:0.6467631931286949\n",
      "train loss:0.5856055364347581\n",
      "train loss:0.5969497286457583\n",
      "train loss:0.631776683588009\n",
      "train loss:0.7237924889585274\n",
      "train loss:0.40014139642202257\n",
      "train loss:0.5386905386247157\n",
      "train loss:0.6254583254327412\n",
      "train loss:0.6466001136399395\n",
      "train loss:0.7727927505533412\n",
      "train loss:0.5154209674232939\n",
      "train loss:0.6891351722568679\n",
      "train loss:0.3552821462211515\n",
      "train loss:0.6464181480856659\n",
      "train loss:0.5984402332816166\n",
      "train loss:1.0178844479238194\n",
      "train loss:0.6150448642587929\n",
      "train loss:0.6022753784888198\n",
      "train loss:0.4932121133332502\n",
      "train loss:0.6143169355583119\n",
      "train loss:0.4840745710730916\n",
      "train loss:0.6823210274088429\n",
      "train loss:0.7328253895812333\n",
      "train loss:0.5014805781108643\n",
      "train loss:0.7794911005619991\n",
      "train loss:0.5515046045495988\n",
      "train loss:0.6778472288335913\n",
      "train loss:0.6329383287381147\n",
      "train loss:0.49075354254433823\n",
      "train loss:0.5420006891829278\n",
      "train loss:0.792033172409426\n",
      "train loss:0.5936690487949072\n",
      "train loss:0.5973174996225372\n",
      "train loss:0.5270059098746404\n",
      "train loss:0.7061155730146952\n",
      "train loss:0.5561591910751852\n",
      "train loss:0.6169404596434063\n",
      "train loss:0.40562243962333683\n",
      "train loss:0.9311410832388315\n",
      "train loss:0.6976973233894441\n",
      "train loss:0.6320999512254276\n",
      "train loss:0.6806457762913429\n",
      "train loss:0.6651781353516781\n",
      "train loss:0.7779572530705368\n",
      "train loss:0.6463930958463318\n",
      "train loss:0.6216734040288243\n",
      "train loss:0.6196449339484609\n",
      "train loss:0.5995507710987332\n",
      "train loss:0.6702913988866955\n",
      "train loss:0.5855811173136743\n",
      "train loss:0.5632791897558806\n",
      "train loss:0.6882951282662118\n",
      "train loss:0.5125742300890315\n",
      "train loss:0.6204084342752771\n",
      "train loss:0.5898307204142579\n",
      "train loss:0.6293574278865898\n",
      "train loss:0.6055104494790765\n",
      "train loss:0.497495927188349\n",
      "train loss:0.40174905456554466\n",
      "train loss:0.6389282288533367\n",
      "train loss:0.35406332625617676\n",
      "train loss:0.596721630090839\n",
      "train loss:0.3343316947143901\n",
      "train loss:0.48457927499637304\n",
      "train loss:0.8322952424917149\n",
      "train loss:0.9148501370937423\n",
      "train loss:0.6461693983346489\n",
      "train loss:0.9224613279304956\n",
      "train loss:0.41727605626785735\n",
      "train loss:0.5956173258541092\n",
      "train loss:0.6134698857322949\n",
      "train loss:0.48385944817342474\n",
      "train loss:0.6036371853476028\n",
      "train loss:0.48421466417938247\n",
      "train loss:0.6176506955260521\n",
      "train loss:0.6036297759677347\n",
      "train loss:0.43948237116839906\n",
      "train loss:0.42779565303112743\n",
      "train loss:0.42318444004152944\n",
      "train loss:0.3815245632494776\n",
      "train loss:0.5004673882368726\n",
      "train loss:1.029283360930147\n",
      "train loss:0.470987697560567\n",
      "train loss:0.48712069391607116\n",
      "train loss:0.36069937513060923\n",
      "train loss:0.8592443546424896\n",
      "train loss:0.3457797649611048\n",
      "train loss:0.50009356857621\n",
      "train loss:0.7836850377394687\n",
      "train loss:0.8084921996578949\n",
      "train loss:0.6416151283913643\n",
      "train loss:0.40776549868871514\n",
      "train loss:0.7146956092201714\n",
      "train loss:0.33842315311690907\n",
      "train loss:0.521556941517033\n",
      "train loss:0.8253925763356864\n",
      "train loss:0.49650265955167405\n",
      "train loss:0.34542010107153265\n",
      "train loss:0.5512059416551662\n",
      "train loss:0.4390004377535375\n",
      "train loss:0.387817582569898\n",
      "train loss:0.37483651329164186\n",
      "train loss:0.3745091331736141\n",
      "train loss:0.8007158495478087\n",
      "train loss:0.5286623254089167\n",
      "train loss:0.50734715012528\n",
      "train loss:0.8366896923148339\n",
      "train loss:0.5210131633359651\n",
      "train loss:0.4903677455479734\n",
      "train loss:0.6441420142158385\n",
      "train loss:0.7745360961226446\n",
      "train loss:0.6368163961209126\n",
      "train loss:0.6354920323116695\n",
      "train loss:0.7155219414254823\n",
      "train loss:0.5487911538967468\n",
      "train loss:0.6151752899404782\n",
      "train loss:0.5472535234573312\n",
      "train loss:0.47236200967593245\n",
      "train loss:0.7670173199681479\n",
      "train loss:0.661058099370036\n",
      "train loss:0.4897279917394372\n",
      "train loss:0.5015056804561815\n",
      "train loss:0.6162408124180805\n",
      "train loss:0.5370369703441589\n",
      "train loss:0.4365309716533684\n",
      "train loss:0.8065083823978616\n",
      "train loss:0.5056701063746516\n",
      "train loss:0.28583912724020855\n",
      "train loss:0.7224379219152054\n",
      "train loss:0.7212442904379088\n",
      "train loss:0.7192959424769858\n",
      "train loss:0.8358454121091385\n",
      "train loss:0.7450694822156745\n",
      "train loss:0.5211185932065032\n",
      "train loss:0.7055518059337627\n",
      "train loss:0.6956140274809806\n",
      "train loss:0.6239013436698382\n",
      "train loss:0.6880376256566347\n",
      "train loss:0.6230069837589249\n",
      "train loss:0.8099462181179915\n",
      "train loss:0.6336090504040184\n",
      "train loss:0.6107871647988856\n",
      "train loss:0.6255924588682362\n",
      "train loss:0.6788981196151921\n",
      "train loss:0.6494547869649184\n",
      "train loss:0.6148017971059934\n",
      "train loss:0.7490178370119472\n",
      "train loss:0.5937981693559902\n",
      "train loss:0.6692014804008046\n",
      "train loss:0.5328492913686607\n",
      "train loss:0.6834203727620356\n",
      "train loss:0.6148286498179498\n",
      "train loss:0.6675092371247808\n",
      "train loss:0.7802876488705606\n",
      "train loss:0.5251698077636022\n",
      "train loss:0.5492083312962366\n",
      "train loss:0.43464781149143894\n",
      "train loss:0.4074254769918789\n",
      "train loss:0.7330100733186254\n",
      "train loss:0.3788082437492198\n",
      "train loss:0.6182431333649421\n",
      "train loss:1.003660604365252\n",
      "train loss:0.34785625153157157\n",
      "train loss:0.8493935658178273\n",
      "train loss:0.9277923720385148\n",
      "train loss:0.8356054733775959\n",
      "train loss:0.46229770343125615\n",
      "train loss:0.5407120625510846\n",
      "train loss:0.5957578514115875\n",
      "train loss:0.5623784564894689\n",
      "train loss:0.6877644242920948\n",
      "train loss:0.512734211767817\n",
      "train loss:0.7427009738255437\n",
      "train loss:0.4393211568213188\n",
      "train loss:0.4969036153919064\n",
      "train loss:0.6684396678145467\n",
      "train loss:0.4583264843937765\n",
      "train loss:0.4458351318045913\n",
      "train loss:0.4998442692949567\n",
      "train loss:0.3852269551649872\n",
      "train loss:0.48893596302071646\n",
      "train loss:0.49303717716855083\n",
      "train loss:0.963547907349956\n",
      "train loss:0.8189483751233382\n",
      "train loss:0.8080637910169408\n",
      "train loss:0.6492586937390539\n",
      "train loss:0.36143540103657834\n",
      "train loss:0.3528718616878372\n",
      "train loss:0.7482272357462214\n",
      "train loss:0.2555648992777776\n",
      "train loss:0.7034141567807132\n",
      "train loss:0.4735951904957421\n",
      "train loss:0.7218719492726534\n",
      "train loss:0.8022533075234616\n",
      "train loss:0.6152449038712424\n",
      "train loss:0.5024495834152886\n",
      "train loss:0.708437727847392\n",
      "train loss:0.5237983549670818\n",
      "train loss:0.5992659140685789\n",
      "train loss:0.5487993488955472\n",
      "train loss:0.6190985270521665\n",
      "train loss:0.6937228443329632\n",
      "train loss:0.6147022120003705\n",
      "train loss:0.6837931952217453\n",
      "train loss:0.6709167308704045\n",
      "train loss:0.6843739438541819\n",
      "train loss:0.6031391638147262\n",
      "train loss:0.747567362634968\n",
      "train loss:0.7250006884162027\n",
      "train loss:0.5824630222644256\n",
      "train loss:0.6731868113384817\n",
      "train loss:0.8157940975145561\n",
      "train loss:0.6022217833898846\n",
      "train loss:0.6296323899184455\n",
      "train loss:0.5591604220038714\n",
      "train loss:0.5441834402984325\n",
      "train loss:0.5681153535959906\n",
      "train loss:0.4953187298134475\n",
      "train loss:0.6908186625160326\n",
      "train loss:0.7202265370841369\n",
      "train loss:0.5975682715425586\n",
      "train loss:0.6274243748661525\n",
      "train loss:0.48064873197014385\n",
      "train loss:0.7137220735803717\n",
      "train loss:0.6218995586334112\n",
      "train loss:0.5026804117904535\n",
      "train loss:0.7214319602583262\n",
      "train loss:0.7140785348076426\n",
      "train loss:0.4206103375543718\n",
      "train loss:0.7136548301441101\n",
      "train loss:0.2964276265859512\n",
      "train loss:0.3997135736194526\n",
      "train loss:0.7136595946068109\n",
      "train loss:0.48890342342282683\n",
      "train loss:0.5074794732806406\n",
      "train loss:0.5151625470385659\n",
      "train loss:0.4894890579306811\n",
      "train loss:0.2375829854174917\n",
      "train loss:0.6132687288459496\n",
      "train loss:0.33021867093544743\n",
      "train loss:0.6481325434226195\n",
      "train loss:1.0670307273872932\n",
      "train loss:0.780253475528228\n",
      "train loss:0.49865640655850874\n",
      "train loss:0.4057161866980296\n",
      "train loss:0.50033500030369\n",
      "train loss:0.5911222214901705\n",
      "train loss:0.5945355840608225\n",
      "train loss:0.534556403470589\n",
      "train loss:0.421516750477578\n",
      "train loss:0.6960022632329801\n",
      "train loss:0.43041523791371716\n",
      "train loss:0.5197663568014891\n",
      "train loss:0.6945233545904169\n",
      "train loss:0.6343517389872116\n",
      "train loss:0.6342670065679423\n",
      "train loss:0.4146407982254511\n",
      "train loss:0.5089860015071672\n",
      "train loss:0.4903099302094283\n",
      "train loss:0.4790227059058513\n",
      "train loss:0.8416228348184953\n",
      "train loss:0.3895948314574662\n",
      "train loss:1.0363552423821047\n",
      "train loss:0.9155805997644721\n",
      "train loss:0.5325625262764595\n",
      "train loss:0.6203331891974775\n",
      "train loss:0.6195705708205347\n",
      "train loss:0.7442761241954511\n",
      "train loss:0.47909637540152916\n",
      "train loss:0.6846550578764065\n",
      "train loss:0.6447265751190877\n",
      "train loss:0.6221274389241437\n",
      "train loss:0.6767632090408557\n",
      "train loss:0.5926995862999528\n",
      "train loss:0.6251742051786057\n",
      "train loss:0.6191665464145643\n",
      "train loss:0.6274461541937619\n",
      "train loss:0.678850890232942\n",
      "train loss:0.6091082300380781\n",
      "train loss:0.8129796514396137\n",
      "train loss:0.4890330577244152\n",
      "train loss:0.5383647972878087\n",
      "train loss:0.6970509140132267\n",
      "train loss:0.528974765010204\n",
      "train loss:0.7832146772693195\n",
      "train loss:0.7646582311767451\n",
      "train loss:0.611461428680019\n",
      "train loss:0.7778968682402987\n",
      "train loss:0.38784495612750497\n",
      "train loss:0.7581220940234575\n",
      "train loss:0.5547859391368868\n",
      "train loss:0.5650586941210121\n",
      "train loss:0.8273685869758183\n",
      "train loss:0.730166338653841\n",
      "train loss:0.5618365848739639\n",
      "train loss:0.6060558089224279\n",
      "train loss:0.6224409463695088\n",
      "train loss:0.6070885680702099\n",
      "train loss:0.6249686088725239\n",
      "train loss:0.5445967113678958\n",
      "train loss:0.5374452264955842\n",
      "train loss:0.6702465491880416\n",
      "train loss:0.694103079718192\n",
      "train loss:0.5206865206918224\n",
      "train loss:0.5284567082481579\n",
      "train loss:0.6029576450675416\n",
      "train loss:0.5148579177033082\n",
      "train loss:0.6155616676223157\n",
      "train loss:0.49530474674405356\n",
      "train loss:0.8759822038752307\n",
      "train loss:0.5234468061075542\n",
      "train loss:0.723280700159841\n",
      "train loss:0.7290288149440046\n",
      "train loss:0.5125053213958599\n",
      "train loss:0.40628846083651987\n",
      "train loss:0.6288369279721694\n",
      "train loss:0.6241632009430973\n",
      "train loss:0.5978054448122595\n",
      "train loss:0.7694471163175262\n",
      "train loss:0.6044903205792069\n",
      "train loss:0.6314980863197638\n",
      "train loss:0.5507474905084966\n",
      "train loss:0.6994464670903767\n",
      "train loss:0.7534366698591627\n",
      "train loss:0.6757516217922641\n",
      "train loss:0.5220423268561958\n",
      "train loss:0.6807607496362751\n",
      "train loss:0.6851024031637147\n",
      "train loss:0.6893009024386094\n",
      "train loss:0.5720631549199571\n",
      "train loss:0.6692456691971216\n",
      "train loss:0.7096133321156678\n",
      "train loss:0.5862107384873876\n",
      "train loss:0.6857551464369902\n",
      "train loss:0.5165017438653187\n",
      "train loss:0.6622777156353006\n",
      "train loss:0.4665858441972765\n",
      "train loss:0.609125437609308\n",
      "train loss:0.5335041886752004\n",
      "train loss:0.5234271639607124\n",
      "train loss:0.8693073770970038\n",
      "train loss:0.8324218193811734\n",
      "train loss:0.6210663915513406\n",
      "train loss:0.6111807014275747\n",
      "train loss:0.5147149523910446\n",
      "train loss:0.617874667810662\n",
      "train loss:0.7028097438174103\n",
      "train loss:0.7924517988839598\n",
      "train loss:0.6854306848143861\n",
      "train loss:0.6016358141725291\n",
      "train loss:0.491328225875329\n",
      "train loss:0.42054036524207233\n",
      "train loss:0.6740886834894873\n",
      "train loss:0.5404525175309508\n",
      "train loss:0.5345306700717544\n",
      "train loss:0.6175214768906312\n",
      "train loss:0.707348657314458\n",
      "train loss:0.7880313598705714\n",
      "train loss:0.450662270480872\n",
      "train loss:0.6133557155741225\n",
      "train loss:0.42290669500368805\n",
      "train loss:0.6048148390488954\n",
      "train loss:0.6125849485710131\n",
      "train loss:0.5156025535418871\n",
      "train loss:0.7344347541055269\n",
      "train loss:0.3750412256749779\n",
      "train loss:0.49653969512435897\n",
      "train loss:0.6079297946450644\n",
      "train loss:0.6163476883581811\n",
      "train loss:0.4919701793214677\n",
      "train loss:0.6282073250466403\n",
      "train loss:0.6196336068083088\n",
      "train loss:0.6101546340970134\n",
      "train loss:0.37389370258492505\n",
      "train loss:0.3781056174819547\n",
      "train loss:0.5187470722071585\n",
      "train loss:0.6448969880731351\n",
      "train loss:0.5166131451925831\n",
      "train loss:0.355200607906089\n",
      "train loss:0.21246592141364026\n",
      "train loss:0.6147798006862466\n",
      "train loss:0.6079906140658753\n",
      "train loss:0.6922223704197549\n",
      "train loss:0.9164537735242545\n",
      "train loss:0.5012580629635528\n",
      "train loss:0.4965198265579128\n",
      "train loss:0.85845716831851\n",
      "train loss:0.8168553013699972\n",
      "train loss:0.6055154874405732\n",
      "train loss:0.4964030930892753\n",
      "train loss:0.5616926234449324\n",
      "train loss:0.6300163380061994\n",
      "train loss:0.6285217921064945\n",
      "train loss:0.6846519872768868\n",
      "train loss:0.6750916315137375\n",
      "train loss:0.6324271671277258\n",
      "train loss:0.48298321162416535\n",
      "train loss:0.5583867006055475\n",
      "train loss:0.6819802266046135\n",
      "train loss:0.5568359591932726\n",
      "train loss:0.47430159227224883\n",
      "train loss:0.3499290022498076\n",
      "train loss:0.39676262454581324\n",
      "train loss:0.24700894166307857\n",
      "train loss:0.3559233459002533\n",
      "train loss:0.48517956440260257\n",
      "train loss:0.7387623539390842\n",
      "train loss:0.5561777593931062\n",
      "train loss:0.7857231502800249\n",
      "train loss:0.8984112211145371\n",
      "train loss:0.543607890899892\n",
      "train loss:0.4921133431249329\n",
      "train loss:0.9543555857030714\n",
      "train loss:0.6255575309728445\n",
      "train loss:0.8187809231131344\n",
      "train loss:0.6133737193972348\n",
      "train loss:0.4363364057792426\n",
      "train loss:0.5636530294206683\n",
      "train loss:0.5596774492171371\n",
      "train loss:0.5735255801327251\n",
      "train loss:0.5684608343860869\n",
      "train loss:0.631001843550711\n",
      "train loss:0.55216758187522\n",
      "train loss:0.5591112840073935\n",
      "train loss:0.622576790633183\n",
      "train loss:0.4463576073682887\n",
      "train loss:0.6162900545634358\n",
      "train loss:0.7126903386279737\n",
      "train loss:0.7256352758369381\n",
      "train loss:0.7140333338109167\n",
      "train loss:0.41919847798118737\n",
      "train loss:0.7089858125322633\n",
      "train loss:0.515181063351813\n",
      "train loss:0.49603472051072195\n",
      "train loss:0.6220173041890515\n",
      "train loss:0.408716975262966\n",
      "train loss:0.7159512926249668\n",
      "train loss:0.7149913460141513\n",
      "train loss:0.505039521261904\n",
      "train loss:0.5071613394340909\n",
      "train loss:0.3934339645507454\n",
      "train loss:0.7215793556557566\n",
      "train loss:0.6176349079470682\n",
      "train loss:0.3848957687301745\n",
      "train loss:0.9613413929727063\n",
      "train loss:0.29268679087695276\n",
      "train loss:0.38291998289361423\n",
      "train loss:0.6370478168013869\n",
      "train loss:0.3874362052877141\n",
      "train loss:0.8742915917715244\n",
      "train loss:0.41488764869400896\n",
      "train loss:0.6156692768183614\n",
      "train loss:0.37852778999005554\n",
      "train loss:0.6170334736598677\n",
      "train loss:0.9445223786534338\n",
      "train loss:0.49903518782678846\n",
      "train loss:0.39989123371655355\n",
      "train loss:0.811746097198944\n",
      "train loss:0.5222159551709332\n",
      "train loss:0.6859062184788713\n",
      "train loss:0.45564735626705133\n",
      "train loss:0.4527837337734126\n",
      "train loss:0.6094194455896745\n",
      "train loss:0.7148208999343675\n",
      "train loss:0.5311183472634781\n",
      "train loss:0.5336929915318123\n",
      "train loss:0.6908534561118941\n",
      "train loss:0.4257085271523363\n",
      "train loss:0.41148445726869215\n",
      "train loss:0.787877337727572\n",
      "train loss:0.7238591465660741\n",
      "train loss:0.7886955909579088\n",
      "train loss:0.7068587255569557\n",
      "train loss:0.45015901977368344\n",
      "train loss:0.6172786207284748\n",
      "train loss:0.6329084168843192\n",
      "train loss:0.5161026212431404\n",
      "train loss:0.618497887621001\n",
      "train loss:0.6990091789670112\n",
      "train loss:0.6180955782670636\n",
      "train loss:0.6088156516801859\n",
      "train loss:0.5319931560807509\n",
      "train loss:0.6291846554291629\n",
      "train loss:0.7613470463017068\n",
      "train loss:0.7478489376398783\n",
      "train loss:0.5518996512765119\n",
      "train loss:0.47367991437023305\n",
      "train loss:0.5451619476686068\n",
      "train loss:0.8510044305514404\n",
      "train loss:0.6814524453108562\n",
      "train loss:0.6125328384693202\n",
      "train loss:0.5459296397956763\n",
      "train loss:0.8658971681967383\n",
      "train loss:0.5497228531840886\n",
      "train loss:0.6234401745802474\n",
      "train loss:0.6858572598422021\n",
      "train loss:0.5587729603917817\n",
      "train loss:0.6248366279239874\n",
      "train loss:0.6734227785591929\n",
      "train loss:0.7208229592901019\n",
      "train loss:0.627986419007226\n",
      "train loss:0.6146235530659164\n",
      "train loss:0.6242001348621062\n",
      "train loss:0.549805395757506\n",
      "train loss:0.5508456878877199\n",
      "train loss:0.5227526026130451\n",
      "train loss:0.4340316137148668\n",
      "train loss:0.6065308269151519\n",
      "train loss:0.613773815962338\n",
      "train loss:0.25253071752344936\n",
      "train loss:0.5035984858980564\n",
      "train loss:0.9182612898780596\n",
      "train loss:0.465927019680829\n",
      "train loss:0.9283164095383022\n",
      "train loss:0.4850733141588286\n",
      "train loss:0.6264792696281989\n",
      "train loss:0.6372934720553861\n",
      "train loss:0.7421449840670572\n",
      "train loss:0.715137151892039\n",
      "train loss:0.7822679181559574\n",
      "train loss:0.5320076089685732\n",
      "train loss:0.552774483351416\n",
      "train loss:0.5532847810106528\n",
      "train loss:0.7415624519096261\n",
      "train loss:0.6172996567949437\n",
      "train loss:0.6282028993111497\n",
      "train loss:0.4729647417021784\n",
      "train loss:0.6198695137288575\n",
      "train loss:0.692574118867382\n",
      "train loss:0.6759286958477666\n",
      "train loss:0.63090629190458\n",
      "train loss:0.5636065148975307\n",
      "train loss:0.622169282197765\n",
      "train loss:0.6903186392648444\n",
      "train loss:0.5458216241002548\n",
      "train loss:0.5351927172645945\n",
      "train loss:0.604199923586427\n",
      "train loss:0.6167579829048124\n",
      "train loss:0.7014440147826437\n",
      "train loss:0.793619234666107\n",
      "train loss:0.5092375678751132\n",
      "train loss:0.531385683482816\n",
      "train loss:0.4188322728466364\n",
      "train loss:0.6105595594466728\n",
      "train loss:0.3896801653399947\n",
      "train loss:0.848952803699126\n",
      "train loss:0.6264060286032225\n",
      "train loss:0.6288770843548226\n",
      "train loss:0.5208602041998129\n",
      "train loss:0.8396926194823742\n",
      "train loss:0.5081957352097193\n",
      "train loss:0.6212335084982518\n",
      "train loss:0.7850298076968028\n",
      "train loss:0.6073015545742987\n",
      "train loss:0.6090193603474493\n",
      "train loss:0.5384623793700027\n",
      "train loss:0.391095600992566\n",
      "train loss:0.5956259344289839\n",
      "train loss:0.5318866118981347\n",
      "train loss:0.5230386830953829\n",
      "train loss:0.619168904173143\n",
      "train loss:0.3086964077158904\n",
      "train loss:0.7101692101624227\n",
      "train loss:0.7256194035899749\n",
      "train loss:0.26740287156140047\n",
      "train loss:0.7892612715921009\n",
      "train loss:0.634736058829969\n",
      "train loss:0.3615516504113464\n",
      "train loss:0.6267675900830139\n",
      "train loss:0.47883955633801867\n",
      "train loss:0.9093562226077815\n",
      "train loss:0.36754216059100286\n",
      "train loss:0.5044964087228683\n",
      "train loss:0.5320027123023887\n",
      "train loss:0.502770163802049\n",
      "train loss:0.7359756569494422\n",
      "train loss:0.6269686965709905\n",
      "train loss:0.6983274546142423\n",
      "train loss:0.41386687386584453\n",
      "train loss:0.5051633493774017\n",
      "train loss:0.41759340589143923\n",
      "train loss:0.8936042342356405\n",
      "train loss:0.5243095772991436\n",
      "train loss:0.41700849230164216\n",
      "train loss:0.5281761902514874\n",
      "train loss:0.523288230652421\n",
      "train loss:0.5089912976430372\n",
      "train loss:0.5035226790216937\n",
      "train loss:0.3887197874808987\n",
      "train loss:0.38410793924031844\n",
      "train loss:0.48174968077952707\n",
      "train loss:0.33297546665307837\n",
      "train loss:1.099671541640567\n",
      "train loss:0.656683173049873\n",
      "train loss:0.6581021049528054\n",
      "train loss:0.6504576154834514\n",
      "train loss:0.7264256611914346\n",
      "train loss:0.6135137625718181\n",
      "train loss:0.50844276456147\n",
      "train loss:0.609296542431847\n",
      "train loss:0.43445878334266547\n",
      "train loss:0.4329364341974542\n",
      "train loss:0.7942860279604537\n",
      "train loss:0.7632454357744124\n",
      "train loss:0.6706049217778554\n",
      "train loss:0.6270372843464742\n",
      "train loss:0.5523725165047697\n",
      "train loss:0.5519521251453885\n",
      "train loss:0.7445426398300665\n",
      "train loss:0.7390351196374583\n",
      "train loss:0.5597705787892069\n",
      "train loss:0.668853520681792\n",
      "train loss:0.5028588666964224\n",
      "train loss:0.4950540141838336\n",
      "train loss:0.6895205515820024\n",
      "train loss:0.534521615208823\n",
      "train loss:0.7603463864464785\n",
      "train loss:0.6944661547360182\n",
      "train loss:0.45258078672513785\n",
      "train loss:0.777593315567871\n",
      "train loss:0.4358623039398698\n",
      "train loss:0.3402118145463638\n",
      "train loss:0.4010855935716641\n",
      "train loss:0.38822786118258296\n",
      "train loss:0.9884615149562889\n",
      "train loss:0.6037112765599013\n",
      "train loss:0.6267956589318358\n",
      "train loss:0.37516493045286803\n",
      "train loss:0.3680884003056904\n",
      "train loss:0.4957777322603694\n",
      "train loss:0.9258444418343025\n",
      "train loss:0.5171986740183102\n",
      "train loss:0.38367877035805076\n",
      "train loss:0.7719725638639202\n",
      "train loss:0.7293807611203731\n",
      "train loss:0.38461554724619096\n",
      "train loss:0.5054028835390845\n",
      "train loss:0.4979317898054706\n",
      "train loss:0.4977351164530114\n",
      "train loss:0.6319520771812361\n",
      "train loss:0.6022136398920773\n",
      "train loss:0.695694298534269\n",
      "train loss:0.513180717336805\n",
      "train loss:0.6098071611947259\n",
      "train loss:0.7106748914438983\n",
      "train loss:0.7031216123114559\n",
      "train loss:0.6074032054222155\n",
      "train loss:0.6100814449851691\n",
      "train loss:0.54243549434437\n",
      "train loss:0.6199710422052207\n",
      "train loss:0.6224945277241548\n",
      "train loss:0.47530326654346455\n",
      "train loss:0.4665841647015664\n",
      "train loss:0.7694149216829069\n",
      "train loss:0.6137626529655904\n",
      "train loss:0.6003452102513158\n",
      "train loss:0.7061713996342134\n",
      "train loss:0.5263859309508612\n",
      "train loss:0.5120764319574671\n",
      "train loss:0.6193604224924052\n",
      "train loss:0.6887639303484576\n",
      "train loss:0.7873119871348115\n",
      "train loss:0.6099816903164001\n",
      "train loss:0.5141352357127046\n",
      "train loss:0.5162174288205228\n",
      "train loss:0.7161048828099402\n",
      "train loss:0.5270660178406958\n",
      "train loss:0.5275110258568827\n",
      "train loss:0.3153808230226726\n",
      "train loss:0.29540266660305897\n",
      "train loss:0.7487833897731411\n",
      "train loss:0.8799395137155471\n",
      "train loss:0.6202888812718528\n",
      "train loss:0.741005278451025\n",
      "train loss:0.5226444794409554\n",
      "train loss:0.6045911706100717\n",
      "train loss:0.6099925298950879\n",
      "train loss:0.4156020374403287\n",
      "train loss:0.403878538522444\n",
      "train loss:0.3965607602879615\n",
      "train loss:0.6324594164839409\n",
      "train loss:0.9497860843745819\n",
      "train loss:0.5094566694319183\n",
      "train loss:0.6283653585397033\n",
      "train loss:0.6001895027199887\n",
      "train loss:0.4020844811721682\n",
      "train loss:0.5069243510072668\n",
      "train loss:0.7130891930916179\n",
      "train loss:0.6099024691621728\n",
      "train loss:0.5138128063496606\n",
      "train loss:0.49991082227110983\n",
      "train loss:0.7185671261814188\n",
      "train loss:0.5181535685959471\n",
      "train loss:0.7126266830708504\n",
      "train loss:0.5153266327625982\n",
      "train loss:0.608331747297732\n",
      "train loss:0.7142063042675162\n",
      "train loss:0.6026252134731973\n",
      "train loss:0.7045526074853969\n",
      "train loss:0.5315327821630468\n",
      "train loss:0.4492602482856758\n",
      "train loss:0.6834410340898568\n",
      "train loss:0.4447881353643428\n",
      "train loss:0.5350644727960889\n",
      "train loss:0.42355678011282294\n",
      "train loss:0.7305620263644991\n",
      "train loss:0.514010400363978\n",
      "train loss:0.5101893921640469\n",
      "train loss:0.7165106994862\n",
      "train loss:0.7418982000861817\n",
      "train loss:0.5170101942847974\n",
      "train loss:0.6177966719931277\n",
      "train loss:0.5217589548955385\n",
      "train loss:0.60829561138916\n",
      "train loss:0.6166831191647668\n",
      "train loss:0.6259530550516474\n",
      "train loss:0.6963007124699214\n",
      "train loss:1.038970121512252\n",
      "train loss:0.5414422144247434\n",
      "train loss:0.4897632781598472\n",
      "train loss:0.48305686346305754\n",
      "train loss:0.6769147305437533\n",
      "train loss:0.6149887527426415\n",
      "train loss:0.5551938848929099\n",
      "train loss:0.6826087240615909\n",
      "train loss:0.5548158821205837\n",
      "train loss:0.541649054096604\n",
      "train loss:0.7523237552460424\n",
      "train loss:0.6096729591835907\n",
      "train loss:0.6107554628716639\n",
      "train loss:0.6848899512534306\n",
      "train loss:0.6816990953122072\n",
      "train loss:0.6832439560882472\n",
      "train loss:0.6078105156491098\n",
      "train loss:0.7578238277260991\n",
      "train loss:0.6071413375284445\n",
      "train loss:0.6871332247557229\n",
      "train loss:0.5457360202747276\n",
      "train loss:0.4007506601272911\n",
      "train loss:0.6831940980467672\n",
      "train loss:0.5366770671908787\n",
      "train loss:0.44170131091739256\n",
      "train loss:0.5074489048022759\n",
      "train loss:0.6030686050881225\n",
      "train loss:0.39512639830274665\n",
      "train loss:0.7601581106010967\n",
      "train loss:0.49926164581048216\n",
      "train loss:0.6334274417725964\n",
      "train loss:1.1095008878571175\n",
      "train loss:0.5052677182792203\n",
      "train loss:0.6067985970704747\n",
      "train loss:0.5120040701914383\n",
      "train loss:0.5020561423577166\n",
      "train loss:0.40790375079840224\n",
      "train loss:0.5069002745859453\n",
      "train loss:0.6202176835617894\n",
      "train loss:0.8153696642921924\n",
      "train loss:0.7164971118498148\n",
      "train loss:0.6094171549265736\n",
      "train loss:0.42744841036714887\n",
      "train loss:0.6987431324276265\n",
      "train loss:0.6898368312888186\n",
      "train loss:0.7625921616803772\n",
      "train loss:0.47339477401902297\n",
      "train loss:0.5362822210323391\n",
      "train loss:0.8142181142384958\n",
      "train loss:0.6220137271683048\n",
      "train loss:0.6192090044203399\n",
      "train loss:0.6882746960278018\n",
      "train loss:0.6161868067933121\n",
      "train loss:0.5582773995323972\n",
      "train loss:0.7346674164389662\n",
      "train loss:0.6208835393532178\n",
      "train loss:0.6165174470100362\n",
      "train loss:0.6191277583009406\n",
      "train loss:0.5520674235550755\n",
      "train loss:0.8006409681809578\n",
      "train loss:0.6189719087621688\n",
      "train loss:0.618941824004341\n",
      "train loss:0.613103260211889\n",
      "train loss:0.6067085885238525\n",
      "train loss:0.6233119615523367\n",
      "train loss:0.5335669148586691\n",
      "train loss:0.6812728866213958\n",
      "train loss:0.7651332597096994\n",
      "train loss:0.6016634517948354\n",
      "train loss:0.7601748887852411\n",
      "train loss:0.4577008263910264\n",
      "train loss:0.6924889881971085\n",
      "train loss:0.6047943858219228\n",
      "train loss:0.5310493090337851\n",
      "train loss:0.68509242408105\n",
      "train loss:0.6024012490264213\n",
      "train loss:0.35850375097366494\n",
      "train loss:0.5185193357422285\n",
      "train loss:0.6993099609816745\n",
      "train loss:0.9793140632201579\n",
      "train loss:0.7096790028162893\n",
      "train loss:0.7768475836075756\n",
      "train loss:0.4656475145661637\n",
      "train loss:0.7451399016091489\n",
      "train loss:0.5624494237739409\n",
      "train loss:0.6807057093866207\n",
      "train loss:0.6816182103683218\n",
      "train loss:0.6678667202768456\n",
      "train loss:0.6277018784543907\n",
      "train loss:0.6760404695522881\n",
      "train loss:0.6244540991145648\n",
      "train loss:0.6245252153800914\n",
      "train loss:0.6248682343486466\n",
      "train loss:0.668472462283371\n",
      "train loss:0.6292930937322605\n",
      "train loss:0.5106291291315198\n",
      "train loss:0.6778019435611301\n",
      "train loss:0.6822164458159314\n",
      "train loss:0.5515510500383229\n",
      "train loss:0.45990448978262577\n",
      "train loss:0.5205171910762859\n",
      "train loss:0.694385153511298\n",
      "train loss:0.7196598911998348\n",
      "train loss:0.6113237983093199\n",
      "train loss:0.7157365914154035\n",
      "train loss:0.8184225075720508\n",
      "train loss:0.6039677559735661\n",
      "train loss:0.5199800044830034\n",
      "train loss:0.4200461064715223\n",
      "train loss:0.4094621356749137\n",
      "train loss:0.6127113924965186\n",
      "train loss:0.8224826852889008\n",
      "train loss:0.6200703285893294\n",
      "train loss:0.5078465942657427\n",
      "train loss:0.31238456492679834\n",
      "train loss:0.4949043439934572\n",
      "train loss:0.8307560274237737\n",
      "train loss:0.4986299424374123\n",
      "train loss:0.3841652959551217\n",
      "train loss:0.6172909840423777\n",
      "train loss:0.8523887655285671\n",
      "train loss:0.7123231819463588\n",
      "train loss:0.5069412518178754\n",
      "train loss:0.4035004288719878\n",
      "train loss:0.6245613861473588\n",
      "train loss:0.4002827103272226\n",
      "train loss:0.5972618317646136\n",
      "train loss:0.6136667656146733\n",
      "train loss:0.6127199932359506\n",
      "train loss:0.6096040986340615\n",
      "train loss:0.6284670968799354\n",
      "train loss:0.6036708535205588\n",
      "train loss:0.39531260075888264\n",
      "train loss:0.523730965240587\n",
      "train loss:0.7190369304806882\n",
      "train loss:0.8171056390744192\n",
      "train loss:0.3238586005390735\n",
      "train loss:0.5084464926108903\n",
      "train loss:0.5258392049120039\n",
      "train loss:0.5008864595380059\n",
      "train loss:0.5276838304590806\n",
      "train loss:0.718615917110621\n",
      "train loss:0.6157857917481194\n",
      "train loss:0.616820983653114\n",
      "train loss:0.7063089222071968\n",
      "train loss:0.7086908982574129\n",
      "train loss:0.5261093726246532\n",
      "train loss:0.5119663019345607\n",
      "train loss:0.5060159942936927\n",
      "train loss:0.7039331027757796\n",
      "train loss:0.5218340516652399\n",
      "train loss:0.5190158437162348\n",
      "train loss:0.8754807654301239\n",
      "train loss:0.536127110189111\n",
      "train loss:0.5151427971968918\n",
      "train loss:0.6967051195626305\n",
      "train loss:0.4394888601586132\n",
      "train loss:0.6977804098075854\n",
      "train loss:0.6120050038496727\n",
      "train loss:0.6109034292165569\n",
      "train loss:0.5191494518112238\n",
      "train loss:0.6109646297648673\n",
      "train loss:0.5973306638307985\n",
      "train loss:0.7787605749976118\n",
      "train loss:0.6975757589815579\n",
      "train loss:0.8428332043488818\n",
      "train loss:0.5368510567508504\n",
      "train loss:0.5514769130902757\n",
      "train loss:0.5390331963695427\n",
      "train loss:0.5525306891545904\n",
      "train loss:0.5423109981670645\n",
      "train loss:0.6153044687545719\n",
      "train loss:0.5278291362632436\n",
      "train loss:0.5226711313995788\n",
      "train loss:0.5199727961682602\n",
      "train loss:0.4149510569679104\n",
      "train loss:0.7097873613770946\n",
      "train loss:0.5068510501207638\n",
      "train loss:0.6393102017840105\n",
      "train loss:0.5038745243112277\n",
      "train loss:0.491664085467273\n",
      "train loss:0.630653703537328\n",
      "train loss:0.5133706286577061\n",
      "train loss:0.4872100594062515\n",
      "train loss:0.5081177735672905\n",
      "train loss:0.6229154258238431\n",
      "train loss:0.6244725643393493\n",
      "train loss:0.617500304568628\n",
      "train loss:0.5030656911265176\n",
      "train loss:0.2518091607833803\n",
      "train loss:0.49088728190070785\n",
      "train loss:0.5002976209979655\n",
      "train loss:0.8802427381582598\n",
      "train loss:0.7234687475743014\n",
      "train loss:0.6278248289532089\n",
      "train loss:0.40698879264280874\n",
      "train loss:0.7961798494245297\n",
      "train loss:0.7744508295754046\n",
      "train loss:0.6870288412171114\n",
      "train loss:0.7910917662585816\n",
      "train loss:0.5756962083028716\n",
      "train loss:0.5270282545556618\n",
      "train loss:0.6807402206081526\n",
      "train loss:0.5880157818877187\n",
      "train loss:0.6312793894133335\n",
      "train loss:0.5323572032363438\n",
      "train loss:0.6772962246537012\n",
      "train loss:0.7267696768258605\n",
      "train loss:0.7733814744975451\n",
      "train loss:0.7229321955620849\n",
      "train loss:0.5800197331584431\n",
      "train loss:0.622020771960319\n",
      "train loss:0.5696515941257545\n",
      "train loss:0.6163702891231313\n",
      "train loss:0.6189648356561037\n",
      "train loss:0.6821094674886897\n",
      "train loss:0.6821529150624055\n",
      "train loss:0.5513190729578287\n",
      "train loss:0.6756510887608735\n",
      "train loss:0.461956494206712\n",
      "train loss:0.5156466016728681\n",
      "train loss:0.6966213622024322\n",
      "train loss:0.7883140244751647\n",
      "train loss:0.6110121772131334\n",
      "train loss:0.7810376086830211\n",
      "train loss:0.3429405869242189\n",
      "train loss:0.6008103250611637\n",
      "train loss:0.5164594117015617\n",
      "train loss:0.7309066158689823\n",
      "train loss:0.7062184212471514\n",
      "train loss:0.5065776761995999\n",
      "train loss:0.8002318603242669\n",
      "train loss:0.42774431931852386\n",
      "train loss:0.873996147209593\n",
      "train loss:0.6134284403560722\n",
      "train loss:0.6738982790410741\n",
      "train loss:0.48256175727693806\n",
      "train loss:0.6758845322284903\n",
      "train loss:0.544695270505594\n",
      "train loss:0.6693380104544908\n",
      "train loss:0.4755600884242033\n",
      "train loss:0.4583004768705967\n",
      "train loss:0.6147691995701317\n",
      "train loss:0.6944902694658573\n",
      "train loss:0.6125321474867171\n",
      "train loss:0.41300544516998994\n",
      "train loss:0.704305045717945\n",
      "train loss:0.3982753479565374\n",
      "train loss:0.5006351451465286\n",
      "train loss:0.6496878682758676\n",
      "train loss:0.23632920780287375\n",
      "train loss:0.791877869396141\n",
      "train loss:0.5090866538818031\n",
      "train loss:0.33646644459340236\n",
      "train loss:0.3431617752940696\n",
      "train loss:0.6816984896721177\n",
      "train loss:0.6661721245092862\n",
      "train loss:0.49149234606585424\n",
      "train loss:0.5126677131149637\n",
      "train loss:0.4834554620089289\n",
      "train loss:0.5132507892503272\n",
      "train loss:0.6423688328102979\n",
      "train loss:0.8969363247851898\n",
      "train loss:0.62629869873199\n",
      "train loss:0.6128948258882414\n",
      "train loss:0.42360494284592126\n",
      "train loss:0.6092164925607441\n",
      "train loss:0.6040473581582646\n",
      "train loss:0.6787438653154331\n",
      "train loss:0.5406554030287151\n",
      "train loss:0.612786593637129\n",
      "train loss:0.6652905673118151\n",
      "train loss:0.5527154644571572\n",
      "train loss:0.4129039150005923\n",
      "train loss:0.5429115968493072\n",
      "train loss:0.5251209886063661\n",
      "train loss:0.700380587504458\n",
      "train loss:0.6131917972836886\n",
      "train loss:0.7797601091901607\n",
      "train loss:0.51249903623846\n",
      "train loss:0.5134080417024125\n",
      "train loss:0.5155828806892526\n",
      "train loss:0.39688704638836386\n",
      "train loss:0.7273845258156559\n",
      "train loss:0.5044603298361029\n",
      "train loss:0.8413534942948127\n",
      "train loss:0.5948297561341913\n",
      "train loss:0.3869840512557964\n",
      "train loss:0.5013866288825999\n",
      "train loss:0.6241514756477178\n",
      "train loss:0.6097203926407251\n",
      "train loss:0.5170425365330475\n",
      "train loss:0.8382561855935935\n",
      "train loss:0.7126432291910726\n",
      "train loss:0.710352216489994\n",
      "train loss:0.4415923340864296\n",
      "train loss:0.5435469252508913\n",
      "train loss:0.5888871861136378\n",
      "train loss:0.7827443342240646\n",
      "train loss:0.6737196817890141\n",
      "train loss:0.7969829231351306\n",
      "train loss:0.7848538897084336\n",
      "train loss:0.5844534535147072\n",
      "train loss:0.5384925598438687\n",
      "train loss:0.5860093044131832\n",
      "train loss:0.5866244376681987\n",
      "train loss:0.5720964633764669\n",
      "train loss:0.6251320697538876\n",
      "train loss:0.6218997391184236\n",
      "train loss:0.6227149046528516\n",
      "train loss:0.5449571256213981\n",
      "train loss:0.5441967625783385\n",
      "train loss:0.5223188943629212\n",
      "train loss:0.5214700001728724\n",
      "train loss:0.808809970053122\n",
      "train loss:0.4893195512280853\n",
      "train loss:0.4940208244458454\n",
      "train loss:0.9371146987661325\n",
      "train loss:0.49134393096099566\n",
      "train loss:0.5144496983825457\n",
      "train loss:0.7608314390952098\n",
      "train loss:0.59623674899864\n",
      "train loss:0.7070922148878556\n",
      "train loss:0.39725573098881706\n",
      "train loss:0.695056966169042\n",
      "train loss:0.5132752120773183\n",
      "train loss:0.5232773897077246\n",
      "train loss:0.6121695871303389\n",
      "train loss:0.5175836845745488\n",
      "train loss:0.3114643412013066\n",
      "train loss:0.6178712593072466\n",
      "train loss:0.6261420219014949\n",
      "train loss:0.6192869600978643\n",
      "train loss:0.6049160629323906\n",
      "train loss:0.7254143623176492\n",
      "train loss:0.5177384657890212\n",
      "train loss:0.6974996836296027\n",
      "train loss:0.5156735446216659\n",
      "train loss:0.5249004160225703\n",
      "train loss:0.5956672506050072\n",
      "train loss:0.4131749630838784\n",
      "train loss:0.4110516188956659\n",
      "train loss:0.8346663450172098\n",
      "train loss:0.6055214143131626\n",
      "train loss:0.7164684592324542\n",
      "train loss:0.7933009050421809\n",
      "train loss:0.7043538540777371\n",
      "train loss:0.4487527043216457\n",
      "train loss:0.3628170946464283\n",
      "train loss:0.7010192289117612\n",
      "train loss:0.6908609376528275\n",
      "train loss:0.5546432923393809\n",
      "train loss:0.44824423084153864\n",
      "train loss:0.527375905517949\n",
      "train loss:0.6233364852062417\n",
      "train loss:0.5166530072860054\n",
      "train loss:0.41225507547980217\n",
      "train loss:0.5050192058072736\n",
      "train loss:0.7351174609452735\n",
      "train loss:0.24926130725718507\n",
      "train loss:0.628494348769402\n",
      "train loss:0.630426719575823\n",
      "train loss:0.6531916221322251\n",
      "train loss:0.6443024418146319\n",
      "train loss:0.3645178681904743\n",
      "train loss:0.8907703284574826\n",
      "train loss:0.637330480102696\n",
      "train loss:0.7283982692924009\n",
      "train loss:0.6252638818595492\n",
      "train loss:0.616738055242036\n",
      "train loss:0.6072596338819205\n",
      "train loss:0.5342963512342213\n",
      "train loss:0.6061987044260274\n",
      "train loss:0.6104422953405166\n",
      "train loss:0.6786335048329424\n",
      "train loss:0.68044338199995\n",
      "train loss:0.7433377672510663\n",
      "train loss:0.6233099645195135\n",
      "train loss:0.6160908067811988\n",
      "train loss:0.7650204463690291\n",
      "train loss:0.5730386886502946\n",
      "train loss:0.6310620567745704\n",
      "train loss:0.4732501058613683\n",
      "train loss:0.626933105504409\n",
      "train loss:0.6287285096608553\n",
      "train loss:0.7307717364205037\n",
      "train loss:0.617693208959008\n",
      "train loss:0.6797154518505756\n",
      "train loss:0.5417574479601139\n",
      "train loss:0.5403680368946235\n",
      "train loss:0.6080186362960938\n",
      "train loss:0.6860792409355493\n",
      "train loss:0.610890515436643\n",
      "train loss:0.5015562350260229\n",
      "train loss:0.620528623738632\n",
      "train loss:0.6109053918394017\n",
      "train loss:0.6168456784365688\n",
      "train loss:0.6236628429376088\n",
      "train loss:0.39760142599247955\n",
      "train loss:0.7258001932680229\n",
      "train loss:0.5146947365096302\n",
      "train loss:0.48926967755720197\n",
      "train loss:0.9389552862986553\n",
      "train loss:0.615188102308705\n",
      "train loss:0.618544292379685\n",
      "train loss:0.42225555417675303\n",
      "train loss:0.516292417513097\n",
      "train loss:0.5223746119572931\n",
      "train loss:0.6867222245465399\n",
      "train loss:0.886269452402116\n",
      "train loss:0.5193549836120877\n",
      "train loss:0.7658115222698456\n",
      "train loss:0.5401574113268105\n",
      "train loss:0.6130982246829574\n",
      "train loss:0.40472814465710866\n",
      "train loss:0.4540777482686692\n",
      "train loss:0.4405267633546365\n",
      "train loss:0.6033296298784937\n",
      "train loss:0.5149091366881576\n",
      "train loss:0.8075571457731628\n",
      "train loss:0.48664726413719867\n",
      "train loss:0.613866315090457\n",
      "train loss:0.4927452552505785\n",
      "train loss:0.5096746557747639\n",
      "train loss:0.5109573474494943\n",
      "train loss:0.7485386139796695\n",
      "train loss:0.37024223852728505\n",
      "train loss:0.49525970167077854\n",
      "train loss:0.5015226062696463\n",
      "train loss:0.6463551089808914\n",
      "train loss:0.7597536732999873\n",
      "train loss:0.5025340025421219\n",
      "train loss:0.6187255576698043\n",
      "train loss:0.625369743487721\n",
      "train loss:0.5001154286730852\n",
      "train loss:0.6215170297589963\n",
      "train loss:0.512699201075495\n",
      "train loss:0.5141858729417129\n",
      "train loss:0.6103084234853963\n",
      "train loss:0.5195182410177728\n",
      "train loss:0.7199215140198894\n",
      "train loss:0.522246642958687\n",
      "train loss:0.4236738989690137\n",
      "train loss:0.6102064947537537\n",
      "train loss:0.6185413178136807\n",
      "train loss:0.7947354886814356\n",
      "train loss:0.5992946591739851\n",
      "train loss:0.4294559240426127\n",
      "train loss:0.7876282028202928\n",
      "train loss:0.5199391974736997\n",
      "train loss:0.440840553337693\n",
      "train loss:0.5271269793515121\n",
      "train loss:0.42875322318593867\n",
      "train loss:0.5089509596774925\n",
      "train loss:0.5036468524088805\n",
      "train loss:0.6235090647842778\n",
      "train loss:0.6293950657488574\n",
      "train loss:0.5059013894043626\n",
      "train loss:0.8636602654959271\n",
      "train loss:0.7195739329193838\n",
      "train loss:0.7109753735297388\n",
      "train loss:0.40019681383455136\n",
      "train loss:0.30446579749889147\n",
      "train loss:0.98223478836545\n",
      "train loss:0.6075953875506194\n",
      "train loss:0.4356539983894649\n",
      "train loss:0.602346438309052\n",
      "train loss:0.8329029545827196\n",
      "train loss:0.6130777983887308\n",
      "train loss:0.536303498522506\n",
      "train loss:0.6198680981865415\n",
      "train loss:0.47819922882868776\n",
      "train loss:0.6086013423393041\n",
      "train loss:0.6130707954823702\n",
      "train loss:0.6788263449816372\n",
      "train loss:0.5154325610933637\n",
      "train loss:0.6186417079422932\n",
      "train loss:0.6025478329792825\n",
      "train loss:0.5162101050005325\n",
      "train loss:0.4240988396533482\n",
      "train loss:0.4120387156089098\n",
      "train loss:0.3823211434867254\n",
      "train loss:0.7594380901654196\n",
      "train loss:0.5110845964953983\n",
      "train loss:0.6258851690693126\n",
      "train loss:0.7645469012675401\n",
      "train loss:0.6315247903198631\n",
      "train loss:0.6365266761422274\n",
      "train loss:0.6216411810929087\n",
      "train loss:0.7269832123994837\n",
      "train loss:0.6144539887197759\n",
      "train loss:0.7876834217457438\n",
      "train loss:0.6099246246438732\n",
      "train loss:0.5395142652713585\n",
      "train loss:0.7468951008586997\n",
      "train loss:0.5667827421274889\n",
      "train loss:0.7381710455143737\n",
      "train loss:0.5922280966114737\n",
      "train loss:0.7692621358433993\n",
      "train loss:0.5859073141387381\n",
      "train loss:0.6324774824245198\n",
      "train loss:0.7512668289325575\n",
      "train loss:0.7109048180804793\n",
      "train loss:0.614370157724545\n",
      "train loss:0.5481800562370577\n",
      "train loss:0.670159357313481\n",
      "train loss:0.7176382270174565\n",
      "train loss:0.502308924371337\n",
      "train loss:0.7217505026344272\n",
      "train loss:0.6191498887323782\n",
      "train loss:0.6629786147665454\n",
      "train loss:0.5006206411741808\n",
      "train loss:0.6211038617199229\n",
      "train loss:0.5914128706921064\n",
      "train loss:0.6800783788330395\n",
      "train loss:0.3525330773265274\n",
      "train loss:0.5044956674639581\n",
      "train loss:0.6240581032677363\n",
      "train loss:0.7295951706048173\n",
      "train loss:0.5023172006915515\n",
      "train loss:0.5027100557793422\n",
      "train loss:0.21454944067928836\n",
      "train loss:0.4888148844664726\n",
      "train loss:0.9515128213806548\n",
      "train loss:0.49478025390417135\n",
      "train loss:0.6530027427066551\n",
      "train loss:0.6318296249712233\n",
      "train loss:0.4934147947753661\n",
      "train loss:0.6272037841151867\n",
      "train loss:0.6271555557683128\n",
      "train loss:0.271596475463087\n",
      "train loss:0.620067221373884\n",
      "train loss:0.49414219067177845\n",
      "train loss:0.7243619854406008\n",
      "train loss:0.625296498761619\n",
      "train loss:0.4158857154963056\n",
      "train loss:0.5139476163112039\n",
      "train loss:0.7156676730194109\n",
      "train loss:0.5042836124089568\n",
      "train loss:0.41628208421902074\n",
      "train loss:0.7123678767552416\n",
      "train loss:0.6092729003785365\n",
      "train loss:0.49514611496354854\n",
      "train loss:0.4145143653377959\n",
      "train loss:0.7190671037214854\n",
      "train loss:0.40778540172727284\n",
      "train loss:0.5075310916539422\n",
      "train loss:0.7123058179586172\n",
      "train loss:0.7059824633945064\n",
      "train loss:0.598189374510962\n",
      "train loss:0.801190974643655\n",
      "train loss:0.3046028852438345\n",
      "train loss:0.5098546835312436\n",
      "train loss:0.5165543227165375\n",
      "train loss:0.5078157660323135\n",
      "train loss:0.4033770106320088\n",
      "train loss:0.5025897602698771\n",
      "train loss:0.6236710431959981\n",
      "train loss:0.48404631678948684\n",
      "train loss:0.4963549276630509\n",
      "train loss:0.7577947656619151\n",
      "train loss:0.6004291495996854\n",
      "train loss:0.7223602240835497\n",
      "train loss:0.5020087390092492\n",
      "train loss:0.6235278972640387\n",
      "train loss:0.49075250235405293\n",
      "train loss:0.5890435382208277\n",
      "train loss:0.5099466870972955\n",
      "train loss:0.3038488599497252\n",
      "train loss:0.49799263200329846\n",
      "train loss:0.6006053043904304\n",
      "train loss:0.374442339439253\n",
      "train loss:0.7098229149782196\n",
      "train loss:0.8462645313880509\n",
      "train loss:0.5909552874165004\n",
      "train loss:0.8188206985970494\n",
      "train loss:0.7003706559725986\n",
      "train loss:0.5146494684257619\n",
      "train loss:0.5260243227893852\n",
      "train loss:0.6068330845080329\n",
      "train loss:0.6063608220580327\n",
      "train loss:0.4618999524047724\n",
      "train loss:0.5273900392924038\n",
      "train loss:0.5301994708832022\n",
      "train loss:0.5813212176993667\n",
      "train loss:0.5905963142218973\n",
      "train loss:0.3214795192694016\n",
      "train loss:0.37492266633870086\n",
      "train loss:0.37883689280033905\n",
      "train loss:0.8820883776612783\n",
      "train loss:0.600133121403975\n",
      "train loss:0.8553935894527754\n",
      "train loss:0.23159625431049363\n",
      "train loss:0.6083303956178155\n",
      "train loss:0.22188749574231298\n",
      "train loss:0.3608695354374477\n",
      "train loss:0.48336422233152004\n",
      "train loss:0.5114035496659882\n",
      "train loss:0.4510267226113087\n",
      "train loss:0.6842476073039491\n",
      "train loss:0.9681642634078967\n",
      "train loss:0.7131259113682409\n",
      "train loss:0.23156137891738152\n",
      "train loss:0.5902547043641815\n",
      "train loss:0.3990307010326058\n",
      "train loss:0.47053269022717403\n",
      "train loss:0.5038696952090584\n",
      "train loss:0.41165924746767074\n",
      "train loss:0.613045505166328\n",
      "train loss:0.36233730692900706\n",
      "train loss:0.49700565605688096\n",
      "train loss:0.5055592228365926\n",
      "train loss:0.7191023792716387\n",
      "train loss:0.6120355690614113\n",
      "train loss:0.7314724842720892\n",
      "train loss:0.7995822997462374\n",
      "train loss:0.6229294499777038\n",
      "train loss:0.5957890771730471\n",
      "train loss:0.6944712668903293\n",
      "train loss:0.5702479718581792\n",
      "train loss:0.45479676792883517\n",
      "train loss:0.8287081148018884\n",
      "train loss:0.5362823588524066\n",
      "train loss:0.5541743495050893\n",
      "train loss:0.6893987471230634\n",
      "train loss:0.675633524255123\n",
      "train loss:0.5400576162399574\n",
      "train loss:0.5463991593478544\n",
      "train loss:0.7326706020513648\n",
      "train loss:0.7056133232573445\n",
      "train loss:0.7641346546632928\n",
      "train loss:0.6866669715516693\n",
      "train loss:0.6504913543717116\n",
      "train loss:0.7116037820622161\n",
      "train loss:0.6298004648645887\n",
      "train loss:0.5305610919490598\n",
      "train loss:0.6645251745969181\n",
      "train loss:0.6293656876958573\n",
      "train loss:0.610101093233915\n",
      "train loss:0.4620533770585702\n",
      "train loss:0.7493455141024323\n",
      "train loss:0.6793650331515539\n",
      "train loss:0.4489301727339563\n",
      "train loss:0.5983369987212697\n",
      "train loss:0.3001718576835223\n",
      "train loss:0.6238564900828025\n",
      "train loss:0.38495559646093314\n",
      "train loss:0.6312903812430823\n",
      "train loss:0.6511070451268429\n",
      "train loss:0.7873227917635678\n",
      "train loss:0.4771753391440255\n",
      "train loss:0.6361063901637947\n",
      "train loss:0.9036434685942215\n",
      "train loss:0.6133847089301627\n",
      "train loss:0.4048921271854435\n",
      "train loss:0.4993777551004787\n",
      "train loss:0.6259178995004856\n",
      "train loss:0.5046120707436776\n",
      "train loss:0.41940450269379503\n",
      "train loss:0.5201728383901552\n",
      "train loss:0.6117887562792205\n",
      "train loss:0.4080462003298179\n",
      "train loss:0.3392050495715122\n",
      "train loss:0.8144113543942153\n",
      "train loss:0.4775690480179865\n",
      "train loss:0.3776122406907262\n",
      "train loss:0.6178409179690445\n",
      "train loss:0.6857842250495924\n",
      "train loss:0.7440903518660611\n",
      "train loss:0.4896244844394576\n",
      "train loss:0.5885810246715172\n",
      "train loss:0.39998407367733846\n",
      "train loss:0.5221268021454655\n",
      "train loss:0.6183389621702642\n",
      "train loss:0.5992634239438372\n",
      "train loss:0.5693681590217803\n",
      "train loss:0.4126571623044354\n",
      "train loss:0.7211464774654183\n",
      "train loss:0.5219700680575972\n",
      "train loss:0.7177485950773083\n",
      "train loss:0.4980616501308628\n",
      "train loss:0.7116350619468856\n",
      "train loss:0.523409515778954\n",
      "train loss:0.7972505804124698\n",
      "train loss:0.6092960925613221\n",
      "train loss:0.5987486074402089\n",
      "train loss:0.45277467831886337\n",
      "train loss:0.6218354998404084\n",
      "train loss:0.4198069625505161\n",
      "train loss:0.5976906406877809\n",
      "train loss:0.7705532893057402\n",
      "train loss:0.5309900169752773\n",
      "train loss:0.4434313460120416\n",
      "train loss:0.6050599615694183\n",
      "train loss:0.6295324175764384\n",
      "train loss:0.3793083123907609\n",
      "train loss:0.2750136772736224\n",
      "train loss:0.22584592983497034\n",
      "train loss:0.5353031286783225\n",
      "train loss:0.49448595298455456\n",
      "train loss:0.49354570730084896\n",
      "train loss:0.5071168061856973\n",
      "train loss:0.4969122581162738\n",
      "train loss:0.7414697839951054\n",
      "train loss:0.14895246024913936\n",
      "train loss:0.874330685705206\n",
      "train loss:0.8822520992360603\n",
      "train loss:0.34584029227402335\n",
      "train loss:0.6465417822617612\n",
      "train loss:0.5981536526416354\n",
      "train loss:0.5052527270704823\n",
      "train loss:0.6307191323136697\n",
      "train loss:0.6201396038272324\n",
      "train loss:0.6843319738005887\n",
      "train loss:0.6629647685875615\n",
      "train loss:0.4918317771463597\n",
      "train loss:0.7518109043865311\n",
      "train loss:0.6327482921666616\n",
      "train loss:0.605444444814522\n",
      "train loss:0.5418181489364937\n",
      "train loss:0.5644566917501004\n",
      "train loss:0.4857105082760076\n",
      "train loss:0.6279015099147637\n",
      "train loss:0.731443640721664\n",
      "train loss:0.621090004390978\n",
      "train loss:0.5061958350952032\n",
      "train loss:0.5148624477924099\n",
      "train loss:0.5067630479357883\n",
      "train loss:0.28866492299227237\n",
      "train loss:0.6048660656209129\n",
      "train loss:0.5095647100905656\n",
      "train loss:0.6567658547970094\n",
      "train loss:0.456335597032093\n",
      "train loss:1.2153769162539165\n",
      "train loss:0.3647352830053938\n",
      "train loss:0.5354779060343343\n",
      "train loss:0.9372214426646744\n",
      "train loss:0.7559318220379543\n",
      "train loss:0.6215121945501584\n",
      "train loss:0.6116950815183131\n",
      "train loss:0.8059729333572507\n",
      "train loss:0.7104853878156533\n",
      "train loss:0.5332536014423291\n",
      "train loss:0.6389432499291829\n",
      "train loss:0.51545719816225\n",
      "train loss:0.7212418662050288\n",
      "train loss:0.6550354845176392\n",
      "train loss:0.6244849523362979\n",
      "train loss:0.6629491495561293\n",
      "train loss:0.6145311644723558\n",
      "train loss:0.5837941290261359\n",
      "train loss:0.6023618743441038\n",
      "train loss:0.7021360131140306\n",
      "train loss:0.6576814578308079\n",
      "train loss:0.5902466268971784\n",
      "train loss:0.5342598305132815\n",
      "train loss:0.4572770559586747\n",
      "train loss:0.5798329625556905\n",
      "train loss:0.758160489067351\n",
      "train loss:0.6355534271608474\n",
      "train loss:0.4997025217302385\n",
      "train loss:0.6859453460705769\n",
      "train loss:0.6925350897903714\n",
      "train loss:0.5399496695074243\n",
      "train loss:0.8975585883499256\n",
      "train loss:0.6903204224848457\n",
      "train loss:0.494161788021979\n",
      "train loss:0.5424526100789849\n",
      "train loss:0.6084225605065725\n",
      "train loss:0.5923387480106131\n",
      "train loss:0.4279161318313494\n",
      "train loss:0.47496058291141424\n",
      "train loss:0.8251580731488705\n",
      "train loss:0.6400899285601123\n",
      "train loss:0.5327454701264217\n",
      "train loss:0.5997574992978347\n",
      "train loss:0.46596627303114807\n",
      "train loss:0.4395327357030686\n",
      "train loss:0.38616579022117953\n",
      "train loss:0.6089005973799084\n",
      "train loss:0.6054150428310043\n",
      "train loss:0.6356833573275098\n",
      "train loss:0.316141376740983\n",
      "train loss:1.027845295847458\n",
      "train loss:0.4962209160169496\n",
      "train loss:0.6365054941432742\n",
      "train loss:0.5024848008383597\n",
      "train loss:0.5782622652070419\n",
      "train loss:0.622900980673749\n",
      "train loss:0.574903424662\n",
      "train loss:0.6574402675852056\n",
      "train loss:0.7554294188639002\n",
      "train loss:0.6139433678578765\n",
      "train loss:0.6805498032713507\n",
      "train loss:0.6732567389469333\n",
      "train loss:0.7163068881884296\n",
      "train loss:0.5905741565491693\n",
      "train loss:0.6698662075124587\n",
      "train loss:0.615138289316076\n",
      "train loss:0.5627637264227175\n",
      "train loss:0.6293719750652957\n",
      "train loss:0.4888947086170652\n",
      "train loss:0.6382461614735955\n",
      "train loss:0.6064639677132773\n",
      "train loss:0.5988035001749584\n",
      "train loss:0.7150625513228329\n",
      "train loss:0.579054528507699\n",
      "train loss:0.5688542989112324\n",
      "train loss:0.6130005508148939\n",
      "train loss:0.6016279841030823\n",
      "train loss:0.5718459371935627\n",
      "train loss:0.7031050988569798\n",
      "train loss:0.6098022028000186\n",
      "train loss:0.908296545673099\n",
      "train loss:0.6572401624867308\n",
      "train loss:0.5869957387182889\n",
      "train loss:0.44861547926910533\n",
      "train loss:0.5024163176304859\n",
      "train loss:0.520067444148595\n",
      "train loss:0.5379766050016995\n",
      "train loss:0.42415182851732086\n",
      "train loss:0.514203062989513\n",
      "train loss:0.273708141121393\n",
      "train loss:0.6397324217251122\n",
      "train loss:0.816952401120479\n",
      "train loss:0.8407401230623737\n",
      "train loss:0.36925765202772115\n",
      "train loss:0.6708783296399095\n",
      "train loss:0.7660741840150938\n",
      "train loss:0.4959087936060781\n",
      "train loss:0.6159550126851162\n",
      "train loss:0.7084810412831856\n",
      "train loss:0.5768874436799762\n",
      "train loss:0.3672444663386264\n",
      "train loss:0.5835056310508093\n",
      "train loss:0.6604088201260861\n",
      "train loss:0.3897328706928757\n",
      "train loss:0.7039271741638288\n",
      "train loss:0.5179712255944587\n",
      "train loss:0.6096525917005104\n",
      "train loss:0.7535781231987755\n",
      "train loss:0.6952882320220922\n",
      "train loss:0.5485396881021526\n",
      "train loss:0.5929122225648796\n",
      "train loss:0.45638719626200946\n",
      "train loss:0.6614517820089204\n",
      "train loss:0.6630855734354333\n",
      "train loss:0.4502112471443386\n",
      "train loss:0.4682249840462216\n",
      "train loss:0.5725140158021151\n",
      "train loss:0.8427010399733099\n",
      "train loss:0.6000217995508054\n",
      "train loss:0.81171519559117\n",
      "train loss:0.7031654332858402\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6617647058823529\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bef4c5bc-62b7-4f43-bd3f-eb923bf007c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet5Layer:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = int((input_size - filter_size + 2 * filter_pad) / filter_stride + 1)\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b3'] = np.zeros(hidden_size)        \n",
    "        self.params['W4'] = weight_init_std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b4'] = np.zeros(hidden_size)\n",
    "        self.params['W5'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b5'] = np.zeros(output_size)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.layers['Affine3'] = Affine(self.params['W4'], self.params['b4'])\n",
    "        self.layers['Affine4'] = Affine(self.params['W5'], self.params['b5'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3,4,5):\n",
    "        # for idx in (1, 2, 3,4,5):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        grads['W4'], grads['b4'] = self.layers['Affine3'].dW, self.layers['Affine3'].db\n",
    "        grads['W5'], grads['b5'] = self.layers['Affine4'].dW, self.layers['Affine4'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af74f96d-2361-45c8-b2a2-980e637564bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.300298718590973\n",
      "=== epoch:1, train acc:0.7, test acc:0.65 ===\n",
      "train loss:2.297649240616901\n",
      "train loss:2.2954909359783167\n",
      "train loss:2.29121456549611\n",
      "train loss:2.285370069035045\n",
      "train loss:2.275604011666046\n",
      "train loss:2.252248165203401\n",
      "train loss:2.2202715478866635\n",
      "train loss:2.175352080370364\n",
      "train loss:2.044328361775077\n",
      "train loss:1.8948495206536413\n",
      "train loss:1.7007444769644693\n",
      "train loss:1.374749340073309\n",
      "train loss:0.989747188805946\n",
      "train loss:0.7353986025497822\n",
      "train loss:0.6354294920267323\n",
      "train loss:0.4999146224374235\n",
      "train loss:0.5780262115786929\n",
      "train loss:1.2471999051716132\n",
      "train loss:0.29939264819476896\n",
      "train loss:0.7836296373345997\n",
      "train loss:0.6168812337884446\n",
      "train loss:0.7120390510488065\n",
      "train loss:0.7348575934042839\n",
      "train loss:0.768463046653386\n",
      "train loss:0.9126951725947405\n",
      "train loss:0.6977971622430833\n",
      "train loss:0.7017946147292363\n",
      "train loss:0.685082671986686\n",
      "train loss:0.3527086817808157\n",
      "train loss:0.858737118513262\n",
      "train loss:0.5569937020553581\n",
      "train loss:0.6748951658522659\n",
      "train loss:0.3549779168259037\n",
      "train loss:0.509302259895069\n",
      "train loss:0.5074026728322683\n",
      "train loss:0.7966262540400327\n",
      "train loss:0.6893383874794402\n",
      "train loss:0.7037167034010349\n",
      "train loss:0.5011628894441749\n",
      "train loss:0.5756068237660024\n",
      "train loss:0.7123495255792515\n",
      "train loss:0.5532259028379749\n",
      "train loss:0.5192985935106961\n",
      "train loss:0.5510348997989667\n",
      "train loss:0.512085503826574\n",
      "train loss:0.6963329479630047\n",
      "train loss:0.4008710957544439\n",
      "train loss:0.6253449067192322\n",
      "train loss:0.7787662267097388\n",
      "train loss:0.7925441133548391\n",
      "train loss:0.3952226716853827\n",
      "train loss:1.125098406281307\n",
      "train loss:0.5920479706095443\n",
      "train loss:0.5566142029303978\n",
      "train loss:0.5497899083027857\n",
      "train loss:0.4720217701747503\n",
      "train loss:0.6049353306540464\n",
      "train loss:0.6093898128981922\n",
      "train loss:0.6169415956743439\n",
      "train loss:0.736071188458191\n",
      "train loss:0.6101752524191999\n",
      "train loss:0.4920567737252036\n",
      "train loss:0.6880296141763301\n",
      "train loss:0.558930447492261\n",
      "train loss:0.5243191570170842\n",
      "train loss:0.7171303380836848\n",
      "train loss:0.6124048533392129\n",
      "train loss:0.3201324924517904\n",
      "train loss:0.7943897517546926\n",
      "train loss:0.8337497182317923\n",
      "train loss:0.8917819631830287\n",
      "train loss:0.6636791437734292\n",
      "train loss:0.6478026842274294\n",
      "train loss:0.5628239304187862\n",
      "train loss:0.6435281827544358\n",
      "train loss:0.7353493102064513\n",
      "train loss:0.47573527759652234\n",
      "train loss:0.5096482458735556\n",
      "train loss:0.7253452316965037\n",
      "train loss:0.7377166435721956\n",
      "train loss:0.6284257227392636\n",
      "train loss:0.5658621321865159\n",
      "train loss:0.6817085884350729\n",
      "train loss:0.5251496532817068\n",
      "train loss:0.6210012162586475\n",
      "train loss:0.5979004481379843\n",
      "train loss:0.40667174289133445\n",
      "train loss:0.5505838208196934\n",
      "train loss:0.503033602457706\n",
      "train loss:0.2040000844091569\n",
      "train loss:0.49279256931426135\n",
      "train loss:0.5135325502606602\n",
      "train loss:0.3321728847639508\n",
      "train loss:0.4801667235948971\n",
      "train loss:0.4862936036736098\n",
      "train loss:1.0059497774522939\n",
      "train loss:0.8454943212390497\n",
      "train loss:0.9051864439138104\n",
      "train loss:0.512012272491123\n",
      "train loss:0.3603460256385039\n",
      "train loss:0.5392117880776678\n",
      "train loss:0.5459637090245737\n",
      "train loss:0.6090850430078258\n",
      "train loss:0.6183428748163373\n",
      "train loss:0.5130594452638856\n",
      "train loss:0.61804903722916\n",
      "train loss:0.6749855541326383\n",
      "train loss:0.6272471620229095\n",
      "train loss:0.4084416276576651\n",
      "train loss:0.6734963484009363\n",
      "train loss:0.44210035661763014\n",
      "train loss:0.28258053568212277\n",
      "train loss:0.7523421312422581\n",
      "train loss:0.3509626031406622\n",
      "train loss:0.9389202436035825\n",
      "train loss:0.32922454015902536\n",
      "train loss:0.502006962902711\n",
      "train loss:0.4905979360114231\n",
      "train loss:0.5141282151833244\n",
      "train loss:0.510891878560361\n",
      "train loss:0.6333633721226776\n",
      "train loss:0.8253160214700456\n",
      "train loss:0.7383954622412509\n",
      "train loss:0.4235680971096535\n",
      "train loss:0.8291126316565715\n",
      "train loss:0.7004639163434117\n",
      "train loss:0.7945413756074551\n",
      "train loss:0.5889157425828857\n",
      "train loss:0.7017939759728994\n",
      "train loss:0.7010733251008643\n",
      "train loss:0.6979608068903185\n",
      "train loss:0.6390790081819997\n",
      "train loss:0.7179942442972205\n",
      "train loss:0.6978187654725315\n",
      "train loss:0.6702516780014381\n",
      "train loss:0.6838525070994292\n",
      "train loss:0.7530217195278202\n",
      "train loss:0.6687524400584325\n",
      "train loss:0.6809958772557289\n",
      "train loss:0.6598199836908281\n",
      "train loss:0.7029509320829077\n",
      "train loss:0.6746132144780173\n",
      "train loss:0.6785414982917327\n",
      "train loss:0.5643530111514801\n",
      "train loss:0.5243799061034228\n",
      "train loss:0.6231978565806392\n",
      "train loss:0.418218775744777\n",
      "train loss:0.7168502132033884\n",
      "train loss:0.5023206471648937\n",
      "train loss:0.558878943788376\n",
      "train loss:0.6389526336063731\n",
      "train loss:0.5345578855695107\n",
      "train loss:0.6777261391259921\n",
      "train loss:0.5057281005797873\n",
      "train loss:0.7706504368666633\n",
      "train loss:0.5498230759288286\n",
      "train loss:0.8136781231395627\n",
      "train loss:0.6178272202073634\n",
      "train loss:0.46741461034825027\n",
      "train loss:0.3866946879421432\n",
      "train loss:0.4673889473199571\n",
      "train loss:0.3707571338999549\n",
      "train loss:0.7059148414736213\n",
      "train loss:0.8755844389627626\n",
      "train loss:0.6026541866504613\n",
      "train loss:0.8109218667512834\n",
      "train loss:0.5457039134885483\n",
      "train loss:0.5531708192579881\n",
      "train loss:0.5583638472525141\n",
      "train loss:0.5580036444718498\n",
      "train loss:0.6002343597674293\n",
      "train loss:0.6036299567104115\n",
      "train loss:0.689304451486682\n",
      "train loss:0.44384718928999706\n",
      "train loss:0.4261610455543786\n",
      "train loss:0.27721730572988224\n",
      "train loss:0.5736654791891496\n",
      "train loss:1.0736035133971549\n",
      "train loss:0.23281230398238523\n",
      "train loss:0.48735572939667604\n",
      "train loss:0.6670415723756502\n",
      "train loss:0.8985070566537889\n",
      "train loss:0.5014050286651492\n",
      "train loss:0.2649757083934471\n",
      "train loss:0.7645435091799098\n",
      "train loss:0.7875008093827303\n",
      "train loss:0.6347977461556651\n",
      "train loss:0.6106407451518547\n",
      "train loss:0.6302947593750707\n",
      "train loss:0.6822718320283058\n",
      "train loss:0.5779837352133067\n",
      "train loss:0.4479873011061472\n",
      "train loss:0.5602548208476147\n",
      "train loss:0.5448616326134219\n",
      "train loss:0.4758764433288434\n",
      "train loss:0.5117371312348784\n",
      "train loss:0.4179805514515885\n",
      "train loss:0.47452230342936785\n",
      "train loss:0.3407208687320133\n",
      "train loss:0.3663128982566234\n",
      "train loss:0.3543619336035932\n",
      "train loss:1.008691423467573\n",
      "train loss:0.5631020209180296\n",
      "train loss:0.3459009305591186\n",
      "train loss:0.6842940790438155\n",
      "train loss:0.8162222731909404\n",
      "train loss:0.36293369402800313\n",
      "train loss:0.730848943928421\n",
      "train loss:0.29888866367750244\n",
      "train loss:0.7046381801894054\n",
      "train loss:0.5186237603601777\n",
      "train loss:0.8918256292826168\n",
      "train loss:0.6152517661896579\n",
      "train loss:0.6966696465543941\n",
      "train loss:0.6015556443999126\n",
      "train loss:0.6954223074815448\n",
      "train loss:0.549910770821043\n",
      "train loss:0.6419281901478742\n",
      "train loss:0.541049504862176\n",
      "train loss:0.568754637777338\n",
      "train loss:0.6111301057391818\n",
      "train loss:0.6272008177014982\n",
      "train loss:0.5995648877328166\n",
      "train loss:0.5287926998653643\n",
      "train loss:0.6047685291468101\n",
      "train loss:0.6423388018711481\n",
      "train loss:0.4877235706194362\n",
      "train loss:0.7424868310039967\n",
      "train loss:0.6029190467774496\n",
      "train loss:0.7626704191174448\n",
      "train loss:0.8336862522183098\n",
      "train loss:0.9217229746948219\n",
      "train loss:0.5510014439415925\n",
      "train loss:0.4513977660979065\n",
      "train loss:0.5745634067222083\n",
      "train loss:0.751743868920469\n",
      "train loss:0.5931524795290256\n",
      "train loss:0.4908112663392485\n",
      "train loss:0.5186009314609054\n",
      "train loss:0.5651320642593418\n",
      "train loss:0.47551878920142876\n",
      "train loss:0.6094428367945446\n",
      "train loss:0.884058754067833\n",
      "train loss:0.6971307546792324\n",
      "train loss:0.7820260310176542\n",
      "train loss:0.6173048995388324\n",
      "train loss:0.6873012817950356\n",
      "train loss:0.6010065451249664\n",
      "train loss:0.603358319340075\n",
      "train loss:0.44685532819591633\n",
      "train loss:0.5313169818284573\n",
      "train loss:0.6158736385178087\n",
      "train loss:0.4387772145864467\n",
      "train loss:0.40717479528207656\n",
      "train loss:0.5201007628124019\n",
      "train loss:0.5000855075683027\n",
      "train loss:0.9114826970824893\n",
      "train loss:0.6373290084434557\n",
      "train loss:0.8684671632120198\n",
      "train loss:1.001107691813813\n",
      "train loss:0.4431971871012489\n",
      "train loss:0.6105785672412504\n",
      "train loss:0.5046047116553589\n",
      "train loss:0.6195578316907762\n",
      "train loss:0.585904181046787\n",
      "train loss:0.6190381484458995\n",
      "train loss:0.6807491850091644\n",
      "train loss:0.6315607776408607\n",
      "train loss:0.5110486066113074\n",
      "train loss:0.4829712092337063\n",
      "train loss:0.5457407285589326\n",
      "train loss:0.6076645321537916\n",
      "train loss:0.8904190571003678\n",
      "train loss:0.3220512475612319\n",
      "train loss:0.8103488461327008\n",
      "train loss:0.406429882289031\n",
      "train loss:0.3976280993397269\n",
      "train loss:0.2303908298078996\n",
      "train loss:0.5013928767170946\n",
      "train loss:0.8518337050625122\n",
      "train loss:0.5115423410076775\n",
      "train loss:0.6562189726817999\n",
      "train loss:0.3356451263601088\n",
      "train loss:0.3598625127000262\n",
      "train loss:0.7032732073533663\n",
      "train loss:0.6443012975105811\n",
      "train loss:0.9829167286840835\n",
      "train loss:0.5071157803733168\n",
      "train loss:0.614531169519094\n",
      "train loss:0.4727412996155408\n",
      "train loss:0.7479120540847769\n",
      "train loss:0.5618353580967307\n",
      "train loss:0.6327651807842465\n",
      "train loss:0.6765884996488292\n",
      "train loss:0.6314061612250506\n",
      "train loss:0.6320636447085227\n",
      "train loss:0.5875293617596261\n",
      "train loss:0.7178119893891147\n",
      "train loss:0.6787674143603096\n",
      "train loss:0.5829407045204943\n",
      "train loss:0.5121450904338565\n",
      "train loss:0.4272567907015864\n",
      "train loss:0.613504523470364\n",
      "train loss:0.6196021687821839\n",
      "train loss:0.40797988108076577\n",
      "train loss:0.6103806224343149\n",
      "train loss:0.4843736008780142\n",
      "train loss:0.3184595756674818\n",
      "train loss:0.6758665599566063\n",
      "train loss:0.8539577195845689\n",
      "train loss:0.48250943263850826\n",
      "train loss:0.698840146556829\n",
      "train loss:0.48515577177150215\n",
      "train loss:0.491603102466139\n",
      "train loss:0.629938262131428\n",
      "train loss:0.5258246690990375\n",
      "train loss:0.8097996770700717\n",
      "train loss:0.5085241326253724\n",
      "train loss:0.6979191946516432\n",
      "train loss:0.4659334497296014\n",
      "train loss:0.617744390063325\n",
      "train loss:0.48437590635907696\n",
      "train loss:0.6173217455398177\n",
      "train loss:0.47439531319634465\n",
      "train loss:0.6100824354461418\n",
      "train loss:0.529896602141217\n",
      "train loss:0.33330412628618716\n",
      "train loss:0.7968531793539952\n",
      "train loss:0.595440805111294\n",
      "train loss:0.5160937522144609\n",
      "train loss:0.4996725504339672\n",
      "train loss:0.3699190188569161\n",
      "train loss:0.6288716832047403\n",
      "train loss:0.4943153802285771\n",
      "train loss:0.8231016106334149\n",
      "train loss:0.4983137071373059\n",
      "train loss:0.6353056420249366\n",
      "train loss:0.6542021230844448\n",
      "train loss:0.8463967820065228\n",
      "train loss:0.5095465399426246\n",
      "train loss:0.44557479097767166\n",
      "train loss:0.6902920708001193\n",
      "train loss:0.6279110198649718\n",
      "train loss:0.5495591182776456\n",
      "train loss:0.6278077912608675\n",
      "train loss:0.4117134733442939\n",
      "train loss:0.46455093309808004\n",
      "train loss:0.7530480336288454\n",
      "train loss:0.6189022325270396\n",
      "train loss:0.4405228598721934\n",
      "train loss:0.7163623356978203\n",
      "train loss:0.512348163355693\n",
      "train loss:0.7355727954780765\n",
      "train loss:0.29112633228762774\n",
      "train loss:0.37785124012068516\n",
      "train loss:1.0153890291679102\n",
      "train loss:0.3678419743468905\n",
      "train loss:0.7388841102918183\n",
      "train loss:0.3772851657610845\n",
      "train loss:0.48021081066666926\n",
      "train loss:0.7272921175482964\n",
      "train loss:0.7198686487808518\n",
      "train loss:0.4894855353491046\n",
      "train loss:0.5991363553555618\n",
      "train loss:0.5262503831163372\n",
      "train loss:0.7028897452910833\n",
      "train loss:0.698733018197105\n",
      "train loss:0.6111927996872734\n",
      "train loss:0.5310510627781926\n",
      "train loss:0.613640707005706\n",
      "train loss:0.5502989274134558\n",
      "train loss:0.534377549079619\n",
      "train loss:0.6112256425099479\n",
      "train loss:0.6190175674119754\n",
      "train loss:0.5238893085385918\n",
      "train loss:0.431482563856114\n",
      "train loss:0.30961100309363143\n",
      "train loss:0.49929447236993596\n",
      "train loss:0.49233254098337387\n",
      "train loss:0.6438852627967612\n",
      "train loss:0.6362257737515123\n",
      "train loss:0.6613791844116024\n",
      "train loss:0.6630239580719268\n",
      "train loss:0.49118583381859066\n",
      "train loss:0.8892370797463103\n",
      "train loss:0.3865976704221471\n",
      "train loss:0.2882411608797327\n",
      "train loss:0.7355981787818677\n",
      "train loss:0.6216772474259076\n",
      "train loss:0.8762534408605669\n",
      "train loss:0.5356714083261845\n",
      "train loss:0.6796001729855916\n",
      "train loss:0.5024624368282655\n",
      "train loss:0.7444149211286264\n",
      "train loss:0.626875375391881\n",
      "train loss:0.6833309216436733\n",
      "train loss:0.5883631147690944\n",
      "train loss:0.6384893799038739\n",
      "train loss:0.6302338955468204\n",
      "train loss:0.5176786196154537\n",
      "train loss:0.6752562695596247\n",
      "train loss:0.5539111069330143\n",
      "train loss:0.7484505635479302\n",
      "train loss:0.4607628658047361\n",
      "train loss:0.6782577835914643\n",
      "train loss:0.5154814372988644\n",
      "train loss:0.7214149127819508\n",
      "train loss:0.8032375765465852\n",
      "train loss:0.7932353090046093\n",
      "train loss:0.7686957433180109\n",
      "train loss:0.534027774252271\n",
      "train loss:0.4668065979057284\n",
      "train loss:0.5994603017926357\n",
      "train loss:0.4598176506880165\n",
      "train loss:0.6838732283152812\n",
      "train loss:0.7505359049351176\n",
      "train loss:0.822388151033292\n",
      "train loss:0.47272397137186595\n",
      "train loss:0.744774395451932\n",
      "train loss:0.8007800871704518\n",
      "train loss:0.7291610795876716\n",
      "train loss:0.5876757769127376\n",
      "train loss:0.5469617637646176\n",
      "train loss:0.482586801397863\n",
      "train loss:0.6797205252021603\n",
      "train loss:0.4456185720455933\n",
      "train loss:0.6110996354166922\n",
      "train loss:0.6970476490255448\n",
      "train loss:0.6064765680790594\n",
      "train loss:0.60637640810441\n",
      "train loss:0.6230184154111621\n",
      "train loss:0.7003566494917007\n",
      "train loss:0.6206524642178346\n",
      "train loss:0.6112160315037718\n",
      "train loss:0.41802191455032106\n",
      "train loss:0.3905483531250372\n",
      "train loss:0.6244112214774147\n",
      "train loss:0.3805471601281001\n",
      "train loss:0.35662811008020023\n",
      "train loss:0.16468168098806454\n",
      "train loss:0.5066860195907276\n",
      "train loss:0.31821098309474943\n",
      "train loss:0.7746875378052632\n",
      "train loss:0.08450609284648979\n",
      "train loss:0.5849391729655258\n",
      "train loss:0.9495268916880958\n",
      "train loss:0.8374396806540269\n",
      "train loss:0.6242132595647882\n",
      "train loss:0.3004637871472312\n",
      "train loss:0.5296943389010129\n",
      "train loss:0.4494551808520241\n",
      "train loss:0.5412527777279729\n",
      "train loss:0.8225734532728082\n",
      "train loss:0.7473248445849439\n",
      "train loss:0.5793455308916481\n",
      "train loss:0.5294375643042121\n",
      "train loss:0.5769137691713333\n",
      "train loss:0.6289498656018677\n",
      "train loss:0.6837507116175459\n",
      "train loss:0.7471276516841816\n",
      "train loss:0.4394333087677148\n",
      "train loss:0.4862053236033114\n",
      "train loss:0.6161746711544933\n",
      "train loss:0.5249340450846687\n",
      "train loss:0.42143704647284386\n",
      "train loss:0.6145758904212912\n",
      "train loss:0.6254030944257251\n",
      "train loss:0.7548794968901706\n",
      "train loss:0.7435394484167257\n",
      "train loss:0.6188829252194238\n",
      "train loss:0.7433640569467734\n",
      "train loss:0.4080405324686106\n",
      "train loss:0.6120737225944163\n",
      "train loss:0.40132319181483955\n",
      "train loss:0.8006548257816141\n",
      "train loss:0.6184680782850924\n",
      "train loss:0.6112861047974094\n",
      "train loss:0.8338504306280496\n",
      "train loss:0.5475392147594944\n",
      "train loss:0.6729872375267427\n",
      "train loss:0.6807855825856246\n",
      "train loss:0.6219368715218656\n",
      "train loss:0.5197851743996046\n",
      "train loss:0.7239982255234729\n",
      "train loss:0.7174880071378543\n",
      "train loss:0.534909367008545\n",
      "train loss:0.5794843133503432\n",
      "train loss:0.5659376120521765\n",
      "train loss:0.5001540217370198\n",
      "train loss:0.6101088334764142\n",
      "train loss:0.4389848427702785\n",
      "train loss:0.6920242225914681\n",
      "train loss:0.6208620412837322\n",
      "train loss:0.7133964368645961\n",
      "train loss:0.7197779576207235\n",
      "train loss:0.525552810751367\n",
      "train loss:0.6075373311703443\n",
      "train loss:0.505784684206866\n",
      "train loss:0.8647856013755897\n",
      "train loss:0.6183074154002693\n",
      "train loss:0.6968514469006774\n",
      "train loss:0.5271118628729052\n",
      "train loss:0.6834305707207208\n",
      "train loss:0.6808990478850262\n",
      "train loss:0.4806318370921546\n",
      "train loss:0.7484773663610877\n",
      "train loss:0.5557890983608093\n",
      "train loss:0.6191626437816252\n",
      "train loss:0.5500051995007602\n",
      "train loss:0.6754456892605075\n",
      "train loss:0.6751963348048203\n",
      "train loss:0.677450538231685\n",
      "train loss:0.5553265610894099\n",
      "train loss:0.6158013310082256\n",
      "train loss:0.4719111482682428\n",
      "train loss:0.531301217291918\n",
      "train loss:0.43588260754706776\n",
      "train loss:0.7087639620649155\n",
      "train loss:0.5001097549021248\n",
      "train loss:0.5026842141932713\n",
      "train loss:0.5015085097948051\n",
      "train loss:0.6411411015449946\n",
      "train loss:0.5071010579641516\n",
      "train loss:0.6387777261656002\n",
      "train loss:0.6508711100545703\n",
      "train loss:0.5004688156970335\n",
      "train loss:0.6468519574072221\n",
      "train loss:0.5028186473289527\n",
      "train loss:0.6393942735686291\n",
      "train loss:0.6124494762132516\n",
      "train loss:0.3935897666502678\n",
      "train loss:0.8116157880345215\n",
      "train loss:0.6894120700424706\n",
      "train loss:0.6089549308917991\n",
      "train loss:0.752195328836027\n",
      "train loss:0.6181836408279203\n",
      "train loss:0.6307703639404924\n",
      "train loss:0.628383393391377\n",
      "train loss:0.5806262802790674\n",
      "train loss:0.5770969420212502\n",
      "train loss:0.5794477550175294\n",
      "train loss:0.5124258284640979\n",
      "train loss:0.6724415864676854\n",
      "train loss:0.7363523922790645\n",
      "train loss:0.6143514729814393\n",
      "train loss:0.6874868028989116\n",
      "train loss:0.6832283309215741\n",
      "train loss:0.45895667689439035\n",
      "train loss:0.8418385015635164\n",
      "train loss:0.3722187213998909\n",
      "train loss:0.33094038183395263\n",
      "train loss:0.7001080854478181\n",
      "train loss:0.7160967698369143\n",
      "train loss:0.3807796723779403\n",
      "train loss:0.6227475240495541\n",
      "train loss:0.6415691631569246\n",
      "train loss:0.7393555786154409\n",
      "train loss:0.6027994806576584\n",
      "train loss:0.6277379869242627\n",
      "train loss:0.5314971262103814\n",
      "train loss:0.49646535051837304\n",
      "train loss:0.61354328731305\n",
      "train loss:0.2970532194784866\n",
      "train loss:0.8228192421807388\n",
      "train loss:0.41538954061205013\n",
      "train loss:0.6154561443796469\n",
      "train loss:0.5201039226947871\n",
      "train loss:0.62103923044982\n",
      "train loss:0.6221276755763944\n",
      "train loss:0.5040985325391409\n",
      "train loss:0.6257295643218056\n",
      "train loss:0.6043612443044769\n",
      "train loss:0.5159213412226797\n",
      "train loss:0.6175990962790465\n",
      "train loss:0.704592231082408\n",
      "train loss:0.6913606834377817\n",
      "train loss:0.6239633182499575\n",
      "train loss:0.6798465437485943\n",
      "train loss:0.5201623063848971\n",
      "train loss:0.6868528517212996\n",
      "train loss:0.6845890541375435\n",
      "train loss:0.5460716298889509\n",
      "train loss:0.415442274394118\n",
      "train loss:0.7473677644884067\n",
      "train loss:0.46722769747188353\n",
      "train loss:0.37179080242387447\n",
      "train loss:0.6183060220616726\n",
      "train loss:0.6140496716432502\n",
      "train loss:0.7212749358011473\n",
      "train loss:0.6242026793442261\n",
      "train loss:0.8016672140774249\n",
      "train loss:0.6124174685720709\n",
      "train loss:0.6072829446681937\n",
      "train loss:0.41378870583538785\n",
      "train loss:0.6070534049766749\n",
      "train loss:0.41379391776774427\n",
      "train loss:0.3976738344033757\n",
      "train loss:0.5008556349235627\n",
      "train loss:0.49797266550599656\n",
      "train loss:0.4927831555875506\n",
      "train loss:0.7983565328495892\n",
      "train loss:0.6318882432445421\n",
      "train loss:0.5149607067731802\n",
      "train loss:0.3631692339971786\n",
      "train loss:0.7689600543084597\n",
      "train loss:0.49036552346607226\n",
      "train loss:0.49699345221166674\n",
      "train loss:0.386274981731589\n",
      "train loss:0.6379115961151233\n",
      "train loss:0.48576679383600985\n",
      "train loss:0.5180538673984516\n",
      "train loss:0.7479547515011694\n",
      "train loss:0.5075442797916678\n",
      "train loss:0.5136677677249052\n",
      "train loss:0.6144387963893113\n",
      "train loss:0.39843464559619723\n",
      "train loss:0.6154682223792963\n",
      "train loss:0.7181780882954222\n",
      "train loss:0.39977081162934913\n",
      "train loss:0.6224388873663507\n",
      "train loss:0.3990299523037689\n",
      "train loss:0.8229500824696995\n",
      "train loss:0.29163959071324397\n",
      "train loss:0.9282891710603\n",
      "train loss:0.40648826203838617\n",
      "train loss:0.5143207283467858\n",
      "train loss:0.5126702370639094\n",
      "train loss:0.392932079078042\n",
      "train loss:0.6202341685802578\n",
      "train loss:0.38447343339481554\n",
      "train loss:0.6555750609921829\n",
      "train loss:0.49613961937318296\n",
      "train loss:0.5002678930545729\n",
      "train loss:0.631367319759548\n",
      "train loss:0.504806244108361\n",
      "train loss:0.20131874597027988\n",
      "train loss:0.5024918991703696\n",
      "train loss:0.6486321129849697\n",
      "train loss:0.4930560223173403\n",
      "train loss:0.34927184150476986\n",
      "train loss:0.9309290757555475\n",
      "train loss:0.4830063757967622\n",
      "train loss:0.3604801814561309\n",
      "train loss:0.6376816013513142\n",
      "train loss:0.5047277283632858\n",
      "train loss:0.4900386425917554\n",
      "train loss:0.5098586465356366\n",
      "train loss:0.2681557587603799\n",
      "train loss:0.6523980638870092\n",
      "train loss:0.2346416287460935\n",
      "train loss:0.7868517778855071\n",
      "train loss:0.981184029483015\n",
      "train loss:0.6147119224657434\n",
      "train loss:0.5139027641985051\n",
      "train loss:0.6072181558667471\n",
      "train loss:0.60568924237994\n",
      "train loss:0.6126614621576424\n",
      "train loss:0.6162662414554654\n",
      "train loss:0.6875195650698858\n",
      "train loss:0.6184935100171334\n",
      "train loss:0.4138704086243729\n",
      "train loss:0.5489035951495579\n",
      "train loss:0.6826539117783256\n",
      "train loss:0.4600826926559593\n",
      "train loss:0.6093546492982516\n",
      "train loss:0.419354022464404\n",
      "train loss:0.2852358700996028\n",
      "train loss:0.6357734586818724\n",
      "train loss:0.8756684792947869\n",
      "train loss:0.7697459556628303\n",
      "train loss:0.36920072963823913\n",
      "train loss:0.3569915308859057\n",
      "train loss:0.497846950603592\n",
      "train loss:0.4853018713863258\n",
      "train loss:0.648550798072\n",
      "train loss:0.18263123585113067\n",
      "train loss:0.17158876081908042\n",
      "train loss:0.676730982496639\n",
      "train loss:0.7213590827711338\n",
      "train loss:1.2152560355009607\n",
      "train loss:0.5071308039195289\n",
      "train loss:0.6984231859718399\n",
      "train loss:0.7570301064540391\n",
      "train loss:0.5660206227825904\n",
      "train loss:0.5947683357052649\n",
      "train loss:0.7355191034847184\n",
      "train loss:0.772096679350356\n",
      "train loss:0.612917075543892\n",
      "train loss:0.6407924907278233\n",
      "train loss:0.6985683784925468\n",
      "train loss:0.6245204472971844\n",
      "train loss:0.6494115712976986\n",
      "train loss:0.7127619338317607\n",
      "train loss:0.6407617081132929\n",
      "train loss:0.6715395471560341\n",
      "train loss:0.6294901206559903\n",
      "train loss:0.6211614317062597\n",
      "train loss:0.6247276970582929\n",
      "train loss:0.5597881992527813\n",
      "train loss:0.5406587280252207\n",
      "train loss:0.5190103420209595\n",
      "train loss:0.4058599900575346\n",
      "train loss:0.6286902296639639\n",
      "train loss:0.9073021118694333\n",
      "train loss:0.7547981122012166\n",
      "train loss:0.6014640914075212\n",
      "train loss:0.5039969271964043\n",
      "train loss:0.6144624124859446\n",
      "train loss:0.7250949445519054\n",
      "train loss:0.7800275561949094\n",
      "train loss:0.37204431059538196\n",
      "train loss:0.6744658027415478\n",
      "train loss:0.6135080471458748\n",
      "train loss:0.5521495409123388\n",
      "train loss:0.6773181844894898\n",
      "train loss:0.5498272704819573\n",
      "train loss:0.7892803434574016\n",
      "train loss:0.5623879769783642\n",
      "train loss:0.6772253978518974\n",
      "train loss:0.5599053104759201\n",
      "train loss:0.7877468262919616\n",
      "train loss:0.6752301945320783\n",
      "train loss:0.5661486341188986\n",
      "train loss:0.6236533617283125\n",
      "train loss:0.6805177906573082\n",
      "train loss:0.6223492851283898\n",
      "train loss:0.6757124961325707\n",
      "train loss:0.6799399894347522\n",
      "train loss:0.43321859685253444\n",
      "train loss:0.5449630501500509\n",
      "train loss:0.3615911682574099\n",
      "train loss:0.5164885717903196\n",
      "train loss:0.49624437292937074\n",
      "train loss:0.6513312096137073\n",
      "train loss:0.7917486336264455\n",
      "train loss:0.5114854208470491\n",
      "train loss:0.49316704496489755\n",
      "train loss:0.7688676791838993\n",
      "train loss:0.7471593572350211\n",
      "train loss:0.5105170205425684\n",
      "train loss:0.37737515669515914\n",
      "train loss:0.5125085616685839\n",
      "train loss:0.9179812004866292\n",
      "train loss:0.5237557845537911\n",
      "train loss:0.5327301507868368\n",
      "train loss:0.3426103155776385\n",
      "train loss:0.7807501468749443\n",
      "train loss:0.5285643560105215\n",
      "train loss:0.7702827728617159\n",
      "train loss:0.758136241306491\n",
      "train loss:0.6176650886989025\n",
      "train loss:0.6792785200048792\n",
      "train loss:0.7360866959457878\n",
      "train loss:0.5681532056381251\n",
      "train loss:0.5766921721140114\n",
      "train loss:0.5703891942057524\n",
      "train loss:0.5652637979013326\n",
      "train loss:0.7946134766234223\n",
      "train loss:0.6174968487778488\n",
      "train loss:0.5606940196502421\n",
      "train loss:0.5471218399871776\n",
      "train loss:0.613550466690954\n",
      "train loss:0.6811273897695724\n",
      "train loss:0.6095937277850634\n",
      "train loss:0.6841539738999441\n",
      "train loss:0.5300440374698854\n",
      "train loss:0.5250641807242006\n",
      "train loss:0.6929449227495185\n",
      "train loss:0.5070325015511854\n",
      "train loss:0.405415143690287\n",
      "train loss:0.38595921028906705\n",
      "train loss:0.49516825677660536\n",
      "train loss:1.057025840958849\n",
      "train loss:0.7442336804507496\n",
      "train loss:0.6087830590367094\n",
      "train loss:0.6180709924168305\n",
      "train loss:0.6152765772707613\n",
      "train loss:0.619400651695732\n",
      "train loss:0.6193122417675452\n",
      "train loss:0.5405448868501888\n",
      "train loss:0.5324340578555733\n",
      "train loss:0.6062220207680415\n",
      "train loss:0.6769671725346422\n",
      "train loss:0.6156817234242922\n",
      "train loss:0.5491918427278437\n",
      "train loss:0.6185002484512372\n",
      "train loss:0.5410416984741492\n",
      "train loss:0.5361743470927444\n",
      "train loss:0.6102355121348648\n",
      "train loss:0.6933834523509503\n",
      "train loss:0.6106993434767666\n",
      "train loss:0.5245344660824426\n",
      "train loss:0.6101953511307561\n",
      "train loss:0.29963727490079195\n",
      "train loss:0.7474980782166671\n",
      "train loss:0.6111653691066136\n",
      "train loss:0.6360931802049634\n",
      "train loss:0.6289262944563437\n",
      "train loss:0.26795471883285743\n",
      "train loss:0.49722216478744496\n",
      "train loss:0.5093736439072998\n",
      "train loss:0.49543903456989424\n",
      "train loss:1.0208625098154767\n",
      "train loss:0.7315860875892674\n",
      "train loss:0.8080191361861422\n",
      "train loss:0.5291344887228344\n",
      "train loss:0.6076445465755135\n",
      "train loss:0.6200039795040035\n",
      "train loss:0.5563284573010503\n",
      "train loss:0.7322835153845135\n",
      "train loss:0.5727009094912622\n",
      "train loss:0.7777574370984088\n",
      "train loss:0.5857418788595585\n",
      "train loss:0.584240852702559\n",
      "train loss:0.6308126792371969\n",
      "train loss:0.6293452424044512\n",
      "train loss:0.5737755902330659\n",
      "train loss:0.5092600213428679\n",
      "train loss:0.4883348118312405\n",
      "train loss:0.5412218137919397\n",
      "train loss:0.6898165793977294\n",
      "train loss:0.5190537887232659\n",
      "train loss:0.6032035732895504\n",
      "train loss:0.4969212018908329\n",
      "train loss:0.3723847947679334\n",
      "train loss:0.49405613074025806\n",
      "train loss:1.0586268181953484\n",
      "train loss:0.63397360381313\n",
      "train loss:0.6375389273626354\n",
      "train loss:0.37988283741999085\n",
      "train loss:0.6327837286700705\n",
      "train loss:0.7266027234848438\n",
      "train loss:0.4011126374711302\n",
      "train loss:0.6165314782005211\n",
      "train loss:0.4171734584802057\n",
      "train loss:0.5155706636289158\n",
      "train loss:0.7114368306289014\n",
      "train loss:0.5169396870225743\n",
      "train loss:0.5146056142648289\n",
      "train loss:0.30001338933167954\n",
      "train loss:0.615672807936377\n",
      "train loss:0.5078745552823527\n",
      "train loss:0.7452447852301339\n",
      "train loss:0.38041552404582535\n",
      "train loss:0.3613074564187258\n",
      "train loss:0.6420740889729792\n",
      "train loss:0.5059096153235602\n",
      "train loss:0.6358955496923453\n",
      "train loss:0.748600170967466\n",
      "train loss:0.6285089009162411\n",
      "train loss:0.5168119638592773\n",
      "train loss:0.6107119504944343\n",
      "train loss:0.5072988405920568\n",
      "train loss:0.4075073341464547\n",
      "train loss:0.6124910330338524\n",
      "train loss:0.5208449831354172\n",
      "train loss:0.6113885072818002\n",
      "train loss:0.611338059172894\n",
      "train loss:0.7120027451468383\n",
      "train loss:0.5160071105684592\n",
      "train loss:0.8690785746628373\n",
      "train loss:0.5306329973222631\n",
      "train loss:0.5263469730937264\n",
      "train loss:0.45231079513242933\n",
      "train loss:0.6120812992230571\n",
      "train loss:0.8498178733540941\n",
      "train loss:0.5380593862822771\n",
      "train loss:0.4566783849211733\n",
      "train loss:0.8349077652279222\n",
      "train loss:0.46034595577938464\n",
      "train loss:0.6102136510064474\n",
      "train loss:0.7719108729816065\n",
      "train loss:0.45008526248832903\n",
      "train loss:0.6125506675291573\n",
      "train loss:0.7699730566794593\n",
      "train loss:0.35654958068309367\n",
      "train loss:0.4324310783078742\n",
      "train loss:0.506954377552156\n",
      "train loss:0.8120355813214166\n",
      "train loss:0.6148990717210545\n",
      "train loss:0.7217863143099837\n",
      "train loss:0.288008328967189\n",
      "train loss:0.5056148935377773\n",
      "train loss:0.6212346117872471\n",
      "train loss:0.5145035956638918\n",
      "train loss:0.7503443775430341\n",
      "train loss:0.8344133338047761\n",
      "train loss:0.389492443953512\n",
      "train loss:0.7213646567388492\n",
      "train loss:0.7020357095220762\n",
      "train loss:0.3384459250473821\n",
      "train loss:0.6916403939800835\n",
      "train loss:0.4379134652805391\n",
      "train loss:0.6068702870209799\n",
      "train loss:0.5228956484664244\n",
      "train loss:0.7054908129458617\n",
      "train loss:0.42528242133727057\n",
      "train loss:0.4151224311356012\n",
      "train loss:0.2859871019142314\n",
      "train loss:0.6214285378491188\n",
      "train loss:0.2166212876703965\n",
      "train loss:0.7951477666385786\n",
      "train loss:0.6521124129651671\n",
      "train loss:0.6569919407538836\n",
      "train loss:0.5153933694495383\n",
      "train loss:0.35657599605893187\n",
      "train loss:0.4924686082029618\n",
      "train loss:0.3547448379202078\n",
      "train loss:0.5003050524910947\n",
      "train loss:0.6454140040516838\n",
      "train loss:0.3587973545511815\n",
      "train loss:0.4945885908082772\n",
      "train loss:0.35537308822788694\n",
      "train loss:0.6491490049799689\n",
      "train loss:0.6450948300199694\n",
      "train loss:0.6269380161248173\n",
      "train loss:0.5035933768146181\n",
      "train loss:0.8267521705742704\n",
      "train loss:0.4075557775722878\n",
      "train loss:0.6957053267447109\n",
      "train loss:0.6916884134067478\n",
      "train loss:0.612435920146962\n",
      "train loss:0.47380772543731703\n",
      "train loss:0.6171698553300058\n",
      "train loss:0.7493896238189457\n",
      "train loss:0.684981061685176\n",
      "train loss:0.7423357355603868\n",
      "train loss:0.569109611347245\n",
      "train loss:0.5088343234968941\n",
      "train loss:0.6233925013224638\n",
      "train loss:0.5637339496709294\n",
      "train loss:0.5460290161518102\n",
      "train loss:0.6109692387755364\n",
      "train loss:0.5340528923574027\n",
      "train loss:0.433287547992653\n",
      "train loss:0.30000164032583065\n",
      "train loss:0.6180745543189319\n",
      "train loss:0.8896132620552237\n",
      "train loss:0.5056660042279273\n",
      "train loss:0.35852797509676015\n",
      "train loss:0.6311805707659727\n",
      "train loss:0.6489311117041192\n",
      "train loss:0.773025618860604\n",
      "train loss:0.9747795964076003\n",
      "train loss:0.4111963345486108\n",
      "train loss:0.5220761389370124\n",
      "train loss:0.4323926528966049\n",
      "train loss:0.6120398827480826\n",
      "train loss:0.5261533867715783\n",
      "train loss:0.7672692567793326\n",
      "train loss:0.6127528004047313\n",
      "train loss:0.6802129597457464\n",
      "train loss:0.6181279154912221\n",
      "train loss:0.48177212919444357\n",
      "train loss:0.5488269547165218\n",
      "train loss:0.7434076877900652\n",
      "train loss:0.614030123790468\n",
      "train loss:0.5506797938952565\n",
      "train loss:0.6752822294598954\n",
      "train loss:0.6823152760442073\n",
      "train loss:0.5400344071424732\n",
      "train loss:0.3769199774074897\n",
      "train loss:0.8392436515833419\n",
      "train loss:0.6915994102222817\n",
      "train loss:0.44209224310034256\n",
      "train loss:0.6170375412024804\n",
      "train loss:0.6855456558821057\n",
      "train loss:0.3197730755817424\n",
      "train loss:0.6174408634435398\n",
      "train loss:0.7089852734137383\n",
      "train loss:0.6119707738804403\n",
      "train loss:0.5110147607326235\n",
      "train loss:0.3906180127198766\n",
      "train loss:0.844932712117686\n",
      "train loss:0.5027346973973186\n",
      "train loss:0.7246654254693785\n",
      "train loss:0.5030969138883654\n",
      "train loss:0.7017955977171767\n",
      "train loss:0.6164395815738939\n",
      "train loss:0.6154028071222319\n",
      "train loss:0.5300131048223953\n",
      "train loss:0.6080305712868551\n",
      "train loss:0.5177434606098322\n",
      "train loss:0.7654715546063009\n",
      "train loss:0.7648636966924252\n",
      "train loss:0.6100719963624947\n",
      "train loss:0.4776501284700198\n",
      "train loss:0.5365816874701538\n",
      "train loss:0.748024185520068\n",
      "train loss:0.5532488295801203\n",
      "train loss:0.6127687502062116\n",
      "train loss:0.683557538863305\n",
      "train loss:0.6773652418740315\n",
      "train loss:0.4040465454286629\n",
      "train loss:0.4522398592085909\n",
      "train loss:0.691575347898888\n",
      "train loss:0.6896131310422076\n",
      "train loss:0.7636794273202157\n",
      "train loss:0.33796508097327227\n",
      "train loss:0.6858191077500532\n",
      "train loss:0.6147629520655132\n",
      "train loss:0.5108175945748674\n",
      "train loss:0.6104570959995793\n",
      "train loss:0.6281957596588754\n",
      "train loss:0.5101124755368781\n",
      "train loss:0.8225652590478774\n",
      "train loss:0.8171506719331184\n",
      "train loss:0.5315071157562221\n",
      "train loss:0.5154525325461434\n",
      "train loss:0.8387853057644445\n",
      "train loss:0.5392219204034867\n",
      "train loss:0.5421019729401964\n",
      "train loss:0.3964971186322935\n",
      "train loss:0.6100993860400277\n",
      "train loss:0.5325569982090121\n",
      "train loss:0.5212213977301042\n",
      "train loss:0.5182020249840245\n",
      "train loss:0.591396076796473\n",
      "train loss:0.2791894354475399\n",
      "train loss:0.5043318605501108\n",
      "train loss:0.7480990972757606\n",
      "train loss:0.4984075512623952\n",
      "train loss:0.7870041075425875\n",
      "train loss:0.8913195533473368\n",
      "train loss:0.5062415387400778\n",
      "train loss:0.5104364542718797\n",
      "train loss:0.619565626943792\n",
      "train loss:0.5122425764362635\n",
      "train loss:0.30679115671692997\n",
      "train loss:0.6092989961562929\n",
      "train loss:0.4140865187404376\n",
      "train loss:0.6113666465076942\n",
      "train loss:0.8034724483857822\n",
      "train loss:0.7206480964441158\n",
      "train loss:0.7008140782398925\n",
      "train loss:0.5298410201316859\n",
      "train loss:0.4379398060810555\n",
      "train loss:0.6921589254660789\n",
      "train loss:0.5271155354675666\n",
      "train loss:0.596526405186417\n",
      "train loss:0.7719731057614226\n",
      "train loss:0.45725658547312564\n",
      "train loss:0.6003105508592446\n",
      "train loss:0.6111926171777309\n",
      "train loss:0.7008073840883072\n",
      "train loss:0.6122713603681712\n",
      "train loss:0.6821707488567353\n",
      "train loss:0.6916572365777737\n",
      "train loss:0.45339480971019713\n",
      "train loss:0.6978437293543459\n",
      "train loss:0.542789304062178\n",
      "train loss:0.6803797523780333\n",
      "train loss:0.5293262389852778\n",
      "train loss:0.603554952140689\n",
      "train loss:0.43712423281914425\n",
      "train loss:0.5074799121037787\n",
      "train loss:0.7797611778651775\n",
      "train loss:0.6189688593128484\n",
      "train loss:0.6133456843133852\n",
      "train loss:0.5076350966077146\n",
      "train loss:0.6035636778091601\n",
      "train loss:0.8829512879135644\n",
      "train loss:0.5989795836882106\n",
      "train loss:0.32513867094937754\n",
      "train loss:0.7943970651494304\n",
      "train loss:0.6124833896734213\n",
      "train loss:0.5188371244700283\n",
      "train loss:0.43243340633837624\n",
      "train loss:0.5955231296465593\n",
      "train loss:0.7142578312269146\n",
      "train loss:0.42491345800735375\n",
      "train loss:0.5128761667156844\n",
      "train loss:0.5067649510978978\n",
      "train loss:0.8470467266441876\n",
      "train loss:0.5124214341377222\n",
      "train loss:0.3727974425849109\n",
      "train loss:0.38240841031852524\n",
      "train loss:0.8661404277742687\n",
      "train loss:0.8077577698970136\n",
      "train loss:0.37977160489594197\n",
      "train loss:0.41178626530547857\n",
      "train loss:0.686632728915318\n",
      "train loss:0.3704692110277637\n",
      "train loss:0.6152948889033493\n",
      "train loss:0.7378395516744891\n",
      "train loss:0.8061653633749495\n",
      "train loss:0.6019992746070465\n",
      "train loss:0.5195115299789761\n",
      "train loss:0.5317378121032003\n",
      "train loss:0.5311662178204344\n",
      "train loss:0.8483053275761444\n",
      "train loss:0.547926483730499\n",
      "train loss:0.44750894888316983\n",
      "train loss:0.7547607356269771\n",
      "train loss:0.6784481179368556\n",
      "train loss:0.6080013775166112\n",
      "train loss:0.7467388903990445\n",
      "train loss:0.48519323202655584\n",
      "train loss:0.5372895094612433\n",
      "train loss:0.7423495613612815\n",
      "train loss:0.5385517603051279\n",
      "train loss:0.6219994771172741\n",
      "train loss:0.6246985174882174\n",
      "train loss:0.45659082311718835\n",
      "train loss:0.4298707635714016\n",
      "train loss:0.5180871734397493\n",
      "train loss:0.5217846725553753\n",
      "train loss:0.3651495958426309\n",
      "train loss:0.3520614876851472\n",
      "train loss:0.6241221043451455\n",
      "train loss:0.669915036215176\n",
      "train loss:0.8172027907025212\n",
      "train loss:0.15411417534150615\n",
      "train loss:0.6441751840728325\n",
      "train loss:0.6613026214268636\n",
      "train loss:0.7504256675601276\n",
      "train loss:0.4846344250996405\n",
      "train loss:0.48016191922282525\n",
      "train loss:0.7122625245367067\n",
      "train loss:0.5405372962030449\n",
      "train loss:0.6048206424469101\n",
      "train loss:0.7654390795902317\n",
      "train loss:0.6155911925731854\n",
      "train loss:0.4863640088428293\n",
      "train loss:0.6312803990564745\n",
      "train loss:0.5006147363237146\n",
      "train loss:0.6602327608424873\n",
      "train loss:0.6811728494931036\n",
      "train loss:0.6798064059004579\n",
      "train loss:0.6807100776667204\n",
      "train loss:0.6027709335927831\n",
      "train loss:0.6840206786050851\n",
      "train loss:0.6860163972046871\n",
      "train loss:0.47111197914191993\n",
      "train loss:0.46895876550075294\n",
      "train loss:0.6034166398353017\n",
      "train loss:0.6114225462824371\n",
      "train loss:0.42656283978837733\n",
      "train loss:0.7185730820261846\n",
      "train loss:0.5922678576370111\n",
      "train loss:0.6182441030025316\n",
      "train loss:0.4774247293047491\n",
      "train loss:0.7302164959939603\n",
      "train loss:0.6432893256984334\n",
      "train loss:0.701132913115854\n",
      "train loss:0.5225751820010445\n",
      "train loss:0.5031256003735161\n",
      "train loss:0.8222399833243342\n",
      "train loss:0.6023675764398428\n",
      "train loss:0.6890156425127513\n",
      "train loss:0.5268335284765084\n",
      "train loss:0.6729914331850788\n",
      "train loss:0.4740278237299095\n",
      "train loss:0.5476687884803968\n",
      "train loss:0.384708049354252\n",
      "train loss:0.6294851764665796\n",
      "train loss:0.5231009622739825\n",
      "train loss:0.6014777066768785\n",
      "train loss:0.7197478475745653\n",
      "train loss:0.5076361169507905\n",
      "train loss:0.6096578568641189\n",
      "train loss:0.6236553410829575\n",
      "train loss:0.7121085968972167\n",
      "train loss:0.7072041096562092\n",
      "train loss:0.784615425987216\n",
      "train loss:0.6019331485814202\n",
      "train loss:0.5879438381547807\n",
      "train loss:0.682299943213612\n",
      "train loss:0.5995814708804461\n",
      "train loss:0.5435593395189702\n",
      "train loss:0.689260219237666\n",
      "train loss:0.8496914222535212\n",
      "train loss:0.6152894151628845\n",
      "train loss:0.6161336794153762\n",
      "train loss:0.564723789167642\n",
      "train loss:0.6241075568480918\n",
      "train loss:0.6266368997589874\n",
      "train loss:0.6110088772422018\n",
      "train loss:0.4972936076408415\n",
      "train loss:0.5490957937057505\n",
      "train loss:0.6839405286847148\n",
      "train loss:0.8866250747113934\n",
      "train loss:0.529698006416685\n",
      "train loss:0.533878313462327\n",
      "train loss:0.5357256159075113\n",
      "train loss:0.6201690233764262\n",
      "train loss:0.6145956203219902\n",
      "train loss:0.6864170272886159\n",
      "train loss:0.3190607182698165\n",
      "train loss:0.5017362796742032\n",
      "train loss:0.619267564379926\n",
      "train loss:0.4853838809703531\n",
      "train loss:0.6137101735817165\n",
      "train loss:0.3567290257145067\n",
      "train loss:0.8919679938390889\n",
      "train loss:0.7745394873217204\n",
      "train loss:0.5139846446200119\n",
      "train loss:0.7882276600232235\n",
      "train loss:0.6062029179470637\n",
      "train loss:0.6934965370073753\n",
      "train loss:0.6732718144877956\n",
      "train loss:0.6632843086657818\n",
      "train loss:0.5538601889515146\n",
      "train loss:0.5132240115597473\n",
      "train loss:0.4572566491700883\n",
      "train loss:0.7331201797070197\n",
      "train loss:0.7939014727249365\n",
      "train loss:0.6121136199004701\n",
      "train loss:0.502336581563327\n",
      "train loss:0.5614116686068542\n",
      "train loss:0.7430086040943539\n",
      "train loss:0.4659531117029144\n",
      "train loss:0.5305510601233794\n",
      "train loss:0.3523355822193125\n",
      "train loss:0.7639458261893729\n",
      "train loss:0.3983394097680082\n",
      "train loss:0.5100525234304879\n",
      "train loss:0.6382330096466033\n",
      "train loss:0.5043583572891002\n",
      "train loss:0.6685842760716074\n",
      "train loss:0.3450910398856804\n",
      "train loss:0.6742890139683286\n",
      "train loss:0.7859409697970301\n",
      "train loss:0.7489191262558865\n",
      "train loss:0.5047572066123834\n",
      "train loss:0.8352910626327619\n",
      "train loss:0.6909665354940492\n",
      "train loss:0.6251809155348521\n",
      "train loss:0.7448638675989706\n",
      "train loss:0.5654270677795584\n",
      "train loss:0.4862355144166323\n",
      "train loss:0.7175347532608767\n",
      "train loss:0.6321791917283479\n",
      "train loss:0.7185796299004594\n",
      "train loss:0.5944386151141414\n",
      "train loss:0.6752025302501601\n",
      "train loss:0.6790805592214204\n",
      "train loss:0.6777117088871188\n",
      "train loss:0.5404389794886781\n",
      "train loss:0.6792599563130307\n",
      "train loss:0.7151034484304284\n",
      "train loss:0.6718051745445389\n",
      "train loss:0.6654929744204332\n",
      "train loss:0.7741091198137682\n",
      "train loss:0.5200088209514112\n",
      "train loss:0.615492340139365\n",
      "train loss:0.6196798411388444\n",
      "train loss:0.5453090194630826\n",
      "train loss:0.5311996641082688\n",
      "train loss:0.6857727994624819\n",
      "train loss:0.603726163447655\n",
      "train loss:0.610502574728475\n",
      "train loss:0.4964487846833746\n",
      "train loss:0.4931744473987704\n",
      "train loss:0.7140769318132221\n",
      "train loss:0.731274426996094\n",
      "train loss:0.47169014440441287\n",
      "train loss:0.7212989206627343\n",
      "train loss:0.6220049891107082\n",
      "train loss:0.5004556577095237\n",
      "train loss:0.5843071055219768\n",
      "train loss:0.4025502507823927\n",
      "train loss:0.5118671248695742\n",
      "train loss:0.8116863749885297\n",
      "train loss:0.7129257055804745\n",
      "train loss:0.606925508664128\n",
      "train loss:0.5207311184841756\n",
      "train loss:0.4317176255504426\n",
      "train loss:0.40808294640084564\n",
      "train loss:0.5025523865773464\n",
      "train loss:0.29005330251097183\n",
      "train loss:0.5104190395808015\n",
      "train loss:0.498149007877603\n",
      "train loss:0.6740701186208677\n",
      "train loss:0.34526997361598005\n",
      "train loss:0.493408798696243\n",
      "train loss:0.6973679489769526\n",
      "train loss:0.7941183940973805\n",
      "train loss:0.6708511238054868\n",
      "train loss:0.3728203093783004\n",
      "train loss:0.5260282120116218\n",
      "train loss:0.37357801877418056\n",
      "train loss:0.5122200376524508\n",
      "train loss:0.6054519380360011\n",
      "train loss:0.39229359906261607\n",
      "train loss:0.8266981308172145\n",
      "train loss:0.8089883423219749\n",
      "train loss:0.6154926414597063\n",
      "train loss:0.45310766398420305\n",
      "train loss:0.5356523399527172\n",
      "train loss:0.4536202634414105\n",
      "train loss:0.6121789993310955\n",
      "train loss:0.4322772353234742\n",
      "train loss:0.4285088407247903\n",
      "train loss:0.6856739241111186\n",
      "train loss:0.7040211933849598\n",
      "train loss:0.6868655728117121\n",
      "train loss:0.5995483122320238\n",
      "train loss:0.4087925014076184\n",
      "train loss:0.6161099561311539\n",
      "train loss:0.6102711643979568\n",
      "train loss:0.5868935323204914\n",
      "train loss:0.370358591413231\n",
      "train loss:0.6026720130657814\n",
      "train loss:0.8070119548373051\n",
      "train loss:0.5124422179212833\n",
      "train loss:0.6034030540126859\n",
      "train loss:0.38943413236239877\n",
      "train loss:0.6124164062853027\n",
      "train loss:0.6241839178891553\n",
      "train loss:0.7127614627112472\n",
      "train loss:0.6188570593656927\n",
      "train loss:0.5204968584096206\n",
      "train loss:0.6077801137862116\n",
      "train loss:0.718476993193983\n",
      "train loss:0.6051033504266734\n",
      "train loss:0.6106773204086471\n",
      "train loss:0.5993573260871407\n",
      "train loss:0.44273420291333265\n",
      "train loss:0.6913642779112357\n",
      "train loss:0.545892625382869\n",
      "train loss:0.6781072724893102\n",
      "train loss:0.7439201936274624\n",
      "train loss:0.6865463471261563\n",
      "train loss:0.4580467787072028\n",
      "train loss:0.44910147474830087\n",
      "train loss:0.5250894296473796\n",
      "train loss:0.4391581711965992\n",
      "train loss:0.6063258826807922\n",
      "train loss:0.6322738425123854\n",
      "train loss:0.6120671868840113\n",
      "train loss:0.623736234120647\n",
      "train loss:0.3734932196123789\n",
      "train loss:0.49352380655124284\n",
      "train loss:0.4958225429985781\n",
      "train loss:0.648690471031299\n",
      "train loss:0.515783693833906\n",
      "train loss:0.3450377152582006\n",
      "train loss:0.32334645359534064\n",
      "train loss:0.8147591698970501\n",
      "train loss:0.35743393258877904\n",
      "train loss:0.502523827237532\n",
      "train loss:0.18333484661351554\n",
      "train loss:0.5092200892735762\n",
      "train loss:0.5198101548005761\n",
      "train loss:0.8302143690310066\n",
      "train loss:0.6368450091126968\n",
      "train loss:0.6107020769782453\n",
      "train loss:0.8290755262190228\n",
      "train loss:0.33144180908867316\n",
      "train loss:0.6094310580714464\n",
      "train loss:0.594056313092992\n",
      "train loss:0.4583604020154966\n",
      "train loss:0.6865517393101641\n",
      "train loss:0.5334136710703431\n",
      "train loss:0.618682550880645\n",
      "train loss:0.7404460468266796\n",
      "train loss:0.6026148430276919\n",
      "train loss:0.6248932852451137\n",
      "train loss:0.6747867342927976\n",
      "train loss:0.5691717540819545\n",
      "train loss:0.48549943374419124\n",
      "train loss:0.5500005316865979\n",
      "train loss:0.5261404454893601\n",
      "train loss:0.43727235274109466\n",
      "train loss:0.5888525606330496\n",
      "train loss:0.5153838806589071\n",
      "train loss:0.3806390874896328\n",
      "train loss:0.6333226651832305\n",
      "train loss:0.5021895208500637\n",
      "train loss:0.3466622678384688\n",
      "train loss:0.5287568211885472\n",
      "train loss:0.34154734096888834\n",
      "train loss:0.5146761259439995\n",
      "train loss:0.6157640822498196\n",
      "train loss:1.0313785992699143\n",
      "train loss:0.6582859708672093\n",
      "train loss:0.722633955319882\n",
      "train loss:0.6009840811042976\n",
      "train loss:0.7417662800589188\n",
      "train loss:0.6142582236523926\n",
      "train loss:0.6216461479860074\n",
      "train loss:0.6666637188953787\n",
      "train loss:0.7489994696484972\n",
      "train loss:0.7486007606964269\n",
      "train loss:0.6412582400783883\n",
      "train loss:0.5082026366535933\n",
      "train loss:0.6408129013192124\n",
      "train loss:0.5407420351794965\n",
      "train loss:0.6728280305557324\n",
      "train loss:0.6150553177350896\n",
      "train loss:0.5174822326259789\n",
      "train loss:0.7369047418890577\n",
      "train loss:0.547937936392061\n",
      "train loss:0.38363470336142225\n",
      "train loss:0.6222615420169972\n",
      "train loss:0.8963278637000709\n",
      "train loss:0.6147020874739424\n",
      "train loss:0.8733566763448746\n",
      "train loss:0.6078967074535796\n",
      "train loss:0.6069118017544944\n",
      "train loss:0.530373874700867\n",
      "train loss:0.603358621166814\n",
      "train loss:0.6785769617083981\n",
      "train loss:0.5991561587306771\n",
      "train loss:0.4434110914428501\n",
      "train loss:0.4407784381366291\n",
      "train loss:0.5898408579937697\n",
      "train loss:0.6889875031138429\n",
      "train loss:0.5902807791545472\n",
      "train loss:0.6101902667257735\n",
      "train loss:0.7096936641095182\n",
      "train loss:0.4903843383004731\n",
      "train loss:0.8813144047082927\n",
      "train loss:0.5999489991591596\n",
      "train loss:0.4316103499707906\n",
      "train loss:0.42661099772148636\n",
      "train loss:0.5179383200152492\n",
      "train loss:0.5973775972317269\n",
      "train loss:0.6018371574171988\n",
      "train loss:0.5012495575874636\n",
      "train loss:0.3845036136330239\n",
      "train loss:0.6967911450675579\n",
      "train loss:0.5000064293216713\n",
      "train loss:0.37896866360065135\n",
      "train loss:0.7806647527545016\n",
      "train loss:0.6393440898335158\n",
      "train loss:0.61495248967176\n",
      "train loss:0.5205675321173456\n",
      "train loss:0.5784956803479194\n",
      "train loss:0.6026068534916645\n",
      "train loss:0.5068246458245326\n",
      "train loss:0.49640248632977224\n",
      "train loss:0.6065563191915944\n",
      "train loss:0.6169675351192232\n",
      "train loss:0.6120208285704598\n",
      "train loss:0.5135939455732175\n",
      "train loss:0.6081525931166014\n",
      "train loss:0.9465533865473518\n",
      "train loss:0.6149466368192743\n",
      "train loss:0.464880020950363\n",
      "train loss:0.5327441014286055\n",
      "train loss:0.5268059836804517\n",
      "train loss:0.6849222185065758\n",
      "train loss:0.6062973234205571\n",
      "train loss:0.7333566584563067\n",
      "train loss:0.4693179879418034\n",
      "train loss:0.6919738202457694\n",
      "train loss:0.5258898119461558\n",
      "train loss:0.5242607868906183\n",
      "train loss:0.6139961021268177\n",
      "train loss:0.5274323643081396\n",
      "train loss:0.6085662757669613\n",
      "train loss:0.6964579803066427\n",
      "train loss:0.7117666117609533\n",
      "train loss:0.8774781084601357\n",
      "train loss:0.5259262333157037\n",
      "train loss:0.6942048460663386\n",
      "train loss:0.4576036982328683\n",
      "train loss:0.7450118487040174\n",
      "train loss:0.5344562531217121\n",
      "train loss:0.8198095508356884\n",
      "train loss:0.6153694035304802\n",
      "train loss:0.6783357932902931\n",
      "train loss:0.7331086366305395\n",
      "train loss:0.8361683774640001\n",
      "train loss:0.5251499207364322\n",
      "train loss:0.6254144923502871\n",
      "train loss:0.6253129626696378\n",
      "train loss:0.6435499953503283\n",
      "train loss:0.6718776392174973\n",
      "train loss:0.5747243530333489\n",
      "train loss:0.5731294209096611\n",
      "train loss:0.614674552544428\n",
      "train loss:0.6791576241641442\n",
      "train loss:0.6141117609030677\n",
      "train loss:0.6042070924663648\n",
      "train loss:0.5420345187476342\n",
      "train loss:0.6093560651784287\n",
      "train loss:0.5333234611330291\n",
      "train loss:0.6958578177241821\n",
      "train loss:0.6103394879326156\n",
      "train loss:0.7820055926921663\n",
      "train loss:0.4191179617598347\n",
      "train loss:0.6155929157091654\n",
      "train loss:0.5818715863489439\n",
      "train loss:0.6148886637397368\n",
      "train loss:0.9940947632408091\n",
      "train loss:0.6029873343598797\n",
      "train loss:0.41582468707803394\n",
      "train loss:0.7755393029327555\n",
      "train loss:0.6102977003113051\n",
      "train loss:0.6017234720751949\n",
      "train loss:0.7506302544675094\n",
      "train loss:0.47020624849196524\n",
      "train loss:0.7338466836420023\n",
      "train loss:0.5281549360385325\n",
      "train loss:0.5992503424236127\n",
      "train loss:0.6083997190554058\n",
      "train loss:0.4836102577309015\n",
      "train loss:0.6158769670107183\n",
      "train loss:0.5944487696663014\n",
      "train loss:0.691542784734672\n",
      "train loss:0.6113174115845273\n",
      "train loss:0.5960594646322677\n",
      "train loss:0.6098652703852915\n",
      "train loss:0.667094512419673\n",
      "train loss:0.6817987564815203\n",
      "train loss:0.6010534191200921\n",
      "train loss:0.6763134091274933\n",
      "train loss:0.706724696018298\n",
      "train loss:0.8267531850887962\n",
      "train loss:0.7392928316544761\n",
      "train loss:0.4674930894334094\n",
      "train loss:0.6119967156929216\n",
      "train loss:0.48074863662907985\n",
      "train loss:0.6743051512850567\n",
      "train loss:0.6071697532562641\n",
      "train loss:0.7331104403577521\n",
      "train loss:0.4141952536680675\n",
      "train loss:0.6001372995347671\n",
      "train loss:0.593724896610835\n",
      "train loss:0.8409954002301671\n",
      "train loss:0.4867840897385635\n",
      "train loss:0.6902402093459017\n",
      "train loss:0.6138023072335393\n",
      "train loss:0.764636666020022\n",
      "train loss:0.4259247970857471\n",
      "train loss:0.6018256328584901\n",
      "train loss:0.6717725241374751\n",
      "train loss:0.6180429921570825\n",
      "train loss:0.4200719820304656\n",
      "train loss:0.6711620259842113\n",
      "train loss:0.5155313599228022\n",
      "train loss:0.5910212560895858\n",
      "train loss:0.9775110878627323\n",
      "train loss:0.7155956802649028\n",
      "train loss:0.6063605329025538\n",
      "train loss:0.773926324350315\n",
      "train loss:0.5335992382506992\n",
      "train loss:0.617174985451507\n",
      "train loss:0.5337029518484179\n",
      "train loss:0.6802745982817913\n",
      "train loss:0.6827311092866267\n",
      "train loss:0.679024583038824\n",
      "train loss:0.5570765500111904\n",
      "train loss:0.6766895011259544\n",
      "train loss:0.49245513331452584\n",
      "train loss:0.48714965483178074\n",
      "train loss:0.5482508711804222\n",
      "train loss:0.607772115393829\n",
      "train loss:0.6860349004908513\n",
      "train loss:0.6092634847643779\n",
      "train loss:0.43283964821018495\n",
      "train loss:0.695137635190306\n",
      "train loss:0.31133933448264534\n",
      "train loss:0.7067296414758758\n",
      "train loss:0.2575793768039044\n",
      "train loss:0.8749947280614432\n",
      "train loss:0.6414024873415787\n",
      "train loss:0.3639670644720432\n",
      "train loss:0.35154233039186716\n",
      "train loss:0.504094870973906\n",
      "train loss:0.17147750113179755\n",
      "train loss:0.1403943854789974\n",
      "train loss:0.7199856755948355\n",
      "train loss:0.28998495784283856\n",
      "train loss:1.0926811452507708\n",
      "train loss:0.5155451626948209\n",
      "train loss:0.6074237392961807\n",
      "train loss:0.4993335211180974\n",
      "train loss:0.36965813386058677\n",
      "train loss:0.7171094913246929\n",
      "train loss:0.6926052355415415\n",
      "train loss:0.6831944184506838\n",
      "train loss:0.6145847217855785\n",
      "train loss:0.4731374733141761\n",
      "train loss:0.6111045793752992\n",
      "train loss:0.4814865857840348\n",
      "train loss:0.4237459512277592\n",
      "train loss:0.626012849914828\n",
      "train loss:0.6128496993469966\n",
      "train loss:0.47516690416986285\n",
      "train loss:0.6178361679786325\n",
      "train loss:0.44715520799075426\n",
      "train loss:0.432797071644812\n",
      "train loss:0.5875598175327126\n",
      "train loss:0.5020560422816744\n",
      "train loss:0.37505838360971816\n",
      "train loss:0.624008083961755\n",
      "train loss:0.8945916267986457\n",
      "train loss:0.48304967443259333\n",
      "train loss:0.620553732564049\n",
      "train loss:0.47844713154801416\n",
      "train loss:0.9918325711477041\n",
      "train loss:0.7714093403513334\n",
      "train loss:0.7810483792047735\n",
      "train loss:0.8168454959218041\n",
      "train loss:0.4757891584764396\n",
      "train loss:0.6854822311749699\n",
      "train loss:0.6769165233823047\n",
      "train loss:0.5833518790636557\n",
      "train loss:0.6314737789243532\n",
      "train loss:0.6397437743008825\n",
      "train loss:0.6536544680350668\n",
      "train loss:0.6376533911282742\n",
      "train loss:0.7302181045966343\n",
      "train loss:0.6919804876846911\n",
      "train loss:0.566510149620951\n",
      "train loss:0.5541649891586192\n",
      "train loss:0.674942594945709\n",
      "train loss:0.8000625142773299\n",
      "train loss:0.5550163924811884\n",
      "train loss:0.6215807791499965\n",
      "train loss:0.6092382998394125\n",
      "train loss:0.592608584906493\n",
      "train loss:0.6083485573809753\n",
      "train loss:0.4261434059600108\n",
      "train loss:0.5231389760326447\n",
      "train loss:0.5016773773790509\n",
      "train loss:0.8117612785248474\n",
      "train loss:0.6126940159321561\n",
      "train loss:0.4965284789374856\n",
      "train loss:0.4677682884153954\n",
      "train loss:0.7274875824289736\n",
      "train loss:0.23149890378682964\n",
      "train loss:0.6391270776645551\n",
      "train loss:0.3549509790511133\n",
      "train loss:0.594574300161419\n",
      "train loss:0.6156081443816059\n",
      "train loss:0.76751510686692\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6617647058823529\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet5Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6987951b-b9f4-4e37-8d42-7e52f80cf53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet7Layer:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = int((input_size - filter_size + 2 * filter_pad) / filter_stride + 1)\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b3'] = np.zeros(hidden_size)        \n",
    "        self.params['W4'] = weight_init_std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b4'] = np.zeros(hidden_size)\n",
    "        self.params['W5'] = weight_init_std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b5'] = np.zeros(hidden_size)\n",
    "        self.params['W6'] = weight_init_std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b6'] = np.zeros(hidden_size)\n",
    "        self.params['W7'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b7'] = np.zeros(output_size)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.layers['Affine3'] = Affine(self.params['W4'], self.params['b4'])\n",
    "        self.layers['Affine4'] = Affine(self.params['W5'], self.params['b5'])\n",
    "        self.layers['Affine5'] = Affine(self.params['W6'], self.params['b6'])\n",
    "        self.layers['Affine6'] = Affine(self.params['W7'], self.params['b7'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3,4,5,6,7):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        grads['W4'], grads['b4'] = self.layers['Affine3'].dW, self.layers['Affine3'].db\n",
    "        grads['W5'], grads['b5'] = self.layers['Affine4'].dW, self.layers['Affine4'].db\n",
    "        grads['W6'], grads['b6'] = self.layers['Affine5'].dW, self.layers['Affine5'].db\n",
    "        grads['W7'], grads['b7'] = self.layers['Affine6'].dW, self.layers['Affine6'].db\n",
    "\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d082961-e29b-4156-97dd-9c247acf51f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3002388202767476\n",
      "=== epoch:1, train acc:0.7, test acc:0.65 ===\n",
      "train loss:2.2975553834244855\n",
      "train loss:2.2943938485468225\n",
      "train loss:2.290904239149401\n",
      "train loss:2.287286064204815\n",
      "train loss:2.280722736408403\n",
      "train loss:2.2747281377785966\n",
      "train loss:2.267430130812123\n",
      "train loss:2.258318875960108\n",
      "train loss:2.241985632134329\n",
      "train loss:2.2193028800375183\n",
      "train loss:2.1863480716891512\n",
      "train loss:2.127819405983845\n",
      "train loss:2.0230786165282906\n",
      "train loss:1.8323999510006694\n",
      "train loss:1.5236624979588753\n",
      "train loss:1.0967064100013422\n",
      "train loss:0.7665785248121592\n",
      "train loss:0.6154160675781347\n",
      "train loss:0.3394318938919303\n",
      "train loss:0.9254815148079876\n",
      "train loss:0.8476672688021075\n",
      "train loss:0.7997057059280236\n",
      "train loss:0.5263935763660894\n",
      "train loss:0.6214579338024937\n",
      "train loss:0.5853496695225062\n",
      "train loss:0.6634001066307305\n",
      "train loss:0.681208794597784\n",
      "train loss:0.6690378741335212\n",
      "train loss:0.6662754616141012\n",
      "train loss:0.7275082809109195\n",
      "train loss:0.4874266004481037\n",
      "train loss:0.7692959308308194\n",
      "train loss:0.6765157486213735\n",
      "train loss:0.6136284215105177\n",
      "train loss:0.7586980737859245\n",
      "train loss:0.6772594864470434\n",
      "train loss:0.4580388066610597\n",
      "train loss:0.5307904884198007\n",
      "train loss:0.1972879974631241\n",
      "train loss:0.5172283164940622\n",
      "train loss:0.7713657096427229\n",
      "train loss:0.522017648308145\n",
      "train loss:1.422446351242499\n",
      "train loss:0.23113788392706813\n",
      "train loss:0.5088002552597682\n",
      "train loss:0.5322135728970551\n",
      "train loss:0.6100867193233521\n",
      "train loss:0.7396216055247683\n",
      "train loss:0.5871117123400866\n",
      "train loss:0.7103427045627297\n",
      "train loss:0.5744882716068099\n",
      "train loss:0.648335355211576\n",
      "train loss:0.6504451085050189\n",
      "train loss:0.5796984755164079\n",
      "train loss:0.5712260445400071\n",
      "train loss:0.6346926456281924\n",
      "train loss:0.5248925521049636\n",
      "train loss:0.6059249941052235\n",
      "train loss:0.5432510828308179\n",
      "train loss:0.6115535816386874\n",
      "train loss:0.6308823879788161\n",
      "train loss:0.7198751110230431\n",
      "train loss:0.7055759666024195\n",
      "train loss:0.7138317069313588\n",
      "train loss:0.5088511299537462\n",
      "train loss:0.7272985212912758\n",
      "train loss:0.9076760079639765\n",
      "train loss:0.533847724289706\n",
      "train loss:0.634125821661285\n",
      "train loss:0.6085022946478833\n",
      "train loss:0.6914551846008614\n",
      "train loss:0.6965516925001787\n",
      "train loss:0.6195828286350759\n",
      "train loss:0.666867969156937\n",
      "train loss:0.694394086539241\n",
      "train loss:0.589808201031113\n",
      "train loss:0.5560590002706172\n",
      "train loss:0.4929998197329587\n",
      "train loss:0.48268345540265456\n",
      "train loss:0.6854060489145575\n",
      "train loss:0.5433486059696426\n",
      "train loss:0.39912685959507826\n",
      "train loss:0.7325442335690548\n",
      "train loss:0.5065760323587998\n",
      "train loss:0.8982960827149638\n",
      "train loss:0.48552645167500563\n",
      "train loss:0.7354284494466705\n",
      "train loss:0.5061442155110102\n",
      "train loss:0.6003415884297187\n",
      "train loss:0.49584126065539885\n",
      "train loss:0.5390748279970189\n",
      "train loss:0.7048118413863481\n",
      "train loss:0.6953652513446575\n",
      "train loss:0.6006937774868282\n",
      "train loss:0.658349083181458\n",
      "train loss:0.6801143526449166\n",
      "train loss:0.4129653467545327\n",
      "train loss:0.47742056785273934\n",
      "train loss:0.45006304575580797\n",
      "train loss:0.519172854905546\n",
      "train loss:0.7097679545375979\n",
      "train loss:0.4977830014924523\n",
      "train loss:0.22505890300885154\n",
      "train loss:0.5149908033476976\n",
      "train loss:0.8792932624997665\n",
      "train loss:0.7951530301818861\n",
      "train loss:0.629266088211439\n",
      "train loss:0.7292551465813728\n",
      "train loss:0.4242289649371937\n",
      "train loss:0.3272281738961648\n",
      "train loss:0.6068685848404064\n",
      "train loss:0.4355797218040438\n",
      "train loss:0.5237169590641122\n",
      "train loss:0.6245894757562935\n",
      "train loss:0.6342227031218319\n",
      "train loss:0.5273648069123149\n",
      "train loss:0.5153980508122599\n",
      "train loss:0.3986330874014159\n",
      "train loss:0.356683247040325\n",
      "train loss:0.5208000061012577\n",
      "train loss:0.31855241077103563\n",
      "train loss:0.326232429593746\n",
      "train loss:0.9132361046835749\n",
      "train loss:0.8288869368208245\n",
      "train loss:0.8805089213257367\n",
      "train loss:0.6095736420527914\n",
      "train loss:0.526975532567249\n",
      "train loss:0.6883177764844741\n",
      "train loss:0.5955667707438249\n",
      "train loss:0.6944536424351785\n",
      "train loss:0.7356230387067344\n",
      "train loss:0.6804178318125202\n",
      "train loss:0.705583488714467\n",
      "train loss:0.7542943147926336\n",
      "train loss:0.6606352211890121\n",
      "train loss:0.6566319773121523\n",
      "train loss:0.6125567226902114\n",
      "train loss:0.5692554648965166\n",
      "train loss:0.5842346239293894\n",
      "train loss:0.5697920336755764\n",
      "train loss:0.6108695203715875\n",
      "train loss:0.7778031243385151\n",
      "train loss:0.514815510934686\n",
      "train loss:0.5029350832002684\n",
      "train loss:0.3552390429576884\n",
      "train loss:0.8011227121340394\n",
      "train loss:0.7961873300775995\n",
      "train loss:0.355191915606541\n",
      "train loss:0.6291606425784935\n",
      "train loss:0.6190490348194417\n",
      "train loss:0.9343658914322674\n",
      "train loss:0.6115209529397194\n",
      "train loss:0.6725143964295486\n",
      "train loss:0.7397660896007779\n",
      "train loss:0.676903591616198\n",
      "train loss:0.6464434711957633\n",
      "train loss:0.6873999014818876\n",
      "train loss:0.6903928460129312\n",
      "train loss:0.6974244979800325\n",
      "train loss:0.6940957390427619\n",
      "train loss:0.6933636351130785\n",
      "train loss:0.6985778685850234\n",
      "train loss:0.7029397009272574\n",
      "train loss:0.6916855227999553\n",
      "train loss:0.6910481250086531\n",
      "train loss:0.6931587237840807\n",
      "train loss:0.6685348359350586\n",
      "train loss:0.6866315541508285\n",
      "train loss:0.6610567171316424\n",
      "train loss:0.6248545419095108\n",
      "train loss:0.6442020406655821\n",
      "train loss:0.6690560557776475\n",
      "train loss:0.46654952981047726\n",
      "train loss:0.4593121345842374\n",
      "train loss:0.6975839342287153\n",
      "train loss:0.5041898608357213\n",
      "train loss:0.8532803668919635\n",
      "train loss:0.7302404576051266\n",
      "train loss:1.026315850896642\n",
      "train loss:0.4160411603711435\n",
      "train loss:0.689051945808546\n",
      "train loss:0.5942828717495191\n",
      "train loss:0.3698466934611744\n",
      "train loss:0.5328258550193656\n",
      "train loss:0.6200919128536353\n",
      "train loss:0.7870295592187339\n",
      "train loss:0.7685848427919125\n",
      "train loss:0.6719599221361741\n",
      "train loss:0.4827998851303238\n",
      "train loss:0.745240839724004\n",
      "train loss:0.7317950774385233\n",
      "train loss:0.7809717050727766\n",
      "train loss:0.6305805519914542\n",
      "train loss:0.5870241796526414\n",
      "train loss:0.6478390458843818\n",
      "train loss:0.4761642470411928\n",
      "train loss:0.7378739437734223\n",
      "train loss:0.499496220696616\n",
      "train loss:0.6763623276706451\n",
      "train loss:0.5295250373580911\n",
      "train loss:0.7701382518089416\n",
      "train loss:0.5908629946485748\n",
      "train loss:0.5188933933093688\n",
      "train loss:0.2878916192952701\n",
      "train loss:0.38100016677123943\n",
      "train loss:0.810397066887097\n",
      "train loss:0.8147211714763468\n",
      "train loss:0.4809911589764876\n",
      "train loss:0.3570093975177968\n",
      "train loss:0.34388612712668587\n",
      "train loss:0.5315251239127863\n",
      "train loss:0.32363797396350136\n",
      "train loss:0.5110276164429169\n",
      "train loss:0.6477198244282337\n",
      "train loss:0.5139796423654841\n",
      "train loss:0.6695865467906228\n",
      "train loss:0.3872940931211161\n",
      "train loss:0.7462132982775141\n",
      "train loss:0.6117009482614183\n",
      "train loss:0.6235692920295317\n",
      "train loss:0.8055635251021507\n",
      "train loss:0.5718670795339577\n",
      "train loss:0.7264397887359978\n",
      "train loss:0.7109735170847568\n",
      "train loss:0.6339257382757539\n",
      "train loss:0.6652116906676506\n",
      "train loss:0.669466473792355\n",
      "train loss:0.713154751608168\n",
      "train loss:0.662792290485094\n",
      "train loss:0.5860013908889903\n",
      "train loss:0.6525993507572705\n",
      "train loss:0.644713369297388\n",
      "train loss:0.5984737458372129\n",
      "train loss:0.5297659256608409\n",
      "train loss:0.7385497174671734\n",
      "train loss:0.6740090229001658\n",
      "train loss:0.6906242109661096\n",
      "train loss:0.5422621176624679\n",
      "train loss:0.6173327342273847\n",
      "train loss:0.701087433709113\n",
      "train loss:0.6211139785019549\n",
      "train loss:0.8008959088937179\n",
      "train loss:0.5329514354063845\n",
      "train loss:0.7043142443630849\n",
      "train loss:0.34776080737733017\n",
      "train loss:0.7055900945866195\n",
      "train loss:0.3062609196779526\n",
      "train loss:0.7293887386217235\n",
      "train loss:0.7138559741305684\n",
      "train loss:0.5992204333290081\n",
      "train loss:0.41059821535588503\n",
      "train loss:0.8050782864212845\n",
      "train loss:0.6090891397579344\n",
      "train loss:0.4171407108502739\n",
      "train loss:0.602983584460677\n",
      "train loss:0.5120421946578548\n",
      "train loss:0.706905449468795\n",
      "train loss:0.4161062574344279\n",
      "train loss:0.5065868261502027\n",
      "train loss:0.741394059985967\n",
      "train loss:0.6006589315030739\n",
      "train loss:0.8088664295161646\n",
      "train loss:0.6350112085523725\n",
      "train loss:0.44611956950257453\n",
      "train loss:0.6185537189962902\n",
      "train loss:0.5366531333503549\n",
      "train loss:0.5227638218331603\n",
      "train loss:0.5260661172241531\n",
      "train loss:0.7111135303392119\n",
      "train loss:0.6060616327945947\n",
      "train loss:0.5172567597938643\n",
      "train loss:0.6047197867257996\n",
      "train loss:0.39729834018171456\n",
      "train loss:0.5180942134942393\n",
      "train loss:0.8489481941694539\n",
      "train loss:0.7289079201827732\n",
      "train loss:0.7958928711289873\n",
      "train loss:0.6739082000338276\n",
      "train loss:0.7234165543848448\n",
      "train loss:0.576922867946584\n",
      "train loss:0.5906083747408143\n",
      "train loss:0.5969928619309048\n",
      "train loss:0.5928257426085094\n",
      "train loss:0.681214860031077\n",
      "train loss:0.782895514026506\n",
      "train loss:0.6740311063380376\n",
      "train loss:0.6776183451033682\n",
      "train loss:0.6109140664377375\n",
      "train loss:0.6083655287496366\n",
      "train loss:0.5961889592532239\n",
      "train loss:0.5872509072800962\n",
      "train loss:0.6263782262099802\n",
      "train loss:0.7888918148697759\n",
      "train loss:0.6647180754065136\n",
      "train loss:0.676795439474374\n",
      "train loss:0.6710664711060573\n",
      "train loss:0.47283472857900255\n",
      "train loss:0.45245328428880693\n",
      "train loss:0.3130093744155178\n",
      "train loss:0.7448204273590431\n",
      "train loss:0.6385828965168626\n",
      "train loss:0.5140879549502124\n",
      "train loss:0.6585915224779224\n",
      "train loss:0.3440935733278693\n",
      "train loss:0.6609993787437228\n",
      "train loss:0.7769671787851822\n",
      "train loss:0.6191858708153017\n",
      "train loss:0.5241686492773193\n",
      "train loss:0.5156028664207831\n",
      "train loss:0.5184605785407135\n",
      "train loss:0.7704602255173623\n",
      "train loss:0.6121567307719858\n",
      "train loss:0.5488404842868072\n",
      "train loss:0.5546804713027491\n",
      "train loss:0.6104612748994508\n",
      "train loss:0.5499036439893512\n",
      "train loss:0.6780707497934738\n",
      "train loss:0.3831863871133686\n",
      "train loss:0.6105917107447727\n",
      "train loss:0.7859833986800229\n",
      "train loss:0.5156998462535365\n",
      "train loss:0.7833701144728853\n",
      "train loss:0.5382639852939205\n",
      "train loss:0.6172848385056373\n",
      "train loss:0.617629656076574\n",
      "train loss:0.41456742795310964\n",
      "train loss:0.4054512686552741\n",
      "train loss:0.5047559068368972\n",
      "train loss:0.6403921786453478\n",
      "train loss:0.8524715828391651\n",
      "train loss:0.4920889798346907\n",
      "train loss:0.6271285995153513\n",
      "train loss:0.6074349656102396\n",
      "train loss:0.5099201141429238\n",
      "train loss:0.6964614578550876\n",
      "train loss:0.6214006461810208\n",
      "train loss:0.34873507706090445\n",
      "train loss:0.7030672554230258\n",
      "train loss:0.7789566461282955\n",
      "train loss:0.5394051742576507\n",
      "train loss:0.5418760841521784\n",
      "train loss:0.752241622005217\n",
      "train loss:0.5429677421352196\n",
      "train loss:0.5369975263018208\n",
      "train loss:0.5467006839703201\n",
      "train loss:0.6123242001842295\n",
      "train loss:0.5238809160251718\n",
      "train loss:0.7128914489771507\n",
      "train loss:0.7006735113960921\n",
      "train loss:0.7015783263313107\n",
      "train loss:0.685610474401509\n",
      "train loss:0.44771732005669496\n",
      "train loss:0.8379559749024127\n",
      "train loss:0.6099010366067887\n",
      "train loss:0.6159393461512022\n",
      "train loss:0.54144992380768\n",
      "train loss:0.680203486599386\n",
      "train loss:0.6076772984040406\n",
      "train loss:0.5491778409033985\n",
      "train loss:0.543937184666706\n",
      "train loss:0.44316937649392074\n",
      "train loss:0.6269755302189424\n",
      "train loss:0.40338213324233435\n",
      "train loss:0.4987155519238641\n",
      "train loss:0.3487512584317012\n",
      "train loss:0.3284826690953276\n",
      "train loss:0.562876884642141\n",
      "train loss:0.32905005531291726\n",
      "train loss:0.5916152923937706\n",
      "train loss:0.5715702679602657\n",
      "train loss:0.5249118959757633\n",
      "train loss:0.3580910336437385\n",
      "train loss:0.359016806494007\n",
      "train loss:0.6634018367137398\n",
      "train loss:0.7252526661422777\n",
      "train loss:0.6108013246076895\n",
      "train loss:0.6132978191339241\n",
      "train loss:0.49849372156229155\n",
      "train loss:0.6809833611878171\n",
      "train loss:0.5828855208571861\n",
      "train loss:0.5320019465851389\n",
      "train loss:0.5768532605844132\n",
      "train loss:0.5041740198625402\n",
      "train loss:0.6160012244727164\n",
      "train loss:0.613824908562841\n",
      "train loss:0.5294134017963245\n",
      "train loss:0.5232130925249185\n",
      "train loss:0.7282946073015192\n",
      "train loss:0.5047276734018864\n",
      "train loss:0.5073540810916228\n",
      "train loss:0.5045801769077809\n",
      "train loss:0.6452836245556324\n",
      "train loss:0.3443682264853083\n",
      "train loss:0.6744728116989875\n",
      "train loss:0.9120987810378305\n",
      "train loss:0.7289443176891067\n",
      "train loss:0.4247211514017718\n",
      "train loss:0.6922281672783225\n",
      "train loss:0.6146748031561369\n",
      "train loss:0.7328577819180142\n",
      "train loss:0.7716325422997847\n",
      "train loss:0.5195096942651315\n",
      "train loss:0.7271177993546851\n",
      "train loss:0.652571590783376\n",
      "train loss:0.6542468840873207\n",
      "train loss:0.6469145190445367\n",
      "train loss:0.6433665106056718\n",
      "train loss:0.5947456277881333\n",
      "train loss:0.5357101336745522\n",
      "train loss:0.5118915832867001\n",
      "train loss:0.6805414851253792\n",
      "train loss:0.6158793076751863\n",
      "train loss:0.42593538037841344\n",
      "train loss:0.8042608144984273\n",
      "train loss:0.3832141444662669\n",
      "train loss:0.9786422899844472\n",
      "train loss:0.6154732787973299\n",
      "train loss:0.26715158549787\n",
      "train loss:0.5098508550392199\n",
      "train loss:0.36394393604735603\n",
      "train loss:0.6469421441455581\n",
      "train loss:0.6537706599543565\n",
      "train loss:0.636203387869679\n",
      "train loss:0.6175957269450938\n",
      "train loss:0.7171058887304691\n",
      "train loss:0.7653293609911539\n",
      "train loss:0.47967794831664634\n",
      "train loss:0.6731852149862892\n",
      "train loss:0.6252335945413344\n",
      "train loss:0.5343924237162819\n",
      "train loss:0.7157250704312311\n",
      "train loss:0.5465113207055494\n",
      "train loss:0.5369978455465717\n",
      "train loss:0.674304270311053\n",
      "train loss:0.5178067784328777\n",
      "train loss:0.5575467985482707\n",
      "train loss:0.615462263775298\n",
      "train loss:0.6833841738505702\n",
      "train loss:0.6844879287630194\n",
      "train loss:0.6065265705807216\n",
      "train loss:0.4285040080261672\n",
      "train loss:0.6082368750003\n",
      "train loss:0.6161701822021524\n",
      "train loss:0.249098031777745\n",
      "train loss:0.4954590725293717\n",
      "train loss:0.34305724521250486\n",
      "train loss:0.705569063106628\n",
      "train loss:0.5036372108668387\n",
      "train loss:0.8381783781809515\n",
      "train loss:0.6461331887829201\n",
      "train loss:0.728642232469727\n",
      "train loss:0.690068450894754\n",
      "train loss:0.6822817055082327\n",
      "train loss:0.5152719089001313\n",
      "train loss:0.6353884391570432\n",
      "train loss:0.6017810012968493\n",
      "train loss:0.6928067161325273\n",
      "train loss:0.6507300748344376\n",
      "train loss:0.5208169955205919\n",
      "train loss:0.5940230261383694\n",
      "train loss:0.6304257343782258\n",
      "train loss:0.5136024918705011\n",
      "train loss:0.6820377781687592\n",
      "train loss:0.5437763546558196\n",
      "train loss:0.5337620508551914\n",
      "train loss:0.7046191355012212\n",
      "train loss:0.299752371895387\n",
      "train loss:0.3728265325941299\n",
      "train loss:0.7980854521243671\n",
      "train loss:0.9238251878494836\n",
      "train loss:0.3620294237493014\n",
      "train loss:0.7593544559086134\n",
      "train loss:0.6163374445319683\n",
      "train loss:0.39367621194443064\n",
      "train loss:0.6242998626948844\n",
      "train loss:0.7954328479190125\n",
      "train loss:0.526439826918866\n",
      "train loss:0.755568228991726\n",
      "train loss:0.6179754213938299\n",
      "train loss:0.617009389159787\n",
      "train loss:0.5587878851652737\n",
      "train loss:0.5629190756303915\n",
      "train loss:0.441891607540823\n",
      "train loss:0.5518053372235008\n",
      "train loss:0.6171855716164896\n",
      "train loss:0.7552752883813444\n",
      "train loss:0.6110138440140888\n",
      "train loss:0.4536523701693208\n",
      "train loss:0.5222582649057994\n",
      "train loss:0.7021084239788277\n",
      "train loss:0.7913484400312738\n",
      "train loss:0.6992361672656059\n",
      "train loss:0.6957924305639838\n",
      "train loss:0.6910573405526156\n",
      "train loss:0.7583303163572299\n",
      "train loss:0.5462266188209224\n",
      "train loss:0.48218112259207924\n",
      "train loss:0.47500231211802524\n",
      "train loss:0.6834755023252556\n",
      "train loss:0.463963952450562\n",
      "train loss:0.5329308446303986\n",
      "train loss:0.5221174215367498\n",
      "train loss:0.5151122788580452\n",
      "train loss:0.51101134770082\n",
      "train loss:0.6203067113209477\n",
      "train loss:0.36521709756728493\n",
      "train loss:0.493649946306974\n",
      "train loss:0.33805459995805326\n",
      "train loss:0.6899463944756924\n",
      "train loss:0.8326261692849937\n",
      "train loss:0.3421899380436651\n",
      "train loss:0.33639103712980545\n",
      "train loss:0.6545957389708775\n",
      "train loss:0.36035791647009396\n",
      "train loss:0.7548832875033867\n",
      "train loss:0.3820675697778127\n",
      "train loss:0.737709068216295\n",
      "train loss:0.5136542038788322\n",
      "train loss:0.5122379682881408\n",
      "train loss:0.5174511627889723\n",
      "train loss:0.5183864842186765\n",
      "train loss:0.6989988618563007\n",
      "train loss:0.695192222254619\n",
      "train loss:0.6132673668127329\n",
      "train loss:0.4496644310736517\n",
      "train loss:0.7688664711442869\n",
      "train loss:0.7569616603607229\n",
      "train loss:0.6854325114979718\n",
      "train loss:0.6834505424648853\n",
      "train loss:0.558830137482186\n",
      "train loss:0.62035928703489\n",
      "train loss:0.6208063105159937\n",
      "train loss:0.6210100212513023\n",
      "train loss:0.6201560401794238\n",
      "train loss:0.5561162479453353\n",
      "train loss:0.6773156623551864\n",
      "train loss:0.6144581473425463\n",
      "train loss:0.6818693688560895\n",
      "train loss:0.612405990393835\n",
      "train loss:0.4567779910327797\n",
      "train loss:0.7716245939160098\n",
      "train loss:0.8391986125608666\n",
      "train loss:0.45508156578986725\n",
      "train loss:0.6870107192437688\n",
      "train loss:0.44921609946424157\n",
      "train loss:0.6144370121359786\n",
      "train loss:0.6987914172542837\n",
      "train loss:0.5207372642087723\n",
      "train loss:0.5162300966202429\n",
      "train loss:0.2981598395842576\n",
      "train loss:0.3798073198635609\n",
      "train loss:0.6271211012225513\n",
      "train loss:0.5021020153877592\n",
      "train loss:0.8221628298227419\n",
      "train loss:0.6472743569247938\n",
      "train loss:0.7564263314234797\n",
      "train loss:0.7241152939426968\n",
      "train loss:0.6127342133004314\n",
      "train loss:0.6879801150527554\n",
      "train loss:0.5538740047325624\n",
      "train loss:0.5560246287284232\n",
      "train loss:0.5643219471910271\n",
      "train loss:0.7843163760166358\n",
      "train loss:0.6301370154870042\n",
      "train loss:0.5320150444403917\n",
      "train loss:0.682011262176508\n",
      "train loss:0.6320352185019467\n",
      "train loss:0.6778148879247192\n",
      "train loss:0.5270803590752805\n",
      "train loss:0.6230739166859405\n",
      "train loss:0.5039911504118179\n",
      "train loss:0.6793965153845504\n",
      "train loss:0.5445702798403516\n",
      "train loss:0.6849258990988014\n",
      "train loss:0.6095803007960087\n",
      "train loss:0.6001173344791906\n",
      "train loss:0.6971816933112842\n",
      "train loss:0.7005104828766672\n",
      "train loss:0.5165451135739765\n",
      "train loss:0.5151345325402086\n",
      "train loss:0.6125054213213413\n",
      "train loss:0.6178872067213645\n",
      "train loss:0.6160673826225629\n",
      "train loss:0.6112157323449083\n",
      "train loss:0.7134727618803294\n",
      "train loss:0.6070451142890625\n",
      "train loss:0.7010926415933231\n",
      "train loss:0.33450091814898253\n",
      "train loss:0.4193130912223393\n",
      "train loss:0.6977072286912719\n",
      "train loss:0.5118620806441715\n",
      "train loss:0.611572094580719\n",
      "train loss:0.5089489767856774\n",
      "train loss:0.6194942404903345\n",
      "train loss:0.5000375801941903\n",
      "train loss:0.6191513060876801\n",
      "train loss:0.7322807215309752\n",
      "train loss:0.7167884056313574\n",
      "train loss:0.7051996906380931\n",
      "train loss:0.774538420023615\n",
      "train loss:0.6773400581259987\n",
      "train loss:0.7394563955829605\n",
      "train loss:0.5162592894439333\n",
      "train loss:0.5225564639251593\n",
      "train loss:0.7739144023534165\n",
      "train loss:0.676976444775351\n",
      "train loss:0.4986461839269082\n",
      "train loss:0.5387189392836387\n",
      "train loss:0.5327372564078379\n",
      "train loss:0.7790224598537226\n",
      "train loss:0.6782245698079596\n",
      "train loss:0.7274224107545436\n",
      "train loss:0.5700984061432137\n",
      "train loss:0.5628648558690024\n",
      "train loss:0.4929133600860813\n",
      "train loss:0.6803143143292613\n",
      "train loss:0.6118903863324199\n",
      "train loss:0.6847236036276164\n",
      "train loss:0.5299153376809186\n",
      "train loss:0.7696722871322501\n",
      "train loss:0.6143639861451812\n",
      "train loss:0.5212605799954072\n",
      "train loss:0.6142319581207789\n",
      "train loss:0.6119980484789159\n",
      "train loss:0.4134501032457608\n",
      "train loss:0.5076459984035597\n",
      "train loss:0.7337704343232994\n",
      "train loss:1.020667115019997\n",
      "train loss:0.5184880054089326\n",
      "train loss:0.6942160771205022\n",
      "train loss:0.614405926113976\n",
      "train loss:0.45868728798330893\n",
      "train loss:0.5398802245160859\n",
      "train loss:0.6115465313920769\n",
      "train loss:0.5352887964221005\n",
      "train loss:0.531674472728468\n",
      "train loss:0.7697022391973112\n",
      "train loss:0.5297186947767842\n",
      "train loss:0.6108795121945587\n",
      "train loss:0.6964878209356962\n",
      "train loss:0.44201766806569776\n",
      "train loss:0.6100066984714104\n",
      "train loss:0.6075434024223894\n",
      "train loss:0.7921956507187472\n",
      "train loss:0.5225429078123394\n",
      "train loss:0.517458507290628\n",
      "train loss:0.5062870339002836\n",
      "train loss:0.3015917319130482\n",
      "train loss:0.6202770311582498\n",
      "train loss:0.5014027379882329\n",
      "train loss:0.7495346133682762\n",
      "train loss:0.3623297484874831\n",
      "train loss:0.630843732938859\n",
      "train loss:0.3551652215268464\n",
      "train loss:0.9108241221523622\n",
      "train loss:0.5015190952813516\n",
      "train loss:0.7461542700378898\n",
      "train loss:0.6243392081871604\n",
      "train loss:0.30584224787391767\n",
      "train loss:0.4056686676167516\n",
      "train loss:0.39968508451091067\n",
      "train loss:0.7240573067683413\n",
      "train loss:0.3951495287693243\n",
      "train loss:0.7285095375926719\n",
      "train loss:0.6135710919448929\n",
      "train loss:0.3980814762638614\n",
      "train loss:0.3828629826453253\n",
      "train loss:0.7428037706014308\n",
      "train loss:0.9471657009885472\n",
      "train loss:0.6122486584194521\n",
      "train loss:0.6080027047688166\n",
      "train loss:0.43448241365902707\n",
      "train loss:0.6116275417923338\n",
      "train loss:0.44408184580414456\n",
      "train loss:0.4344243145452844\n",
      "train loss:0.5235403173973032\n",
      "train loss:0.7106177440200708\n",
      "train loss:0.6113443421820098\n",
      "train loss:0.5130822730090847\n",
      "train loss:0.6984732945581897\n",
      "train loss:0.6092045470304909\n",
      "train loss:0.7961392363707966\n",
      "train loss:0.6064446595358907\n",
      "train loss:0.6974053442429119\n",
      "train loss:0.522978757625682\n",
      "train loss:0.7592778383252218\n",
      "train loss:0.6134178678937804\n",
      "train loss:0.5405108241575481\n",
      "train loss:0.6070341254292356\n",
      "train loss:0.7546588076280834\n",
      "train loss:0.6826394399299304\n",
      "train loss:0.5511579197299328\n",
      "train loss:0.8061011302594867\n",
      "train loss:0.5539533506143155\n",
      "train loss:0.5561746079132208\n",
      "train loss:0.7385719862750961\n",
      "train loss:0.5568371700758838\n",
      "train loss:0.6163945797957061\n",
      "train loss:0.5490391238071057\n",
      "train loss:0.5515094526473898\n",
      "train loss:0.6142108911743516\n",
      "train loss:0.7468763138214196\n",
      "train loss:0.611683025407636\n",
      "train loss:0.5950113465941415\n",
      "train loss:0.5264879542404668\n",
      "train loss:0.4293749967093506\n",
      "train loss:0.5194082276291622\n",
      "train loss:0.3849156818399987\n",
      "train loss:0.37014498719306194\n",
      "train loss:0.47806462899199653\n",
      "train loss:0.4967709085370421\n",
      "train loss:0.32365074012085016\n",
      "train loss:0.9116357646913047\n",
      "train loss:0.5150749109208488\n",
      "train loss:0.33450691968667445\n",
      "train loss:0.8295066060739609\n",
      "train loss:0.49575181927461476\n",
      "train loss:0.5021124313373818\n",
      "train loss:0.385280295399433\n",
      "train loss:0.2601347954014135\n",
      "train loss:0.7397107145554199\n",
      "train loss:0.5046654213409991\n",
      "train loss:0.8035662973583427\n",
      "train loss:0.6852473697778318\n",
      "train loss:0.5985960156864766\n",
      "train loss:0.5321528239944342\n",
      "train loss:0.6081481877548219\n",
      "train loss:0.4629056538503679\n",
      "train loss:0.5307613521415826\n",
      "train loss:0.6005919647697584\n",
      "train loss:0.5241342811856768\n",
      "train loss:0.3461398938657404\n",
      "train loss:0.7017859981672043\n",
      "train loss:0.5255094283594224\n",
      "train loss:0.5993752459834962\n",
      "train loss:0.2842515478358338\n",
      "train loss:0.6214847418971661\n",
      "train loss:0.6243068155761946\n",
      "train loss:0.360085549245481\n",
      "train loss:0.7726504889657296\n",
      "train loss:0.5069651572517718\n",
      "train loss:1.013390544116741\n",
      "train loss:0.8263004702653486\n",
      "train loss:0.6194814448434989\n",
      "train loss:0.5983633720271262\n",
      "train loss:0.7650888080216105\n",
      "train loss:0.6902495256799123\n",
      "train loss:0.4935184873218878\n",
      "train loss:0.622244886695848\n",
      "train loss:0.5088035136532636\n",
      "train loss:0.6936008897712537\n",
      "train loss:0.5143073540072696\n",
      "train loss:0.5769654402976486\n",
      "train loss:0.6829299794702574\n",
      "train loss:0.6787924953899551\n",
      "train loss:0.7431705369889785\n",
      "train loss:0.5619790595505048\n",
      "train loss:0.6214796907041312\n",
      "train loss:0.681354538416162\n",
      "train loss:0.5517983996798972\n",
      "train loss:0.6109647657809134\n",
      "train loss:0.548364625699575\n",
      "train loss:0.6891777896309901\n",
      "train loss:0.5326946591682286\n",
      "train loss:0.52724387303409\n",
      "train loss:0.7725101208426376\n",
      "train loss:0.6180884571207496\n",
      "train loss:0.6932692069422668\n",
      "train loss:0.6840390684232117\n",
      "train loss:0.4207054126013329\n",
      "train loss:0.781078450644521\n",
      "train loss:0.5206059178587432\n",
      "train loss:0.5131503051739225\n",
      "train loss:0.6118663047989521\n",
      "train loss:0.9025317094659968\n",
      "train loss:0.7723006607954136\n",
      "train loss:0.5289984445797465\n",
      "train loss:0.5352179204146582\n",
      "train loss:0.5259868877418576\n",
      "train loss:0.5234149510420862\n",
      "train loss:0.526924307604129\n",
      "train loss:0.43227823414521493\n",
      "train loss:0.603945790927691\n",
      "train loss:0.6137440450282041\n",
      "train loss:0.5017935718394099\n",
      "train loss:0.4971525668230167\n",
      "train loss:0.3872188624555845\n",
      "train loss:0.865491044948697\n",
      "train loss:0.501100774138346\n",
      "train loss:0.4944411678207425\n",
      "train loss:0.5087277152888992\n",
      "train loss:0.7448521041116101\n",
      "train loss:0.6148971491210983\n",
      "train loss:0.7494911108388425\n",
      "train loss:0.5130415202249081\n",
      "train loss:0.38104807272572827\n",
      "train loss:0.5000187329735295\n",
      "train loss:0.39784094898956673\n",
      "train loss:0.616454913147933\n",
      "train loss:0.5031899562118798\n",
      "train loss:0.9578158701034152\n",
      "train loss:0.619880763626338\n",
      "train loss:0.4122664035278672\n",
      "train loss:0.6276222645755825\n",
      "train loss:0.5090197674072343\n",
      "train loss:0.41058607855657564\n",
      "train loss:0.7060726979830154\n",
      "train loss:0.7011501081219399\n",
      "train loss:0.6085399551087247\n",
      "train loss:0.7029846807398956\n",
      "train loss:0.6003341702456998\n",
      "train loss:0.3464851787332533\n",
      "train loss:0.4298524922872815\n",
      "train loss:0.6100548519226525\n",
      "train loss:0.4167419542087458\n",
      "train loss:0.7030703164264114\n",
      "train loss:0.8037306669499749\n",
      "train loss:0.5113072977792619\n",
      "train loss:0.5939244342114636\n",
      "train loss:0.29575796480236016\n",
      "train loss:0.5067633823760467\n",
      "train loss:0.8441780161822303\n",
      "train loss:0.7050263032070798\n",
      "train loss:0.6147010340235833\n",
      "train loss:0.7186117926880826\n",
      "train loss:0.6099017391080018\n",
      "train loss:0.593641072967871\n",
      "train loss:0.6141368150680045\n",
      "train loss:0.7769569914431791\n",
      "train loss:0.6040117662453695\n",
      "train loss:0.6891827835517746\n",
      "train loss:0.6793370850179582\n",
      "train loss:0.5419456654769041\n",
      "train loss:0.47899018206544\n",
      "train loss:0.8188289985864621\n",
      "train loss:0.4887836052290244\n",
      "train loss:0.5411138571604506\n",
      "train loss:0.48016009173285024\n",
      "train loss:0.47275240657062884\n",
      "train loss:0.4523882241606475\n",
      "train loss:0.6156202408467811\n",
      "train loss:0.7667071665432861\n",
      "train loss:0.42563709778462344\n",
      "train loss:0.7987997478194203\n",
      "train loss:0.5073861941144411\n",
      "train loss:0.5897287224579186\n",
      "train loss:0.5014790450306241\n",
      "train loss:0.6120409728075631\n",
      "train loss:0.475914012666138\n",
      "train loss:0.8469907828575852\n",
      "train loss:0.5996633181767812\n",
      "train loss:0.6139135006600416\n",
      "train loss:0.8470012508442462\n",
      "train loss:0.803400563599159\n",
      "train loss:0.6119355645856331\n",
      "train loss:0.6797109818476623\n",
      "train loss:0.7575130632599502\n",
      "train loss:0.6071370938590673\n",
      "train loss:0.6178450622723489\n",
      "train loss:0.5515806107239036\n",
      "train loss:0.6197566254767294\n",
      "train loss:0.6235456111680395\n",
      "train loss:0.4927494637502859\n",
      "train loss:0.6784388083449115\n",
      "train loss:0.6260571271902126\n",
      "train loss:0.6296745597700252\n",
      "train loss:0.6759301653921496\n",
      "train loss:0.48638113052933507\n",
      "train loss:0.8651933212296378\n",
      "train loss:0.42427831297900254\n",
      "train loss:0.48365860701885877\n",
      "train loss:0.6056323422375337\n",
      "train loss:0.5993392940606401\n",
      "train loss:0.6024484893124047\n",
      "train loss:0.6777392763670769\n",
      "train loss:0.7086299949369359\n",
      "train loss:0.5901945117924707\n",
      "train loss:0.4327179277904964\n",
      "train loss:0.6805568599987916\n",
      "train loss:0.6244854929628976\n",
      "train loss:0.40301312614450213\n",
      "train loss:0.5051186195978261\n",
      "train loss:0.744071903539561\n",
      "train loss:0.3819985653021427\n",
      "train loss:0.7954809732293437\n",
      "train loss:0.5983616078285938\n",
      "train loss:0.5063610928790216\n",
      "train loss:0.7409442866135376\n",
      "train loss:0.6948130476955279\n",
      "train loss:0.6178391151519562\n",
      "train loss:0.6121554088368766\n",
      "train loss:0.6831841917315886\n",
      "train loss:0.6050108986281286\n",
      "train loss:0.7015521752800643\n",
      "train loss:0.44247402738364894\n",
      "train loss:0.600494733427114\n",
      "train loss:0.5335915432548616\n",
      "train loss:0.52651176258032\n",
      "train loss:0.5066177606025599\n",
      "train loss:0.6015297016896118\n",
      "train loss:0.6919615410960905\n",
      "train loss:0.6152596839238142\n",
      "train loss:0.5007966466354583\n",
      "train loss:0.6161298957653653\n",
      "train loss:0.7068423248459913\n",
      "train loss:0.5351483751707055\n",
      "train loss:0.5743537502611762\n",
      "train loss:0.5031433634185137\n",
      "train loss:0.6713777825514622\n",
      "train loss:0.6029257565549089\n",
      "train loss:0.725776066851855\n",
      "train loss:0.7093649693948137\n",
      "train loss:0.5193513506276387\n",
      "train loss:0.6166361167004882\n",
      "train loss:0.6064336125471558\n",
      "train loss:0.6230660696772731\n",
      "train loss:0.6122737654496005\n",
      "train loss:0.696343917732194\n",
      "train loss:0.6016727862992182\n",
      "train loss:0.614661801381926\n",
      "train loss:0.5239511971302732\n",
      "train loss:0.526809847185149\n",
      "train loss:0.5258335376682531\n",
      "train loss:0.6828313273913036\n",
      "train loss:0.43493609004731193\n",
      "train loss:0.43254692262430394\n",
      "train loss:0.5101235941510744\n",
      "train loss:0.594161616225572\n",
      "train loss:0.5171325804079899\n",
      "train loss:0.4793420710920122\n",
      "train loss:0.6246037122034596\n",
      "train loss:0.5894922992318318\n",
      "train loss:0.5908407092500068\n",
      "train loss:0.6363751254499703\n",
      "train loss:0.6189304665077925\n",
      "train loss:0.5082260709288169\n",
      "train loss:0.6319102837767971\n",
      "train loss:0.7374338596394635\n",
      "train loss:0.6094321986880111\n",
      "train loss:0.38360486085592455\n",
      "train loss:0.7003212558735279\n",
      "train loss:0.48798562119282957\n",
      "train loss:0.46831869135970994\n",
      "train loss:0.48598362382981525\n",
      "train loss:0.6004578821048219\n",
      "train loss:0.4878007805801225\n",
      "train loss:0.8233790478871679\n",
      "train loss:0.7117637514178499\n",
      "train loss:0.6092840600878404\n",
      "train loss:0.7530695756518053\n",
      "train loss:0.6472930120252155\n",
      "train loss:0.8546731911890341\n",
      "train loss:0.4575979260847867\n",
      "train loss:0.580336566747391\n",
      "train loss:0.667688277236859\n",
      "train loss:0.6117580329400008\n",
      "train loss:0.6381100579581622\n",
      "train loss:0.5465920584503418\n",
      "train loss:0.6802069437334751\n",
      "train loss:0.7413258473215949\n",
      "train loss:0.7304303043621265\n",
      "train loss:0.5525889110474218\n",
      "train loss:0.483319707385338\n",
      "train loss:0.5417312338414921\n",
      "train loss:0.7534749250482551\n",
      "train loss:0.6033465234625085\n",
      "train loss:0.6017476229047833\n",
      "train loss:0.5315009990867694\n",
      "train loss:0.5376111723235937\n",
      "train loss:0.6950714618102692\n",
      "train loss:0.7859937270943415\n",
      "train loss:0.683057687541076\n",
      "train loss:0.8222224563788126\n",
      "train loss:0.6811647061108287\n",
      "train loss:0.7436933499515235\n",
      "train loss:0.6091950760089244\n",
      "train loss:0.6071215670710068\n",
      "train loss:0.4971997025456486\n",
      "train loss:0.5649127122956956\n",
      "train loss:0.5548327966297897\n",
      "train loss:0.5556739317553395\n",
      "train loss:0.4774319437518897\n",
      "train loss:0.6159930508708718\n",
      "train loss:0.5322759949961484\n",
      "train loss:0.6122425886117504\n",
      "train loss:0.6218965425505208\n",
      "train loss:0.8661823774114596\n",
      "train loss:0.7853471173066755\n",
      "train loss:0.4305402302104452\n",
      "train loss:0.6132549845733102\n",
      "train loss:0.31742334189099675\n",
      "train loss:0.7093181038203558\n",
      "train loss:0.5026367421943647\n",
      "train loss:0.4943464228612907\n",
      "train loss:0.6106736120995927\n",
      "train loss:0.7036244453330094\n",
      "train loss:0.6176295854139946\n",
      "train loss:0.6092817693448817\n",
      "train loss:0.5939713926482683\n",
      "train loss:0.5107202299070477\n",
      "train loss:0.4946419247391535\n",
      "train loss:0.2756518340765475\n",
      "train loss:0.7000911494135077\n",
      "train loss:0.4931582151609898\n",
      "train loss:0.5007441323970416\n",
      "train loss:0.564016728872655\n",
      "train loss:0.48699463638879426\n",
      "train loss:0.49185784194278004\n",
      "train loss:0.7734992562076549\n",
      "train loss:0.3544611109679656\n",
      "train loss:0.8666241648487464\n",
      "train loss:0.828796756884841\n",
      "train loss:0.5724726290890206\n",
      "train loss:0.36959008445571107\n",
      "train loss:0.3077866312793676\n",
      "train loss:0.708871979071928\n",
      "train loss:0.6148114611551161\n",
      "train loss:0.501951186798957\n",
      "train loss:0.7952551596194387\n",
      "train loss:0.8102527824776338\n",
      "train loss:0.33132254347334156\n",
      "train loss:0.5101186312433004\n",
      "train loss:0.6095749415593836\n",
      "train loss:0.6931308634032172\n",
      "train loss:0.608581175504945\n",
      "train loss:0.3393147469751612\n",
      "train loss:0.6118269095453783\n",
      "train loss:0.4183261469098035\n",
      "train loss:0.6050429817255168\n",
      "train loss:0.6036642086151656\n",
      "train loss:0.6075377728971361\n",
      "train loss:0.8913898225546593\n",
      "train loss:0.6183561192584589\n",
      "train loss:0.6153388068621692\n",
      "train loss:0.5990730977938027\n",
      "train loss:0.327516649460166\n",
      "train loss:0.49582179897114065\n",
      "train loss:0.5015598945390478\n",
      "train loss:0.49521542143949515\n",
      "train loss:0.8189548076123689\n",
      "train loss:0.6977657012230288\n",
      "train loss:0.7069314356528753\n",
      "train loss:0.6147756501353713\n",
      "train loss:0.6858915628074438\n",
      "train loss:0.6089131962322053\n",
      "train loss:0.542544893363617\n",
      "train loss:0.42700567686005864\n",
      "train loss:0.5207412544615719\n",
      "train loss:0.5995733251795642\n",
      "train loss:0.6083290513765917\n",
      "train loss:0.7017703134787285\n",
      "train loss:0.5941354176405673\n",
      "train loss:0.595914074921793\n",
      "train loss:0.5094871796771832\n",
      "train loss:0.604155679412197\n",
      "train loss:0.5253266197884223\n",
      "train loss:0.6877490037649899\n",
      "train loss:0.6049005563447675\n",
      "train loss:0.5126307916865486\n",
      "train loss:0.8674219040928497\n",
      "train loss:0.5196820707153039\n",
      "train loss:0.6877764196664194\n",
      "train loss:0.6004341374580296\n",
      "train loss:0.5124297406917024\n",
      "train loss:0.5311865172000613\n",
      "train loss:0.515957362688025\n",
      "train loss:0.4184102036249647\n",
      "train loss:0.7780836208457492\n",
      "train loss:0.4008686729186568\n",
      "train loss:0.6103448048394137\n",
      "train loss:0.5057859139458614\n",
      "train loss:0.5731162997837604\n",
      "train loss:0.35975632392878215\n",
      "train loss:0.4896961406163934\n",
      "train loss:0.5816763525579118\n",
      "train loss:0.5036336626156153\n",
      "train loss:0.49055257684164105\n",
      "train loss:0.7829476359423955\n",
      "train loss:0.7053071748034336\n",
      "train loss:0.7912897095260298\n",
      "train loss:0.6257945085483391\n",
      "train loss:0.7082331459099549\n",
      "train loss:0.8643961841283145\n",
      "train loss:0.5624157830888512\n",
      "train loss:0.764024736963363\n",
      "train loss:0.5880946363310015\n",
      "train loss:0.5570781390424095\n",
      "train loss:0.5937047255903306\n",
      "train loss:0.7372735447152609\n",
      "train loss:0.5000140472472611\n",
      "train loss:0.6741856032913958\n",
      "train loss:0.5021714002692226\n",
      "train loss:0.5506112445074047\n",
      "train loss:0.6031492879139012\n",
      "train loss:0.6024240478945443\n",
      "train loss:0.541958909419311\n",
      "train loss:0.6681040254737762\n",
      "train loss:0.7648225931066756\n",
      "train loss:0.7437133479131273\n",
      "train loss:0.6198921180945797\n",
      "train loss:0.6073208811243526\n",
      "train loss:0.4696352179972143\n",
      "train loss:0.5288016015706749\n",
      "train loss:0.5126370308480188\n",
      "train loss:0.5170581865084061\n",
      "train loss:0.6054902549735746\n",
      "train loss:0.6257135670766151\n",
      "train loss:0.5063031321490359\n",
      "train loss:0.6156374726808373\n",
      "train loss:0.35205518327761426\n",
      "train loss:0.5661857433074742\n",
      "train loss:0.4582539732288565\n",
      "train loss:0.47531590227528364\n",
      "train loss:0.6781099382096758\n",
      "train loss:0.8632307860779429\n",
      "train loss:0.5026674051078446\n",
      "train loss:0.786031244825317\n",
      "train loss:0.6778317705286769\n",
      "train loss:0.7988543919146373\n",
      "train loss:0.5706700868594136\n",
      "train loss:0.5328843461529115\n",
      "train loss:0.5424910253609962\n",
      "train loss:0.6313826754872125\n",
      "train loss:0.6783001933207651\n",
      "train loss:0.63188927073667\n",
      "train loss:0.5096427364477691\n",
      "train loss:0.54016498677877\n",
      "train loss:0.6294605305651938\n",
      "train loss:0.6294751139029786\n",
      "train loss:0.5690926063856477\n",
      "train loss:0.6777310560967406\n",
      "train loss:0.562080717655166\n",
      "train loss:0.5529957835579505\n",
      "train loss:0.5448487558494857\n",
      "train loss:0.6069817656450437\n",
      "train loss:0.527151235632893\n",
      "train loss:0.6966177432107837\n",
      "train loss:0.519090381188501\n",
      "train loss:0.41935740962314016\n",
      "train loss:0.40104740920518545\n",
      "train loss:0.6116701424811306\n",
      "train loss:0.7401370948968988\n",
      "train loss:0.5049819031619165\n",
      "train loss:0.49984427875901094\n",
      "train loss:0.754697761880351\n",
      "train loss:0.37124704398434294\n",
      "train loss:0.8910236903995316\n",
      "train loss:0.7493765184277218\n",
      "train loss:0.5000555180344561\n",
      "train loss:0.38757178245936846\n",
      "train loss:0.6183913580455199\n",
      "train loss:0.5015781016358127\n",
      "train loss:0.38545864863403045\n",
      "train loss:0.8391110685775562\n",
      "train loss:0.6094147192256589\n",
      "train loss:0.5069475270456456\n",
      "train loss:0.5005024316755692\n",
      "train loss:0.40124216848301425\n",
      "train loss:0.913467851448841\n",
      "train loss:0.9203107915193712\n",
      "train loss:0.4235244659004424\n",
      "train loss:0.417325046177047\n",
      "train loss:0.610585024986658\n",
      "train loss:0.6983314131880791\n",
      "train loss:0.6123319207002816\n",
      "train loss:0.42133320040632505\n",
      "train loss:0.7856128841521858\n",
      "train loss:0.7757671920101374\n",
      "train loss:0.6127780045160283\n",
      "train loss:0.5225525033205349\n",
      "train loss:0.449201748234914\n",
      "train loss:0.763660958816222\n",
      "train loss:0.5313281465223207\n",
      "train loss:0.5341066321404208\n",
      "train loss:0.5320384253632552\n",
      "train loss:0.5239776816669277\n",
      "train loss:0.853648349366717\n",
      "train loss:0.5283910538044084\n",
      "train loss:0.8540371365429695\n",
      "train loss:0.4379285971310202\n",
      "train loss:0.5232614471004878\n",
      "train loss:0.3476481765584865\n",
      "train loss:0.6067397643658703\n",
      "train loss:0.5160220652646652\n",
      "train loss:0.7012657890192202\n",
      "train loss:0.5210596004397289\n",
      "train loss:0.40908346135284723\n",
      "train loss:0.7270928518763313\n",
      "train loss:0.592561562891501\n",
      "train loss:0.5063065073512951\n",
      "train loss:0.7296475742218087\n",
      "train loss:0.4966855593274973\n",
      "train loss:0.8230830930702602\n",
      "train loss:0.3851533854720278\n",
      "train loss:0.6162724813987305\n",
      "train loss:0.6179571297938045\n",
      "train loss:0.7214156358855128\n",
      "train loss:0.6260827838781146\n",
      "train loss:0.40211215106782544\n",
      "train loss:0.612834482404484\n",
      "train loss:0.3970439163220217\n",
      "train loss:0.8986746606099064\n",
      "train loss:0.6996667045962806\n",
      "train loss:0.329544829610583\n",
      "train loss:0.5108159831119632\n",
      "train loss:0.5023357471801305\n",
      "train loss:0.9894870927538204\n",
      "train loss:0.5162758147096725\n",
      "train loss:0.5032575387624312\n",
      "train loss:0.7871165158732948\n",
      "train loss:0.6115157208138154\n",
      "train loss:0.673922299382313\n",
      "train loss:0.6973580070560568\n",
      "train loss:0.6712042170005883\n",
      "train loss:0.6150400580132589\n",
      "train loss:0.6566279475485141\n",
      "train loss:0.6845336946241656\n",
      "train loss:0.6779975159707275\n",
      "train loss:0.5490288336557433\n",
      "train loss:0.6820290576259891\n",
      "train loss:0.5511184716921583\n",
      "train loss:0.6174673158661513\n",
      "train loss:0.613552340736028\n",
      "train loss:0.5358768767587071\n",
      "train loss:0.7527965689828123\n",
      "train loss:0.7418864742605283\n",
      "train loss:0.40975469750015653\n",
      "train loss:0.6759733519188798\n",
      "train loss:0.6918841154191041\n",
      "train loss:0.6839395681019707\n",
      "train loss:0.47060871104154334\n",
      "train loss:0.8356099850092547\n",
      "train loss:0.5458254672340102\n",
      "train loss:0.607866525895778\n",
      "train loss:0.6107118932274054\n",
      "train loss:0.5419363000176534\n",
      "train loss:0.5316253548645304\n",
      "train loss:0.7649826767663683\n",
      "train loss:0.6960635368786244\n",
      "train loss:0.44888158569331627\n",
      "train loss:0.6079954665737172\n",
      "train loss:0.6057842413142264\n",
      "train loss:0.4283117999605544\n",
      "train loss:0.51785148391174\n",
      "train loss:0.5014001794430993\n",
      "train loss:0.6881975463759094\n",
      "train loss:0.49906502226078897\n",
      "train loss:0.4755752963839939\n",
      "train loss:0.624438489863004\n",
      "train loss:0.5024856873258448\n",
      "train loss:0.65845127671472\n",
      "train loss:0.4632398449437556\n",
      "train loss:0.8803501149661173\n",
      "train loss:0.6842973004210432\n",
      "train loss:0.6146090090195194\n",
      "train loss:0.8255715699921138\n",
      "train loss:0.6001638819217548\n",
      "train loss:0.5214052034825104\n",
      "train loss:0.5254213328253088\n",
      "train loss:0.5184586818301369\n",
      "train loss:0.682650127266726\n",
      "train loss:0.6876551344012244\n",
      "train loss:0.4623449250673389\n",
      "train loss:0.6116391540281774\n",
      "train loss:0.5440471629935445\n",
      "train loss:0.6113233300794632\n",
      "train loss:0.8312390016355333\n",
      "train loss:0.7542162047900189\n",
      "train loss:0.47151162157451276\n",
      "train loss:0.4672103626034237\n",
      "train loss:0.6101634650209184\n",
      "train loss:0.7588957829785666\n",
      "train loss:0.4624442191617225\n",
      "train loss:0.5334144470749294\n",
      "train loss:0.45229683143741645\n",
      "train loss:0.6111193120313692\n",
      "train loss:0.6899975000205745\n",
      "train loss:0.5217481932735907\n",
      "train loss:0.6153926651607564\n",
      "train loss:0.5119348754046694\n",
      "train loss:0.5087437044548232\n",
      "train loss:0.6125372655491549\n",
      "train loss:0.8126774054087742\n",
      "train loss:0.6112405290145186\n",
      "train loss:0.504759974297631\n",
      "train loss:0.7124449960066929\n",
      "train loss:0.5104583911661822\n",
      "train loss:0.29196599380570387\n",
      "train loss:0.39535874021092887\n",
      "train loss:0.7265806990925271\n",
      "train loss:0.6139370553595327\n",
      "train loss:0.7382055818469959\n",
      "train loss:0.7351945528565763\n",
      "train loss:0.725622231853362\n",
      "train loss:0.5039750367060063\n",
      "train loss:0.29623760957658885\n",
      "train loss:0.9180217916984452\n",
      "train loss:0.5070949059365513\n",
      "train loss:0.7043371292471099\n",
      "train loss:0.6969769442421714\n",
      "train loss:0.5226723161351161\n",
      "train loss:0.8733300832936605\n",
      "train loss:0.6077417553048797\n",
      "train loss:0.530717368339301\n",
      "train loss:0.539520214410109\n",
      "train loss:0.532737507202873\n",
      "train loss:0.4616427429672953\n",
      "train loss:0.3757282946841713\n",
      "train loss:0.8486597621559527\n",
      "train loss:0.6261783884378261\n",
      "train loss:0.5222771771829913\n",
      "train loss:0.6908814875598848\n",
      "train loss:0.5232403708462197\n",
      "train loss:0.4373248595705871\n",
      "train loss:0.6121859831855623\n",
      "train loss:0.6057121305336384\n",
      "train loss:0.5169069390083874\n",
      "train loss:0.5130007392321981\n",
      "train loss:0.7026981315500135\n",
      "train loss:0.7049044676560817\n",
      "train loss:0.7052560184939415\n",
      "train loss:0.611659846357087\n",
      "train loss:0.4090320872397384\n",
      "train loss:0.40103974427972544\n",
      "train loss:0.606334914948701\n",
      "train loss:0.6146903248030041\n",
      "train loss:0.9278601153945594\n",
      "train loss:0.2962930824247044\n",
      "train loss:0.397867176486279\n",
      "train loss:0.6253439868185333\n",
      "train loss:0.3860817750920923\n",
      "train loss:0.6037580422689265\n",
      "train loss:0.5012578337605939\n",
      "train loss:0.6132842207942719\n",
      "train loss:0.3702324109354595\n",
      "train loss:0.6286816315446826\n",
      "train loss:0.6048280968244111\n",
      "train loss:0.6257389931708749\n",
      "train loss:0.6277427767889675\n",
      "train loss:0.49368097890312485\n",
      "train loss:0.7363074998884543\n",
      "train loss:0.8524817528315671\n",
      "train loss:0.8152718015299204\n",
      "train loss:0.3237733109491614\n",
      "train loss:0.6000861473900124\n",
      "train loss:0.5239619320317397\n",
      "train loss:0.5931595830702807\n",
      "train loss:0.6032515412538629\n",
      "train loss:0.5260725676857791\n",
      "train loss:0.760107353157213\n",
      "train loss:0.6809266064129404\n",
      "train loss:0.6178064241147408\n",
      "train loss:0.4632466742912219\n",
      "train loss:0.5314238138299887\n",
      "train loss:0.46432361003457634\n",
      "train loss:0.45150538243502564\n",
      "train loss:0.34319515218608443\n",
      "train loss:0.4626360120267112\n",
      "train loss:0.5083927078064934\n",
      "train loss:0.6117352443813204\n",
      "train loss:0.3812719452057315\n",
      "train loss:0.5012704572108954\n",
      "train loss:0.6327638524166026\n",
      "train loss:0.8773322081544338\n",
      "train loss:0.8750434585913249\n",
      "train loss:0.3546411485853719\n",
      "train loss:0.3666813412330886\n",
      "train loss:0.7456750803665086\n",
      "train loss:0.36892273453531466\n",
      "train loss:0.6103623045654115\n",
      "train loss:0.49356254602127414\n",
      "train loss:0.508751886234883\n",
      "train loss:0.38550995509345176\n",
      "train loss:0.6290737383530148\n",
      "train loss:1.1215517222105826\n",
      "train loss:0.5056398196308398\n",
      "train loss:0.5817287915406028\n",
      "train loss:0.7164286001711876\n",
      "train loss:0.5952864057553616\n",
      "train loss:0.7814965554055526\n",
      "train loss:0.6827364814233339\n",
      "train loss:0.5366207717839961\n",
      "train loss:0.600921240213878\n",
      "train loss:0.6835370136902459\n",
      "train loss:0.4752209498216932\n",
      "train loss:0.5416830766748612\n",
      "train loss:0.40827834779749034\n",
      "train loss:0.547694908430632\n",
      "train loss:0.6040010112583929\n",
      "train loss:0.534201575651268\n",
      "train loss:0.6807603805470043\n",
      "train loss:0.5260027241136097\n",
      "train loss:0.34081586568973143\n",
      "train loss:0.41938340958315984\n",
      "train loss:0.2975540446603648\n",
      "train loss:0.4926211405957249\n",
      "train loss:0.7511483773375247\n",
      "train loss:0.751056255855443\n",
      "train loss:0.5124673661702217\n",
      "train loss:0.48920294296381384\n",
      "train loss:0.6455687552021192\n",
      "train loss:0.774525806649243\n",
      "train loss:0.5023374722274804\n",
      "train loss:0.9984683563602299\n",
      "train loss:0.7041505630238698\n",
      "train loss:0.6088751278431698\n",
      "train loss:0.6005602924337076\n",
      "train loss:0.7712454667441264\n",
      "train loss:0.521248171406025\n",
      "train loss:0.5254580268653981\n",
      "train loss:0.3718996734380295\n",
      "train loss:0.8182022932952048\n",
      "train loss:0.5987151481418131\n",
      "train loss:0.6135829862819923\n",
      "train loss:0.7512134661371583\n",
      "train loss:0.5347874627642172\n",
      "train loss:0.4724327165006025\n",
      "train loss:0.5340559134164039\n",
      "train loss:0.6070107776603614\n",
      "train loss:0.5120710269138647\n",
      "train loss:0.44561928472133355\n",
      "train loss:0.343493605605191\n",
      "train loss:0.5190467173746017\n",
      "train loss:0.8178828137040115\n",
      "train loss:0.6234666343742038\n",
      "train loss:0.5085982607453178\n",
      "train loss:0.381871320629266\n",
      "train loss:0.6185761681026695\n",
      "train loss:0.5093335257652791\n",
      "train loss:0.5896825894887704\n",
      "train loss:0.5043607724705357\n",
      "train loss:0.358310402583684\n",
      "train loss:0.7372900189135102\n",
      "train loss:0.49677992589403086\n",
      "train loss:0.32588871272839814\n",
      "train loss:0.7624520380355492\n",
      "train loss:0.9213758294131111\n",
      "train loss:0.7270777202341736\n",
      "train loss:0.7067947677881008\n",
      "train loss:0.5058806430960898\n",
      "train loss:0.7905223529199736\n",
      "train loss:0.5983888021166772\n",
      "train loss:0.5140132431923918\n",
      "train loss:0.7544139803061906\n",
      "train loss:0.5204180337290241\n",
      "train loss:0.643956281928622\n",
      "train loss:0.5863848720862486\n",
      "train loss:0.6111921488544286\n",
      "train loss:0.4267455052505425\n",
      "train loss:0.7586144865540153\n",
      "train loss:0.6032135007515457\n",
      "train loss:0.6136735199336106\n",
      "train loss:0.5272341410326167\n",
      "train loss:0.9155823835791432\n",
      "train loss:0.5333849692260803\n",
      "train loss:0.6719795684043905\n",
      "train loss:0.5253011338037283\n",
      "train loss:0.4474758569640488\n",
      "train loss:0.8512283162091151\n",
      "train loss:0.816065723403729\n",
      "train loss:0.6140347217781457\n",
      "train loss:0.6094992249068107\n",
      "train loss:0.6190212660615881\n",
      "train loss:0.7397145262656935\n",
      "train loss:0.4808906202396397\n",
      "train loss:0.49187463808850607\n",
      "train loss:0.6668419508128316\n",
      "train loss:0.6728768927083805\n",
      "train loss:0.835168550679068\n",
      "train loss:0.5506923162078725\n",
      "train loss:0.615429860422118\n",
      "train loss:0.41644945503114916\n",
      "train loss:0.5358153817515022\n",
      "train loss:0.7688716734615411\n",
      "train loss:0.6053489953487929\n",
      "train loss:0.35847165874707043\n",
      "train loss:0.4263871001952945\n",
      "train loss:0.6375181726163367\n",
      "train loss:0.6213504818936959\n",
      "train loss:0.7062452210960647\n",
      "train loss:0.9134519966983934\n",
      "train loss:0.4940257975178775\n",
      "train loss:0.7881145237335858\n",
      "train loss:0.6870894659636976\n",
      "train loss:0.6808262658182752\n",
      "train loss:0.5366474752263616\n",
      "train loss:0.603462084344294\n",
      "train loss:0.6114665694828708\n",
      "train loss:0.5351386887149939\n",
      "train loss:0.6142243685402653\n",
      "train loss:0.47549328550082004\n",
      "train loss:0.5473944456389052\n",
      "train loss:0.7537522235411138\n",
      "train loss:0.6834575338619074\n",
      "train loss:0.5965552721715094\n",
      "train loss:0.6727981637300563\n",
      "train loss:0.4717328109295715\n",
      "train loss:0.6637150229035849\n",
      "train loss:0.6171211660503275\n",
      "train loss:0.8229841854238703\n",
      "train loss:0.4471794868929509\n",
      "train loss:0.6788059937627954\n",
      "train loss:0.5915825724663493\n",
      "train loss:0.4218233715869273\n",
      "train loss:0.60316336831067\n",
      "train loss:0.5127278983378811\n",
      "train loss:0.4983354940827551\n",
      "train loss:0.6974465055417255\n",
      "train loss:0.29555489366556603\n",
      "train loss:0.5060210561345584\n",
      "train loss:0.7145739534296565\n",
      "train loss:0.6288883762957649\n",
      "train loss:0.5236511807121904\n",
      "train loss:0.630668639688744\n",
      "train loss:0.4662023583756841\n",
      "train loss:0.5715898133480881\n",
      "train loss:1.022218097236592\n",
      "train loss:0.9716148324137814\n",
      "train loss:0.6838160509620239\n",
      "train loss:0.8568240298767489\n",
      "train loss:0.8174371654109438\n",
      "train loss:0.609755780330491\n",
      "train loss:0.5618254475561457\n",
      "train loss:0.6763599579896232\n",
      "train loss:0.5211909603992207\n",
      "train loss:0.5307573807896759\n",
      "train loss:0.5830848657899252\n",
      "train loss:0.5765438938771861\n",
      "train loss:0.5730324148305146\n",
      "train loss:0.6241417414247908\n",
      "train loss:0.6227303948778655\n",
      "train loss:0.5579497347082671\n",
      "train loss:0.4868650597000645\n",
      "train loss:0.5388629729374035\n",
      "train loss:0.6909401058488067\n",
      "train loss:0.6055057211141833\n",
      "train loss:0.5144997506291025\n",
      "train loss:0.6665273900785941\n",
      "train loss:0.6086933807022455\n",
      "train loss:0.3899902406086106\n",
      "train loss:0.6049845705431354\n",
      "train loss:0.5036611547710035\n",
      "train loss:0.8861424960201398\n",
      "train loss:0.8242048561264456\n",
      "train loss:0.5153977073128257\n",
      "train loss:0.40749759610974257\n",
      "train loss:0.693462926624216\n",
      "train loss:0.5235990831559192\n",
      "train loss:0.42238595379955435\n",
      "train loss:0.6682849689477932\n",
      "train loss:0.5893216656239822\n",
      "train loss:0.4276079696374381\n",
      "train loss:0.6099207173405101\n",
      "train loss:0.5817229223776649\n",
      "train loss:0.5045961992210384\n",
      "train loss:0.5130746633346621\n",
      "train loss:0.4944718191779951\n",
      "train loss:0.515670834688666\n",
      "train loss:0.6358934056158432\n",
      "train loss:0.22036150317762737\n",
      "train loss:0.6080365282622814\n",
      "train loss:0.655639011839954\n",
      "train loss:0.469717439458284\n",
      "train loss:0.522027822499233\n",
      "train loss:0.332523065260139\n",
      "train loss:0.4974527995018548\n",
      "train loss:0.1552945272911171\n",
      "train loss:1.0570608353786584\n",
      "train loss:0.4876388113763112\n",
      "train loss:0.6115806545228182\n",
      "train loss:0.5006610183058438\n",
      "train loss:0.4279928308863652\n",
      "train loss:0.40412773880214764\n",
      "train loss:0.5930475096660952\n",
      "train loss:0.4070647376721013\n",
      "train loss:0.4919586349211119\n",
      "train loss:0.5807924020317705\n",
      "train loss:0.4002280506233304\n",
      "train loss:0.34978922513246646\n",
      "train loss:0.4398049986978633\n",
      "train loss:0.5388349550243321\n",
      "train loss:1.0211626336514146\n",
      "train loss:0.7165962247152826\n",
      "train loss:0.3370372504347747\n",
      "train loss:0.7988229324184999\n",
      "train loss:0.3856198920468106\n",
      "train loss:0.5205304430435171\n",
      "train loss:0.5731045834675561\n",
      "train loss:0.6234659586782032\n",
      "train loss:0.5851329654997995\n",
      "train loss:0.5819671043739572\n",
      "train loss:0.3931129945436062\n",
      "train loss:0.9398399823470873\n",
      "train loss:0.7991700834205475\n",
      "train loss:1.008871949275441\n",
      "train loss:0.44114977114828385\n",
      "train loss:0.439203834885946\n",
      "train loss:0.5206697900524289\n",
      "train loss:0.5257365599219082\n",
      "train loss:0.5168783234601118\n",
      "train loss:0.34602797808264185\n",
      "train loss:0.6991341022188617\n",
      "train loss:0.532514177796316\n",
      "train loss:0.612217738399239\n",
      "train loss:0.8538862361828817\n",
      "train loss:0.7649633512975644\n",
      "train loss:0.4094891271245021\n",
      "train loss:0.34379824255647823\n",
      "train loss:0.6147817953870005\n",
      "train loss:0.398732165828796\n",
      "train loss:0.595686341854607\n",
      "train loss:0.6968800115847442\n",
      "train loss:0.606711345473995\n",
      "train loss:0.7112894182184292\n",
      "train loss:0.9063294445710541\n",
      "train loss:0.6024499051170454\n",
      "train loss:0.6226449946205395\n",
      "train loss:0.5076373889832962\n",
      "train loss:0.4221472294509335\n",
      "train loss:0.42132581471668634\n",
      "train loss:0.5239050198871057\n",
      "train loss:0.7114391933799482\n",
      "train loss:0.4729902591955842\n",
      "train loss:0.7463564890956953\n",
      "train loss:0.6007341987333517\n",
      "train loss:0.6111886561843811\n",
      "train loss:0.8424937284774288\n",
      "train loss:0.598405390944659\n",
      "train loss:0.789150489343716\n",
      "train loss:0.6208555635406203\n",
      "train loss:0.5234759025402388\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6617647058823529\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet7Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "388f9387-6be2-496a-a5a4-29f3d85ab809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet7Layer:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = int((input_size - filter_size + 2 * filter_pad) / filter_stride + 1)\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b3'] = np.zeros(hidden_size)        \n",
    "        self.params['W4'] = weight_init_std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b4'] = np.zeros(hidden_size)\n",
    "        self.params['W5'] = weight_init_std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b5'] = np.zeros(hidden_size)\n",
    "        self.params['W6'] = weight_init_std * np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b6'] = np.zeros(hidden_size)\n",
    "        self.params['W7'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b7'] = np.zeros(output_size)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Sigmoid()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Sigmoid()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.layers['Relu3'] = Sigmoid()  # 추가된 ReLU 활성화 함수\n",
    "        self.layers['Affine3'] = Affine(self.params['W4'], self.params['b4'])\n",
    "        self.layers['Relu4'] = Sigmoid()  # 추가된 ReLU 활성화 함\n",
    "        self.layers['Affine4'] = Affine(self.params['W5'], self.params['b5'])\n",
    "        self.layers['Relu5'] = Sigmoid()  # 추가된 ReLU 활성화 함\n",
    "        self.layers['Affine5'] = Affine(self.params['W6'], self.params['b6'])\n",
    "        self.layers['Relu6'] = Sigmoid()  # 추가된 ReLU 활성화 함\n",
    "        self.layers['Affine6'] = Affine(self.params['W7'], self.params['b7'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3,4,5,6,7):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        grads['W4'], grads['b4'] = self.layers['Affine3'].dW, self.layers['Affine3'].db\n",
    "        grads['W5'], grads['b5'] = self.layers['Affine4'].dW, self.layers['Affine4'].db\n",
    "        grads['W6'], grads['b6'] = self.layers['Affine5'].dW, self.layers['Affine5'].db\n",
    "        grads['W7'], grads['b7'] = self.layers['Affine6'].dW, self.layers['Affine6'].db\n",
    "\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6846bd6-1c86-4852-82b0-ead718aae9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2622155579089585\n",
      "=== epoch:1, train acc:0.7, test acc:0.65 ===\n",
      "train loss:2.1736192992844168\n",
      "train loss:2.1029727597880337\n",
      "train loss:2.0057717025064528\n",
      "train loss:1.9480715768775838\n",
      "train loss:1.8508828244554514\n",
      "train loss:1.7817081390428862\n",
      "train loss:1.6753632473199478\n",
      "train loss:1.6411566986219852\n",
      "train loss:1.52889883815578\n",
      "train loss:1.4488426995780692\n",
      "train loss:1.417025681920395\n",
      "train loss:1.3188363723436123\n",
      "train loss:1.2461335629965866\n",
      "train loss:1.1451749963183817\n",
      "train loss:1.074657208422506\n",
      "train loss:0.9656456531590282\n",
      "train loss:1.0350991660194684\n",
      "train loss:0.9871741827847605\n",
      "train loss:0.7193608671442777\n",
      "train loss:0.9123668591209491\n",
      "train loss:0.5988821258064481\n",
      "train loss:0.7841482775026474\n",
      "train loss:0.9312704746849763\n",
      "train loss:0.6479351114536934\n",
      "train loss:0.9156563853968163\n",
      "train loss:0.6101052479899307\n",
      "train loss:0.6997147598998769\n",
      "train loss:0.9008610578084179\n",
      "train loss:0.7869142540576888\n",
      "train loss:1.0796620182669405\n",
      "train loss:0.6670281970012567\n",
      "train loss:0.7545246450799107\n",
      "train loss:0.5664368165869441\n",
      "train loss:0.6525533404597189\n",
      "train loss:0.7323839360471804\n",
      "train loss:0.6468119061483635\n",
      "train loss:0.5671328276743897\n",
      "train loss:0.6427222441261169\n",
      "train loss:0.565322980570661\n",
      "train loss:0.5623238611175682\n",
      "train loss:0.5581622407078326\n",
      "train loss:0.6354148308065816\n",
      "train loss:0.5496795342562666\n",
      "train loss:0.6329641914245336\n",
      "train loss:0.6321670868568078\n",
      "train loss:0.5382114960688709\n",
      "train loss:0.8193970447703744\n",
      "train loss:0.5349121633974612\n",
      "train loss:0.6302224241111196\n",
      "train loss:0.6298022990614285\n",
      "train loss:0.5304084803067541\n",
      "train loss:0.5282596581459684\n",
      "train loss:0.8310194658131511\n",
      "train loss:0.6284406784038313\n",
      "train loss:0.5281747281536645\n",
      "train loss:0.6275243881169109\n",
      "train loss:0.6269679926273837\n",
      "train loss:0.6263734050166587\n",
      "train loss:0.5283521106947846\n",
      "train loss:0.5273245219625623\n",
      "train loss:0.5257372086036621\n",
      "train loss:0.6264651972755737\n",
      "train loss:0.5230501367907183\n",
      "train loss:0.6272366512117616\n",
      "train loss:0.6272997671019354\n",
      "train loss:0.41090383391883567\n",
      "train loss:0.7375713727922399\n",
      "train loss:0.628085245781227\n",
      "train loss:0.8377143525115578\n",
      "train loss:0.6248932764876536\n",
      "train loss:0.6236004542106042\n",
      "train loss:0.6225856083910563\n",
      "train loss:0.798164499283076\n",
      "train loss:0.5375376331091332\n",
      "train loss:0.6215305484178307\n",
      "train loss:0.6972927247099129\n",
      "train loss:0.6932178827743904\n",
      "train loss:0.48775743611177225\n",
      "train loss:0.6241776489487859\n",
      "train loss:0.6901062908600628\n",
      "train loss:0.4184154587546908\n",
      "train loss:0.549102330194133\n",
      "train loss:0.3775207877329157\n",
      "train loss:0.5306451033528867\n",
      "train loss:0.6215757607713933\n",
      "train loss:0.6235989742763283\n",
      "train loss:0.6259300693164974\n",
      "train loss:0.6281716412245032\n",
      "train loss:0.858113369851923\n",
      "train loss:0.5130495787634074\n",
      "train loss:0.3931464870277948\n",
      "train loss:0.3874036528707748\n",
      "train loss:0.7593421693358429\n",
      "train loss:0.2508768857183169\n",
      "train loss:0.5083185280674286\n",
      "train loss:0.907878563730814\n",
      "train loss:0.7700443024653947\n",
      "train loss:0.7617327677627326\n",
      "train loss:0.7507587788631609\n",
      "train loss:0.847417878119043\n",
      "train loss:0.4131243188420871\n",
      "train loss:0.5209943629873519\n",
      "train loss:0.6196705745954161\n",
      "train loss:0.7081454592152262\n",
      "train loss:0.5332127419154798\n",
      "train loss:0.3621432538143655\n",
      "train loss:0.7026936442825457\n",
      "train loss:0.5337790845727375\n",
      "train loss:0.7018231020305621\n",
      "train loss:0.7787741049357639\n",
      "train loss:0.4589185331133702\n",
      "train loss:0.6184062654818281\n",
      "train loss:0.7727209022175916\n",
      "train loss:0.6930139117183475\n",
      "train loss:0.690126894733485\n",
      "train loss:0.48245900904204725\n",
      "train loss:0.5499606507315542\n",
      "train loss:0.546442885925765\n",
      "train loss:0.6932488904416066\n",
      "train loss:0.6934763307492828\n",
      "train loss:0.5417029478134583\n",
      "train loss:0.6179471739946402\n",
      "train loss:0.6176625511149718\n",
      "train loss:0.6174609640333604\n",
      "train loss:0.617335983534437\n",
      "train loss:0.5302824100167587\n",
      "train loss:0.33768156525785553\n",
      "train loss:0.6191236258220492\n",
      "train loss:0.40923743292534176\n",
      "train loss:0.7345587406008074\n",
      "train loss:0.737968527237706\n",
      "train loss:0.393070169039512\n",
      "train loss:0.5086552847225924\n",
      "train loss:0.872093604994383\n",
      "train loss:0.7474547874816532\n",
      "train loss:0.6265777305799793\n",
      "train loss:0.844789799809781\n",
      "train loss:0.6206895573319903\n",
      "train loss:0.5186381788528039\n",
      "train loss:0.6178547215045415\n",
      "train loss:0.7073149349934712\n",
      "train loss:0.7820674927242791\n",
      "train loss:0.5400674196001852\n",
      "train loss:0.544299513208742\n",
      "train loss:0.6186487910333565\n",
      "train loss:0.5490841964565601\n",
      "train loss:0.5478470215094086\n",
      "train loss:0.5445881685696149\n",
      "train loss:0.6171984934469265\n",
      "train loss:0.6167201295300461\n",
      "train loss:0.6163933680906213\n",
      "train loss:0.6162082655842072\n",
      "train loss:0.7004351748761095\n",
      "train loss:0.5296786277846018\n",
      "train loss:0.7031331032981297\n",
      "train loss:0.3421762913671922\n",
      "train loss:0.6170507561064261\n",
      "train loss:0.8086062937560479\n",
      "train loss:0.4207063776598643\n",
      "train loss:0.6183805763379846\n",
      "train loss:0.8172625392321354\n",
      "train loss:0.8999901933095634\n",
      "train loss:0.5253534676029804\n",
      "train loss:0.6158135433335755\n",
      "train loss:0.6157380740425479\n",
      "train loss:0.7736027379182889\n",
      "train loss:0.46313552934728036\n",
      "train loss:0.762149303866667\n",
      "train loss:0.7520822671338177\n",
      "train loss:0.7407360282294517\n",
      "train loss:0.6793212474597035\n",
      "train loss:0.5329462579460358\n",
      "train loss:0.6314232439032395\n",
      "train loss:0.531546457990036\n",
      "train loss:0.6789906754789387\n",
      "train loss:0.6792675453158903\n",
      "train loss:0.5111690141071835\n",
      "train loss:0.5580138632521305\n",
      "train loss:0.5475111282584886\n",
      "train loss:0.5371574261431196\n",
      "train loss:0.6152751561450198\n",
      "train loss:0.7970098824186381\n",
      "train loss:0.7091482420983286\n",
      "train loss:0.3199074538577064\n",
      "train loss:0.5140265274907719\n",
      "train loss:0.7300249916253152\n",
      "train loss:0.7322780072551012\n",
      "train loss:0.5101059061238896\n",
      "train loss:0.7350486982265243\n",
      "train loss:0.3952011106853394\n",
      "train loss:0.7369660130735187\n",
      "train loss:0.7332380655318929\n",
      "train loss:0.51137665760242\n",
      "train loss:0.7260368908674357\n",
      "train loss:0.5144582586637907\n",
      "train loss:0.4101553555150791\n",
      "train loss:0.4067595404280776\n",
      "train loss:0.620067502929011\n",
      "train loss:0.9323717252809951\n",
      "train loss:0.6174810083685979\n",
      "train loss:0.5177780576224078\n",
      "train loss:0.8942157313668557\n",
      "train loss:0.43542260575363106\n",
      "train loss:0.5267467405914069\n",
      "train loss:0.5270515073638123\n",
      "train loss:0.5262025433482161\n",
      "train loss:0.5243929808588572\n",
      "train loss:0.7059690889127992\n",
      "train loss:0.3308721951784319\n",
      "train loss:0.713041899391444\n",
      "train loss:0.9893228419875797\n",
      "train loss:0.4298691920695208\n",
      "train loss:0.3294397827995662\n",
      "train loss:0.5176239762641421\n",
      "train loss:0.7173247277897737\n",
      "train loss:0.9092106018103454\n",
      "train loss:0.5195838061960141\n",
      "train loss:0.5198937522292734\n",
      "train loss:0.5194219205031321\n",
      "train loss:0.7108366853150507\n",
      "train loss:0.7079683021039113\n",
      "train loss:0.7906623230041722\n",
      "train loss:0.6143782042750774\n",
      "train loss:0.5335872582622383\n",
      "train loss:0.5347808446820004\n",
      "train loss:0.5343338773924362\n",
      "train loss:0.6144833189844988\n",
      "train loss:0.7721565978682283\n",
      "train loss:0.6149055531610734\n",
      "train loss:0.6900248867223288\n",
      "train loss:0.5415631686700781\n",
      "train loss:0.4628037344375774\n",
      "train loss:0.766674369193648\n",
      "train loss:0.6150689024092445\n",
      "train loss:0.457657283791528\n",
      "train loss:0.3597494591925897\n",
      "train loss:0.6144287979112375\n",
      "train loss:0.5187607974370545\n",
      "train loss:0.5141643251749952\n",
      "train loss:0.5103345171703251\n",
      "train loss:0.8479637940999749\n",
      "train loss:0.6230349888234971\n",
      "train loss:0.5071751076213677\n",
      "train loss:0.8555728686341032\n",
      "train loss:0.622494264100553\n",
      "train loss:0.508898761995842\n",
      "train loss:0.5091044664786755\n",
      "train loss:0.6209449850404003\n",
      "train loss:0.833252080609666\n",
      "train loss:0.6171801717567745\n",
      "train loss:0.6157871948611815\n",
      "train loss:0.6147815907836848\n",
      "train loss:0.4291213068659223\n",
      "train loss:0.42741148294369724\n",
      "train loss:0.7074776492424597\n",
      "train loss:0.5211492907297273\n",
      "train loss:0.422796118304292\n",
      "train loss:0.5164094532187953\n",
      "train loss:0.7174947185292625\n",
      "train loss:0.7169916266851425\n",
      "train loss:0.5152610294318002\n",
      "train loss:0.6164791278470195\n",
      "train loss:0.41032446257339555\n",
      "train loss:0.7209710724002284\n",
      "train loss:0.7188393993549211\n",
      "train loss:0.5149742645656479\n",
      "train loss:0.6161997244702635\n",
      "train loss:0.7135183985678589\n",
      "train loss:0.4194028524548372\n",
      "train loss:0.6153035132616826\n",
      "train loss:0.6151184613806271\n",
      "train loss:0.4184140562552181\n",
      "train loss:0.6158416204704668\n",
      "train loss:0.7146492106219597\n",
      "train loss:0.3108342115340955\n",
      "train loss:0.6173022503334323\n",
      "train loss:0.7215842544758917\n",
      "train loss:0.7193337266301001\n",
      "train loss:0.8115473254314459\n",
      "train loss:0.6143317731770261\n",
      "train loss:0.523064276566256\n",
      "train loss:0.33829854416282806\n",
      "train loss:0.7053568089799832\n",
      "train loss:0.5219265260112617\n",
      "train loss:0.7051165419803962\n",
      "train loss:0.7026337596946807\n",
      "train loss:0.34162960310661894\n",
      "train loss:0.7046592853055031\n",
      "train loss:0.5218742520145474\n",
      "train loss:0.5200266016656117\n",
      "train loss:0.6146845406268886\n",
      "train loss:0.6149818336413686\n",
      "train loss:0.41446470671212976\n",
      "train loss:0.4059302756706683\n",
      "train loss:0.2799039529999711\n",
      "train loss:0.5050604467810449\n",
      "train loss:0.5033856504245843\n",
      "train loss:0.8992848653665529\n",
      "train loss:0.7676351385410576\n",
      "train loss:0.5028874997196603\n",
      "train loss:1.0158226609918857\n",
      "train loss:0.7482319444884818\n",
      "train loss:0.26863338618244836\n",
      "train loss:0.5067665460151504\n",
      "train loss:0.39057504040117746\n",
      "train loss:0.38841148071307235\n",
      "train loss:0.6244615660319\n",
      "train loss:0.38458646515258776\n",
      "train loss:0.6267695883509571\n",
      "train loss:0.8639535844014017\n",
      "train loss:0.5062640089107556\n",
      "train loss:0.2675644269400355\n",
      "train loss:0.8575064570957922\n",
      "train loss:0.6218778140084835\n",
      "train loss:0.5083723628776184\n",
      "train loss:0.8334570821856719\n",
      "train loss:0.5129969296700123\n",
      "train loss:0.61549207107112\n",
      "train loss:0.6144548384657492\n",
      "train loss:0.7046515920957981\n",
      "train loss:0.5255416397711892\n",
      "train loss:0.8555698633286303\n",
      "train loss:0.6142575614534148\n",
      "train loss:0.6157094865236331\n",
      "train loss:0.6173551819888796\n",
      "train loss:0.6189127714460451\n",
      "train loss:0.6789771885425901\n",
      "train loss:0.5648038572568496\n",
      "train loss:0.6218363233268219\n",
      "train loss:0.5624532438300347\n",
      "train loss:0.4917719087682798\n",
      "train loss:0.6834521586501976\n",
      "train loss:0.6151744366644538\n",
      "train loss:0.6141532910870906\n",
      "train loss:0.5335876481189628\n",
      "train loss:0.6961964427877037\n",
      "train loss:0.6130345881208715\n",
      "train loss:0.7008476516722915\n",
      "train loss:0.7008197333484296\n",
      "train loss:0.6130695718643782\n",
      "train loss:0.7828090225409715\n",
      "train loss:0.5292501632402871\n",
      "train loss:0.5289703780349229\n",
      "train loss:0.6129646922252214\n",
      "train loss:0.6962150997540941\n",
      "train loss:0.7727461759250576\n",
      "train loss:0.5353641261395139\n",
      "train loss:0.7624553730625087\n",
      "train loss:0.6850379769668095\n",
      "train loss:0.6167643245763885\n",
      "train loss:0.6179774830084288\n",
      "train loss:0.6188961812628709\n",
      "train loss:0.5571277255545967\n",
      "train loss:0.7394875462024901\n",
      "train loss:0.5589140795976474\n",
      "train loss:0.6187365791489223\n",
      "train loss:0.48500840949442325\n",
      "train loss:0.8156480212645544\n",
      "train loss:0.5470393159602096\n",
      "train loss:0.6148593565981328\n",
      "train loss:0.6869481738796674\n",
      "train loss:0.5384732851464556\n",
      "train loss:0.6903583887472264\n",
      "train loss:0.45085323701769975\n",
      "train loss:0.6964622998944858\n",
      "train loss:0.525543943594224\n",
      "train loss:0.427201066291451\n",
      "train loss:0.6145962133054428\n",
      "train loss:0.2986678311459377\n",
      "train loss:0.5071705892320377\n",
      "train loss:0.3807446666225766\n",
      "train loss:0.633168985647161\n",
      "train loss:0.6380338690424199\n",
      "train loss:0.6415506252517702\n",
      "train loss:0.6435076063098113\n",
      "train loss:1.1851686820702962\n",
      "train loss:0.3686291992710544\n",
      "train loss:0.6324863763479949\n",
      "train loss:0.24468744345144056\n",
      "train loss:0.7555183432839125\n",
      "train loss:0.7468142368812906\n",
      "train loss:0.2660337608684558\n",
      "train loss:0.5054467965667093\n",
      "train loss:0.5055896266206041\n",
      "train loss:0.38560752231307754\n",
      "train loss:0.6248757090380546\n",
      "train loss:0.6244980000713497\n",
      "train loss:0.9636684273122469\n",
      "train loss:0.39712373890506153\n",
      "train loss:0.6181040916412462\n",
      "train loss:0.6165965742016292\n",
      "train loss:0.7144814793004042\n",
      "train loss:0.4195652044810842\n",
      "train loss:0.4192198482620203\n",
      "train loss:0.8033012617630376\n",
      "train loss:0.7038412489654708\n",
      "train loss:0.5243726846822684\n",
      "train loss:0.43550877350565925\n",
      "train loss:0.7001568796256419\n",
      "train loss:0.6976921024982614\n",
      "train loss:0.6943570840329789\n",
      "train loss:0.5329416613475131\n",
      "train loss:0.5330882958397956\n",
      "train loss:0.6915239567760308\n",
      "train loss:0.5341438644320904\n",
      "train loss:0.532210473421853\n",
      "train loss:0.5290571496980137\n",
      "train loss:0.5251044316328687\n",
      "train loss:0.5207808836782741\n",
      "train loss:0.6139460956822362\n",
      "train loss:0.5140540938771517\n",
      "train loss:0.40251226019885805\n",
      "train loss:0.6204080025565641\n",
      "train loss:0.9591450009508298\n",
      "train loss:0.3918801106179277\n",
      "train loss:0.8470675239940391\n",
      "train loss:0.3935565446707298\n",
      "train loss:0.5067975819798651\n",
      "train loss:0.2682663941050726\n",
      "train loss:0.6256394468783844\n",
      "train loss:0.7472783057258676\n",
      "train loss:0.743940743643257\n",
      "train loss:0.6227257638680672\n",
      "train loss:0.3898534335708968\n",
      "train loss:0.621533691297467\n",
      "train loss:0.5069056965363817\n",
      "train loss:0.506928375909097\n",
      "train loss:0.6206311921304911\n",
      "train loss:0.2746788109536918\n",
      "train loss:0.5052523134783031\n",
      "train loss:0.5043264772263789\n",
      "train loss:0.7477049569481059\n",
      "train loss:0.3799033577432519\n",
      "train loss:0.7503885816997811\n",
      "train loss:0.7458200503934612\n",
      "train loss:0.738245288019447\n",
      "train loss:0.5076428144449407\n",
      "train loss:0.7246496712978296\n",
      "train loss:0.8131409415167103\n",
      "train loss:0.7911302951540536\n",
      "train loss:0.5311394537186639\n",
      "train loss:0.7578943728958418\n",
      "train loss:0.5517987583150068\n",
      "train loss:0.619532286064714\n",
      "train loss:0.5653229903089727\n",
      "train loss:0.5664605203235197\n",
      "train loss:0.6216965905647462\n",
      "train loss:0.6771954753402516\n",
      "train loss:0.6214665790371079\n",
      "train loss:0.6206065356062119\n",
      "train loss:0.6782086913026628\n",
      "train loss:0.7346129524697945\n",
      "train loss:0.6210211066612852\n",
      "train loss:0.6773702660410101\n",
      "train loss:0.5631595749508651\n",
      "train loss:0.557800711468642\n",
      "train loss:0.4814235427791146\n",
      "train loss:0.5388042932244597\n",
      "train loss:0.7708589400513775\n",
      "train loss:0.8507215712717293\n",
      "train loss:0.44774059859973914\n",
      "train loss:0.6961177667204688\n",
      "train loss:0.6961622520686271\n",
      "train loss:0.6123244963846065\n",
      "train loss:0.6123185498221243\n",
      "train loss:0.5269122429597793\n",
      "train loss:0.6976533825584906\n",
      "train loss:0.6963007904637427\n",
      "train loss:0.5282030160038624\n",
      "train loss:0.6952403434741008\n",
      "train loss:0.5291187421420409\n",
      "train loss:0.5274826833645021\n",
      "train loss:0.6122968872731485\n",
      "train loss:0.6984973650684748\n",
      "train loss:0.5248990292521453\n",
      "train loss:0.4304970703890774\n",
      "train loss:0.42012549996501625\n",
      "train loss:0.4076171460217699\n",
      "train loss:0.618861772862525\n",
      "train loss:0.5057257316488316\n",
      "train loss:0.7447679267954974\n",
      "train loss:0.5035013432415246\n",
      "train loss:0.6288740288359879\n",
      "train loss:0.6296400289751054\n",
      "train loss:0.6294933029818859\n",
      "train loss:0.8722855879666096\n",
      "train loss:0.38312445938674283\n",
      "train loss:0.6235649280346405\n",
      "train loss:0.734940653923144\n",
      "train loss:0.39567297052959194\n",
      "train loss:0.5082770596861472\n",
      "train loss:0.6180273938640699\n",
      "train loss:0.5097309288186771\n",
      "train loss:0.5098224564400173\n",
      "train loss:0.7221623418764705\n",
      "train loss:0.5116364783219729\n",
      "train loss:0.815376916268149\n",
      "train loss:0.4164909686063384\n",
      "train loss:0.41495688810748277\n",
      "train loss:0.3036922247973254\n",
      "train loss:0.5091911496920294\n",
      "train loss:0.3912565615943787\n",
      "train loss:0.857149597534756\n",
      "train loss:0.6234134167600243\n",
      "train loss:0.9651581615005606\n",
      "train loss:0.5076546066034894\n",
      "train loss:0.8288540518193408\n",
      "train loss:0.6145933963808532\n",
      "train loss:0.6131429421665666\n",
      "train loss:0.7002693893629498\n",
      "train loss:0.6122325575594321\n",
      "train loss:0.6128658007229382\n",
      "train loss:0.6139072497210991\n",
      "train loss:0.5451806292250522\n",
      "train loss:0.615293978763138\n",
      "train loss:0.5473905206340122\n",
      "train loss:0.6149981438362266\n",
      "train loss:0.6146409797844182\n",
      "train loss:0.5416532278394752\n",
      "train loss:0.6870877114837691\n",
      "train loss:0.5362712442853556\n",
      "train loss:0.6124191132367727\n",
      "train loss:0.6931496966586617\n",
      "train loss:0.6932202330442601\n",
      "train loss:0.612246553167356\n",
      "train loss:0.6924215419543763\n",
      "train loss:0.6124233361239486\n",
      "train loss:0.6124919294387025\n",
      "train loss:0.532707241539894\n",
      "train loss:0.35818332211608367\n",
      "train loss:0.6122724668985022\n",
      "train loss:0.6127758344845086\n",
      "train loss:0.7081795011485071\n",
      "train loss:0.6134855723071491\n",
      "train loss:0.8937735285859573\n",
      "train loss:0.32890161516350797\n",
      "train loss:0.7070815680215914\n",
      "train loss:0.42141084317176236\n",
      "train loss:0.8024453311322297\n",
      "train loss:0.7045519749086518\n",
      "train loss:0.5217618188407227\n",
      "train loss:0.6122528566725993\n",
      "train loss:0.7826705290233373\n",
      "train loss:0.5290362656826268\n",
      "train loss:0.5297465235824794\n",
      "train loss:0.5289399083824368\n",
      "train loss:0.774641856688495\n",
      "train loss:0.5312377989072237\n",
      "train loss:0.612197558806388\n",
      "train loss:0.6917881965217064\n",
      "train loss:0.7644337471136033\n",
      "train loss:0.5392384965522743\n",
      "train loss:0.6853624847962199\n",
      "train loss:0.5433126200118983\n",
      "train loss:0.7510336154251954\n",
      "train loss:0.47766250906437435\n",
      "train loss:0.6832048822111345\n",
      "train loss:0.6824871028454054\n",
      "train loss:0.5471293099404727\n",
      "train loss:0.6146288927777658\n",
      "train loss:0.6141310216992468\n",
      "train loss:0.6136176245789244\n",
      "train loss:0.458500641131784\n",
      "train loss:0.917339711161042\n",
      "train loss:0.5357818365690615\n",
      "train loss:0.4514442954628267\n",
      "train loss:0.6943719431607924\n",
      "train loss:0.4384164256923134\n",
      "train loss:0.7016339116559009\n",
      "train loss:0.6124539617278518\n",
      "train loss:0.7047727265776378\n",
      "train loss:0.6125708668438505\n",
      "train loss:0.42266878201805297\n",
      "train loss:0.4143840012634392\n",
      "train loss:0.5110007749763565\n",
      "train loss:0.5080471092132238\n",
      "train loss:0.5056111959846241\n",
      "train loss:0.6245543696941389\n",
      "train loss:0.7476016427675053\n",
      "train loss:0.6259623317380243\n",
      "train loss:0.25108737639178175\n",
      "train loss:0.502088432502682\n",
      "train loss:0.5016616684351951\n",
      "train loss:0.3645688237005614\n",
      "train loss:0.35930223230119646\n",
      "train loss:0.7898857950622428\n",
      "train loss:0.6461632737838585\n",
      "train loss:0.20694379546233727\n",
      "train loss:1.0800930873849943\n",
      "train loss:0.6419583570846454\n",
      "train loss:0.36303259172972613\n",
      "train loss:0.7686664237865408\n",
      "train loss:0.8792849737551537\n",
      "train loss:0.5047235714920906\n",
      "train loss:0.7284725311614519\n",
      "train loss:0.7149820829553303\n",
      "train loss:0.7899329769732535\n",
      "train loss:0.6895962761296224\n",
      "train loss:0.6812857491818394\n",
      "train loss:0.6762251916117743\n",
      "train loss:0.6742014238268471\n",
      "train loss:0.5615318918088936\n",
      "train loss:0.6748989715802752\n",
      "train loss:0.5785549591905732\n",
      "train loss:0.7060429285322789\n",
      "train loss:0.6100294972439496\n",
      "train loss:0.6389223637188662\n",
      "train loss:0.5473123463138141\n",
      "train loss:0.7216376173959029\n",
      "train loss:0.6233597656371772\n",
      "train loss:0.5593873975630738\n",
      "train loss:0.5471639542070117\n",
      "train loss:0.6127491831934624\n",
      "train loss:0.6119141930320197\n",
      "train loss:0.7868549170292873\n",
      "train loss:0.6122997429594754\n",
      "train loss:0.6128092616735532\n",
      "train loss:0.3123340125547319\n",
      "train loss:0.5096652834452556\n",
      "train loss:0.5063956976866042\n",
      "train loss:0.503991444005805\n",
      "train loss:0.6284524696161758\n",
      "train loss:0.5018297595635479\n",
      "train loss:0.5013915379280987\n",
      "train loss:0.36071865609697673\n",
      "train loss:0.6456697170881821\n",
      "train loss:0.6476498279350572\n",
      "train loss:0.35330363478658744\n",
      "train loss:0.6519622503778879\n",
      "train loss:0.1967748409927093\n",
      "train loss:0.5034823336699372\n",
      "train loss:0.5040448375664557\n",
      "train loss:0.6620756821991364\n",
      "train loss:0.6601322133164286\n",
      "train loss:0.503233755977951\n",
      "train loss:0.5028050585055693\n",
      "train loss:0.5024090882917478\n",
      "train loss:0.5020624049366593\n",
      "train loss:0.5017758291792658\n",
      "train loss:0.6450234520681357\n",
      "train loss:0.6404621915445943\n",
      "train loss:0.5013602839313556\n",
      "train loss:0.7603040483815009\n",
      "train loss:0.5030933331119884\n",
      "train loss:0.5043708239399661\n",
      "train loss:0.8431512337384713\n",
      "train loss:0.6157963557376321\n",
      "train loss:0.6134451251630804\n",
      "train loss:0.426373719062011\n",
      "train loss:0.42893686218509136\n",
      "train loss:0.7009684660047373\n",
      "train loss:0.6968997697081283\n",
      "train loss:0.5290150397521451\n",
      "train loss:0.5296634156542523\n",
      "train loss:0.6119311390615638\n",
      "train loss:0.5291688820079028\n",
      "train loss:0.6118432127187847\n",
      "train loss:0.775652085728286\n",
      "train loss:0.7678497647408993\n",
      "train loss:0.5371899356707862\n",
      "train loss:0.5376856193099459\n",
      "train loss:0.612836314982437\n",
      "train loss:0.6127961860469879\n",
      "train loss:0.6873174706115825\n",
      "train loss:0.5376522872617311\n",
      "train loss:0.8301775500136511\n",
      "train loss:0.5425138650312833\n",
      "train loss:0.5414582396127067\n",
      "train loss:0.46108207745107216\n",
      "train loss:0.3619563182574511\n",
      "train loss:0.5208580377151952\n",
      "train loss:0.6137260681376467\n",
      "train loss:0.40253620781057975\n",
      "train loss:0.6205842729779205\n",
      "train loss:0.5036981644785153\n",
      "train loss:0.9949104319245852\n",
      "train loss:0.7474772248419645\n",
      "train loss:0.7422631175059236\n",
      "train loss:0.6210842161694113\n",
      "train loss:0.39264969696422086\n",
      "train loss:0.5067667007180071\n",
      "train loss:0.3919785567718969\n",
      "train loss:0.3878714790525985\n",
      "train loss:0.6234726476992162\n",
      "train loss:0.6239924475999321\n",
      "train loss:0.38122039327949886\n",
      "train loss:0.3764226542620519\n",
      "train loss:0.6305779513458222\n",
      "train loss:0.5016877743485165\n",
      "train loss:0.6335450374776298\n",
      "train loss:1.1351478079947879\n",
      "train loss:0.8589001708207171\n",
      "train loss:0.725614976703205\n",
      "train loss:0.5140264804473434\n",
      "train loss:0.7918028509914236\n",
      "train loss:0.3585837817268255\n",
      "train loss:0.5311895121487661\n",
      "train loss:0.6123183091953002\n",
      "train loss:0.6867263655311272\n",
      "train loss:0.5419249422295149\n",
      "train loss:0.7503194462603979\n",
      "train loss:0.5496984097331858\n",
      "train loss:0.616066121362177\n",
      "train loss:0.5500377359639411\n",
      "train loss:0.7450356646555746\n",
      "train loss:0.6164969701760002\n",
      "train loss:0.6164592296123939\n",
      "train loss:0.4808267533791746\n",
      "train loss:0.6139257113348422\n",
      "train loss:0.6860206843450993\n",
      "train loss:0.5362235617940885\n",
      "train loss:0.6120146929772702\n",
      "train loss:0.5274499480708036\n",
      "train loss:0.7833830875650664\n",
      "train loss:0.43252125145227704\n",
      "train loss:0.7040923246126812\n",
      "train loss:0.6125267974737618\n",
      "train loss:0.612777875086631\n",
      "train loss:0.41690671875706836\n",
      "train loss:0.713906754273127\n",
      "train loss:0.40847375881876263\n",
      "train loss:0.7205970983994571\n",
      "train loss:0.6161034054974135\n",
      "train loss:0.400770381432895\n",
      "train loss:0.5074563375303665\n",
      "train loss:0.8405604357453456\n",
      "train loss:0.8314785952517149\n",
      "train loss:0.7169397713377754\n",
      "train loss:0.515258919108618\n",
      "train loss:0.6126938276636699\n",
      "train loss:0.4255486903752194\n",
      "train loss:0.4230541233908715\n",
      "train loss:0.6129600430539062\n",
      "train loss:0.8002028280678515\n",
      "train loss:0.5193312932999842\n",
      "train loss:0.7029842270087899\n",
      "train loss:0.42943978591930343\n",
      "train loss:0.702306615878206\n",
      "train loss:0.611896733098502\n",
      "train loss:0.6985295013189825\n",
      "train loss:0.6950001844887967\n",
      "train loss:0.6909322583671343\n",
      "train loss:0.536095019842957\n",
      "train loss:0.5369676432782982\n",
      "train loss:0.5359118258341672\n",
      "train loss:0.6122437958027678\n",
      "train loss:0.5319686745492787\n",
      "train loss:0.5281173279393396\n",
      "train loss:0.6972539705659115\n",
      "train loss:0.5231862252698292\n",
      "train loss:0.612113754840491\n",
      "train loss:0.5178758924317814\n",
      "train loss:0.5147057040278963\n",
      "train loss:0.40570579420119535\n",
      "train loss:0.6180784154079882\n",
      "train loss:0.6200867683306195\n",
      "train loss:0.6216518367056714\n",
      "train loss:0.7384280339522868\n",
      "train loss:0.5049181253877146\n",
      "train loss:0.9595523521408633\n",
      "train loss:0.6175098844295897\n",
      "train loss:0.2927641492184879\n",
      "train loss:0.7216801606655108\n",
      "train loss:0.6149644685056016\n",
      "train loss:0.6139568689640242\n",
      "train loss:0.7081716581432601\n",
      "train loss:0.42503283225536864\n",
      "train loss:0.7027064463350379\n",
      "train loss:0.698189395043076\n",
      "train loss:0.44103769852919295\n",
      "train loss:0.3474424041048278\n",
      "train loss:0.5203200325490936\n",
      "train loss:0.31701507089471564\n",
      "train loss:0.5101187071805574\n",
      "train loss:0.27329708727683877\n",
      "train loss:0.37551546372126143\n",
      "train loss:0.7693060828300006\n",
      "train loss:0.5011010090825171\n",
      "train loss:0.5015607507586671\n",
      "train loss:0.34950640554507334\n",
      "train loss:0.81567503652698\n",
      "train loss:0.6608286782193702\n",
      "train loss:0.8133300345475029\n",
      "train loss:0.34869294238170756\n",
      "train loss:0.9496612075915204\n",
      "train loss:0.9231650540330338\n",
      "train loss:0.501343508807892\n",
      "train loss:0.5023829768519625\n",
      "train loss:0.6232731609765606\n",
      "train loss:0.5066889022591572\n",
      "train loss:0.72149445384493\n",
      "train loss:0.8043450033454921\n",
      "train loss:0.5237921868443688\n",
      "train loss:0.6118040928649613\n",
      "train loss:0.45795629165070545\n",
      "train loss:0.5372939441652422\n",
      "train loss:0.5374424587347886\n",
      "train loss:0.5357388622597629\n",
      "train loss:0.7636813139722795\n",
      "train loss:0.45735100261432293\n",
      "train loss:0.6119581553206416\n",
      "train loss:0.7692434186136942\n",
      "train loss:0.5325448198523959\n",
      "train loss:0.5302010429069035\n",
      "train loss:0.6939924226387952\n",
      "train loss:0.6934810084132259\n",
      "train loss:0.44384690040398933\n",
      "train loss:0.6116437537569634\n",
      "train loss:0.7817077001568132\n",
      "train loss:0.4379774307080823\n",
      "train loss:0.5221772469796496\n",
      "train loss:0.6121853974429934\n",
      "train loss:0.5173671777495127\n",
      "train loss:0.4128851682130672\n",
      "train loss:0.5100573091133206\n",
      "train loss:0.7272099327193009\n",
      "train loss:0.5066448952639111\n",
      "train loss:0.6210440815449367\n",
      "train loss:0.849022449061416\n",
      "train loss:0.5062585035690411\n",
      "train loss:0.5062033610240736\n",
      "train loss:0.7311834055546014\n",
      "train loss:0.8308604793656654\n",
      "train loss:0.7150005299136237\n",
      "train loss:0.4187429380462868\n",
      "train loss:0.7943563868763792\n",
      "train loss:0.4355326177770107\n",
      "train loss:0.5251171889600001\n",
      "train loss:0.6953955968774101\n",
      "train loss:0.4438216640624873\n",
      "train loss:0.6116090714347657\n",
      "train loss:0.4382101740134304\n",
      "train loss:0.42881288626494973\n",
      "train loss:0.612869073442957\n",
      "train loss:0.41065746933183006\n",
      "train loss:0.3986606828386024\n",
      "train loss:0.6212339627977246\n",
      "train loss:0.5033846283342691\n",
      "train loss:0.6287349460231417\n",
      "train loss:0.5015400979684715\n",
      "train loss:0.7661346095735904\n",
      "train loss:0.3659399300269884\n",
      "train loss:0.9030254518402211\n",
      "train loss:0.7622895270139025\n",
      "train loss:0.6283653871890875\n",
      "train loss:0.7436307637124876\n",
      "train loss:0.3894520146791564\n",
      "train loss:0.39186326078724665\n",
      "train loss:0.836481813229021\n",
      "train loss:0.40265302855198215\n",
      "train loss:0.6154367500227389\n",
      "train loss:0.614360116582889\n",
      "train loss:0.5143570549926523\n",
      "train loss:0.802352920729066\n",
      "train loss:0.6119193047673741\n",
      "train loss:0.5239964857775553\n",
      "train loss:0.5252695618839104\n",
      "train loss:0.525237638403105\n",
      "train loss:0.7792010794353548\n",
      "train loss:0.6116903177648398\n",
      "train loss:0.7662451924197431\n",
      "train loss:0.6852401764722916\n",
      "train loss:0.47350022924342083\n",
      "train loss:0.543497260524541\n",
      "train loss:0.683618165804522\n",
      "train loss:0.6141382563978193\n",
      "train loss:0.6140721505054846\n",
      "train loss:0.8140746019601328\n",
      "train loss:0.7401827794064169\n",
      "train loss:0.4979783910285935\n",
      "train loss:0.42605026426811027\n",
      "train loss:0.6812608568693037\n",
      "train loss:0.6144101569273576\n",
      "train loss:0.6134970161899991\n",
      "train loss:0.6127416109437226\n",
      "train loss:0.5335450772890612\n",
      "train loss:0.6928119564514641\n",
      "train loss:0.6939644903258145\n",
      "train loss:0.5267553928445748\n",
      "train loss:0.6116026396281729\n",
      "train loss:0.6117288973519921\n",
      "train loss:0.520224073677353\n",
      "train loss:0.5171934549639312\n",
      "train loss:0.4119695649576758\n",
      "train loss:0.7200226669242389\n",
      "train loss:0.6165661877128336\n",
      "train loss:0.6173437536038521\n",
      "train loss:0.9333862958193648\n",
      "train loss:0.5113737660934355\n",
      "train loss:0.7144613533673401\n",
      "train loss:0.3119884450960962\n",
      "train loss:0.6140921964633814\n",
      "train loss:0.7123045962434734\n",
      "train loss:0.6130128301239751\n",
      "train loss:0.6125390090021087\n",
      "train loss:0.5185937848691585\n",
      "train loss:0.6121857059022294\n",
      "train loss:0.5193233513357974\n",
      "train loss:0.518340605187926\n",
      "train loss:0.6125849948718092\n",
      "train loss:0.5163044408190769\n",
      "train loss:0.613307675320047\n",
      "train loss:0.7106556333289954\n",
      "train loss:0.5153190794421466\n",
      "train loss:0.5143076040406903\n",
      "train loss:0.6139838866021458\n",
      "train loss:0.6141075848524145\n",
      "train loss:0.6140860083893103\n",
      "train loss:0.30322894226214314\n",
      "train loss:0.5090497048266499\n",
      "train loss:0.6183210501497219\n",
      "train loss:0.5061108545197395\n",
      "train loss:0.846873423291896\n",
      "train loss:0.39069561222391436\n",
      "train loss:1.059726950122457\n",
      "train loss:0.4006950419750205\n",
      "train loss:0.5095905052082489\n",
      "train loss:0.7190465333668998\n",
      "train loss:0.5125978957815003\n",
      "train loss:0.711842136139632\n",
      "train loss:0.4181388349261055\n",
      "train loss:0.6128774602075794\n",
      "train loss:0.7060478083085608\n",
      "train loss:0.7887598221731551\n",
      "train loss:0.5264048783475748\n",
      "train loss:0.4424430130693149\n",
      "train loss:0.5261872895638776\n",
      "train loss:0.6115051064556667\n",
      "train loss:0.5237175382056537\n",
      "train loss:0.6997976835498405\n",
      "train loss:0.42997156883632803\n",
      "train loss:0.7040199969314028\n",
      "train loss:0.703400834089344\n",
      "train loss:0.4255195491129416\n",
      "train loss:0.5166723579876169\n",
      "train loss:0.41232287307675347\n",
      "train loss:0.615560045929626\n",
      "train loss:0.7227389382700304\n",
      "train loss:0.825296732054999\n",
      "train loss:0.5111433389445291\n",
      "train loss:0.40465866921745636\n",
      "train loss:1.019310209108768\n",
      "train loss:0.6129893910079203\n",
      "train loss:0.32332379931443067\n",
      "train loss:0.5159027737337541\n",
      "train loss:0.5147937608256457\n",
      "train loss:0.8072271589564941\n",
      "train loss:0.7977880642311354\n",
      "train loss:0.5217108330884213\n",
      "train loss:0.697606050797642\n",
      "train loss:0.527640290464166\n",
      "train loss:0.69217635507493\n",
      "train loss:0.9033800485950897\n",
      "train loss:0.47773021048153375\n",
      "train loss:0.5489666859456779\n",
      "train loss:0.54913487953702\n",
      "train loss:0.5468052806277237\n",
      "train loss:0.5424739777775669\n",
      "train loss:0.5367739413220327\n",
      "train loss:0.6117042411791094\n",
      "train loss:0.5261735736401775\n",
      "train loss:0.7865905262323577\n",
      "train loss:0.6116843275278544\n",
      "train loss:0.7015507429640954\n",
      "train loss:0.5206019003536488\n",
      "train loss:0.518568469406459\n",
      "train loss:0.5160453405696853\n",
      "train loss:0.6136790231756845\n",
      "train loss:0.7140555801244692\n",
      "train loss:0.6139436146698388\n",
      "train loss:0.5128598549101195\n",
      "train loss:0.7147007870267251\n",
      "train loss:0.5132456152145114\n",
      "train loss:0.40858931736851484\n",
      "train loss:0.5100069735844579\n",
      "train loss:0.5081989018394347\n",
      "train loss:0.728568090032684\n",
      "train loss:0.5068856739778981\n",
      "train loss:0.6193678879948932\n",
      "train loss:0.7301672547582854\n",
      "train loss:0.507417444798277\n",
      "train loss:0.507369176899978\n",
      "train loss:0.6181004979041795\n",
      "train loss:0.6175111051205173\n",
      "train loss:0.8256234886445156\n",
      "train loss:0.5125098948345584\n",
      "train loss:0.5138030859645909\n",
      "train loss:0.8031943050420469\n",
      "train loss:0.42545002335830284\n",
      "train loss:0.5193909708323635\n",
      "train loss:0.6119478768087039\n",
      "train loss:0.6118191765610849\n",
      "train loss:0.4270564020181222\n",
      "train loss:0.517513173257721\n",
      "train loss:0.5150372277153586\n",
      "train loss:0.6140691436007044\n",
      "train loss:0.6147262689739257\n",
      "train loss:0.29321835098329124\n",
      "train loss:0.7287537302348314\n",
      "train loss:0.5061490269857014\n",
      "train loss:0.5048141676122146\n",
      "train loss:0.5036341806944271\n",
      "train loss:0.9795623086840184\n",
      "train loss:0.7359546447208105\n",
      "train loss:0.3923285672981601\n",
      "train loss:0.6187479294304643\n",
      "train loss:0.5076093873356659\n",
      "train loss:0.5078477015469877\n",
      "train loss:0.5077433550143484\n",
      "train loss:0.6177250319941558\n",
      "train loss:0.39595301673226213\n",
      "train loss:0.3912137773431719\n",
      "train loss:0.6216525080018804\n",
      "train loss:0.6223945435861007\n",
      "train loss:0.5039607285091454\n",
      "train loss:0.5034055914440506\n",
      "train loss:0.2506415189719493\n",
      "train loss:0.7570875162114331\n",
      "train loss:0.7557619170002421\n",
      "train loss:0.7504300676565883\n",
      "train loss:0.5033194007842993\n",
      "train loss:0.38242640460223826\n",
      "train loss:0.3807989017368022\n",
      "train loss:0.5026985497291483\n",
      "train loss:0.6267689461362421\n",
      "train loss:0.7475404548855814\n",
      "train loss:0.25595303416883486\n",
      "train loss:0.5024438519659193\n",
      "train loss:0.7503154783409303\n",
      "train loss:0.6251082344163417\n",
      "train loss:0.2565016275495365\n",
      "train loss:0.6261503468188014\n",
      "train loss:0.9788288017788552\n",
      "train loss:0.6197436450892941\n",
      "train loss:0.3965371256976812\n",
      "train loss:0.8259891954988738\n",
      "train loss:0.6136240382952528\n",
      "train loss:0.6122787067665254\n",
      "train loss:0.5215159339098061\n",
      "train loss:0.5236588104111605\n",
      "train loss:0.6113936522280875\n",
      "train loss:0.5264687339477682\n",
      "train loss:0.6944016422397853\n",
      "train loss:0.691623613087068\n",
      "train loss:0.6883228819981172\n",
      "train loss:0.6848568787701531\n",
      "train loss:0.544757307924506\n",
      "train loss:0.5453474037979064\n",
      "train loss:0.5435563698106074\n",
      "train loss:0.6131944012945902\n",
      "train loss:0.7553669265198877\n",
      "train loss:0.541166726988843\n",
      "train loss:0.6128037939458589\n",
      "train loss:0.6124671440581877\n",
      "train loss:0.6121507670682858\n",
      "train loss:0.6118769931809689\n",
      "train loss:0.6116599560459205\n",
      "train loss:0.844028637023459\n",
      "train loss:0.6868067192187346\n",
      "train loss:0.3853281990075014\n",
      "train loss:0.8325908147049439\n",
      "train loss:0.5401723018250555\n",
      "train loss:0.8844171394467357\n",
      "train loss:0.6163989346195359\n",
      "train loss:0.67741740033763\n",
      "train loss:0.7280360949573682\n",
      "train loss:0.6259444316506266\n",
      "train loss:0.6284249479732192\n",
      "train loss:0.6297644464437183\n",
      "train loss:0.6735874077446653\n",
      "train loss:0.5862558863472515\n",
      "train loss:0.6737484570421856\n",
      "train loss:0.6272166499265865\n",
      "train loss:0.518193651476312\n",
      "train loss:0.618992800022692\n",
      "train loss:0.6795015429887956\n",
      "train loss:0.6815614548901407\n",
      "train loss:0.5414150603533978\n",
      "train loss:0.7607730456047623\n",
      "train loss:0.6120867367694235\n",
      "train loss:0.6117258416593881\n",
      "train loss:0.4437852668616261\n",
      "train loss:0.43013907963809606\n",
      "train loss:0.7082380016342466\n",
      "train loss:0.4098754393629845\n",
      "train loss:0.508231843084795\n",
      "train loss:0.7329813563975425\n",
      "train loss:0.8476291027855611\n",
      "train loss:0.5055929783049591\n",
      "train loss:0.2663664116246086\n",
      "train loss:0.624861184819862\n",
      "train loss:0.6259881298476913\n",
      "train loss:0.6263164207112343\n",
      "train loss:0.8649750169072759\n",
      "train loss:0.6215976817640356\n",
      "train loss:0.5060100413442374\n",
      "train loss:0.5069158776163077\n",
      "train loss:1.1295447306550916\n",
      "train loss:0.41879174560697213\n",
      "train loss:0.5193049467992493\n",
      "train loss:0.42952734675354864\n",
      "train loss:0.6997497034731008\n",
      "train loss:0.34335852738706885\n",
      "train loss:0.6116780737558903\n",
      "train loss:0.7884011010122579\n",
      "train loss:0.43314152255043215\n",
      "train loss:0.4273253632557853\n",
      "train loss:0.7058079711900309\n",
      "train loss:0.6123553952547248\n",
      "train loss:0.5162895566906247\n",
      "train loss:0.4130520134361297\n",
      "train loss:0.6149521858863987\n",
      "train loss:0.8215480839921024\n",
      "train loss:0.6146332693327581\n",
      "train loss:0.7142046272810008\n",
      "train loss:0.6131181330962475\n",
      "train loss:0.6124936486448759\n",
      "train loss:0.5182683020264607\n",
      "train loss:0.5184018813081902\n",
      "train loss:0.5176865320618681\n",
      "train loss:0.3163571058719182\n",
      "train loss:0.7154515451692747\n",
      "train loss:0.7159214352147105\n",
      "train loss:0.6142259834984121\n",
      "train loss:0.7133681444158726\n",
      "train loss:0.6130686919558631\n",
      "train loss:0.6125453788772828\n",
      "train loss:0.6121030452073072\n",
      "train loss:0.7014440493138563\n",
      "train loss:0.523614382146009\n",
      "train loss:0.6960022331923763\n",
      "train loss:0.5282815720553187\n",
      "train loss:0.5283773440111511\n",
      "train loss:0.3510429694967283\n",
      "train loss:0.4266355788596082\n",
      "train loss:0.4131804096907211\n",
      "train loss:0.6162738254601131\n",
      "train loss:0.3910424592449427\n",
      "train loss:0.624356685603037\n",
      "train loss:0.6279033037509543\n",
      "train loss:0.6305821951861799\n",
      "train loss:0.6322029171911042\n",
      "train loss:0.8883595540480401\n",
      "train loss:0.5018652501334625\n",
      "train loss:0.6272764919915028\n",
      "train loss:0.7444441729701705\n",
      "train loss:0.7343443317900367\n",
      "train loss:0.6168012804436622\n",
      "train loss:0.5117455603383756\n",
      "train loss:0.6131528678054549\n",
      "train loss:0.7038521433359566\n",
      "train loss:0.5238291551984479\n",
      "train loss:0.6113574411432741\n",
      "train loss:0.6115837438528666\n",
      "train loss:0.6878650500148351\n",
      "train loss:0.6130977094492482\n",
      "train loss:0.5433354235908066\n",
      "train loss:0.6822746139648453\n",
      "train loss:0.5471211201137567\n",
      "train loss:0.6812944973245172\n",
      "train loss:0.40522819127389287\n",
      "train loss:0.5383918364349898\n",
      "train loss:0.5317337548948615\n",
      "train loss:0.6952226559457831\n",
      "train loss:0.8624517923195368\n",
      "train loss:0.6113580420038083\n",
      "train loss:0.6113598632390798\n",
      "train loss:0.3496837487662373\n",
      "train loss:0.7008418395994614\n",
      "train loss:0.7013474591584012\n",
      "train loss:0.5204857189864924\n",
      "train loss:0.7916082761420092\n",
      "train loss:0.7836750913422039\n",
      "train loss:0.6928658968007076\n",
      "train loss:0.6119439382601314\n",
      "train loss:0.6126557952645726\n",
      "train loss:0.3888658222867526\n",
      "train loss:0.7589489935677551\n",
      "train loss:0.384610382947579\n",
      "train loss:0.689264513918791\n",
      "train loss:0.6117033260754986\n",
      "train loss:0.5297569599211253\n",
      "train loss:0.6943900898069433\n",
      "train loss:0.43778697138933353\n",
      "train loss:0.7003874544640813\n",
      "train loss:0.700942624089746\n",
      "train loss:0.6115871215067891\n",
      "train loss:0.8697391736781345\n",
      "train loss:0.9233024348083114\n",
      "train loss:0.877899677703833\n",
      "train loss:0.6208579986397188\n",
      "train loss:0.6737801606446218\n",
      "train loss:0.7417093955407781\n",
      "train loss:0.6769655655987656\n",
      "train loss:0.6228123101525372\n",
      "train loss:0.6960508006584945\n",
      "train loss:0.6402530472490887\n",
      "train loss:0.6646923029673752\n",
      "train loss:0.6586406314534212\n",
      "train loss:0.5920142213031525\n",
      "train loss:0.6372778483669876\n",
      "train loss:0.5302626555595887\n",
      "train loss:0.5571774560815814\n",
      "train loss:0.5389994536056626\n",
      "train loss:0.6959318256171118\n",
      "train loss:0.41935002743697425\n",
      "train loss:0.6165916330210958\n",
      "train loss:0.7359564174293038\n",
      "train loss:0.37867636903116975\n",
      "train loss:0.5012141648843789\n",
      "train loss:0.5008697830155625\n",
      "train loss:0.3539759776617767\n",
      "train loss:0.3474186870335882\n",
      "train loss:0.8249399999838684\n",
      "train loss:0.5057233251000792\n",
      "train loss:0.5068707721677312\n",
      "train loss:0.6750525153342529\n",
      "train loss:0.9992106168527348\n",
      "train loss:0.5046608874264037\n",
      "train loss:0.8129370498496741\n",
      "train loss:0.5017842873058225\n",
      "train loss:0.784355302450194\n",
      "train loss:0.5010446415919347\n",
      "train loss:0.5017622392736576\n",
      "train loss:0.7453488021477814\n",
      "train loss:0.6195879306561483\n",
      "train loss:0.40112400335783194\n",
      "train loss:0.2976699289982347\n",
      "train loss:0.7177857300554746\n",
      "train loss:0.7118372294678049\n",
      "train loss:0.4201214224387739\n",
      "train loss:0.6124397574328048\n",
      "train loss:0.5189436741184252\n",
      "train loss:0.7916448728463525\n",
      "train loss:0.7803926588375829\n",
      "train loss:0.6118136415027196\n",
      "train loss:0.6124981265486313\n",
      "train loss:0.465024507779163\n",
      "train loss:0.7549709479153836\n",
      "train loss:0.4729051780475733\n",
      "train loss:0.7509559613570275\n",
      "train loss:0.6150331725755653\n",
      "train loss:0.680530668013873\n",
      "train loss:0.4843613917858014\n",
      "train loss:0.47574678571153506\n",
      "train loss:0.7547675685154219\n",
      "train loss:0.385791596828687\n",
      "train loss:0.6909867639468501\n",
      "train loss:0.5278742314706585\n",
      "train loss:0.6116021762724249\n",
      "train loss:0.7898329982079626\n",
      "train loss:0.8716317758346431\n",
      "train loss:0.5258522295643183\n",
      "train loss:0.6115080280726378\n",
      "train loss:0.7747522646733558\n",
      "train loss:0.364803710350165\n",
      "train loss:0.7729805420346997\n",
      "train loss:0.6118233522082305\n",
      "train loss:0.45067604767001307\n",
      "train loss:0.611600504071674\n",
      "train loss:0.527498451963006\n",
      "train loss:0.7791040399074868\n",
      "train loss:0.6940391900213724\n",
      "train loss:0.5290327814242011\n",
      "train loss:0.5277596329906651\n",
      "train loss:0.6952231500633782\n",
      "train loss:0.611497696272722\n",
      "train loss:0.6114948005359249\n",
      "train loss:0.4392440795600625\n",
      "train loss:0.5223203245333353\n",
      "train loss:0.5188841623459004\n",
      "train loss:0.6129412154536811\n",
      "train loss:0.4108274450299277\n",
      "train loss:0.7203704639579074\n",
      "train loss:0.8249935364994657\n",
      "train loss:0.5104144767708578\n",
      "train loss:0.6158076181712884\n",
      "train loss:0.615638863029219\n",
      "train loss:0.40321639025136136\n",
      "train loss:0.5086466978438168\n",
      "train loss:0.6177724092853614\n",
      "train loss:0.7266140453270047\n",
      "train loss:0.7232575765637307\n",
      "train loss:0.9164688354005538\n",
      "train loss:0.6125234862098926\n",
      "train loss:0.3332474374934596\n",
      "train loss:0.7015259932500426\n",
      "train loss:0.8616716456213671\n",
      "train loss:0.6119914492740314\n",
      "train loss:0.5389028836483133\n",
      "train loss:0.3903613904668486\n",
      "train loss:0.6862819894607599\n",
      "train loss:0.3823498250417029\n",
      "train loss:0.6903578331276645\n",
      "train loss:0.7680144603049046\n",
      "train loss:0.6119655705722882\n",
      "train loss:0.5329028209094198\n",
      "train loss:0.7674781021875411\n",
      "train loss:0.5338965361427355\n",
      "train loss:0.6893378099713223\n",
      "train loss:0.7616192811686828\n",
      "train loss:0.5393805595961221\n",
      "train loss:0.7545105815883553\n",
      "train loss:0.8097715067167638\n",
      "train loss:0.5563974662835931\n",
      "train loss:0.7862726254365473\n",
      "train loss:0.5173007178490524\n",
      "train loss:0.5147331327669559\n",
      "train loss:0.6209085206005027\n",
      "train loss:0.6195122141671922\n",
      "train loss:0.4918603634479271\n",
      "train loss:0.6146378440012609\n",
      "train loss:0.5391674154648991\n",
      "train loss:0.7662084370503643\n",
      "train loss:0.5300069425781065\n",
      "train loss:0.4357068213108703\n",
      "train loss:0.612227678242488\n",
      "train loss:0.7103848211459225\n",
      "train loss:0.5127400271220628\n",
      "train loss:0.40198749853495225\n",
      "train loss:0.7290235086538812\n",
      "train loss:0.3897232280095165\n",
      "train loss:0.7406730435678535\n",
      "train loss:0.5035370825179905\n",
      "train loss:0.5027706563440615\n",
      "train loss:0.5021317859419419\n",
      "train loss:0.37122688522180397\n",
      "train loss:0.892678057285847\n",
      "train loss:0.7585794797302613\n",
      "train loss:0.5020314668278058\n",
      "train loss:0.6270040487754859\n",
      "train loss:0.624790213726096\n",
      "train loss:0.5040850011507854\n",
      "train loss:0.38580619134490146\n",
      "train loss:0.737246644327809\n",
      "train loss:0.6195111995777974\n",
      "train loss:0.7259443138604236\n",
      "train loss:0.40321260577376855\n",
      "train loss:0.6151947124685393\n",
      "train loss:0.6142246117791967\n",
      "train loss:0.514026560616623\n",
      "train loss:0.5145105310117185\n",
      "train loss:0.5143696153069717\n",
      "train loss:0.6135015167536576\n",
      "train loss:0.7104516551585561\n",
      "train loss:0.7988650066840475\n",
      "train loss:0.521131111409381\n",
      "train loss:0.5224021024795323\n",
      "train loss:0.6115126906304711\n",
      "train loss:0.6114460146852407\n",
      "train loss:0.6955511239639574\n",
      "train loss:0.7711209456110397\n",
      "train loss:0.6872136052020107\n",
      "train loss:0.8789519874987102\n",
      "train loss:0.5586455113984925\n",
      "train loss:0.7275515711083218\n",
      "train loss:0.5279314118270646\n",
      "train loss:0.6274313305291479\n",
      "train loss:0.5306796706838333\n",
      "train loss:0.6744095524987131\n",
      "train loss:0.6242670593910049\n",
      "train loss:0.6224768904005671\n",
      "train loss:0.7293263407048368\n",
      "train loss:0.7785139733643229\n",
      "train loss:0.571309748164873\n",
      "train loss:0.5676621212369157\n",
      "train loss:0.676288970490803\n",
      "train loss:0.6193463482464789\n",
      "train loss:0.5557952977145797\n",
      "train loss:0.6154450560674325\n",
      "train loss:0.46981391054291566\n",
      "train loss:0.6119444622563783\n",
      "train loss:0.6114350108260782\n",
      "train loss:0.6986273639681104\n",
      "train loss:0.6117627416195344\n",
      "train loss:0.612213048730592\n",
      "train loss:0.41658119195035415\n",
      "train loss:0.6145558698192654\n",
      "train loss:0.6157770464592673\n",
      "train loss:0.6168201398406348\n",
      "train loss:0.6175886879225059\n",
      "train loss:0.5070909284431646\n",
      "train loss:0.5060228683429943\n",
      "train loss:0.38682976610061587\n",
      "train loss:0.7424569403526371\n",
      "train loss:0.6236333441384638\n",
      "train loss:0.7410920061894225\n",
      "train loss:0.26413129755548437\n",
      "train loss:0.5033023212095079\n",
      "train loss:0.8634210523767637\n",
      "train loss:0.6221819286358576\n",
      "train loss:0.387572714253981\n",
      "train loss:0.5047256672879794\n",
      "train loss:0.736029303349624\n",
      "train loss:0.5059502855704162\n",
      "train loss:0.6190272728879231\n",
      "train loss:0.7262342417298664\n",
      "train loss:0.7194409404080144\n",
      "train loss:0.7116403022436789\n",
      "train loss:0.6120571183686802\n",
      "train loss:0.6114778323064\n",
      "train loss:0.4411763366166035\n",
      "train loss:0.6114361971305218\n",
      "train loss:0.6914570365757935\n",
      "train loss:0.8338894886654131\n",
      "train loss:0.39730484333417315\n",
      "train loss:0.613532255438056\n",
      "train loss:0.6137873660332616\n",
      "train loss:0.46972716728546554\n",
      "train loss:0.6126912492512767\n",
      "train loss:0.6122439762130667\n",
      "train loss:0.36747555612358174\n",
      "train loss:0.5239632032207242\n",
      "train loss:0.5181665303882063\n",
      "train loss:0.6137304525879664\n",
      "train loss:0.7185593153432659\n",
      "train loss:0.7209956462400628\n",
      "train loss:0.616149796095263\n",
      "train loss:0.3990362539953257\n",
      "train loss:0.618410695348582\n",
      "train loss:0.3911279202910048\n",
      "train loss:0.38463764256425703\n",
      "train loss:0.5026302761283994\n",
      "train loss:0.501698059975703\n",
      "train loss:0.7621199024255289\n",
      "train loss:0.8879345202271681\n",
      "train loss:0.8748019840524032\n",
      "train loss:0.8549805299666952\n",
      "train loss:0.6175140168381803\n",
      "train loss:0.910017688883644\n",
      "train loss:0.33453076699041606\n",
      "train loss:0.34178277053585454\n",
      "train loss:0.522880909164315\n",
      "train loss:0.5229964263334239\n",
      "train loss:0.6115540963463\n",
      "train loss:0.4310632412382486\n",
      "train loss:0.702673597481705\n",
      "train loss:0.519693879014072\n",
      "train loss:0.5178479854456247\n",
      "train loss:0.6128036729651396\n",
      "train loss:0.6131427766638363\n",
      "train loss:0.7106195133850362\n",
      "train loss:0.612974459726648\n",
      "train loss:0.6128042684326268\n",
      "train loss:0.5162609798795856\n",
      "train loss:0.6128406471183123\n",
      "train loss:0.6127618270471971\n",
      "train loss:0.5161285781684497\n",
      "train loss:0.7085636715144468\n",
      "train loss:0.6124547090357804\n",
      "train loss:0.6122242161442338\n",
      "train loss:0.4231639901884069\n",
      "train loss:0.6125438913716748\n",
      "train loss:0.7068806613213935\n",
      "train loss:0.5177196971089398\n",
      "train loss:0.7968163647367826\n",
      "train loss:0.7873538498609457\n",
      "train loss:0.6941463912454827\n",
      "train loss:0.5325687019541194\n",
      "train loss:0.6121973755510708\n",
      "train loss:0.4606104124652224\n",
      "train loss:0.6122612568965705\n",
      "train loss:0.6872684181742061\n",
      "train loss:0.7570685136089093\n",
      "train loss:0.6827849297126521\n",
      "train loss:0.615215712929064\n",
      "train loss:0.6794427680958868\n",
      "train loss:0.6176820094219153\n",
      "train loss:0.6772290211177868\n",
      "train loss:0.6761836413153535\n",
      "train loss:0.6752761899346247\n",
      "train loss:0.6240227488556153\n",
      "train loss:0.5725552837013704\n",
      "train loss:0.5120725510159445\n",
      "train loss:0.6768930115832732\n",
      "train loss:0.6177764326123724\n",
      "train loss:0.6793695508855528\n",
      "train loss:0.6154308167316486\n",
      "train loss:0.681810259899711\n",
      "train loss:0.6140312896158364\n",
      "train loss:0.46588016754304934\n",
      "train loss:0.6886357997900192\n",
      "train loss:0.4478651601567966\n",
      "train loss:0.6968468282351515\n",
      "train loss:0.6115322445157216\n",
      "train loss:0.5192529684244197\n",
      "train loss:0.8000839628503817\n",
      "train loss:0.6123914480248855\n",
      "train loss:0.6124977926453176\n",
      "train loss:0.5161913437125805\n",
      "train loss:0.31116192505597307\n",
      "train loss:0.8189267345985376\n",
      "train loss:0.6147379806482818\n",
      "train loss:0.7164981530636766\n",
      "train loss:0.7134011150005432\n",
      "train loss:0.7088587172472859\n",
      "train loss:0.7033784235771586\n",
      "train loss:0.5231249297796118\n",
      "train loss:0.6113613269758498\n",
      "train loss:0.6114005364179073\n",
      "train loss:0.6909475615626315\n",
      "train loss:0.6121266299528628\n",
      "train loss:0.6126478665012113\n",
      "train loss:0.5399368373902453\n",
      "train loss:0.46303048214553905\n",
      "train loss:0.6877064945465825\n",
      "train loss:0.6121169791281231\n",
      "train loss:0.6882973488438165\n",
      "train loss:0.5345604072851312\n",
      "train loss:0.6117468661712305\n",
      "train loss:0.5304155912586854\n",
      "train loss:0.7736535685651398\n",
      "train loss:0.611480641878392\n",
      "train loss:0.6917742660855886\n",
      "train loss:0.44798540069902526\n",
      "train loss:0.693445319818475\n",
      "train loss:0.3540311796326997\n",
      "train loss:0.4289985805802763\n",
      "train loss:0.5155372184740262\n",
      "train loss:0.5112536107678538\n",
      "train loss:0.3958496997878379\n",
      "train loss:0.7376293591221067\n",
      "train loss:0.742014261313207\n",
      "train loss:0.5031581076936412\n",
      "train loss:0.8668316226996566\n",
      "train loss:0.7414228187246851\n",
      "train loss:0.7346544243001453\n",
      "train loss:0.5071921392833034\n",
      "train loss:0.6166456508406739\n",
      "train loss:0.6150858820348893\n",
      "train loss:0.5128329647611845\n",
      "train loss:0.30845518122638715\n",
      "train loss:0.7150870607010951\n",
      "train loss:0.8081376568278174\n",
      "train loss:0.41994022469299014\n",
      "train loss:0.6124399454305595\n",
      "train loss:0.32172776195244446\n",
      "train loss:0.4123578543001429\n",
      "train loss:0.5105326115763286\n",
      "train loss:0.8278214020945116\n",
      "train loss:0.8233827858854642\n",
      "train loss:0.7148824148833521\n",
      "train loss:0.7092345278368233\n",
      "train loss:0.3256463569261948\n",
      "train loss:0.6123420850650846\n",
      "train loss:0.6121246212636554\n",
      "train loss:0.7028954236544639\n",
      "train loss:0.5216240640294566\n",
      "train loss:0.6115104935710994\n",
      "train loss:0.5226603563262748\n",
      "train loss:0.5217272074623528\n",
      "train loss:0.6116966298744095\n",
      "train loss:0.5195289812578858\n",
      "train loss:0.3214283170571108\n",
      "train loss:0.614078016781536\n",
      "train loss:0.6152494121198524\n",
      "train loss:0.7215007523437351\n",
      "train loss:0.6160921526807835\n",
      "train loss:0.8238315800549902\n",
      "train loss:0.5116611403973627\n",
      "train loss:0.7142765974710723\n",
      "train loss:0.6130399968806891\n",
      "train loss:0.6123859768921462\n",
      "train loss:0.8779309262727935\n",
      "train loss:0.6934978643772751\n",
      "train loss:0.612121370966529\n",
      "train loss:0.4663428967740697\n",
      "train loss:0.5410902943059996\n",
      "train loss:0.6132993408982258\n",
      "train loss:0.46668552201375224\n",
      "train loss:0.6864804689427554\n",
      "train loss:0.6123657883252311\n",
      "train loss:0.7602269974020285\n",
      "train loss:0.7550932320271486\n",
      "train loss:0.7475547376165974\n",
      "train loss:0.48589085940335136\n",
      "train loss:0.550276966661429\n",
      "train loss:0.6151347311406045\n",
      "train loss:0.6811060887939137\n",
      "train loss:0.4042403524665342\n",
      "train loss:0.7560450816038844\n",
      "train loss:0.3825749868879892\n",
      "train loss:0.691702640526487\n",
      "train loss:0.6113381599830073\n",
      "train loss:0.4337188969747924\n",
      "train loss:0.6121262182213566\n",
      "train loss:0.41392171552068424\n",
      "train loss:0.5097319800385698\n",
      "train loss:0.5064599659539295\n",
      "train loss:0.7383717693973987\n",
      "train loss:0.5034253377141747\n",
      "train loss:0.5023590239197475\n",
      "train loss:0.5015924685477093\n",
      "train loss:0.5011000291630421\n",
      "train loss:0.5008433072634663\n",
      "train loss:0.6386433813909214\n",
      "train loss:0.5007839204360373\n",
      "train loss:0.5008289497711386\n",
      "train loss:0.781612843294947\n",
      "train loss:0.36008584220256246\n",
      "train loss:0.6406104880141348\n",
      "train loss:0.9075120868440374\n",
      "train loss:0.3681455432775222\n",
      "train loss:0.501318029445245\n",
      "train loss:0.9995187800516565\n",
      "train loss:0.8516075665673162\n",
      "train loss:0.6162969065507105\n",
      "train loss:0.5139065659875064\n",
      "train loss:0.703880201854645\n",
      "train loss:0.6951282064199971\n",
      "train loss:0.760984216193377\n",
      "train loss:0.4782506455953566\n",
      "train loss:0.5519995514535939\n",
      "train loss:0.4903547877851146\n",
      "train loss:0.48597260612415694\n",
      "train loss:0.6148519557374534\n",
      "train loss:0.6140344137038943\n",
      "train loss:0.5403160904219768\n",
      "train loss:0.6122530091612645\n",
      "train loss:0.7664920081221265\n",
      "train loss:0.4506033564740541\n",
      "train loss:0.6114764676709749\n",
      "train loss:0.43458680018537227\n",
      "train loss:0.6121399212565619\n",
      "train loss:0.6129023547140111\n",
      "train loss:0.5133313690492362\n",
      "train loss:0.5105008068742548\n",
      "train loss:0.5079596926133919\n",
      "train loss:0.6197931495303843\n",
      "train loss:0.3859824176625163\n",
      "train loss:0.8635150224622661\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6617647058823529\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet7Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22be5d5a-c604-4cf1-bfb9-a0caf7e59b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), #필터 수 조절해서 성능 조절\n",
    "                 conv_param_1 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=10):\n",
    "        # 가중치 초기화===========\n",
    "        # 각 층의 뉴런 하나당 앞 층의 몇 개 뉴런과 연결되는가（TODO: 자동 계산되게 바꿀 것）\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLU를 사용할 때의 권장 초깃값\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = wight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = wight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fd471b1-374f-461b-84c4-7b22dade8389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.0719775451053626\n",
      "=== epoch:1, train acc:0.7, test acc:0.61 ===\n",
      "train loss:2.1800809121178353\n",
      "train loss:1.9636100748377714\n",
      "train loss:1.6551257925811629\n",
      "train loss:1.574160530305424\n",
      "train loss:1.7821058609378855\n",
      "train loss:0.8553402525695274\n",
      "train loss:2.1451963192932526\n",
      "train loss:2.162223591559808\n",
      "train loss:0.719055386003319\n",
      "train loss:1.6434724438415773\n",
      "train loss:1.8630841875674435\n",
      "train loss:1.3837112785090953\n",
      "train loss:2.216024642472302\n",
      "train loss:2.0179047992615704\n",
      "train loss:1.557615721347896\n",
      "train loss:1.2609492868045327\n",
      "train loss:1.5950664293010963\n",
      "train loss:1.401170570339697\n",
      "train loss:2.1427611169659153\n",
      "train loss:1.6595075377099076\n",
      "train loss:1.7673580901603443\n",
      "train loss:1.7365094842220636\n",
      "train loss:1.7322291971250217\n",
      "train loss:1.7269642358945316\n",
      "train loss:1.6702431889401486\n",
      "train loss:2.4638170623602433\n",
      "train loss:1.4896966018586368\n",
      "train loss:1.523643921940315\n",
      "train loss:1.5996065869705616\n",
      "train loss:1.317800304035552\n",
      "train loss:1.6333577462142195\n",
      "train loss:1.7803079149696743\n",
      "train loss:1.8050867156979233\n",
      "train loss:1.6465723808703174\n",
      "train loss:1.2367028361147878\n",
      "train loss:1.872023873255684\n",
      "train loss:0.9664949538927547\n",
      "train loss:1.280950581383028\n",
      "train loss:2.366485991996523\n",
      "train loss:0.8241916256595466\n",
      "train loss:1.3945753957506342\n",
      "train loss:0.8045277920178906\n",
      "train loss:1.7730311415192506\n",
      "train loss:2.0557974785144926\n",
      "train loss:1.050120549132573\n",
      "train loss:1.8661832624124834\n",
      "train loss:2.0923549978251037\n",
      "train loss:1.1327114852401245\n",
      "train loss:1.7293472738259943\n",
      "train loss:1.0678084745915206\n",
      "train loss:1.0952676303584952\n",
      "train loss:1.29994515731474\n",
      "train loss:1.2479125649019362\n",
      "train loss:1.0666368844473184\n",
      "train loss:1.5024529255039587\n",
      "train loss:1.1352673415944678\n",
      "train loss:1.775944267018593\n",
      "train loss:1.0636193559085796\n",
      "train loss:1.443130803694333\n",
      "train loss:1.1102627563755232\n",
      "train loss:1.6943237652863528\n",
      "train loss:1.8672252610137714\n",
      "train loss:1.4743585463866609\n",
      "train loss:1.2393836748778533\n",
      "train loss:1.612201477205551\n",
      "train loss:1.606083970016926\n",
      "train loss:2.0023078808405566\n",
      "train loss:1.6713525002569427\n",
      "train loss:1.8917429476300722\n",
      "train loss:1.5030211385553365\n",
      "train loss:1.902737467576888\n",
      "train loss:1.4043575387297245\n",
      "train loss:1.8866594759950523\n",
      "train loss:1.7819741036100605\n",
      "train loss:1.4050348418708134\n",
      "train loss:1.6417744197425812\n",
      "train loss:1.4858662228320099\n",
      "train loss:1.7930614964896197\n",
      "train loss:1.3340273615904423\n",
      "train loss:1.4726281375890555\n",
      "train loss:1.6622044815464156\n",
      "train loss:1.3543631102360196\n",
      "train loss:2.087078589565932\n",
      "train loss:1.4168988486449485\n",
      "train loss:1.7500488868486017\n",
      "train loss:1.4992107110692459\n",
      "train loss:0.9399959534615638\n",
      "train loss:1.516012974083845\n",
      "train loss:1.228433828610589\n",
      "train loss:0.8865602078254312\n",
      "train loss:1.4978957799579566\n",
      "train loss:1.4373712423105878\n",
      "train loss:1.5817706461942154\n",
      "train loss:1.750584368744954\n",
      "train loss:1.41761086872971\n",
      "train loss:1.4167496025350883\n",
      "train loss:1.650042054048339\n",
      "train loss:1.1373279682572615\n",
      "train loss:1.8146241971326802\n",
      "train loss:1.4126934432537162\n",
      "train loss:1.5280858432422826\n",
      "train loss:1.3859882785710615\n",
      "train loss:1.2183526281058084\n",
      "train loss:1.1208088477316804\n",
      "train loss:1.5215050938394612\n",
      "train loss:1.6471418890477874\n",
      "train loss:1.680949540851858\n",
      "train loss:1.683997907260497\n",
      "train loss:1.2644885393066163\n",
      "train loss:0.5419927462464916\n",
      "train loss:2.0967221240570724\n",
      "train loss:1.2593560856966617\n",
      "train loss:1.1335246719566585\n",
      "train loss:1.294722675902406\n",
      "train loss:0.6371460218481858\n",
      "train loss:1.3098850352769498\n",
      "train loss:2.345048813560924\n",
      "train loss:1.673588962884321\n",
      "train loss:1.4020048057045833\n",
      "train loss:0.5914006113388574\n",
      "train loss:2.379027910273026\n",
      "train loss:2.2034101472438037\n",
      "train loss:1.2960682540388404\n",
      "train loss:1.7522125635075807\n",
      "train loss:1.4315210713931674\n",
      "train loss:1.0516316632410425\n",
      "train loss:1.0307494470797274\n",
      "train loss:1.282527112675016\n",
      "train loss:1.8255562149207971\n",
      "train loss:1.1747031416340534\n",
      "train loss:1.6219810504351357\n",
      "train loss:1.7557048409515819\n",
      "train loss:1.968993974900107\n",
      "train loss:1.5033595960426414\n",
      "train loss:1.4408916743799813\n",
      "train loss:1.615050125441451\n",
      "train loss:1.996161513768283\n",
      "train loss:1.5184687648425552\n",
      "train loss:1.760802926764632\n",
      "train loss:1.5698113977449597\n",
      "train loss:1.8003535006167755\n",
      "train loss:1.44534595568054\n",
      "train loss:1.277049974096766\n",
      "train loss:1.333599898396623\n",
      "train loss:1.7857612645283765\n",
      "train loss:1.5110721012524673\n",
      "train loss:1.4151310174621412\n",
      "train loss:1.25728174944632\n",
      "train loss:1.0900918027209827\n",
      "train loss:1.7733181942554541\n",
      "train loss:1.4801591398555742\n",
      "train loss:1.6790806543669674\n",
      "train loss:1.7701554617540516\n",
      "train loss:1.0376608675469283\n",
      "train loss:1.450089460895407\n",
      "train loss:1.2878684097939983\n",
      "train loss:1.5342282701396779\n",
      "train loss:1.2181116148380042\n",
      "train loss:1.3930346471318047\n",
      "train loss:1.6143182354420325\n",
      "train loss:1.7351584279134191\n",
      "train loss:0.9866439196025395\n",
      "train loss:1.669166854187494\n",
      "train loss:1.1470037774237396\n",
      "train loss:1.4594543445725376\n",
      "train loss:2.4843131419326356\n",
      "train loss:1.5549997858248692\n",
      "train loss:1.1227936272826882\n",
      "train loss:1.0626696917663954\n",
      "train loss:1.8686490599215297\n",
      "train loss:1.3522287296935502\n",
      "train loss:0.8551065591601196\n",
      "train loss:1.1064562410728647\n",
      "train loss:1.6083910476495569\n",
      "train loss:1.476519348786031\n",
      "train loss:1.3330843773680274\n",
      "train loss:1.2737864365931046\n",
      "train loss:1.3617957497745643\n",
      "train loss:1.4890604947830863\n",
      "train loss:1.39431571275002\n",
      "train loss:1.529428000970801\n",
      "train loss:1.5409476494043512\n",
      "train loss:1.355799278726386\n",
      "train loss:1.89316991625888\n",
      "train loss:1.613610854619229\n",
      "train loss:1.3017567165561108\n",
      "train loss:1.611433471028223\n",
      "train loss:1.6897526475826647\n",
      "train loss:1.11840798307878\n",
      "train loss:1.8981574905123266\n",
      "train loss:1.0917672138489345\n",
      "train loss:1.753337323420332\n",
      "train loss:1.0316592733411067\n",
      "train loss:1.1964721708724346\n",
      "train loss:1.6667525649019193\n",
      "train loss:1.1482937902248769\n",
      "train loss:2.0069987341517175\n",
      "train loss:1.538479614771286\n",
      "train loss:1.1083332852843009\n",
      "train loss:0.8638102013219904\n",
      "train loss:1.3191617427823106\n",
      "train loss:1.2534309528281564\n",
      "train loss:1.147918859979328\n",
      "train loss:1.824233626615089\n",
      "train loss:1.1397524126407133\n",
      "train loss:1.671966443209636\n",
      "train loss:1.329305053176095\n",
      "train loss:1.537389910544149\n",
      "train loss:1.713825518812768\n",
      "train loss:1.24651908918873\n",
      "train loss:1.8681236975316182\n",
      "train loss:1.4032277545443068\n",
      "train loss:1.089519766365306\n",
      "train loss:1.3374256933695974\n",
      "train loss:1.607513956514829\n",
      "train loss:1.3821291901406538\n",
      "train loss:1.0183089778370227\n",
      "train loss:0.6562703540817006\n",
      "train loss:1.6132341981444174\n",
      "train loss:0.720099147247066\n",
      "train loss:1.3095366840647682\n",
      "train loss:2.285115409281951\n",
      "train loss:1.1428035282538422\n",
      "train loss:1.0917969561071128\n",
      "train loss:1.1663329267132638\n",
      "train loss:1.329079209288152\n",
      "train loss:1.6315964239875846\n",
      "train loss:1.527954987816845\n",
      "train loss:1.3480456333550264\n",
      "train loss:1.6746798509286833\n",
      "train loss:1.3783902974028215\n",
      "train loss:1.8033180090268424\n",
      "train loss:1.234543470313826\n",
      "train loss:1.54885280994764\n",
      "train loss:1.3877740501193383\n",
      "train loss:0.8832760116388034\n",
      "train loss:1.14885295742166\n",
      "train loss:0.8621940159208782\n",
      "train loss:0.8917545315320536\n",
      "train loss:1.689717864428151\n",
      "train loss:1.5580326976804937\n",
      "train loss:1.4606073181437154\n",
      "train loss:1.3593817524840048\n",
      "train loss:1.564624236576313\n",
      "train loss:1.289956278217899\n",
      "train loss:1.4690569231912904\n",
      "train loss:2.0646175951844983\n",
      "train loss:1.9928989214635124\n",
      "train loss:1.0751231488891928\n",
      "train loss:0.8222091246019392\n",
      "train loss:1.511897481517525\n",
      "train loss:1.3418624481205055\n",
      "train loss:1.2477978228837117\n",
      "train loss:1.316325435735238\n",
      "train loss:1.9988858138275678\n",
      "train loss:0.6437277059727582\n",
      "train loss:1.3561760730229808\n",
      "train loss:1.1903366795609245\n",
      "train loss:1.4041925705628864\n",
      "train loss:1.8071169978764619\n",
      "train loss:1.6444563402169112\n",
      "train loss:0.8507681509855918\n",
      "train loss:1.3755501103053054\n",
      "train loss:1.6372583392848923\n",
      "train loss:1.104783662666038\n",
      "train loss:1.0845106877077615\n",
      "train loss:1.7487522334410666\n",
      "train loss:1.5262371172159475\n",
      "train loss:1.2869139422853757\n",
      "train loss:1.6591663263637553\n",
      "train loss:1.2760455781957476\n",
      "train loss:1.352043671877397\n",
      "train loss:1.5115003636890871\n",
      "train loss:1.9172972192101825\n",
      "train loss:1.5295231277030887\n",
      "train loss:1.351567994012283\n",
      "train loss:1.7415819010522298\n",
      "train loss:1.6060632642841095\n",
      "train loss:1.47594044375175\n",
      "train loss:1.718835862225643\n",
      "train loss:1.3212459380456332\n",
      "train loss:1.5522501837629092\n",
      "train loss:0.9995205716468719\n",
      "train loss:1.1021769096598635\n",
      "train loss:1.2152619052314846\n",
      "train loss:1.3869361078804967\n",
      "train loss:1.8509366684708233\n",
      "train loss:1.3769391901208192\n",
      "train loss:0.8862230282831416\n",
      "train loss:1.1799752902175913\n",
      "train loss:1.5519397004285433\n",
      "train loss:1.2922552452323086\n",
      "train loss:1.2790237324727083\n",
      "train loss:1.9264026877229603\n",
      "train loss:1.6987809285956306\n",
      "train loss:1.4711154688432535\n",
      "train loss:1.1332044458784887\n",
      "train loss:1.3861819189940396\n",
      "train loss:1.4272083868577288\n",
      "train loss:1.246118213873093\n",
      "train loss:1.2561666961864442\n",
      "train loss:1.2298942288606014\n",
      "train loss:1.695264954425049\n",
      "train loss:1.4747066068277868\n",
      "train loss:1.4881894017758717\n",
      "train loss:1.5218934809371691\n",
      "train loss:1.3543407302612012\n",
      "train loss:1.3765235020207798\n",
      "train loss:1.4062333913978722\n",
      "train loss:1.1591347780816992\n",
      "train loss:1.8871769576103699\n",
      "train loss:1.2404777120844306\n",
      "train loss:1.6812172927073856\n",
      "train loss:0.9541420715848178\n",
      "train loss:1.4589773526003698\n",
      "train loss:1.2639930379468114\n",
      "train loss:1.7914544875115506\n",
      "train loss:1.370018792721417\n",
      "train loss:1.4738217528480049\n",
      "train loss:2.3665916537018665\n",
      "train loss:1.5721674209585776\n",
      "train loss:1.4988253368379103\n",
      "train loss:1.5173042467580522\n",
      "train loss:1.8793050695244535\n",
      "train loss:1.6054114781811717\n",
      "train loss:1.0282733293232666\n",
      "train loss:1.0927928769887183\n",
      "train loss:1.484227558301993\n",
      "train loss:1.2752591897188084\n",
      "train loss:0.8157889982128573\n",
      "train loss:1.5311512358670765\n",
      "train loss:1.5405149216548522\n",
      "train loss:1.5716811897080192\n",
      "train loss:1.355986982816935\n",
      "train loss:1.6638541619334295\n",
      "train loss:1.4671441000226535\n",
      "train loss:1.5948602327096768\n",
      "train loss:0.9109409810072904\n",
      "train loss:1.389351072409914\n",
      "train loss:1.8162872035628337\n",
      "train loss:1.0095584903781563\n",
      "train loss:1.0454699462379968\n",
      "train loss:1.3733310374474759\n",
      "train loss:1.7541514855058775\n",
      "train loss:1.2851032916295473\n",
      "train loss:1.2461995785907822\n",
      "train loss:1.7499312099656907\n",
      "train loss:1.5861281363798427\n",
      "train loss:1.5411825679404623\n",
      "train loss:1.474307251020487\n",
      "train loss:1.2099212577334237\n",
      "train loss:1.6340816253534194\n",
      "train loss:1.3827942467750354\n",
      "train loss:1.4106973541445675\n",
      "train loss:1.7988885103109218\n",
      "train loss:1.9031251553116828\n",
      "train loss:1.7461920876908188\n",
      "train loss:1.38898204227027\n",
      "train loss:1.4445539091051396\n",
      "train loss:1.1608892809040294\n",
      "train loss:0.9167956313995953\n",
      "train loss:1.7742842073752871\n",
      "train loss:1.2787723766158914\n",
      "train loss:1.6796569318699734\n",
      "train loss:1.2721816009597275\n",
      "train loss:1.1726481687288988\n",
      "train loss:1.612818588572879\n",
      "train loss:1.523199123258717\n",
      "train loss:1.6936650570625595\n",
      "train loss:1.1843578222803797\n",
      "train loss:1.0931372123917986\n",
      "train loss:1.702848941609703\n",
      "train loss:1.4559924242001552\n",
      "train loss:1.5166481754945589\n",
      "train loss:1.3541127997702158\n",
      "train loss:1.105203107163429\n",
      "train loss:1.2387469530353143\n",
      "train loss:1.524642924619275\n",
      "train loss:1.668094023421031\n",
      "train loss:1.4080622716045936\n",
      "train loss:1.246588273965529\n",
      "train loss:1.3848167207273177\n",
      "train loss:1.7750518748520014\n",
      "train loss:1.3245080404193446\n",
      "train loss:1.4225697863189632\n",
      "train loss:1.2880832805758784\n",
      "train loss:1.3199765974598718\n",
      "train loss:1.805050216903188\n",
      "train loss:1.247607145958018\n",
      "train loss:1.686869045194071\n",
      "train loss:1.476770271619357\n",
      "train loss:1.6988654892361734\n",
      "train loss:1.2399472830742488\n",
      "train loss:1.8133226382942893\n",
      "train loss:1.2791623018239902\n",
      "train loss:1.137805707203482\n",
      "train loss:1.656048389497236\n",
      "train loss:1.1565187523450915\n",
      "train loss:1.5302825507067856\n",
      "train loss:1.6274456038297493\n",
      "train loss:1.4891662243164692\n",
      "train loss:1.1790613934278111\n",
      "train loss:1.2207848942782786\n",
      "train loss:1.4307814724756212\n",
      "train loss:0.8950918959293055\n",
      "train loss:1.7056532618953284\n",
      "train loss:1.0057362077700815\n",
      "train loss:1.3156118107939538\n",
      "train loss:1.5182300709439511\n",
      "train loss:1.140435716844234\n",
      "train loss:1.240889874564035\n",
      "train loss:1.478160319978043\n",
      "train loss:0.8532054541418297\n",
      "train loss:1.077524531632201\n",
      "train loss:1.3356871808805586\n",
      "train loss:1.1713739592366195\n",
      "train loss:0.9407806371610746\n",
      "train loss:1.8400084685713587\n",
      "train loss:1.9552528444759587\n",
      "train loss:1.3358391540708259\n",
      "train loss:1.73911369816251\n",
      "train loss:1.3694084342593098\n",
      "train loss:1.3129831505093714\n",
      "train loss:1.6042686442128655\n",
      "train loss:1.6110697354029049\n",
      "train loss:1.3812413944027941\n",
      "train loss:1.1874176992497907\n",
      "train loss:1.1413941870284794\n",
      "train loss:1.4975363200581113\n",
      "train loss:1.0694787147526015\n",
      "train loss:1.129832693838472\n",
      "train loss:1.464998862837969\n",
      "train loss:1.6260263056229547\n",
      "train loss:0.958485277488483\n",
      "train loss:1.0936480429168323\n",
      "train loss:1.5311306114260508\n",
      "train loss:1.2427093772144744\n",
      "train loss:1.6250417564134714\n",
      "train loss:1.391760960138544\n",
      "train loss:1.2257512891893758\n",
      "train loss:1.2434436347353366\n",
      "train loss:1.3673696921131566\n",
      "train loss:1.7856165733854206\n",
      "train loss:1.1532130000533878\n",
      "train loss:1.7696652731362619\n",
      "train loss:1.9390985460269916\n",
      "train loss:1.6457407106215403\n",
      "train loss:1.3166983149821418\n",
      "train loss:1.4517545562496157\n",
      "train loss:1.0313799277515634\n",
      "train loss:1.2265424652078736\n",
      "train loss:1.1008214249841664\n",
      "train loss:1.6997531822553076\n",
      "train loss:1.3157787002523602\n",
      "train loss:1.3294650394281402\n",
      "train loss:1.1190765584393207\n",
      "train loss:1.5218869098813315\n",
      "train loss:1.2178541076810212\n",
      "train loss:1.7486974618518853\n",
      "train loss:1.2552142563416393\n",
      "train loss:1.8126288179117835\n",
      "train loss:1.1494722396091295\n",
      "train loss:1.3872694119045432\n",
      "train loss:1.3422776005164383\n",
      "train loss:1.4425350068777492\n",
      "train loss:1.1439842885895655\n",
      "train loss:1.6673774840497604\n",
      "train loss:1.3463790306242274\n",
      "train loss:1.0582309276475446\n",
      "train loss:1.5322096987050133\n",
      "train loss:1.137418049752446\n",
      "train loss:0.8284449509210109\n",
      "train loss:1.3506451652078801\n",
      "train loss:1.3349051345676561\n",
      "train loss:1.2588368670299463\n",
      "train loss:1.3691172468118185\n",
      "train loss:1.0118927567136173\n",
      "train loss:1.3066772390650807\n",
      "train loss:1.424105516847747\n",
      "train loss:1.2204956019208828\n",
      "train loss:1.715034453715728\n",
      "train loss:1.6964767252012607\n",
      "train loss:0.8225195865963142\n",
      "train loss:1.4645926948098682\n",
      "train loss:1.3284520527168526\n",
      "train loss:1.6957293450027138\n",
      "train loss:1.3093792431886346\n",
      "train loss:1.3393187003871652\n",
      "train loss:1.6407358619682522\n",
      "train loss:1.1118530289057291\n",
      "train loss:1.3270748260772511\n",
      "train loss:1.0725155408251408\n",
      "train loss:1.3741729063048127\n",
      "train loss:1.4287506700789059\n",
      "train loss:1.32094280024024\n",
      "train loss:1.3856699765832539\n",
      "train loss:1.6490229972622334\n",
      "train loss:1.43327105624365\n",
      "train loss:1.5177000435943973\n",
      "train loss:0.8800847563711633\n",
      "train loss:1.3703218569193591\n",
      "train loss:0.9203947967514378\n",
      "train loss:0.7376688790336936\n",
      "train loss:1.2042871899145629\n",
      "train loss:1.399559559490012\n",
      "train loss:1.1069714081056963\n",
      "train loss:1.5758590756028776\n",
      "train loss:1.3758883382427676\n",
      "train loss:1.9697924540555813\n",
      "train loss:1.9575327080471125\n",
      "train loss:1.189562809518554\n",
      "train loss:1.4073152398297701\n",
      "train loss:0.9634978720912848\n",
      "train loss:1.846874039317962\n",
      "train loss:1.215345498393304\n",
      "train loss:1.301607698873518\n",
      "train loss:1.4089085018994323\n",
      "train loss:1.8328215089373714\n",
      "train loss:0.9870168007356999\n",
      "train loss:1.4546132754560697\n",
      "train loss:1.5730549647131575\n",
      "train loss:1.204893233683773\n",
      "train loss:1.60859949760425\n",
      "train loss:1.5879556638435819\n",
      "train loss:1.611881015393621\n",
      "train loss:1.1130527360498919\n",
      "train loss:1.212025537571378\n",
      "train loss:1.385968473452954\n",
      "train loss:1.3303275976394382\n",
      "train loss:1.3688429013290409\n",
      "train loss:1.3424673591346448\n",
      "train loss:2.0285013030304624\n",
      "train loss:1.0924303365088892\n",
      "train loss:0.7447778832421768\n",
      "train loss:1.7941238348601978\n",
      "train loss:1.660339022119686\n",
      "train loss:1.3934373228603847\n",
      "train loss:0.7463447152866803\n",
      "train loss:1.2188896365269648\n",
      "train loss:1.7010947486235772\n",
      "train loss:1.394121982023052\n",
      "train loss:1.5182883201091244\n",
      "train loss:1.4937892041197134\n",
      "train loss:1.6878976830717778\n",
      "train loss:1.2006230604902963\n",
      "train loss:1.7653858358116523\n",
      "train loss:1.3315108092055914\n",
      "train loss:1.3106997872322461\n",
      "train loss:1.8875783826945463\n",
      "train loss:1.4559438940872251\n",
      "train loss:1.364127347063963\n",
      "train loss:1.3870762675881048\n",
      "train loss:1.1998660888241877\n",
      "train loss:0.9738651023453821\n",
      "train loss:1.0876376736832878\n",
      "train loss:1.9243991581162674\n",
      "train loss:1.7500208282631244\n",
      "train loss:0.9259857067649968\n",
      "train loss:1.346659468127859\n",
      "train loss:1.4274931698087316\n",
      "train loss:1.0631704812150182\n",
      "train loss:1.2535182436619974\n",
      "train loss:1.2201873387225297\n",
      "train loss:0.8577782446099134\n",
      "train loss:1.4416284525884095\n",
      "train loss:1.3521001448980356\n",
      "train loss:1.8434505679115403\n",
      "train loss:1.6288830845875004\n",
      "train loss:1.4157635810798717\n",
      "train loss:2.0945318412399354\n",
      "train loss:1.1113459711083062\n",
      "train loss:1.3786936122259656\n",
      "train loss:0.7976096085967462\n",
      "train loss:1.8333012216004239\n",
      "train loss:1.697572177434434\n",
      "train loss:1.0843365393940796\n",
      "train loss:1.9292968204913308\n",
      "train loss:1.3810469335242208\n",
      "train loss:1.178243649289575\n",
      "train loss:1.1589600736467598\n",
      "train loss:1.4952547725845022\n",
      "train loss:1.145373508024235\n",
      "train loss:1.125686634956371\n",
      "train loss:1.5882280844987122\n",
      "train loss:1.3992521620037839\n",
      "train loss:1.4767275305526248\n",
      "train loss:1.0148713702970737\n",
      "train loss:1.0978427270549582\n",
      "train loss:1.8831361236727648\n",
      "train loss:1.2360011726245348\n",
      "train loss:1.33978918823718\n",
      "train loss:1.2775066873908827\n",
      "train loss:1.2675653354155814\n",
      "train loss:1.6704598677173295\n",
      "train loss:1.6289117341124393\n",
      "train loss:1.2758414593274159\n",
      "train loss:1.4003333799811908\n",
      "train loss:1.4134177505564531\n",
      "train loss:1.539118834188948\n",
      "train loss:1.1958144765554803\n",
      "train loss:1.215525787385135\n",
      "train loss:1.528574431198771\n",
      "train loss:1.7372857719994719\n",
      "train loss:0.8870321604186773\n",
      "train loss:1.5399215666691102\n",
      "train loss:1.5984196062984946\n",
      "train loss:1.4757721901096077\n",
      "train loss:1.467836032233944\n",
      "train loss:1.2913172566431128\n",
      "train loss:1.093737823415521\n",
      "train loss:1.8386236872562225\n",
      "train loss:1.0953376617871764\n",
      "train loss:1.7374177197131118\n",
      "train loss:1.267321348125876\n",
      "train loss:1.6348503790490008\n",
      "train loss:1.4944443425095966\n",
      "train loss:1.5632532248864026\n",
      "train loss:1.1403656203932349\n",
      "train loss:1.2341357654977905\n",
      "train loss:1.090143735166384\n",
      "train loss:1.3179780284682987\n",
      "train loss:1.3970558693835218\n",
      "train loss:1.4719311089408238\n",
      "train loss:1.7006111549112373\n",
      "train loss:1.1213700671136437\n",
      "train loss:1.3143840433671312\n",
      "train loss:1.2583473967796053\n",
      "train loss:1.3420138641488735\n",
      "train loss:1.3112347403555664\n",
      "train loss:1.147484497030263\n",
      "train loss:1.3036374818911614\n",
      "train loss:1.9440006182711507\n",
      "train loss:1.7096303202824537\n",
      "train loss:1.2416229964325391\n",
      "train loss:1.0751536406700868\n",
      "train loss:1.6742892049078997\n",
      "train loss:1.4505912543085273\n",
      "train loss:1.175443086028656\n",
      "train loss:1.5537483413985698\n",
      "train loss:1.8449598476981897\n",
      "train loss:0.8424546227011979\n",
      "train loss:1.354505786303235\n",
      "train loss:2.3808952155850487\n",
      "train loss:1.2572019833761354\n",
      "train loss:1.032278757301631\n",
      "train loss:1.6116790772120109\n",
      "train loss:1.4445055504888664\n",
      "train loss:0.9066160217678648\n",
      "train loss:2.311954819883757\n",
      "train loss:1.4684266881490593\n",
      "train loss:0.7179402880680819\n",
      "train loss:1.3995593149255756\n",
      "train loss:1.4632165820247653\n",
      "train loss:1.5436407739811735\n",
      "train loss:1.3847912379967995\n",
      "train loss:1.328813983068581\n",
      "train loss:0.6136801802389232\n",
      "train loss:1.8167690521919238\n",
      "train loss:1.7346900032420904\n",
      "train loss:1.358712447315322\n",
      "train loss:1.3947164013766076\n",
      "train loss:1.0517164174832663\n",
      "train loss:1.5601978636271738\n",
      "train loss:1.4072770762231737\n",
      "train loss:0.8530625997498102\n",
      "train loss:1.2288290906064385\n",
      "train loss:1.6664998125602708\n",
      "train loss:1.3596125087339133\n",
      "train loss:1.2802212770962194\n",
      "train loss:0.8235898582053075\n",
      "train loss:1.6655089657800228\n",
      "train loss:1.7681213512102374\n",
      "train loss:1.5735531146326205\n",
      "train loss:0.9315451726831313\n",
      "train loss:1.1507274705055237\n",
      "train loss:1.4394027798022495\n",
      "train loss:0.812452953361297\n",
      "train loss:1.5251797943576064\n",
      "train loss:2.1560319113550537\n",
      "train loss:1.2407031994714754\n",
      "train loss:1.1083709491068627\n",
      "train loss:0.7710098223987641\n",
      "train loss:1.1246524707084697\n",
      "train loss:1.4331818892983619\n",
      "train loss:1.020383286462613\n",
      "train loss:1.6230773802031688\n",
      "train loss:1.1672322239684214\n",
      "train loss:1.4940371534635102\n",
      "train loss:1.5475383460206635\n",
      "train loss:1.6831389911782977\n",
      "train loss:1.7100177357382518\n",
      "train loss:1.6753128775298585\n",
      "train loss:1.2784210852824418\n",
      "train loss:1.4410935802124805\n",
      "train loss:1.4997230570182152\n",
      "train loss:1.4306014000179215\n",
      "train loss:1.7453293101785192\n",
      "train loss:1.2172085168994156\n",
      "train loss:1.8232520029007397\n",
      "train loss:1.6809349035580214\n",
      "train loss:1.1825553600041863\n",
      "train loss:1.7075776514819423\n",
      "train loss:1.5945517569723626\n",
      "train loss:0.8935169495243173\n",
      "train loss:0.785029681516409\n",
      "train loss:0.8622600602099105\n",
      "train loss:2.183823352725523\n",
      "train loss:1.2110783647759524\n",
      "train loss:1.3837481190452467\n",
      "train loss:1.2005404253959973\n",
      "train loss:0.7326801876630644\n",
      "train loss:0.9289660069324108\n",
      "train loss:1.454015219534583\n",
      "train loss:1.8316024971840885\n",
      "train loss:1.868819775595902\n",
      "train loss:1.300546929637213\n",
      "train loss:1.3926748072047457\n",
      "train loss:1.4695554490000469\n",
      "train loss:1.564002239583433\n",
      "train loss:1.2871406962637537\n",
      "train loss:1.2905509451996453\n",
      "train loss:1.1663847530191953\n",
      "train loss:1.5910699305458733\n",
      "train loss:1.66801237613952\n",
      "train loss:1.5066288214634855\n",
      "train loss:1.6177181785200645\n",
      "train loss:1.4544143033930501\n",
      "train loss:1.3068915189946584\n",
      "train loss:1.5152423665948627\n",
      "train loss:1.1688623089356174\n",
      "train loss:1.608007523631087\n",
      "train loss:1.5485229389656348\n",
      "train loss:1.7445270704854572\n",
      "train loss:1.4496759355812552\n",
      "train loss:1.9835361594055159\n",
      "train loss:1.254262848397239\n",
      "train loss:1.4062482508048362\n",
      "train loss:1.2239759801892092\n",
      "train loss:1.7130583449396226\n",
      "train loss:1.0234476960870151\n",
      "train loss:1.4087844943825047\n",
      "train loss:1.5432297363820955\n",
      "train loss:1.7942457908172167\n",
      "train loss:1.1333710741791219\n",
      "train loss:1.1033015950502594\n",
      "train loss:1.462684392719035\n",
      "train loss:1.5006960256697826\n",
      "train loss:1.7655852057986987\n",
      "train loss:1.6851459872624475\n",
      "train loss:1.3628919166911977\n",
      "train loss:1.7831113933437102\n",
      "train loss:1.0691567344205697\n",
      "train loss:1.3787060107416333\n",
      "train loss:1.3614105869934439\n",
      "train loss:2.044626174374078\n",
      "train loss:1.4066315283044877\n",
      "train loss:1.0446343471087558\n",
      "train loss:1.283622825032062\n",
      "train loss:1.2233081965473669\n",
      "train loss:1.0313483428757126\n",
      "train loss:1.4528336566504232\n",
      "train loss:1.193340092585669\n",
      "train loss:1.5787151948083769\n",
      "train loss:1.0815658458782473\n",
      "train loss:1.1187993316082285\n",
      "train loss:1.0932935997998898\n",
      "train loss:1.330685769854833\n",
      "train loss:0.9231323461416572\n",
      "train loss:1.5188822575766963\n",
      "train loss:1.5000847961735215\n",
      "train loss:1.520426838766076\n",
      "train loss:1.3463303302906464\n",
      "train loss:1.2074588730111797\n",
      "train loss:1.6087618729384936\n",
      "train loss:1.24505737903913\n",
      "train loss:1.5548854464908244\n",
      "train loss:1.8110180738185133\n",
      "train loss:1.1394499888162033\n",
      "train loss:2.1203475691014644\n",
      "train loss:1.6011781738381512\n",
      "train loss:1.5116126050725174\n",
      "train loss:1.647692825308825\n",
      "train loss:1.0707976414631915\n",
      "train loss:0.6961012438074\n",
      "train loss:1.9077190850919163\n",
      "train loss:1.113916701495487\n",
      "train loss:1.2602531499997016\n",
      "train loss:1.9339570438556497\n",
      "train loss:1.083977631488311\n",
      "train loss:1.4272429753603533\n",
      "train loss:1.0465837881028481\n",
      "train loss:1.059836747809781\n",
      "train loss:1.642018918782839\n",
      "train loss:1.1065229890122252\n",
      "train loss:1.2676904900578996\n",
      "train loss:1.7240918739808966\n",
      "train loss:1.2456182397040931\n",
      "train loss:1.67454886697669\n",
      "train loss:1.3282826735764373\n",
      "train loss:1.922485195478576\n",
      "train loss:0.9126389049244736\n",
      "train loss:1.3910695362086787\n",
      "train loss:1.4848682465761818\n",
      "train loss:1.4250609235458096\n",
      "train loss:1.434853998616375\n",
      "train loss:1.8653876362844848\n",
      "train loss:1.6717523034135535\n",
      "train loss:1.4670282856640608\n",
      "train loss:1.6948646939586118\n",
      "train loss:1.725206697269559\n",
      "train loss:0.8365566178159132\n",
      "train loss:0.8034484840387435\n",
      "train loss:1.4362501317925866\n",
      "train loss:1.710961952890774\n",
      "train loss:1.437951483579908\n",
      "train loss:1.1367429342697586\n",
      "train loss:0.9911487837190249\n",
      "train loss:1.177648538720935\n",
      "train loss:1.4898378342681045\n",
      "train loss:1.2810692308223937\n",
      "train loss:1.2265935943529562\n",
      "train loss:1.0966920602553778\n",
      "train loss:1.5291919851752376\n",
      "train loss:1.289338941462608\n",
      "train loss:1.6267895369110588\n",
      "train loss:1.6232165533990408\n",
      "train loss:1.3826420376361803\n",
      "train loss:1.7537057284645499\n",
      "train loss:1.614546413576901\n",
      "train loss:1.463256328158965\n",
      "train loss:1.6106316725433067\n",
      "train loss:1.9245894817295188\n",
      "train loss:1.9383690976010093\n",
      "train loss:0.9362548526195852\n",
      "train loss:2.178499960589782\n",
      "train loss:1.0624004740903192\n",
      "train loss:1.6220763543066075\n",
      "train loss:0.6343401581150808\n",
      "train loss:1.4356173559174317\n",
      "train loss:1.7825903962031748\n",
      "train loss:1.1028814558008395\n",
      "train loss:1.378991829722686\n",
      "train loss:0.9180448260760106\n",
      "train loss:1.5029685612310488\n",
      "train loss:1.6034438976810008\n",
      "train loss:1.433684889604552\n",
      "train loss:1.5383791539848486\n",
      "train loss:1.6497971934317435\n",
      "train loss:1.8278490147442157\n",
      "train loss:1.2889464062759621\n",
      "train loss:1.7925996742543315\n",
      "train loss:0.9869502319277486\n",
      "train loss:1.352840331816518\n",
      "train loss:1.442905885278234\n",
      "train loss:1.2096818157482259\n",
      "train loss:1.0628350704769902\n",
      "train loss:1.4834444617495945\n",
      "train loss:1.2542372587835415\n",
      "train loss:1.5603361409851482\n",
      "train loss:1.6077297305877427\n",
      "train loss:1.464805553563553\n",
      "train loss:1.6357982788107157\n",
      "train loss:1.4894845205239666\n",
      "train loss:1.46966800137522\n",
      "train loss:1.3365354970823735\n",
      "train loss:1.2235818749805112\n",
      "train loss:1.3551280620885064\n",
      "train loss:1.1782682486915956\n",
      "train loss:1.3376657479889356\n",
      "train loss:1.35730773540369\n",
      "train loss:1.1936305560227514\n",
      "train loss:1.1454728920556754\n",
      "train loss:1.0539023735914446\n",
      "train loss:1.4777854964517183\n",
      "train loss:1.3316146464833483\n",
      "train loss:1.0631672309848976\n",
      "train loss:1.388847888843423\n",
      "train loss:0.9840868005334347\n",
      "train loss:1.5284711104752216\n",
      "train loss:1.5937265344935638\n",
      "train loss:1.444167611189973\n",
      "train loss:2.058455230403788\n",
      "train loss:1.7027523978354533\n",
      "train loss:1.9218991788881847\n",
      "train loss:0.8695733552731426\n",
      "train loss:1.6671952524453748\n",
      "train loss:1.0711840504743906\n",
      "train loss:0.7018230570270785\n",
      "train loss:1.856155804894157\n",
      "train loss:0.9931905480525014\n",
      "train loss:1.5313296408027273\n",
      "train loss:0.8088849003513005\n",
      "train loss:1.5289363945803225\n",
      "train loss:1.356585172507018\n",
      "train loss:1.4820073862052117\n",
      "train loss:1.466255403205482\n",
      "train loss:1.739226064990111\n",
      "train loss:1.360718437327687\n",
      "train loss:1.4874934709208056\n",
      "train loss:1.07268196714748\n",
      "train loss:1.4790936394011664\n",
      "train loss:1.2545215822448368\n",
      "train loss:1.3619152281465483\n",
      "train loss:1.078631334642358\n",
      "train loss:1.7603244915972212\n",
      "train loss:1.3520525690092065\n",
      "train loss:1.1861195122200914\n",
      "train loss:1.422932252903656\n",
      "train loss:1.1647892795709698\n",
      "train loss:1.3021574706999395\n",
      "train loss:1.2107252163504865\n",
      "train loss:0.7912310867360808\n",
      "train loss:1.5117223345835347\n",
      "train loss:1.2375390343842656\n",
      "train loss:1.4628486254496917\n",
      "train loss:1.2734409673043416\n",
      "train loss:1.2135458709356282\n",
      "train loss:1.196550337443304\n",
      "train loss:1.5778127058969762\n",
      "train loss:1.7662828129552994\n",
      "train loss:1.5068239337333076\n",
      "train loss:1.6725516808332739\n",
      "train loss:1.515207923327916\n",
      "train loss:1.5360938303815386\n",
      "train loss:1.4741378030656063\n",
      "train loss:1.2474471860262004\n",
      "train loss:1.2196173741582965\n",
      "train loss:2.0302417820371703\n",
      "train loss:1.353357371287295\n",
      "train loss:1.2747011407218864\n",
      "train loss:1.0525246587522668\n",
      "train loss:1.238947594369947\n",
      "train loss:1.9399792486677785\n",
      "train loss:1.6338149572520386\n",
      "train loss:1.5808693849926896\n",
      "train loss:1.3159351786628113\n",
      "train loss:1.4302346776171269\n",
      "train loss:1.7669830822258965\n",
      "train loss:1.229610342961956\n",
      "train loss:1.3150082621588428\n",
      "train loss:1.1800011769002747\n",
      "train loss:1.4362101889582384\n",
      "train loss:1.3039725449699753\n",
      "train loss:1.040846752927807\n",
      "train loss:1.4794462592805186\n",
      "train loss:1.7957521897941304\n",
      "train loss:0.8806209457691565\n",
      "train loss:1.488949607424709\n",
      "train loss:0.7755949948644842\n",
      "train loss:1.5246839216779982\n",
      "train loss:1.3392381631206676\n",
      "train loss:1.672933862025993\n",
      "train loss:1.151282274706641\n",
      "train loss:1.2532976755473393\n",
      "train loss:1.3710760856838626\n",
      "train loss:1.5240342112690208\n",
      "train loss:1.4214977040323489\n",
      "train loss:1.2538046480859735\n",
      "train loss:1.163225505793549\n",
      "train loss:1.1686573162846519\n",
      "train loss:1.443457123634442\n",
      "train loss:1.5742196854387769\n",
      "train loss:1.0519853963188195\n",
      "train loss:1.5517599277056044\n",
      "train loss:1.115935797640633\n",
      "train loss:1.119130048275203\n",
      "train loss:1.0091072432461126\n",
      "train loss:1.1833235616567195\n",
      "train loss:1.1058066794830885\n",
      "train loss:1.3316238821846198\n",
      "train loss:1.2477572838779951\n",
      "train loss:1.644959899129196\n",
      "train loss:1.4806362154958852\n",
      "train loss:1.1169791799898658\n",
      "train loss:1.5929868562508076\n",
      "train loss:0.9705534772807172\n",
      "train loss:1.2722791950853662\n",
      "train loss:1.2674715458499364\n",
      "train loss:1.0878197505097977\n",
      "train loss:1.4172638930389325\n",
      "train loss:1.2948473275113672\n",
      "train loss:1.5875970351540167\n",
      "train loss:1.6425623143693737\n",
      "train loss:1.389009613465968\n",
      "train loss:1.2503442060297307\n",
      "train loss:1.677585607117391\n",
      "train loss:1.0647917500864008\n",
      "train loss:1.3492136873850178\n",
      "train loss:1.6906685226357046\n",
      "train loss:1.699246055710462\n",
      "train loss:0.9685909975095954\n",
      "train loss:1.0066705318293194\n",
      "train loss:1.3197378530383213\n",
      "train loss:1.1348587376548351\n",
      "train loss:1.6761070386938115\n",
      "train loss:1.0880997077652654\n",
      "train loss:1.5357985184315615\n",
      "train loss:1.5630960490794898\n",
      "train loss:1.1534630076266637\n",
      "train loss:1.5270185311577933\n",
      "train loss:1.3996959959258504\n",
      "train loss:1.877672171418311\n",
      "train loss:1.38021054467707\n",
      "train loss:1.29548795947442\n",
      "train loss:1.6596841832762927\n",
      "train loss:1.5799274230577898\n",
      "train loss:1.481155518615674\n",
      "train loss:1.3709431530561482\n",
      "train loss:1.333843646192943\n",
      "train loss:1.857285347004872\n",
      "train loss:1.5272804798635513\n",
      "train loss:1.3676465539499065\n",
      "train loss:1.1734167531405026\n",
      "train loss:1.5892533933559752\n",
      "train loss:0.9639450225458879\n",
      "train loss:1.6973280820726948\n",
      "train loss:1.2209235907502527\n",
      "train loss:1.3782154448434203\n",
      "train loss:1.5473529490201092\n",
      "train loss:1.7551365908995078\n",
      "train loss:1.4798356577962208\n",
      "train loss:1.6112154189522534\n",
      "train loss:0.928326759146136\n",
      "train loss:0.9684080866894289\n",
      "train loss:1.5530105376027488\n",
      "train loss:1.5505272615098813\n",
      "train loss:1.7616226710195455\n",
      "train loss:1.780511447478736\n",
      "train loss:1.744142816234668\n",
      "train loss:0.9354916630929928\n",
      "train loss:1.347712310088015\n",
      "train loss:1.4523732649098235\n",
      "train loss:1.7209926946633527\n",
      "train loss:1.1536574262671169\n",
      "train loss:1.2695597208574367\n",
      "train loss:1.3783073030950632\n",
      "train loss:1.0569661810641489\n",
      "train loss:1.4230647505717267\n",
      "train loss:1.5205290333537609\n",
      "train loss:1.258942248297886\n",
      "train loss:0.9270940288168628\n",
      "train loss:1.5072982560379218\n",
      "train loss:0.8704515463287551\n",
      "train loss:1.214611607672706\n",
      "train loss:1.646234091790633\n",
      "train loss:1.2252807779606272\n",
      "train loss:1.7274362629514197\n",
      "train loss:0.9405818966619066\n",
      "train loss:1.6101031529200664\n",
      "train loss:1.4191562195105798\n",
      "train loss:1.3245427002363053\n",
      "train loss:1.4860307579552758\n",
      "train loss:0.9245505331359023\n",
      "train loss:1.1842809634133984\n",
      "train loss:1.019756562565848\n",
      "train loss:1.4522979532791678\n",
      "train loss:1.2792586056521338\n",
      "train loss:1.2839403993550378\n",
      "train loss:1.3382020049977383\n",
      "train loss:1.8533342287505534\n",
      "train loss:1.3502236761096742\n",
      "train loss:1.559534345101786\n",
      "train loss:1.2486356108859047\n",
      "train loss:1.657136264649574\n",
      "train loss:1.236625978722178\n",
      "train loss:1.4197904052924404\n",
      "train loss:1.1573772103753193\n",
      "train loss:1.337665661665722\n",
      "train loss:1.535203228841209\n",
      "train loss:1.115067606452404\n",
      "train loss:1.0016309996985044\n",
      "train loss:1.475070209901403\n",
      "train loss:1.4542674040290406\n",
      "train loss:1.8338902761604636\n",
      "train loss:1.1004696615115375\n",
      "train loss:1.2627389479349913\n",
      "train loss:1.8126339461745054\n",
      "train loss:1.6295863917313569\n",
      "train loss:1.1813985882576148\n",
      "train loss:1.1230363548903584\n",
      "train loss:0.9998171210390343\n",
      "train loss:1.9048303688249617\n",
      "train loss:0.6729387905594708\n",
      "train loss:1.6413062625434882\n",
      "train loss:1.1472702309173897\n",
      "train loss:1.3602556666262704\n",
      "train loss:1.4095191697845988\n",
      "train loss:1.1739017450666158\n",
      "train loss:1.0484845964012846\n",
      "train loss:1.0548230384343753\n",
      "train loss:1.3891155149785877\n",
      "train loss:1.3330091767757326\n",
      "train loss:1.3840847148380295\n",
      "train loss:1.4832384874474092\n",
      "train loss:1.4811148968452676\n",
      "train loss:1.6021884034019547\n",
      "train loss:1.3647506599785617\n",
      "train loss:0.7731663321880535\n",
      "train loss:1.2136759350119601\n",
      "train loss:1.4655426097503976\n",
      "train loss:1.3574693350032863\n",
      "train loss:1.6440803815053513\n",
      "train loss:1.272219266557228\n",
      "train loss:1.375682626679484\n",
      "train loss:1.3711464898362584\n",
      "train loss:1.8691275261645148\n",
      "train loss:1.9532435981210443\n",
      "train loss:1.6631490100515998\n",
      "train loss:0.9938406587938596\n",
      "train loss:1.5524837573054382\n",
      "train loss:1.4575527619030901\n",
      "train loss:1.3656438368110337\n",
      "train loss:1.5084859011706526\n",
      "train loss:1.6015175298092028\n",
      "train loss:1.4368700803617278\n",
      "train loss:1.4048856049645315\n",
      "train loss:1.572837524004113\n",
      "train loss:1.1524109738686734\n",
      "train loss:1.206337319111311\n",
      "train loss:1.4865604472030554\n",
      "train loss:1.4143143434872607\n",
      "train loss:1.2272342769580071\n",
      "train loss:1.6806605841537796\n",
      "train loss:1.6676966263094415\n",
      "train loss:1.521564080350095\n",
      "train loss:1.6238717408588221\n",
      "train loss:1.2440055303640427\n",
      "train loss:1.2242666320184825\n",
      "train loss:1.924139996077581\n",
      "train loss:2.147819160026085\n",
      "train loss:1.0850372071813648\n",
      "train loss:1.8540817933434977\n",
      "train loss:1.0343127707411615\n",
      "train loss:1.455091906187538\n",
      "train loss:0.9976243524455155\n",
      "train loss:1.5214787236840173\n",
      "train loss:1.6565642979610247\n",
      "train loss:1.3122111075096154\n",
      "train loss:1.1923115653971244\n",
      "train loss:1.3369119659172992\n",
      "train loss:1.5847396129753002\n",
      "train loss:1.1748203707027722\n",
      "train loss:1.1418257291927225\n",
      "train loss:1.3392174364228528\n",
      "train loss:1.3618563263632835\n",
      "train loss:1.0907685354379875\n",
      "train loss:0.7376857126186701\n",
      "train loss:1.584244454236314\n",
      "train loss:1.5128440017358074\n",
      "train loss:1.3724408309621603\n",
      "train loss:1.5865460254431505\n",
      "train loss:1.5001524737305798\n",
      "train loss:1.2154433464457646\n",
      "train loss:1.519253218912501\n",
      "train loss:0.9860755671578522\n",
      "train loss:1.7526312855695951\n",
      "train loss:1.6752340518161013\n",
      "train loss:1.3126579863014511\n",
      "train loss:1.8001314036659959\n",
      "train loss:1.2734694793996457\n",
      "train loss:1.7911914187062863\n",
      "train loss:1.7547376065755749\n",
      "train loss:1.3132097316414657\n",
      "train loss:1.4478749357956067\n",
      "train loss:1.2036446971588208\n",
      "train loss:1.3257064215824435\n",
      "train loss:1.1027302962020378\n",
      "train loss:1.090750428416205\n",
      "train loss:1.4778168583853806\n",
      "train loss:0.9112546779959085\n",
      "train loss:1.4810977430300234\n",
      "train loss:1.5212155933328952\n",
      "train loss:1.334320377206829\n",
      "train loss:1.7166357037141005\n",
      "train loss:0.8087869639192826\n",
      "train loss:1.3170225128768958\n",
      "train loss:0.9984558011141533\n",
      "train loss:1.4310425251780137\n",
      "train loss:1.282907427051562\n",
      "train loss:1.0924971066961695\n",
      "train loss:1.2113626031147453\n",
      "train loss:1.4688025194646221\n",
      "train loss:1.306117340522672\n",
      "train loss:1.270236702617043\n",
      "train loss:1.4869775167524923\n",
      "train loss:1.4643638680740616\n",
      "train loss:1.498756720242569\n",
      "train loss:1.381254625316242\n",
      "train loss:1.385467476947587\n",
      "train loss:1.0243939785890415\n",
      "train loss:1.1902139773748697\n",
      "train loss:1.4721041546023481\n",
      "train loss:1.2015854841708857\n",
      "train loss:1.549497721013295\n",
      "train loss:1.645175690199083\n",
      "train loss:1.283256702458725\n",
      "train loss:1.1215513811068853\n",
      "train loss:1.4659464610374404\n",
      "train loss:1.2266788751161448\n",
      "train loss:1.5226566398740942\n",
      "train loss:1.0466079219705642\n",
      "train loss:1.5579631282911552\n",
      "train loss:1.2285664512319252\n",
      "train loss:1.3325552765927589\n",
      "train loss:1.5747770659306317\n",
      "train loss:1.2887196525292486\n",
      "train loss:1.6783918146774828\n",
      "train loss:1.6360139588036067\n",
      "train loss:1.4541288757972575\n",
      "train loss:1.4037554915454873\n",
      "train loss:1.7178913867827053\n",
      "train loss:1.6389066477245133\n",
      "train loss:1.372969965369054\n",
      "train loss:1.5095435378577542\n",
      "train loss:1.1357778599399766\n",
      "train loss:1.5280028044614222\n",
      "train loss:1.2167455078910288\n",
      "train loss:1.2710671793326656\n",
      "train loss:1.2252689319643677\n",
      "train loss:1.5192339127207863\n",
      "train loss:1.4403758593562022\n",
      "train loss:1.2753829535957917\n",
      "train loss:1.088098357401887\n",
      "train loss:1.3683604117088126\n",
      "train loss:1.6076515313639845\n",
      "train loss:1.197023376906363\n",
      "train loss:1.9836405335304927\n",
      "train loss:1.5533713006655137\n",
      "train loss:1.1375949496603872\n",
      "train loss:1.3205270502695863\n",
      "train loss:0.9636865850220279\n",
      "train loss:1.0320030753257994\n",
      "train loss:1.5074034025950116\n",
      "train loss:1.2505020091800938\n",
      "train loss:1.2573386921479826\n",
      "train loss:1.2734851118778256\n",
      "train loss:0.5995825710893453\n",
      "train loss:1.15061120850615\n",
      "train loss:1.269098072741531\n",
      "train loss:1.3704378353670172\n",
      "train loss:1.217538750207523\n",
      "train loss:1.2600363408389739\n",
      "train loss:1.1574586942219098\n",
      "train loss:0.8087140875163901\n",
      "train loss:1.164830650537879\n",
      "train loss:1.2773887893786635\n",
      "train loss:1.3970733533810462\n",
      "train loss:1.6301686552895216\n",
      "train loss:1.3829284724881734\n",
      "train loss:1.380409905753823\n",
      "train loss:1.021979230025074\n",
      "train loss:1.4341581318127778\n",
      "train loss:1.3419069801106036\n",
      "train loss:1.6461638643852858\n",
      "train loss:1.2767616614502215\n",
      "train loss:1.2378966290047821\n",
      "train loss:0.7023183136510205\n",
      "train loss:1.3064645616227173\n",
      "train loss:1.7578575230094082\n",
      "train loss:1.5829631967117437\n",
      "train loss:1.3906053362706152\n",
      "train loss:0.8226262885558736\n",
      "train loss:1.1135567896281593\n",
      "train loss:1.4150238324514384\n",
      "train loss:1.597399463610709\n",
      "train loss:0.5852931473629195\n",
      "train loss:1.5874708401928208\n",
      "train loss:1.7336236957838858\n",
      "train loss:1.634885473303299\n",
      "train loss:1.1452522025559329\n",
      "train loss:1.3405216152957589\n",
      "train loss:1.482962689775246\n",
      "train loss:1.4998776692347353\n",
      "train loss:1.2713499265628578\n",
      "train loss:1.729839135412833\n",
      "train loss:1.612335965345465\n",
      "train loss:1.1657482072763998\n",
      "train loss:1.2240342439196714\n",
      "train loss:1.3569732058688053\n",
      "train loss:1.619787383355042\n",
      "train loss:0.8275204501051192\n",
      "train loss:1.7909305953847174\n",
      "train loss:1.5544642271409488\n",
      "train loss:1.1170970974465195\n",
      "train loss:1.2988190900098664\n",
      "train loss:1.2293904691877264\n",
      "train loss:1.4867935851907643\n",
      "train loss:1.111411544857011\n",
      "train loss:1.7147565203788013\n",
      "train loss:1.380426092993437\n",
      "train loss:1.7906915174367957\n",
      "train loss:1.5015893208309354\n",
      "train loss:1.521627089971042\n",
      "train loss:1.1492893248245726\n",
      "train loss:1.3030478273216972\n",
      "train loss:1.3458891167048237\n",
      "train loss:1.3457781976449943\n",
      "train loss:1.5041401410269235\n",
      "train loss:1.4879190831589935\n",
      "train loss:1.3196910022748254\n",
      "train loss:1.1213523161792844\n",
      "train loss:1.2375464130781362\n",
      "train loss:0.6866242100192989\n",
      "train loss:1.8555763810748942\n",
      "train loss:0.6805889791455896\n",
      "train loss:1.8603112387926715\n",
      "train loss:1.3521408054383524\n",
      "train loss:1.5322624847154198\n",
      "train loss:1.4441470114005188\n",
      "train loss:1.1518707749737989\n",
      "train loss:1.3535510112644253\n",
      "train loss:1.4621058608222535\n",
      "train loss:1.4037669639054016\n",
      "train loss:1.5483473853396101\n",
      "train loss:1.443612078056691\n",
      "train loss:1.5821493303651115\n",
      "train loss:1.1007305695951808\n",
      "train loss:1.3439185409715733\n",
      "train loss:1.7666265397051195\n",
      "train loss:1.0563563299431116\n",
      "train loss:1.4664339681230647\n",
      "train loss:1.290282305875894\n",
      "train loss:1.566912825511852\n",
      "train loss:1.4423162904485098\n",
      "train loss:0.8996481490810178\n",
      "train loss:1.437262471040058\n",
      "train loss:1.059499817106489\n",
      "train loss:1.1215893603637608\n",
      "train loss:1.5571088379677376\n",
      "train loss:1.2629877272878\n",
      "train loss:1.1775411565966076\n",
      "train loss:1.2835030939361167\n",
      "train loss:1.5649078000047716\n",
      "train loss:1.2761586700018035\n",
      "train loss:1.6531980565314441\n",
      "train loss:1.5501003996699434\n",
      "train loss:1.4329985720495233\n",
      "train loss:1.2428405707625305\n",
      "train loss:1.1248842402588628\n",
      "train loss:1.708055121022434\n",
      "train loss:0.9013848933722887\n",
      "train loss:1.5725780739742354\n",
      "train loss:1.6228037756981515\n",
      "train loss:1.5684205865580456\n",
      "train loss:1.241953068263714\n",
      "train loss:1.2906100266483644\n",
      "train loss:1.4255144676536418\n",
      "train loss:1.442901982276688\n",
      "train loss:1.5838209777638688\n",
      "train loss:1.2657173925129088\n",
      "train loss:0.9185008113191684\n",
      "train loss:1.2138939719745825\n",
      "train loss:1.6537035751439153\n",
      "train loss:1.3193763856266736\n",
      "train loss:1.3128523227782805\n",
      "train loss:0.7681544278367112\n",
      "train loss:1.5248105245862218\n",
      "train loss:0.8428901648589537\n",
      "train loss:1.59694844933512\n",
      "train loss:1.2035523571142672\n",
      "train loss:1.3099463189277836\n",
      "train loss:1.4957028243638368\n",
      "train loss:1.2749880512694425\n",
      "train loss:1.1190341199150378\n",
      "train loss:1.3786235455480431\n",
      "train loss:1.5703072249434904\n",
      "train loss:1.3336172798942985\n",
      "train loss:1.5388579412030747\n",
      "train loss:1.1638843852613707\n",
      "train loss:0.9233400166165133\n",
      "train loss:1.8732317747079605\n",
      "train loss:1.1754803485524261\n",
      "train loss:2.098175080780437\n",
      "train loss:1.2461326359046154\n",
      "train loss:1.7123097855804421\n",
      "train loss:1.0003610239591858\n",
      "train loss:1.3759882699637953\n",
      "train loss:1.1865995672330365\n",
      "train loss:1.0429577180352454\n",
      "train loss:1.3144457749705192\n",
      "train loss:1.0392211356016567\n",
      "train loss:1.2133887140284156\n",
      "train loss:1.200123871764524\n",
      "train loss:0.9570501502131229\n",
      "train loss:1.3441866046576678\n",
      "train loss:1.4350027593841528\n",
      "train loss:1.2344343630568049\n",
      "train loss:1.4571963683253257\n",
      "train loss:1.26519453452279\n",
      "train loss:1.342091655448339\n",
      "train loss:1.9327019501394247\n",
      "train loss:1.1706602105743398\n",
      "train loss:1.5832517612261197\n",
      "train loss:0.36128990741654354\n",
      "train loss:1.7281109902689848\n",
      "train loss:1.8799757080465038\n",
      "train loss:1.589439535520945\n",
      "train loss:1.4340340678048094\n",
      "train loss:0.6075160738017106\n",
      "train loss:1.375166358388459\n",
      "train loss:1.5800117008776193\n",
      "train loss:1.2435309291504602\n",
      "train loss:0.873208046560338\n",
      "train loss:1.507558731220041\n",
      "train loss:1.463793087336095\n",
      "train loss:1.2761089717218117\n",
      "train loss:1.5173701458747115\n",
      "train loss:1.4839353981137566\n",
      "train loss:1.9463659041739176\n",
      "train loss:1.0593247994513384\n",
      "train loss:1.6691933856663959\n",
      "train loss:1.2267951133937636\n",
      "train loss:1.9634753291747302\n",
      "train loss:1.717024666645474\n",
      "train loss:1.8944244829596009\n",
      "train loss:1.8077521088903161\n",
      "train loss:1.4173520095134644\n",
      "train loss:1.3606165815838707\n",
      "train loss:1.3214731942964768\n",
      "train loss:1.4142010137234986\n",
      "train loss:1.3957840742959804\n",
      "train loss:1.247938990533517\n",
      "train loss:1.6842488938858153\n",
      "train loss:1.6770128946499312\n",
      "train loss:1.7942670091171835\n",
      "train loss:0.9422491414188615\n",
      "train loss:1.709988557914419\n",
      "train loss:1.1210329726925483\n",
      "train loss:1.5250732877176807\n",
      "train loss:1.2290713404416818\n",
      "train loss:2.1183122656265914\n",
      "train loss:1.5483310547999978\n",
      "train loss:1.7395129252615669\n",
      "train loss:1.5066915338657723\n",
      "train loss:1.9770744920899133\n",
      "train loss:0.796996514735744\n",
      "train loss:1.1590134176112108\n",
      "train loss:1.1265760588145723\n",
      "train loss:1.2396393095618596\n",
      "train loss:1.0331084220822795\n",
      "train loss:1.5215629516413078\n",
      "train loss:1.6404708041151246\n",
      "train loss:1.5901369576083693\n",
      "train loss:1.7351524312130198\n",
      "train loss:1.556175631978329\n",
      "train loss:1.46218745493131\n",
      "train loss:1.4374420913547472\n",
      "train loss:1.912792988187428\n",
      "train loss:1.6832430309438267\n",
      "train loss:1.620445173723232\n",
      "train loss:1.3073042161848918\n",
      "train loss:0.563974602278672\n",
      "train loss:0.943899254747838\n",
      "train loss:1.9568626005799932\n",
      "train loss:1.7284335268226787\n",
      "train loss:1.2074778699271385\n",
      "train loss:1.3885481453547879\n",
      "train loss:1.3669655003816599\n",
      "train loss:1.3259850534942834\n",
      "train loss:1.8516738960103076\n",
      "train loss:1.5088220741941571\n",
      "train loss:1.7264088103932305\n",
      "train loss:1.4169946534894635\n",
      "train loss:1.766166758002728\n",
      "train loss:0.9934049254466645\n",
      "train loss:1.4236991071927965\n",
      "train loss:1.8476971618317581\n",
      "train loss:1.7675578934189573\n",
      "train loss:1.3293494424928118\n",
      "train loss:1.3928815038536446\n",
      "train loss:1.3582722845169477\n",
      "train loss:1.1676224091224794\n",
      "train loss:1.3735554174426858\n",
      "train loss:1.4994182952144102\n",
      "train loss:1.7909129653984344\n",
      "train loss:1.5326518765445354\n",
      "train loss:0.6046513791352854\n",
      "train loss:1.3244261973331874\n",
      "train loss:1.0906586907634621\n",
      "train loss:1.6212675798646274\n",
      "train loss:1.9808151272925527\n",
      "train loss:1.3875483937323694\n",
      "train loss:1.2958862817299517\n",
      "train loss:1.2339426345447377\n",
      "train loss:1.7567849623906127\n",
      "train loss:1.6255149876672703\n",
      "train loss:1.475516939959963\n",
      "train loss:1.072734533791092\n",
      "train loss:1.1117654878380705\n",
      "train loss:1.3921562657934994\n",
      "train loss:1.4883494795225538\n",
      "train loss:1.4522093128795701\n",
      "train loss:1.616454181303784\n",
      "train loss:1.422240876290544\n",
      "train loss:1.723077115912161\n",
      "train loss:1.0803302830058568\n",
      "train loss:0.7461343803773147\n",
      "train loss:1.2013945841049947\n",
      "train loss:1.2110632258672185\n",
      "train loss:1.5132032018481667\n",
      "train loss:1.6311301830021008\n",
      "train loss:1.4855076778076877\n",
      "train loss:1.7087932341816388\n",
      "train loss:1.4714759111604885\n",
      "train loss:1.549219712831245\n",
      "train loss:1.1408672075111064\n",
      "train loss:1.1394851491062388\n",
      "train loss:0.8707519311079448\n",
      "train loss:1.0559094423039284\n",
      "train loss:1.2795190729577424\n",
      "train loss:1.1286320845973377\n",
      "train loss:1.5045212921770363\n",
      "train loss:1.673133247114399\n",
      "train loss:1.5494555566374166\n",
      "train loss:1.4141384218014657\n",
      "train loss:1.2390630236443845\n",
      "train loss:1.8363069989787633\n",
      "train loss:2.042554827170351\n",
      "train loss:1.2777328823722283\n",
      "train loss:1.085512693238425\n",
      "train loss:1.5133720596748998\n",
      "train loss:1.333705185198323\n",
      "train loss:1.2410252591648006\n",
      "train loss:1.3575495005305\n",
      "train loss:1.2627452715963656\n",
      "train loss:1.0118073762159179\n",
      "train loss:0.8576947048235779\n",
      "train loss:1.3124003859915576\n",
      "train loss:1.4475668554969796\n",
      "train loss:1.397644705316372\n",
      "train loss:1.8262991655632566\n",
      "train loss:1.488815447207947\n",
      "train loss:1.8979615745648435\n",
      "train loss:1.3566724545859647\n",
      "train loss:1.9375159351205795\n",
      "train loss:1.199517677928174\n",
      "train loss:1.8223398943118916\n",
      "train loss:1.2820936862159145\n",
      "train loss:1.5158263001994456\n",
      "train loss:1.3622302493031808\n",
      "train loss:1.6279118777111308\n",
      "train loss:1.364931730342074\n",
      "train loss:1.86908040437508\n",
      "train loss:1.8275183942064182\n",
      "train loss:1.783520776249405\n",
      "train loss:1.4828323376224524\n",
      "train loss:1.605931355327895\n",
      "train loss:1.0368027021633073\n",
      "train loss:1.471614139967884\n",
      "train loss:1.5033196009324763\n",
      "train loss:1.6783639597014055\n",
      "train loss:1.598207627064542\n",
      "train loss:1.0573503405005922\n",
      "train loss:1.1434175455705784\n",
      "train loss:0.9333146728820321\n",
      "train loss:1.216518834864822\n",
      "train loss:1.266601967556746\n",
      "train loss:1.2390220612290457\n",
      "train loss:1.498465226806575\n",
      "train loss:1.7865236090530678\n",
      "train loss:1.3011452943300479\n",
      "train loss:1.4326715497795686\n",
      "train loss:1.115756141482478\n",
      "train loss:0.8928970356699628\n",
      "train loss:1.7081365112568503\n",
      "train loss:1.191949106714273\n",
      "train loss:1.3515243510821344\n",
      "train loss:1.4466125358136144\n",
      "train loss:1.3322176793442326\n",
      "train loss:1.2548791601862264\n",
      "train loss:1.3229472833991531\n",
      "train loss:1.4525128738198565\n",
      "train loss:1.79251162783839\n",
      "train loss:0.650850924099424\n",
      "train loss:1.9074607230433795\n",
      "train loss:1.168570484270486\n",
      "train loss:1.3082505086134009\n",
      "train loss:0.9428766869191705\n",
      "train loss:1.6012057404452882\n",
      "train loss:1.3512940213126146\n",
      "train loss:1.43228397986348\n",
      "train loss:1.3373192901646316\n",
      "train loss:0.9254780960854502\n",
      "train loss:0.9187903246287936\n",
      "train loss:1.2498827018486645\n",
      "train loss:1.1851607319332689\n",
      "train loss:1.4889931690154279\n",
      "train loss:1.2598632776439014\n",
      "train loss:1.377157253613912\n",
      "train loss:1.4596775987516224\n",
      "train loss:2.010599132311259\n",
      "train loss:1.0409289791573697\n",
      "train loss:1.3976380711056717\n",
      "train loss:1.5171510214593402\n",
      "train loss:1.7183287382554906\n",
      "train loss:1.6175297138626719\n",
      "train loss:1.606299074203715\n",
      "train loss:1.3464523619809114\n",
      "train loss:1.279607613403995\n",
      "train loss:1.0668053417077303\n",
      "train loss:0.9782075055298847\n",
      "train loss:1.609955007188167\n",
      "train loss:1.1958644346992875\n",
      "train loss:1.0731942405408472\n",
      "train loss:1.4241642222178645\n",
      "train loss:1.73481506866639\n",
      "train loss:1.17934510796573\n",
      "train loss:1.13997436473529\n",
      "train loss:1.6707431353608873\n",
      "train loss:1.0557531242974802\n",
      "train loss:1.072659997715186\n",
      "train loss:1.1667051591155717\n",
      "train loss:1.1294140318065284\n",
      "train loss:1.1972844386658383\n",
      "train loss:1.250331519700852\n",
      "train loss:1.4038548326348255\n",
      "train loss:1.3855663522873338\n",
      "train loss:1.0916814132567154\n",
      "train loss:1.5274168283233054\n",
      "train loss:1.7632171586817589\n",
      "train loss:1.1435249680589412\n",
      "train loss:1.417817429922581\n",
      "train loss:1.6623594447878105\n",
      "train loss:1.3735197130925603\n",
      "train loss:1.4795756457265188\n",
      "train loss:1.0815359014288473\n",
      "train loss:1.639088390839524\n",
      "train loss:1.5940781743035566\n",
      "train loss:0.8519542633528456\n",
      "train loss:0.8556685872658572\n",
      "train loss:1.3095602708931557\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6617647058823529\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 20\n",
    "\n",
    "network = DeepConvNet()\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09821b09-9686-43ca-baed-610188b60390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
