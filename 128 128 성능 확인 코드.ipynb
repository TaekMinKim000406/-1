{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182ecc48-3d27-49f7-af46-f86d636d39da",
   "metadata": {},
   "source": [
    "픽셀을 128 x 128일 때 성능을 검증하기 위한 코드입니다.\n",
    "원래는 정의해 놓은 함수를 불러와서 사용해야 했으나, 검증 과정에서 일부 함수를 직접 코드를 복사해와서 테스트 한 코드도 많습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d5ff52-e3eb-4b92-94e7-9e5a8e64f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(current_dir)\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "image_folder_path = './G1020/Images/'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "774e56f4-c59f-45fb-9959-cfd55d24f058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Hue images shape: (1020, 1, 28, 28)\n",
      "Labels shape: (1020,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('G1020.csv')\n",
    "image_files = df['imageID'].tolist()  # 이미지 파일 이름이 있는 열 이름\n",
    "labels = df['binaryLabels'].values  # 레이블이 있는 열 이름\n",
    "\n",
    "# 이미지 크기 설정\n",
    "target_size = (28, 28)\n",
    "\n",
    "# cv2로 이미지 읽고 전처리\n",
    "def load_and_extract_hue(image_path, target_size):\n",
    "    image = cv2.imread(image_path)  # 이미지 읽기 (기본 BGR 형식)\n",
    "    if image is None:\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return None\n",
    "    image = cv2.resize(image, target_size)  # 이미지 크기 조정\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)  # BGR을 HSV로 변환\n",
    "    hue_channel = hsv_image[:, :, 0]  # Hue 채널만 추출\n",
    "    hue_channel = hue_channel / 180.0  # Hue 값 정규화 (0~1 범위로 스케일링, Hue 범위는 0-179)\n",
    "    return hue_channel\n",
    "\n",
    "# 모든 이미지를 불러와서 리스트에 저장\n",
    "images = [load_and_extract_hue(image_folder_path + img_path, target_size) for img_path in image_files]\n",
    "images = np.array([img for img in images if img is not None])  # None 값 제거\n",
    "\n",
    "# 차원 추가하여 (1020, 1, 128, 128) 형태로 변환\n",
    "images = images[:, np.newaxis, :, :]\n",
    "\n",
    "print(f\"Processed Hue images shape: {images.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a619268-97b7-4407-850d-c494df6eaf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299114715745361\n",
      "=== epoch:1, train acc:0.3, test acc:0.35 ===\n",
      "train loss:2.2952759389177113\n",
      "train loss:2.2872113767696556\n",
      "train loss:2.2757070889892246\n",
      "train loss:2.256803138366368\n",
      "train loss:2.224379310110783\n",
      "train loss:2.186794958498419\n",
      "train loss:2.1343941470789174\n",
      "train loss:2.055350904231305\n",
      "train loss:1.9875604510778544\n",
      "train loss:1.8664347012911144\n",
      "train loss:1.7323561006615662\n",
      "train loss:1.5814499769928\n",
      "train loss:1.4086798400710738\n",
      "train loss:1.2455491701441679\n",
      "train loss:1.0427143720833054\n",
      "train loss:0.9450021261133363\n",
      "train loss:0.6965478797147304\n",
      "train loss:0.7101846967663834\n",
      "train loss:0.6755809507796537\n",
      "train loss:0.6651283876848142\n",
      "train loss:0.513361261729715\n",
      "train loss:0.6655281194037348\n",
      "train loss:0.8487836640284534\n",
      "train loss:0.830360665558435\n",
      "train loss:0.32733835191957367\n",
      "train loss:0.3313824765267569\n",
      "train loss:0.6869040825618504\n",
      "train loss:0.8968981181445157\n",
      "train loss:0.3612209699890836\n",
      "train loss:0.6285578790533477\n",
      "train loss:0.5304414998472097\n",
      "train loss:0.6272014077527335\n",
      "train loss:0.4506668575975743\n",
      "train loss:0.5407351594881011\n",
      "train loss:0.6161653296055066\n",
      "train loss:0.41853949420613584\n",
      "train loss:0.6007283134007793\n",
      "train loss:0.6264297616613259\n",
      "train loss:0.3794727571672545\n",
      "train loss:0.6423931554232187\n",
      "train loss:0.8796306145183387\n",
      "train loss:0.5120914205192498\n",
      "train loss:0.5320721733028055\n",
      "train loss:0.7007531916160099\n",
      "train loss:0.612255202467804\n",
      "train loss:0.63042214911479\n",
      "train loss:0.4729749814653589\n",
      "train loss:0.6187415570037192\n",
      "train loss:0.542884685446323\n",
      "train loss:0.7827144725904096\n",
      "train loss:0.5115237389144404\n",
      "train loss:0.6869477673068843\n",
      "train loss:0.4315409479899429\n",
      "train loss:0.5288672294798379\n",
      "train loss:0.6536043545261337\n",
      "train loss:0.5374690055369576\n",
      "train loss:0.5040206247044677\n",
      "train loss:0.6527115134890862\n",
      "train loss:0.4812462299551143\n",
      "train loss:0.4983974589941328\n",
      "train loss:0.7448674127761933\n",
      "train loss:0.6143782106770561\n",
      "train loss:0.24827633007230204\n",
      "train loss:0.49689127875729644\n",
      "train loss:0.6309025960805335\n",
      "train loss:0.6221228955856756\n",
      "train loss:0.6404013098545259\n",
      "train loss:0.5967751480669037\n",
      "train loss:0.6011962204462559\n",
      "train loss:0.5316778121470558\n",
      "train loss:0.5363314484728339\n",
      "train loss:0.5176682966955048\n",
      "train loss:0.5264345012321485\n",
      "train loss:0.5844041401301088\n",
      "train loss:0.6100981273221056\n",
      "train loss:0.6087626634003882\n",
      "train loss:0.8288986407178982\n",
      "train loss:0.6132276705616404\n",
      "train loss:0.5229667232506096\n",
      "train loss:0.7588370633701231\n",
      "train loss:0.5530776973264077\n",
      "train loss:0.6727001366718655\n",
      "train loss:0.5765811544574104\n",
      "train loss:0.7577925808911565\n",
      "train loss:0.5926098869285423\n",
      "train loss:0.588324651379081\n",
      "train loss:0.5387487054221702\n",
      "train loss:0.5605412820676741\n",
      "train loss:0.5927609836336929\n",
      "train loss:0.7181188306664655\n",
      "train loss:0.6346492333523228\n",
      "train loss:0.3571029282019365\n",
      "train loss:0.8473889874579859\n",
      "train loss:0.49855448056256313\n",
      "train loss:0.6271870568258032\n",
      "train loss:0.7079473314520686\n",
      "train loss:0.260134379270037\n",
      "train loss:0.8756991010853902\n",
      "train loss:0.624045500392874\n",
      "train loss:0.43231460887054707\n",
      "train loss:0.5135094463871305\n",
      "train loss:0.753349768338826\n",
      "train loss:0.6119159399133557\n",
      "train loss:0.5324811839395844\n",
      "train loss:0.6122609773632501\n",
      "train loss:0.6943360104194666\n",
      "train loss:0.3674126739124486\n",
      "train loss:0.7668657614895504\n",
      "train loss:0.41914633042857447\n",
      "train loss:0.5890344181270006\n",
      "train loss:0.705039317493463\n",
      "train loss:0.6445224238613501\n",
      "train loss:0.7245982777707384\n",
      "train loss:0.6950238513624469\n",
      "train loss:0.33875657244912794\n",
      "train loss:0.7328032228535074\n",
      "train loss:0.7132766812092205\n",
      "train loss:0.5224268795402522\n",
      "train loss:0.44481362660915486\n",
      "train loss:0.5461445966633864\n",
      "train loss:0.7107901829575408\n",
      "train loss:0.8839589764030578\n",
      "train loss:0.6881668250034835\n",
      "train loss:0.7369923137258674\n",
      "train loss:0.5280539281832253\n",
      "train loss:0.620931432023937\n",
      "train loss:0.6569361685917471\n",
      "train loss:0.577002874234146\n",
      "train loss:0.4835314032770671\n",
      "train loss:0.532059452905768\n",
      "train loss:0.523550367203812\n",
      "train loss:0.7337788684482223\n",
      "train loss:0.5951539648574702\n",
      "train loss:0.5300699156043716\n",
      "train loss:0.3186658177694768\n",
      "train loss:0.31294057986886814\n",
      "train loss:0.4868280157199205\n",
      "train loss:1.0401581022556072\n",
      "train loss:0.5243971363960409\n",
      "train loss:0.5050601035580804\n",
      "train loss:0.955142494173008\n",
      "train loss:0.8702973294583511\n",
      "train loss:0.6128999336736594\n",
      "train loss:0.5220379830389915\n",
      "train loss:0.6221053018445438\n",
      "train loss:0.6246366610475154\n",
      "train loss:0.719421005827105\n",
      "train loss:0.6408332350983205\n",
      "train loss:0.6738897404198816\n",
      "train loss:0.6341492264420056\n",
      "train loss:0.6377137693679666\n",
      "train loss:0.6132583355764677\n",
      "train loss:0.5895297217087999\n",
      "train loss:0.4905110676586988\n",
      "train loss:0.6065021282170349\n",
      "train loss:0.6431848484578531\n",
      "train loss:0.8996025677701753\n",
      "train loss:0.7117390842665607\n",
      "train loss:0.5988982742984427\n",
      "train loss:0.6012808929855716\n",
      "train loss:0.40130424722735053\n",
      "train loss:0.5190368712930832\n",
      "train loss:0.6018513871792287\n",
      "train loss:0.49086947375610884\n",
      "train loss:0.5082750532943683\n",
      "train loss:1.0200962244655933\n",
      "train loss:0.3678064482647508\n",
      "train loss:0.7349237543871314\n",
      "train loss:0.4963330904228787\n",
      "train loss:0.6648366759456021\n",
      "train loss:0.620517251999601\n",
      "train loss:0.5166720571771213\n",
      "train loss:0.6179433033891317\n",
      "train loss:0.51696836130563\n",
      "train loss:0.7923867455085911\n",
      "train loss:0.5896151367723842\n",
      "train loss:0.44813351362748344\n",
      "train loss:0.43714408707751956\n",
      "train loss:0.641393197165936\n",
      "train loss:0.6033308395442669\n",
      "train loss:0.7063060310718805\n",
      "train loss:0.6036562982571991\n",
      "train loss:0.6187417017973112\n",
      "train loss:0.6212505821321446\n",
      "train loss:0.6335605612307038\n",
      "train loss:0.4305850682977203\n",
      "train loss:0.5327353347790776\n",
      "train loss:0.6992008828109829\n",
      "train loss:0.32083743100421225\n",
      "train loss:0.6054121406957761\n",
      "train loss:0.6276244402333677\n",
      "train loss:0.596391005054452\n",
      "train loss:0.6353420545641805\n",
      "train loss:0.95871129502919\n",
      "train loss:0.5269913047003003\n",
      "train loss:0.6935009021336762\n",
      "train loss:0.5292511030172831\n",
      "train loss:0.4570528040550467\n",
      "train loss:0.6854488829327283\n",
      "train loss:0.5290735530420132\n",
      "train loss:0.6921107895746844\n",
      "train loss:0.6755124806978534\n",
      "train loss:0.4468476447550372\n",
      "train loss:0.7669643880644592\n",
      "train loss:0.36202986865413944\n",
      "train loss:0.3382581783737437\n",
      "train loss:0.5150267731433702\n",
      "train loss:0.5139874226634414\n",
      "train loss:0.47441008675365354\n",
      "train loss:0.5206848396698012\n",
      "train loss:0.7485266745353701\n",
      "train loss:0.49703967426249\n",
      "train loss:0.8097800055456474\n",
      "train loss:0.31028245749380645\n",
      "train loss:0.4913442477118089\n",
      "train loss:0.2065286057148254\n",
      "train loss:0.7747597293662276\n",
      "train loss:0.4636558399123527\n",
      "train loss:0.6530154981871517\n",
      "train loss:0.47691856231807045\n",
      "train loss:0.6125722651537757\n",
      "train loss:0.5020189469237069\n",
      "train loss:0.7395531441050968\n",
      "train loss:0.3464188812830611\n",
      "train loss:0.8267494059645017\n",
      "train loss:0.825571250147491\n",
      "train loss:0.5198483870639119\n",
      "train loss:0.5574224799670907\n",
      "train loss:0.6868111273379032\n",
      "train loss:0.7611909826200196\n",
      "train loss:0.6708741382896053\n",
      "train loss:0.6247744994056714\n",
      "train loss:0.6717301954481258\n",
      "train loss:0.6467264516623489\n",
      "train loss:0.6452141562247514\n",
      "train loss:0.6784213255343852\n",
      "train loss:0.7051939184280809\n",
      "train loss:0.7203088909490085\n",
      "train loss:0.6634691334366911\n",
      "train loss:0.6425765343377192\n",
      "train loss:0.634406160961273\n",
      "train loss:0.6778573858938216\n",
      "train loss:0.6478713366207524\n",
      "train loss:0.6464218030248133\n",
      "train loss:0.6751641410263413\n",
      "train loss:0.5734488996836917\n",
      "train loss:0.6115762743896\n",
      "train loss:0.47514055662754195\n",
      "train loss:0.46227700150759876\n",
      "train loss:0.6178051811159713\n",
      "train loss:0.6295026699188557\n",
      "train loss:0.7711020272150841\n",
      "train loss:0.8375795446059362\n",
      "train loss:0.40640362205092967\n",
      "train loss:0.5933971947688206\n",
      "train loss:0.36971809694560626\n",
      "train loss:0.7865178117800087\n",
      "train loss:0.7508273535970217\n",
      "train loss:0.39707212133332404\n",
      "train loss:0.6539535039114576\n",
      "train loss:0.6244006412821648\n",
      "train loss:0.394753114660481\n",
      "train loss:0.3911517458990849\n",
      "train loss:0.6231128471874136\n",
      "train loss:0.40016650971407375\n",
      "train loss:0.7186596136962813\n",
      "train loss:0.41585927972519804\n",
      "train loss:0.379067615665518\n",
      "train loss:0.6422127508562779\n",
      "train loss:0.7818667999947876\n",
      "train loss:0.37460778400130984\n",
      "train loss:0.49092319383796684\n",
      "train loss:0.6492694358001035\n",
      "train loss:0.3702057682229012\n",
      "train loss:0.239938947622427\n",
      "train loss:0.8153803829536436\n",
      "train loss:0.7444902680197005\n",
      "train loss:0.47759681747302773\n",
      "train loss:0.4730230397531662\n",
      "train loss:0.24381473187553207\n",
      "train loss:0.8897080096429407\n",
      "train loss:0.49120327591701407\n",
      "train loss:0.6085776278476647\n",
      "train loss:0.8194215318406541\n",
      "train loss:0.40440263004981974\n",
      "train loss:0.8246670611742462\n",
      "train loss:0.6143731380993784\n",
      "train loss:0.42749814139111486\n",
      "train loss:0.7619869037697384\n",
      "train loss:0.5395655481834494\n",
      "train loss:0.5293855802865949\n",
      "train loss:0.5852716369034958\n",
      "train loss:0.621325828210632\n",
      "train loss:0.47520755878907395\n",
      "train loss:0.45485269404367745\n",
      "train loss:0.6205208854579453\n",
      "train loss:0.622557863805743\n",
      "train loss:0.7154376516553356\n",
      "train loss:0.6916810337422629\n",
      "train loss:0.6945529129756081\n",
      "train loss:0.5971156030870317\n",
      "train loss:0.33702085736050297\n",
      "train loss:0.6059104638694959\n",
      "train loss:0.774515259392833\n",
      "train loss:0.4158072995275554\n",
      "train loss:0.7279127896976426\n",
      "train loss:0.6174925098810974\n",
      "train loss:0.4149108873138226\n",
      "train loss:0.5111320624305729\n",
      "train loss:0.598792657945366\n",
      "train loss:0.5118820188212296\n",
      "train loss:0.9061152113499912\n",
      "train loss:0.5040805181787443\n",
      "train loss:0.7972963060120263\n",
      "train loss:0.6256051955091898\n",
      "train loss:0.7697602415300759\n",
      "train loss:0.4487488902933771\n",
      "train loss:0.5996897939966782\n",
      "train loss:0.6215954370529209\n",
      "train loss:0.4714777298038066\n",
      "train loss:0.6004297941439523\n",
      "train loss:0.6876336247045812\n",
      "train loss:0.5207357428327902\n",
      "train loss:0.697032207715853\n",
      "train loss:0.6146565073258111\n",
      "train loss:0.6750995071941334\n",
      "train loss:0.7029419926121323\n",
      "train loss:0.7102242673731595\n",
      "train loss:0.616270239998428\n",
      "train loss:0.6083306586701772\n",
      "train loss:0.5617180921964581\n",
      "train loss:0.3789525958231203\n",
      "train loss:0.5961371130804466\n",
      "train loss:0.616879832463332\n",
      "train loss:0.6306579142966142\n",
      "train loss:0.621411805172863\n",
      "train loss:0.6254920249530423\n",
      "train loss:0.6090407755836109\n",
      "train loss:0.5931834257758034\n",
      "train loss:0.41206635335529923\n",
      "train loss:0.6989069089049135\n",
      "train loss:0.619515357056182\n",
      "train loss:0.6312970599194326\n",
      "train loss:0.3831161037112959\n",
      "train loss:0.3594645666042626\n",
      "train loss:0.6268384441660955\n",
      "train loss:0.9809764173656864\n",
      "train loss:0.9541369350516764\n",
      "train loss:0.40606937612679717\n",
      "train loss:0.3186889404687734\n",
      "train loss:0.8730065725646714\n",
      "train loss:0.7729614117216205\n",
      "train loss:0.5633551779136089\n",
      "train loss:0.795893608711364\n",
      "train loss:0.5784341243037661\n",
      "train loss:0.620062738594308\n",
      "train loss:0.5349643305181093\n",
      "train loss:0.5748567092998063\n",
      "train loss:0.6744114808871422\n",
      "train loss:0.6147674942700118\n",
      "train loss:0.49714669351392365\n",
      "train loss:0.6094610030646341\n",
      "train loss:0.6862211745087905\n",
      "train loss:0.7050008847891338\n",
      "train loss:0.6040670639866395\n",
      "train loss:0.5283266465566928\n",
      "train loss:0.41239585307093385\n",
      "train loss:0.6403589083376744\n",
      "train loss:0.6499325922445106\n",
      "train loss:0.6552531324263015\n",
      "train loss:0.6338538266972715\n",
      "train loss:0.6116718594336613\n",
      "train loss:0.9180945812283223\n",
      "train loss:0.7150261344543534\n",
      "train loss:0.5166360225875729\n",
      "train loss:0.6023097985164345\n",
      "train loss:0.4311601379202835\n",
      "train loss:0.5306854770159569\n",
      "train loss:0.6795423194724978\n",
      "train loss:0.5316000008532547\n",
      "train loss:0.5303178119589427\n",
      "train loss:0.7862693330714184\n",
      "train loss:0.6944782186640223\n",
      "train loss:0.5361050889256613\n",
      "train loss:0.7710318341011935\n",
      "train loss:0.8247186767527419\n",
      "train loss:0.6742199493670248\n",
      "train loss:0.6120270332423589\n",
      "train loss:0.6841270895913467\n",
      "train loss:0.5338032531917553\n",
      "train loss:0.5144349054239814\n",
      "train loss:0.5010947128199619\n",
      "train loss:0.545478916067822\n",
      "train loss:0.46543468940533217\n",
      "train loss:0.538164320363309\n",
      "train loss:0.42015543333763317\n",
      "train loss:0.8294538221848133\n",
      "train loss:0.6233941620680082\n",
      "train loss:0.8765328080680528\n",
      "train loss:0.26974350140730086\n",
      "train loss:0.6096763554507807\n",
      "train loss:0.8611796964316023\n",
      "train loss:0.7597136939984517\n",
      "train loss:0.7110613053791066\n",
      "train loss:0.42251058857477863\n",
      "train loss:0.8301186564757759\n",
      "train loss:0.6899647548654675\n",
      "train loss:0.4356904129391862\n",
      "train loss:0.7086439717212304\n",
      "train loss:0.6852519917161957\n",
      "train loss:0.6653151075123163\n",
      "train loss:0.43172620261358086\n",
      "train loss:0.6222855138594137\n",
      "train loss:0.5568956138074659\n",
      "train loss:0.5322491296367313\n",
      "train loss:0.6187459860219379\n",
      "train loss:0.4471602445842394\n",
      "train loss:0.7181600555306781\n",
      "train loss:0.5148358743538023\n",
      "train loss:0.5157439948836784\n",
      "train loss:0.7114159026639078\n",
      "train loss:0.6145550181914559\n",
      "train loss:0.3937287824406516\n",
      "train loss:0.7238165680246818\n",
      "train loss:0.2836174403634565\n",
      "train loss:0.5174488921274117\n",
      "train loss:0.5069500361620631\n",
      "train loss:0.21876270197159622\n",
      "train loss:0.6659966910281676\n",
      "train loss:0.7128566438960756\n",
      "train loss:0.6708033961654871\n",
      "train loss:0.8932957316694852\n",
      "train loss:0.5113983381671126\n",
      "train loss:0.37484673823657555\n",
      "train loss:0.7822896953154448\n",
      "train loss:0.7392555339463266\n",
      "train loss:0.5990300307796058\n",
      "train loss:0.4400080258418518\n",
      "train loss:0.6169070268139437\n",
      "train loss:0.6069169803323489\n",
      "train loss:0.6055882205113956\n",
      "train loss:0.68748756706375\n",
      "train loss:0.6927209962587437\n",
      "train loss:0.4779175131988014\n",
      "train loss:0.6812138988704126\n",
      "train loss:0.5649522490292629\n",
      "train loss:0.5379398922257639\n",
      "train loss:0.6758611043015575\n",
      "train loss:0.5560786644806193\n",
      "train loss:0.6370680769967219\n",
      "train loss:0.7546393074606376\n",
      "train loss:0.6824946930525229\n",
      "train loss:0.6228074366073356\n",
      "train loss:0.6779785722179389\n",
      "train loss:0.5292469830920569\n",
      "train loss:0.7652112673939373\n",
      "train loss:0.4877650398737927\n",
      "train loss:0.6874010616192254\n",
      "train loss:0.8151110998799627\n",
      "train loss:0.6106516039812894\n",
      "train loss:0.6697616493567848\n",
      "train loss:0.5484357045979691\n",
      "train loss:0.621006028243127\n",
      "train loss:0.49595158678737217\n",
      "train loss:0.6729999493122043\n",
      "train loss:0.6113604810918949\n",
      "train loss:0.5356765456057346\n",
      "train loss:0.8473339777646473\n",
      "train loss:0.8333700031780665\n",
      "train loss:0.6002464914600705\n",
      "train loss:0.7453052387366542\n",
      "train loss:0.623679274845818\n",
      "train loss:0.5056771291028341\n",
      "train loss:0.679234279478542\n",
      "train loss:0.5601882765260837\n",
      "train loss:0.5453105242035416\n",
      "train loss:0.5436357977558857\n",
      "train loss:0.7652333353035432\n",
      "train loss:0.6959634088555727\n",
      "train loss:0.6113934190584084\n",
      "train loss:0.6247995641819725\n",
      "train loss:0.6617703774642332\n",
      "train loss:0.6771157910608749\n",
      "train loss:0.526533108494027\n",
      "train loss:0.5160819060424682\n",
      "train loss:0.5231128550707715\n",
      "train loss:0.5121007430350005\n",
      "train loss:0.7190154764781715\n",
      "train loss:0.6222611514315267\n",
      "train loss:0.6056337941098454\n",
      "train loss:0.7236349075261355\n",
      "train loss:0.4186517467587104\n",
      "train loss:0.6257895687029567\n",
      "train loss:0.6140827761502158\n",
      "train loss:0.6053020542102608\n",
      "train loss:0.39477723736260606\n",
      "train loss:0.5187712549158441\n",
      "train loss:0.8266698019558607\n",
      "train loss:0.5946435331056424\n",
      "train loss:0.8082970280533466\n",
      "train loss:0.59963498294916\n",
      "train loss:0.7700132122191541\n",
      "train loss:0.3756755718252274\n",
      "train loss:0.6924140769654222\n",
      "train loss:0.6721339398370759\n",
      "train loss:0.796400356498453\n",
      "train loss:0.6694427373658378\n",
      "train loss:0.6780927841746167\n",
      "train loss:0.6047203115339675\n",
      "train loss:0.5529630719287482\n",
      "train loss:0.5050694011726122\n",
      "train loss:0.6260906650476172\n",
      "train loss:0.6818249152411081\n",
      "train loss:0.6175425242993892\n",
      "train loss:0.7414394978834694\n",
      "train loss:0.6040582115733298\n",
      "train loss:0.6120094820224476\n",
      "train loss:0.6970699875997391\n",
      "train loss:0.55354525891392\n",
      "train loss:0.6196098056633248\n",
      "train loss:0.6946176977352977\n",
      "train loss:0.4578467535159191\n",
      "train loss:0.5581546757812115\n",
      "train loss:0.6058540136602307\n",
      "train loss:0.5068974954443358\n",
      "train loss:0.5147314795860808\n",
      "train loss:0.6162738513320877\n",
      "train loss:0.7132461642324743\n",
      "train loss:0.6081906014023932\n",
      "train loss:0.6313070534966767\n",
      "train loss:0.4813683196286657\n",
      "train loss:0.41128250022482477\n",
      "train loss:0.5032076084026876\n",
      "train loss:0.47252751764697365\n",
      "train loss:0.8633122475362196\n",
      "train loss:0.7218544992644185\n",
      "train loss:1.0372234216384484\n",
      "train loss:0.7827168745405839\n",
      "train loss:0.6940295094775227\n",
      "train loss:0.5596041617754237\n",
      "train loss:0.5760091607342911\n",
      "train loss:0.5847636245102446\n",
      "train loss:0.6292735499883781\n",
      "train loss:0.5327192455554303\n",
      "train loss:0.6195631073689878\n",
      "train loss:0.673921806472527\n",
      "train loss:0.6830813330198675\n",
      "train loss:0.6173264318264616\n",
      "train loss:0.68084411574066\n",
      "train loss:0.5727277594450314\n",
      "train loss:0.6828457209532192\n",
      "train loss:0.7335658072773079\n",
      "train loss:0.6115523720113458\n",
      "train loss:0.6291041609117685\n",
      "train loss:0.4889228815235084\n",
      "train loss:0.6104395087252915\n",
      "train loss:0.6771145414462669\n",
      "train loss:0.6906632715840774\n",
      "train loss:0.4410536873136036\n",
      "train loss:0.5087548101089749\n",
      "train loss:0.597908020521724\n",
      "train loss:0.8167409390249759\n",
      "train loss:0.7014709363323579\n",
      "train loss:0.8129484332738753\n",
      "train loss:0.847371674876349\n",
      "train loss:0.6086333689255354\n",
      "train loss:0.466069474077338\n",
      "train loss:0.5291596978691635\n",
      "train loss:0.6852503576416671\n",
      "train loss:0.3729274333078778\n",
      "train loss:0.5990279738756111\n",
      "train loss:0.6287767315416477\n",
      "train loss:0.5313424021460948\n",
      "train loss:0.6038513990411054\n",
      "train loss:0.6996644635072109\n",
      "train loss:0.4179360356032461\n",
      "train loss:0.6407381933009441\n",
      "train loss:0.637266208648021\n",
      "train loss:0.6279313243127481\n",
      "train loss:0.6265597737213409\n",
      "train loss:0.7183420043611118\n",
      "train loss:0.717271692134852\n",
      "train loss:0.6208229386102703\n",
      "train loss:0.6014174687538176\n",
      "train loss:0.5395522001534931\n",
      "train loss:0.5176911228658659\n",
      "train loss:0.607406525662876\n",
      "train loss:0.7501610816457203\n",
      "train loss:0.6763177867602668\n",
      "train loss:0.8328801562222707\n",
      "train loss:0.4115003411154108\n",
      "train loss:0.5474975404138306\n",
      "train loss:0.47727183615805024\n",
      "train loss:0.6141949585586711\n",
      "train loss:0.36981552580170474\n",
      "train loss:0.5219410076752318\n",
      "train loss:0.9119778065235165\n",
      "train loss:0.6137663680964589\n",
      "train loss:0.4140695384186016\n",
      "train loss:0.6306026993991533\n",
      "train loss:0.7270101235777832\n",
      "train loss:0.8030132766819955\n",
      "train loss:0.705581346190588\n",
      "train loss:0.42971827970990006\n",
      "train loss:0.32112273094762794\n",
      "train loss:0.501915230258284\n",
      "train loss:0.7152175384109932\n",
      "train loss:0.7113364928059396\n",
      "train loss:0.5299434746104936\n",
      "train loss:0.7059692811601265\n",
      "train loss:0.8027057707144986\n",
      "train loss:0.5101641663427643\n",
      "train loss:0.7932607720418389\n",
      "train loss:0.8227278083480212\n",
      "train loss:0.4975771334918787\n",
      "train loss:0.7275632218505015\n",
      "train loss:0.7274469101121508\n",
      "train loss:0.6312156154590937\n",
      "train loss:0.7161334556468782\n",
      "train loss:0.6106629911344081\n",
      "train loss:0.6107278844172845\n",
      "train loss:0.6773474008457325\n",
      "train loss:0.6146940857385466\n",
      "train loss:0.6025975364743271\n",
      "train loss:0.49646970775174326\n",
      "train loss:0.570124378656195\n",
      "train loss:0.6272023673119724\n",
      "train loss:0.8004956649988795\n",
      "train loss:0.5517976401421378\n",
      "train loss:0.4617140861665299\n",
      "train loss:0.5154488169507255\n",
      "train loss:0.6049286279681825\n",
      "train loss:0.7094487055825607\n",
      "train loss:0.6119557610718633\n",
      "train loss:0.6102933625969963\n",
      "train loss:0.5982007882793111\n",
      "train loss:0.3773780623311718\n",
      "train loss:0.4854203466743314\n",
      "train loss:0.6190062278520022\n",
      "train loss:0.21323154857492174\n",
      "train loss:0.3463228337667953\n",
      "train loss:0.6584885070540777\n",
      "train loss:0.6511831051101937\n",
      "train loss:0.51063473613033\n",
      "train loss:0.6700349400053665\n",
      "train loss:0.6469540042285666\n",
      "train loss:0.6406688796958424\n",
      "train loss:0.7473264805554238\n",
      "train loss:0.6182947766726541\n",
      "train loss:0.7415364260693952\n",
      "train loss:0.9146208521987752\n",
      "train loss:0.6860189918909304\n",
      "train loss:0.6179433687777871\n",
      "train loss:0.6360644960679621\n",
      "train loss:0.5854420168635593\n",
      "train loss:0.6880473221097034\n",
      "train loss:0.6368540826606891\n",
      "train loss:0.6144847369982122\n",
      "train loss:0.6083829693964581\n",
      "train loss:0.5694062798979196\n",
      "train loss:0.5947069414104469\n",
      "train loss:0.5321825566207513\n",
      "train loss:0.7317534150069659\n",
      "train loss:0.6296413558981282\n",
      "train loss:0.4853481299866737\n",
      "train loss:0.4737838326268351\n",
      "train loss:0.6164210065989095\n",
      "train loss:0.8026816047617957\n",
      "train loss:0.6045765781297542\n",
      "train loss:0.5126367140399318\n",
      "train loss:0.2833371325226679\n",
      "train loss:0.4014009689316117\n",
      "train loss:0.6407078583896745\n",
      "train loss:1.0512927619996817\n",
      "train loss:0.3651111820731705\n",
      "train loss:0.3659906086634953\n",
      "train loss:0.4968476311928848\n",
      "train loss:0.48692337413691156\n",
      "train loss:1.198826938177989\n",
      "train loss:0.5020086830976097\n",
      "train loss:0.4985710508127024\n",
      "train loss:0.4852734018945135\n",
      "train loss:0.6167141638817796\n",
      "train loss:0.6137876968699076\n",
      "train loss:0.49480007642372065\n",
      "train loss:0.7142994840685344\n",
      "train loss:0.5133140521521303\n",
      "train loss:0.6130329863385395\n",
      "train loss:0.7824851929317151\n",
      "train loss:0.4388668525362055\n",
      "train loss:0.6984034670668517\n",
      "train loss:0.4555372281719891\n",
      "train loss:0.4459796590503233\n",
      "train loss:0.6867501085880238\n",
      "train loss:0.6245631771787479\n",
      "train loss:0.6937560576533931\n",
      "train loss:0.6860074378302607\n",
      "train loss:0.7585746501135929\n",
      "train loss:0.5338376842790539\n",
      "train loss:0.4621387683162938\n",
      "train loss:0.4542983417054044\n",
      "train loss:0.43902878956772706\n",
      "train loss:0.8006083159652381\n",
      "train loss:0.43200013359008915\n",
      "train loss:0.4097831842710728\n",
      "train loss:0.7051631686360581\n",
      "train loss:0.7052656475402149\n",
      "train loss:0.6255583916925876\n",
      "train loss:0.6196480543717804\n",
      "train loss:0.38675322598482503\n",
      "train loss:0.8488940431460467\n",
      "train loss:0.6239603317423481\n",
      "train loss:0.39862732734818906\n",
      "train loss:1.0314834084623474\n",
      "train loss:0.4031481548881489\n",
      "train loss:0.40799341645715737\n",
      "train loss:0.7873874602386021\n",
      "train loss:0.623153293819503\n",
      "train loss:0.5578310882363354\n",
      "train loss:0.6945443813071344\n",
      "train loss:0.5526353078758811\n",
      "train loss:0.6813821043501973\n",
      "train loss:0.6903644816279387\n",
      "train loss:0.5472077775023021\n",
      "train loss:0.5447465141988481\n",
      "train loss:0.6130458499395908\n",
      "train loss:0.6113121742209431\n",
      "train loss:0.6193108354417893\n",
      "train loss:0.8121084050925742\n",
      "train loss:0.734431214557576\n",
      "train loss:0.6850068997020194\n",
      "train loss:0.6121256166514586\n",
      "train loss:0.7456100351982908\n",
      "train loss:0.6263376920965202\n",
      "train loss:0.6239243074605992\n",
      "train loss:0.4611061027741698\n",
      "train loss:0.5010373116506172\n",
      "train loss:0.4937980534001203\n",
      "train loss:0.6735892732221493\n",
      "train loss:0.5176600096717954\n",
      "train loss:0.5179101842861965\n",
      "train loss:0.5131321660745481\n",
      "train loss:0.524952266444553\n",
      "train loss:0.8233788080418749\n",
      "train loss:0.6361286652181134\n",
      "train loss:0.7537441999429861\n",
      "train loss:0.4899840346709737\n",
      "train loss:0.3666549584697727\n",
      "train loss:0.48660817391083544\n",
      "train loss:0.8970007842359367\n",
      "train loss:0.37035717750388447\n",
      "train loss:0.8557030021594529\n",
      "train loss:0.6170098512642587\n",
      "train loss:0.5054613936101208\n",
      "train loss:0.7329579007674591\n",
      "train loss:0.8094896790248398\n",
      "train loss:0.5072769036640573\n",
      "train loss:0.6957551106733135\n",
      "train loss:0.6068881063165419\n",
      "train loss:0.6102610530353288\n",
      "train loss:0.538534858934766\n",
      "train loss:0.5513666651463363\n",
      "train loss:0.5565935029535127\n",
      "train loss:0.8155459075355511\n",
      "train loss:0.47890030314293447\n",
      "train loss:0.479384198739537\n",
      "train loss:0.6184144098939542\n",
      "train loss:0.6849003014077332\n",
      "train loss:0.6004474425544719\n",
      "train loss:0.53204166974128\n",
      "train loss:0.5353651790091775\n",
      "train loss:0.4367134982159828\n",
      "train loss:0.5120338689380313\n",
      "train loss:0.49280549519783073\n",
      "train loss:0.47612461885706125\n",
      "train loss:0.4845651070685338\n",
      "train loss:0.6418885162319354\n",
      "train loss:0.9179402369350583\n",
      "train loss:0.35190528324104875\n",
      "train loss:0.3591230309532645\n",
      "train loss:0.6589818020037884\n",
      "train loss:0.3463529494504398\n",
      "train loss:1.2542976659410638\n",
      "train loss:0.6182495814623612\n",
      "train loss:0.48319215205229343\n",
      "train loss:0.6127503570511594\n",
      "train loss:0.5191599912059742\n",
      "train loss:0.6958696373893699\n",
      "train loss:0.5268369081428551\n",
      "train loss:0.43629987600832854\n",
      "train loss:0.5230000175580396\n",
      "train loss:0.6853901934487509\n",
      "train loss:0.4250573543789473\n",
      "train loss:0.5272235008399622\n",
      "train loss:0.5205694677592463\n",
      "train loss:0.42172045076916964\n",
      "train loss:0.4012807808546407\n",
      "train loss:0.5904063753888267\n",
      "train loss:0.3861486387605651\n",
      "train loss:0.6344686761879288\n",
      "train loss:0.630865935293772\n",
      "train loss:0.6736721331025871\n",
      "train loss:0.5127130006121814\n",
      "train loss:0.36658193523816085\n",
      "train loss:0.9088896725733886\n",
      "train loss:0.36931315674014653\n",
      "train loss:0.5073059061687912\n",
      "train loss:0.5148654503482931\n",
      "train loss:0.34226420605236274\n",
      "train loss:0.7560414827609598\n",
      "train loss:0.4923869067607149\n",
      "train loss:0.869548706063503\n",
      "train loss:0.6114872189878435\n",
      "train loss:0.7519591874897202\n",
      "train loss:0.7260938258781897\n",
      "train loss:0.6152678748681242\n",
      "train loss:0.5072763918369736\n",
      "train loss:0.3711110606039277\n",
      "train loss:0.6165332546710343\n",
      "train loss:0.45963452062592436\n",
      "train loss:0.6911587854429869\n",
      "train loss:0.5255594499279854\n",
      "train loss:0.6197238643923237\n",
      "train loss:0.611370053597317\n",
      "train loss:0.5346087125973789\n",
      "train loss:0.5289753257638311\n",
      "train loss:0.32137009402392164\n",
      "train loss:0.7136466320737097\n",
      "train loss:0.49612513638945127\n",
      "train loss:0.7196774484862501\n",
      "train loss:0.6110441692013698\n",
      "train loss:0.5033038741270495\n",
      "train loss:0.384845944693917\n",
      "train loss:0.6191402658943965\n",
      "train loss:0.5172190266926722\n",
      "train loss:0.9855253760876141\n",
      "train loss:0.5214013686628558\n",
      "train loss:0.5056367294273489\n",
      "train loss:0.38357849967995905\n",
      "train loss:0.40361481142759886\n",
      "train loss:0.5196740433701155\n",
      "train loss:0.8455852540608987\n",
      "train loss:0.7470291858017581\n",
      "train loss:0.40952824955205963\n",
      "train loss:0.5125600686434757\n",
      "train loss:0.27742969154525277\n",
      "train loss:0.49689328471188965\n",
      "train loss:0.2487086594814843\n",
      "train loss:0.8763737767268147\n",
      "train loss:0.6570893133072462\n",
      "train loss:0.4904622475686601\n",
      "train loss:0.6315753078303828\n",
      "train loss:0.5185431584019731\n",
      "train loss:0.6406245905792953\n",
      "train loss:0.9577558905085238\n",
      "train loss:0.3981732172266706\n",
      "train loss:0.6184790236337311\n",
      "train loss:0.3005465294549068\n",
      "train loss:0.705343375938948\n",
      "train loss:0.4263800373835533\n",
      "train loss:0.40907180744872473\n",
      "train loss:0.8170658216697294\n",
      "train loss:0.6143433999435819\n",
      "train loss:0.7123380360672114\n",
      "train loss:0.95547571081638\n",
      "train loss:0.6053986855619227\n",
      "train loss:0.7627684509076059\n",
      "train loss:0.6139975212638482\n",
      "train loss:0.4302198056532892\n",
      "train loss:0.5661285699905161\n",
      "train loss:0.4838409657855592\n",
      "train loss:0.6095757998381351\n",
      "train loss:0.6795710518273942\n",
      "train loss:0.6136181943392002\n",
      "train loss:0.5448469135050852\n",
      "train loss:0.5334774185074036\n",
      "train loss:0.6172228214905612\n",
      "train loss:0.7794838947895225\n",
      "train loss:0.7613003538952554\n",
      "train loss:0.5402997454194274\n",
      "train loss:0.6096583699079258\n",
      "train loss:0.6978077434538019\n",
      "train loss:0.358801035829999\n",
      "train loss:0.5249000598344904\n",
      "train loss:0.7887401256720183\n",
      "train loss:0.6158504576679247\n",
      "train loss:0.6997802533310669\n",
      "train loss:0.6151381835240516\n",
      "train loss:0.6988038994353818\n",
      "train loss:0.6133609153094537\n",
      "train loss:0.4311153268164386\n",
      "train loss:0.42862838731215785\n",
      "train loss:0.6901100362893946\n",
      "train loss:0.6047714942033228\n",
      "train loss:0.32046754886621165\n",
      "train loss:0.39863184797929174\n",
      "train loss:0.6314153090843307\n",
      "train loss:0.6307164632192258\n",
      "train loss:0.48846300894767525\n",
      "train loss:0.3852067996910009\n",
      "train loss:0.47618155279199276\n",
      "train loss:0.7791656316120414\n",
      "train loss:0.6294413041647905\n",
      "train loss:0.5059479249651947\n",
      "train loss:0.8957132723859079\n",
      "train loss:0.36653950051124295\n",
      "train loss:0.4943663700310402\n",
      "train loss:0.6355768040318407\n",
      "train loss:0.6229667538753614\n",
      "train loss:0.7337983073850141\n",
      "train loss:0.5972119066259427\n",
      "train loss:0.5203781518996932\n",
      "train loss:0.8759175084525186\n",
      "train loss:0.8475929531530143\n",
      "train loss:0.543692906482433\n",
      "train loss:0.6756672831471409\n",
      "train loss:0.6247073374095111\n",
      "train loss:0.5748597350724238\n",
      "train loss:0.5244228152921739\n",
      "train loss:0.5801506791634745\n",
      "train loss:0.5697389882162835\n",
      "train loss:0.7227187282655565\n",
      "train loss:0.5095127610612661\n",
      "train loss:0.670485402613537\n",
      "train loss:0.5539903569021812\n",
      "train loss:0.5421684462902214\n",
      "train loss:0.5358216453945576\n",
      "train loss:0.4517971008106466\n",
      "train loss:0.5170553791571493\n",
      "train loss:0.40798443383036875\n",
      "train loss:0.6175815673024572\n",
      "train loss:0.37400132281245074\n",
      "train loss:0.6276739391976657\n",
      "train loss:0.5202419267008377\n",
      "train loss:0.931680315719856\n",
      "train loss:0.19293670742606048\n",
      "train loss:0.6435223577567706\n",
      "train loss:0.186942819754859\n",
      "train loss:0.529410312380555\n",
      "train loss:0.6570804885595709\n",
      "train loss:0.3346003341009613\n",
      "train loss:0.49379086717177645\n",
      "train loss:0.3646272427972993\n",
      "train loss:1.0310459503110012\n",
      "train loss:0.8476213728675939\n",
      "train loss:0.5006584486289352\n",
      "train loss:0.6187806669923377\n",
      "train loss:0.8400863884440796\n",
      "train loss:0.530632050881474\n",
      "train loss:0.6036326954205101\n",
      "train loss:0.8551163421435832\n",
      "train loss:0.6886199532188465\n",
      "train loss:0.6289835835809378\n",
      "train loss:0.5631740752708213\n",
      "train loss:0.6383364211258773\n",
      "train loss:0.6416284956508698\n",
      "train loss:0.592206621309392\n",
      "train loss:0.5972972048671247\n",
      "train loss:0.6922800299509355\n",
      "train loss:0.5407807168072485\n",
      "train loss:0.6340629236967918\n",
      "train loss:0.4662751007646416\n",
      "train loss:0.4957527106611603\n",
      "train loss:0.5471028180849817\n",
      "train loss:0.38118864880072867\n",
      "train loss:0.617237923603527\n",
      "train loss:0.6163679717025271\n",
      "train loss:0.5086533313878986\n",
      "train loss:0.38053699613329167\n",
      "train loss:0.6290326291410995\n",
      "train loss:0.6484283594875998\n",
      "train loss:0.34713031501382485\n",
      "train loss:0.8159099347273688\n",
      "train loss:0.835245682634504\n",
      "train loss:0.35189196541135426\n",
      "train loss:0.6687249861365303\n",
      "train loss:0.7743190219795506\n",
      "train loss:0.8830393832110335\n",
      "train loss:0.6120968231592563\n",
      "train loss:0.5034375739064101\n",
      "train loss:0.5207855835211401\n",
      "train loss:0.6169592192772414\n",
      "train loss:0.4375326413678198\n",
      "train loss:0.5218002539247875\n",
      "train loss:0.5214893435294711\n",
      "train loss:0.5187253500596941\n",
      "train loss:0.5250665072149988\n",
      "train loss:0.519478084265084\n",
      "train loss:0.5944212463907612\n",
      "train loss:0.6103145174749781\n",
      "train loss:0.5008212412273092\n",
      "train loss:0.6162547975963028\n",
      "train loss:0.7358016708116633\n",
      "train loss:0.7209958476889154\n",
      "train loss:0.7080095913920496\n",
      "train loss:0.5173356429874866\n",
      "train loss:0.6179769519663274\n",
      "train loss:0.7106115758107225\n",
      "train loss:0.4270034863383237\n",
      "train loss:0.5120782368683319\n",
      "train loss:0.6987837706719895\n",
      "train loss:0.7919768687849568\n",
      "train loss:0.6882044400490803\n",
      "train loss:0.6143548767474966\n",
      "train loss:0.6921949365440853\n",
      "train loss:0.6078544720805527\n",
      "train loss:0.6125248874219891\n",
      "train loss:0.6115838862547165\n",
      "train loss:0.6182028617510211\n",
      "train loss:0.48148827450050236\n",
      "train loss:0.5453465347155266\n",
      "train loss:0.5440450877316393\n",
      "train loss:0.7673693665630843\n",
      "train loss:0.5421233812896256\n",
      "train loss:0.4499116809061425\n",
      "train loss:0.7626606622274557\n",
      "train loss:0.6100578293567644\n",
      "train loss:0.5226949912179261\n",
      "train loss:0.7888941205421982\n",
      "train loss:0.7079432792751863\n",
      "train loss:0.7808434000385528\n",
      "train loss:0.6962188052137036\n",
      "train loss:0.5355297581005745\n",
      "train loss:0.6882629343535032\n",
      "train loss:0.470875772514425\n",
      "train loss:0.6858044641809512\n",
      "train loss:0.6126347871623705\n",
      "train loss:0.6775443613633715\n",
      "train loss:0.5403109763646535\n",
      "train loss:0.6093144375179766\n",
      "train loss:0.5368762116427589\n",
      "train loss:0.6055766788282216\n",
      "train loss:0.8316769760270626\n",
      "train loss:0.7562876420847289\n",
      "train loss:0.46706708914063083\n",
      "train loss:0.8123274740461497\n",
      "train loss:0.4736438872374933\n",
      "train loss:0.6758560728670744\n",
      "train loss:0.3999936501873995\n",
      "train loss:0.7559339817367763\n",
      "train loss:0.6164626917899165\n",
      "train loss:0.6858254613404183\n",
      "train loss:0.6973595789071164\n",
      "train loss:0.38647319371088895\n",
      "train loss:0.5304930865374177\n",
      "train loss:0.3442280932560804\n",
      "train loss:0.8107062580265237\n",
      "train loss:0.6099600602043244\n",
      "train loss:0.808630712112187\n",
      "train loss:0.7868840617541064\n",
      "train loss:0.5158508872299461\n",
      "train loss:0.773150326264187\n",
      "train loss:0.6142402917724398\n",
      "train loss:0.6102362348769608\n",
      "train loss:0.696322710956712\n",
      "train loss:0.7466321650855272\n",
      "train loss:0.7407823405929804\n",
      "train loss:0.6708483673560445\n",
      "train loss:0.503584373733813\n",
      "train loss:0.5634423856633587\n",
      "train loss:0.614925076385717\n",
      "train loss:0.7288262090092339\n",
      "train loss:0.6754195609760425\n",
      "train loss:0.510756588997164\n",
      "train loss:0.6206546562354583\n",
      "train loss:0.5647081520272103\n",
      "train loss:0.5580683586200345\n",
      "train loss:0.5485815611314414\n",
      "train loss:0.6785125714599586\n",
      "train loss:0.527930159533067\n",
      "train loss:0.5236399686628053\n",
      "train loss:0.529844464484637\n",
      "train loss:0.6104040843788826\n",
      "train loss:0.8038506687044699\n",
      "train loss:0.609814661796247\n",
      "train loss:0.7317602667438712\n",
      "train loss:0.7171294336378253\n",
      "train loss:0.5148981602097213\n",
      "train loss:0.5000339275844625\n",
      "train loss:0.5090183317565036\n",
      "train loss:0.403423011453022\n",
      "train loss:0.6370275617363128\n",
      "train loss:0.9312652618293662\n",
      "train loss:0.5111577807751225\n",
      "train loss:0.7251469899197038\n",
      "train loss:0.5189197532726089\n",
      "train loss:0.776834467703163\n",
      "train loss:0.42925842595189423\n",
      "train loss:0.5946551253192557\n",
      "train loss:0.6102082305543052\n",
      "train loss:0.5139403030363799\n",
      "train loss:0.6911706476302559\n",
      "train loss:0.612953082207356\n",
      "train loss:0.6870421172423987\n",
      "train loss:0.5311884672955988\n",
      "train loss:0.6212134702627137\n",
      "train loss:0.6161858152925062\n",
      "train loss:0.6156219071290049\n",
      "train loss:0.5206838845822028\n",
      "train loss:0.702156603781374\n",
      "train loss:0.44783499059820003\n",
      "train loss:0.6092472978136481\n",
      "train loss:0.6956020517105623\n",
      "train loss:0.5239753722012036\n",
      "train loss:0.5069596638283068\n",
      "train loss:0.6301671167572352\n",
      "train loss:0.7123550619253969\n",
      "train loss:0.5298139474117856\n",
      "train loss:0.41687055461710776\n",
      "train loss:0.4081610359574942\n",
      "train loss:0.5157951520118167\n",
      "train loss:0.6146067205070174\n",
      "train loss:0.5055575053865682\n",
      "train loss:0.5117216886064855\n",
      "train loss:0.23789599137913328\n",
      "train loss:0.35181160072366124\n",
      "train loss:0.4863574093006102\n",
      "train loss:0.34792187691121385\n",
      "train loss:1.0314385180952403\n",
      "train loss:0.3372482741978331\n",
      "train loss:0.3522479227855583\n",
      "train loss:0.8419795985565482\n",
      "train loss:0.8320936928468854\n",
      "train loss:0.48935830483403553\n",
      "train loss:0.6263443095823287\n",
      "train loss:0.6424349605245587\n",
      "train loss:0.712565674453171\n",
      "train loss:0.9019959559981618\n",
      "train loss:0.6149362863527179\n",
      "train loss:0.6088684431012983\n",
      "train loss:0.6156127583599792\n",
      "train loss:0.6770559739679319\n",
      "train loss:0.6232419664444182\n",
      "train loss:0.5787577178358201\n",
      "train loss:0.6365147728666056\n",
      "train loss:0.5885630284084552\n",
      "train loss:0.6810186127883779\n",
      "train loss:0.5464788237807832\n",
      "train loss:0.5386669876663552\n",
      "train loss:0.631022474307186\n",
      "train loss:0.6725152297925951\n",
      "train loss:0.4488115985685567\n",
      "train loss:0.68251503722441\n",
      "train loss:0.547987875264125\n",
      "train loss:0.46163734149599345\n",
      "train loss:0.44287857514101603\n",
      "train loss:0.5148377197352682\n",
      "train loss:0.6156647964980501\n",
      "train loss:0.7252730879776033\n",
      "train loss:0.5974815079565927\n",
      "train loss:0.5069949200477268\n",
      "train loss:0.7503756431128089\n",
      "train loss:0.35977592109982803\n",
      "train loss:0.21308644370579702\n",
      "train loss:0.6565420932827563\n",
      "train loss:0.6691170103962875\n",
      "train loss:0.5106617332762273\n",
      "train loss:0.3660623441308195\n",
      "train loss:0.49090993165739993\n",
      "train loss:0.7974182560894757\n",
      "train loss:0.809049418467449\n",
      "train loss:0.37932641889107693\n",
      "train loss:0.6364759817582145\n",
      "train loss:0.6225009067879399\n",
      "train loss:0.7225685046683195\n",
      "train loss:0.5126334363667366\n",
      "train loss:0.6209443945087629\n",
      "train loss:0.5951174602579089\n",
      "train loss:0.428155260191049\n",
      "train loss:0.6107136027603979\n",
      "train loss:0.699394056543559\n",
      "train loss:0.6028941236895696\n",
      "train loss:0.5291740423966013\n",
      "train loss:0.7636807583170316\n",
      "train loss:0.6866902153931168\n",
      "train loss:0.4675868894288541\n",
      "train loss:0.46213688317900986\n",
      "train loss:0.7534146853379559\n",
      "train loss:0.534905321533209\n",
      "train loss:0.6094207748340421\n",
      "train loss:0.598323830315767\n",
      "train loss:0.5274488692312694\n",
      "train loss:0.5350627111230005\n",
      "train loss:0.435378517291535\n",
      "train loss:0.6110557519810594\n",
      "train loss:0.6132722480662437\n",
      "train loss:0.6057006861998012\n",
      "train loss:0.5013171349687224\n",
      "train loss:0.5105058157537729\n",
      "train loss:0.5008344601485214\n",
      "train loss:0.6171652088105839\n",
      "train loss:0.49617124372995125\n",
      "train loss:0.3641242307374727\n",
      "train loss:0.21641448227624335\n",
      "train loss:0.49490937185658446\n",
      "train loss:0.3526314812446366\n",
      "train loss:0.34644137656774854\n",
      "train loss:1.0393111595616455\n",
      "train loss:0.6528476154729649\n",
      "train loss:0.48133272341699485\n",
      "train loss:0.8342196746461635\n",
      "train loss:0.773312422503383\n",
      "train loss:0.5059866438326492\n",
      "train loss:0.6213338252741173\n",
      "train loss:0.7292785356937201\n",
      "train loss:0.796456446209943\n",
      "train loss:0.43088276145725757\n",
      "train loss:0.6144353037167978\n",
      "train loss:0.6787141168033298\n",
      "train loss:0.6169854764001608\n",
      "train loss:0.5539690960737462\n",
      "train loss:0.6762315872277467\n",
      "train loss:0.6284793774519309\n",
      "train loss:0.6758418053657425\n",
      "train loss:0.6705135013503043\n",
      "train loss:0.4684637060266284\n",
      "train loss:0.6297330166404155\n",
      "train loss:0.5667101001856947\n",
      "train loss:0.6171418720432951\n",
      "train loss:0.6865636838931174\n",
      "train loss:0.6803574692318417\n",
      "train loss:0.681526138139617\n",
      "train loss:0.5546980367226826\n",
      "train loss:0.6765826850962372\n",
      "train loss:0.6826734000567651\n",
      "train loss:0.6157208105510122\n",
      "train loss:0.6827631363097835\n",
      "train loss:0.539866063805879\n",
      "train loss:0.46456787365966273\n",
      "train loss:0.6108520099170761\n",
      "train loss:0.4491116652308504\n",
      "train loss:0.6023303584884727\n",
      "train loss:0.6058263442721812\n",
      "train loss:0.6278611336540008\n",
      "train loss:0.6262867908609795\n",
      "train loss:0.6193694240720526\n",
      "train loss:0.6197729609117347\n",
      "train loss:0.7266656909366248\n",
      "train loss:0.3983866297398397\n",
      "train loss:0.3770174316125791\n",
      "train loss:0.6275360706202746\n",
      "train loss:0.7462640692078238\n",
      "train loss:0.38468409104147344\n",
      "train loss:0.7327603246549216\n",
      "train loss:0.615081767678004\n",
      "train loss:0.4967724967896161\n",
      "train loss:0.5194546917522985\n",
      "train loss:0.6270335827950075\n",
      "train loss:0.6128980274713035\n",
      "train loss:0.5977433461338181\n",
      "train loss:0.6987288512709194\n",
      "train loss:0.6128501409183562\n",
      "train loss:0.5193725880003155\n",
      "train loss:0.5146587336858883\n",
      "train loss:0.6124521451892095\n",
      "train loss:0.4151608505157263\n",
      "train loss:0.6025874665639607\n",
      "train loss:0.5130316605600123\n",
      "train loss:0.4088958305941418\n",
      "train loss:0.5039711036185636\n",
      "train loss:0.8141091028472347\n",
      "train loss:0.714275328881849\n",
      "train loss:0.2824281148846242\n",
      "train loss:0.6250270332746812\n",
      "train loss:0.9677706582111316\n",
      "train loss:0.6228600017691174\n",
      "train loss:0.7843562766190274\n",
      "train loss:0.6936000437344616\n",
      "train loss:0.5445965517463531\n",
      "train loss:0.6103157116183914\n",
      "train loss:0.6044095131218375\n",
      "train loss:0.5470257973160323\n",
      "train loss:0.5369340970857933\n",
      "train loss:0.6120689513001788\n",
      "train loss:0.6077305189252001\n",
      "train loss:0.6089575733902332\n",
      "train loss:0.6730938592360015\n",
      "train loss:0.6759889766294953\n",
      "train loss:0.5507652875145739\n",
      "train loss:0.5351286965221701\n",
      "train loss:0.7588366813692241\n",
      "train loss:0.5310513300498375\n",
      "train loss:0.6029519581604841\n",
      "train loss:0.6946096616859186\n",
      "train loss:0.5288408751966267\n",
      "train loss:0.5276365848840563\n",
      "train loss:0.6923357204484232\n",
      "train loss:0.5173622563003399\n",
      "train loss:0.6220547711798267\n",
      "train loss:0.6232378402718298\n",
      "train loss:0.6152478648724039\n",
      "train loss:0.3261129898618963\n",
      "train loss:0.50370445730527\n",
      "train loss:0.5993650160341012\n",
      "train loss:0.7260563346717527\n",
      "train loss:0.6155081627874261\n",
      "train loss:0.39516522638943846\n",
      "train loss:0.6158589811938544\n",
      "train loss:0.9627204344369964\n",
      "train loss:0.6223475699924554\n",
      "train loss:0.8142082316299202\n",
      "train loss:0.6069586219420539\n",
      "train loss:0.6042768479718864\n",
      "train loss:0.7753151117627454\n",
      "train loss:0.6035443171881243\n",
      "train loss:0.6111075451919427\n",
      "train loss:0.6157244768746354\n",
      "train loss:0.6813133248298078\n",
      "train loss:0.6258117253439044\n",
      "train loss:0.5563020820241682\n",
      "train loss:0.6693832075484971\n",
      "train loss:0.6263925061167268\n",
      "train loss:0.6328671557092233\n",
      "train loss:0.5696183884927342\n",
      "train loss:0.5665605070016508\n",
      "train loss:0.5063794567642472\n",
      "train loss:0.4786280458261786\n",
      "train loss:0.5351745778373846\n",
      "train loss:0.6071817507115024\n",
      "train loss:0.6184634954643573\n",
      "train loss:0.7854676173846832\n",
      "train loss:0.8038335347930825\n",
      "train loss:0.6081344810839814\n",
      "train loss:0.6186515768528367\n",
      "train loss:0.6148267767815044\n",
      "train loss:0.7947588081748209\n",
      "train loss:0.6857060795112722\n",
      "train loss:0.34828078730133305\n",
      "train loss:0.4436007625660934\n",
      "train loss:0.7123835864567124\n",
      "train loss:0.42231828988641296\n",
      "train loss:0.8937913806795089\n",
      "train loss:0.7052054659034904\n",
      "train loss:0.42143567136823695\n",
      "train loss:0.602570447835055\n",
      "train loss:0.6041157334899451\n",
      "train loss:0.6116981040648387\n",
      "train loss:0.6148063333207635\n",
      "train loss:0.6163145910403475\n",
      "train loss:0.6023435567997417\n",
      "train loss:0.49881911430810566\n",
      "train loss:0.5195373392187234\n",
      "train loss:0.6005171262046952\n",
      "train loss:0.4174842905662766\n",
      "train loss:0.6064431913110466\n",
      "train loss:0.5140384286719126\n",
      "train loss:0.6032279223849917\n",
      "train loss:0.4002834712786368\n",
      "train loss:0.38100955154844934\n",
      "train loss:0.5066862418123986\n",
      "train loss:0.7566252886291217\n",
      "train loss:0.914288043718108\n",
      "train loss:0.8576548540352908\n",
      "train loss:0.7123868011447514\n",
      "train loss:0.8011688488517018\n",
      "train loss:0.32755223971363884\n",
      "train loss:0.7022806014936109\n",
      "train loss:0.6238780950440391\n",
      "train loss:0.6931392240258814\n",
      "train loss:0.6097697383294423\n",
      "train loss:0.6904506498957927\n",
      "train loss:0.4898893917234208\n",
      "train loss:0.5517726119896569\n",
      "train loss:0.5501645648219387\n",
      "train loss:0.6175589527845784\n",
      "train loss:0.5453919680803867\n",
      "train loss:0.6769545490786294\n",
      "train loss:0.7486584541386799\n",
      "train loss:0.5450197118768518\n",
      "train loss:0.6196153680540288\n",
      "train loss:0.6050593266831313\n",
      "train loss:0.6186231155101403\n",
      "train loss:0.46585921742625347\n",
      "train loss:0.6900413090288401\n",
      "train loss:0.6869230961846869\n",
      "train loss:0.6189067206637322\n",
      "train loss:0.43914737375133583\n",
      "train loss:0.6163524803495625\n",
      "train loss:0.6907764824732341\n",
      "train loss:0.6044250029426567\n",
      "train loss:0.6949832057166708\n",
      "train loss:0.5152342093323401\n",
      "train loss:0.7080928768400142\n",
      "train loss:0.8701792253131349\n",
      "train loss:0.7050098019444067\n",
      "train loss:0.6949743465750917\n",
      "train loss:0.5346746534485557\n",
      "train loss:0.6129235852268641\n",
      "train loss:0.6127894961825091\n",
      "train loss:0.482921825399956\n",
      "train loss:0.5412020799412918\n",
      "train loss:0.5992903746590177\n",
      "train loss:0.7596634623909163\n",
      "train loss:0.6703999592179559\n",
      "train loss:0.532100478765295\n",
      "train loss:0.6109617000737554\n",
      "train loss:0.6861560106362681\n",
      "train loss:0.5390234730454861\n",
      "train loss:0.6955494648207369\n",
      "train loss:0.690528209775964\n",
      "train loss:0.6840843521239113\n",
      "train loss:0.5362809194231906\n",
      "train loss:0.6012321007577248\n",
      "train loss:0.6126609921077464\n",
      "train loss:0.5238207053776355\n",
      "train loss:0.5297719997287781\n",
      "train loss:0.5373787142583698\n",
      "train loss:0.5137134319340279\n",
      "train loss:0.3143073522534049\n",
      "train loss:0.7105333699150354\n",
      "train loss:0.3925948238771476\n",
      "train loss:0.629399113787831\n",
      "train loss:0.8684732762706753\n",
      "train loss:0.3539097324856001\n",
      "train loss:0.7411927651810626\n",
      "train loss:0.49893560841540713\n",
      "train loss:0.4898363136528081\n",
      "train loss:0.7387901552319588\n",
      "train loss:0.6232494003026238\n",
      "train loss:0.7266132791813524\n",
      "train loss:0.6092417419987013\n",
      "train loss:0.7164824912441995\n",
      "train loss:0.5129296151048981\n",
      "train loss:0.6035141814456864\n",
      "train loss:0.6168656489796657\n",
      "train loss:0.6178690361938098\n",
      "train loss:0.534078672538554\n",
      "train loss:0.614509488894776\n",
      "train loss:0.6066025017660897\n",
      "train loss:0.5252480931624175\n",
      "train loss:0.6102619015610131\n",
      "train loss:0.7638780469585406\n",
      "train loss:0.6819015556783913\n",
      "train loss:0.46377925336685155\n",
      "train loss:0.747004962331028\n",
      "train loss:0.6722897134828616\n",
      "train loss:0.6851948948166544\n",
      "train loss:0.5429632492729856\n",
      "train loss:0.6154618001947283\n",
      "train loss:0.6018375704864493\n",
      "train loss:0.7451546631897851\n",
      "train loss:0.745915560568596\n",
      "train loss:0.546841143962483\n",
      "train loss:0.6799787566760601\n",
      "train loss:0.7336282519664098\n",
      "train loss:0.43087530772860916\n",
      "train loss:0.7458790326847484\n",
      "train loss:0.6770995814810472\n",
      "train loss:0.604437093709224\n",
      "train loss:0.674306038992639\n",
      "train loss:0.7915718447926257\n",
      "train loss:0.6069786791980197\n",
      "train loss:0.498766150979766\n",
      "train loss:0.5468224412485041\n",
      "train loss:0.7453020591074708\n",
      "train loss:0.42815939800749936\n",
      "train loss:0.4659322042254904\n",
      "train loss:0.6246887449991476\n",
      "train loss:0.6873205370216121\n",
      "train loss:0.5278518034948726\n",
      "train loss:0.6961643424039299\n",
      "train loss:0.7830073430763624\n",
      "train loss:0.5188740807135749\n",
      "train loss:0.5107646312336385\n",
      "train loss:0.4129185815063833\n",
      "train loss:0.5938965440644297\n",
      "train loss:0.6136718896071123\n",
      "train loss:0.7126290518094474\n",
      "train loss:0.3843420979972555\n",
      "train loss:0.742302295880034\n",
      "train loss:0.6190716500317459\n",
      "train loss:0.7300093879182011\n",
      "train loss:0.626909122540516\n",
      "train loss:0.5148424378159202\n",
      "train loss:0.8135642666736661\n",
      "train loss:0.6060594533929684\n",
      "train loss:0.7948777843930055\n",
      "train loss:0.4514381749221183\n",
      "train loss:0.6095359692825731\n",
      "train loss:0.5318725213115937\n",
      "train loss:0.5392571042746799\n",
      "train loss:0.538254207908801\n",
      "train loss:0.5979616798999291\n",
      "train loss:0.8143832307793557\n",
      "train loss:0.5346573947211068\n",
      "train loss:0.45550407793238873\n",
      "train loss:0.6086121312642462\n",
      "train loss:0.7683946236411517\n",
      "train loss:0.613976993315358\n",
      "train loss:0.6937420045698265\n",
      "train loss:0.6612540804782077\n",
      "train loss:0.44770328489167693\n",
      "train loss:0.60922873555373\n",
      "train loss:0.7543863983675203\n",
      "train loss:0.8982685973920175\n",
      "train loss:0.4684326862874286\n",
      "train loss:0.45739189879150616\n",
      "train loss:0.8281994132445739\n",
      "train loss:0.3946751816012293\n",
      "train loss:0.6922238453947994\n",
      "train loss:0.5168862108119756\n",
      "train loss:0.6122653519105492\n",
      "train loss:0.6182318728263011\n",
      "train loss:0.6224998590911203\n",
      "train loss:0.6104495843923513\n",
      "train loss:0.4180103424598561\n",
      "train loss:0.42441876646716326\n",
      "train loss:0.6180704949008121\n",
      "train loss:0.8290414535165794\n",
      "train loss:0.5074615259377236\n",
      "train loss:0.5202298217779453\n",
      "train loss:0.5993336200112169\n",
      "train loss:0.5957563499526619\n",
      "train loss:0.92990048387624\n",
      "train loss:0.711652017660161\n",
      "train loss:0.5953388374115773\n",
      "train loss:0.6986688687603035\n",
      "train loss:0.5275626438829111\n",
      "train loss:0.6809517699972449\n",
      "train loss:0.5247117397599265\n",
      "train loss:0.6890072438533855\n",
      "train loss:0.3861859785605794\n",
      "train loss:0.6053345220658264\n",
      "train loss:0.4451048175654491\n",
      "train loss:0.7649211489846189\n",
      "train loss:0.674233538521233\n",
      "train loss:0.4436428479038608\n",
      "train loss:0.6988529224789651\n",
      "train loss:0.41923320880010345\n",
      "train loss:0.5850473658874114\n",
      "train loss:0.5924602311391259\n",
      "train loss:0.6033111837242959\n",
      "train loss:0.6213817040895067\n",
      "train loss:0.6083070862399691\n",
      "train loss:0.6373140566552855\n",
      "train loss:0.4140453651152141\n",
      "train loss:0.8244734942117071\n",
      "train loss:0.5237630420447081\n",
      "train loss:0.5115041479842395\n",
      "train loss:0.5133154052469859\n",
      "train loss:0.5118927465002819\n",
      "train loss:0.5891309354153375\n",
      "train loss:0.7261006515568071\n",
      "train loss:0.5068652202800372\n",
      "train loss:0.3910023557343992\n",
      "train loss:0.4033419972903204\n",
      "train loss:0.4938424068731397\n",
      "train loss:0.6223603651831222\n",
      "train loss:0.6513212704987112\n",
      "train loss:0.6289833908990767\n",
      "train loss:0.3687796765407536\n",
      "train loss:0.8603013700837996\n",
      "train loss:1.0810827897141166\n",
      "train loss:0.6927390057106976\n",
      "train loss:0.4218409494657937\n",
      "train loss:0.4336209047398315\n",
      "train loss:0.44304624363319933\n",
      "train loss:0.42252593050020637\n",
      "train loss:0.6080406719994584\n",
      "train loss:0.6272398088845013\n",
      "train loss:0.39675989847540183\n",
      "train loss:0.6945487595335986\n",
      "train loss:0.5080231110699619\n",
      "train loss:0.4046359215231772\n",
      "train loss:0.3963230976027795\n",
      "train loss:0.5156395067137939\n",
      "train loss:0.6886628756505498\n",
      "train loss:0.5150352396681319\n",
      "train loss:0.7702880178934524\n",
      "train loss:0.37494075409408056\n",
      "train loss:0.626611633225824\n",
      "train loss:0.4812541305972716\n",
      "train loss:0.7351722968411462\n",
      "train loss:0.6003651705789289\n",
      "train loss:0.6073984864924482\n",
      "train loss:0.37911018660000045\n",
      "train loss:0.5068515138901116\n",
      "train loss:0.7435478874361432\n",
      "train loss:0.6054565183055336\n",
      "train loss:0.5966116795074852\n",
      "train loss:0.4044967670988446\n",
      "train loss:0.5735436345418233\n",
      "train loss:0.6253076224248098\n",
      "train loss:0.5096195949484055\n",
      "train loss:0.49423893302127625\n",
      "train loss:0.607342268651266\n",
      "train loss:0.4039720490232111\n",
      "train loss:0.8310187426779094\n",
      "train loss:0.5040703594933087\n",
      "train loss:0.6943378888339853\n",
      "train loss:0.6166497363580303\n",
      "train loss:0.6074852653900121\n",
      "train loss:0.5882408345733383\n",
      "train loss:0.5182650957587512\n",
      "train loss:0.6147712118582713\n",
      "train loss:0.5384632633423635\n",
      "train loss:0.41715531593417376\n",
      "train loss:0.6911030421233836\n",
      "train loss:0.509956627915096\n",
      "train loss:0.5082343800711693\n",
      "train loss:0.6149115157678191\n",
      "train loss:0.5214851837729356\n",
      "train loss:0.7254269419586248\n",
      "train loss:0.3925196487205854\n",
      "train loss:0.4977581864009283\n",
      "train loss:0.6283612242231874\n",
      "train loss:0.7168715674510697\n",
      "train loss:0.36462927887892926\n",
      "train loss:0.5000946340415354\n",
      "train loss:0.7589026901271475\n",
      "train loss:0.7196152212971387\n",
      "train loss:0.559811446822277\n",
      "train loss:0.49667458365175643\n",
      "train loss:0.6123592689852467\n",
      "train loss:0.6134372970913275\n",
      "train loss:0.7272294194191607\n",
      "train loss:0.603115050290672\n",
      "train loss:0.5214839919837342\n",
      "train loss:0.5969081191970541\n",
      "train loss:0.5248990819759706\n",
      "train loss:0.4306952967305427\n",
      "train loss:0.6048753097660459\n",
      "train loss:0.5190587771467797\n",
      "train loss:0.4232155454143182\n",
      "train loss:0.5160020891681149\n",
      "train loss:0.39884088305272647\n",
      "train loss:0.6284400096435019\n",
      "train loss:0.608061991290856\n",
      "train loss:0.5095737664960184\n",
      "train loss:0.621262799156091\n",
      "train loss:0.8826263868532521\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6617647058823529\n"
     ]
    }
   ],
   "source": [
    "from mymethod.neural_network import *\n",
    "from common.trainer import Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet3Layer(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "049ff570-9180-44a6-9673-69c3702f9c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.0262690777760453\n",
      "=== epoch:1, train acc:0.7, test acc:0.63 ===\n",
      "train loss:2.2404881357291053\n",
      "train loss:2.2015569707436873\n",
      "train loss:1.0344168206949351\n",
      "train loss:2.039315841224476\n",
      "train loss:1.7942754423420535\n",
      "train loss:2.2715692297677004\n",
      "train loss:1.7332602523752136\n",
      "train loss:1.8409789384577884\n",
      "train loss:2.145710456817768\n",
      "train loss:1.5033178010315893\n",
      "train loss:0.9963202274157072\n",
      "train loss:1.9589717212554336\n",
      "train loss:1.5360352247413818\n",
      "train loss:1.8408652696529295\n",
      "train loss:1.288656168239756\n",
      "train loss:1.4911821528330387\n",
      "train loss:1.377948420091246\n",
      "train loss:1.6725964312862736\n",
      "train loss:2.0937796514885942\n",
      "train loss:1.6056728478574183\n",
      "train loss:2.1782459406937797\n",
      "train loss:1.2814115925846095\n",
      "train loss:1.7331143333321013\n",
      "train loss:1.843718259594624\n",
      "train loss:1.4565325049523508\n",
      "train loss:1.9646825134432384\n",
      "train loss:2.0432454242580986\n",
      "train loss:0.9798496749767966\n",
      "train loss:1.7561495437797046\n",
      "train loss:2.026430854368071\n",
      "train loss:1.6972630685055328\n",
      "train loss:1.4845119078097833\n",
      "train loss:2.3984018241804392\n",
      "train loss:1.6652104567373425\n",
      "train loss:1.8887568670280306\n",
      "train loss:1.5381463508623912\n",
      "train loss:1.7041279462226384\n",
      "train loss:1.1932142288646357\n",
      "train loss:1.4464268913038971\n",
      "train loss:1.930256553897283\n",
      "train loss:1.7279281675887972\n",
      "train loss:1.7102277996622708\n",
      "train loss:1.693459052334902\n",
      "train loss:2.020026325729938\n",
      "train loss:1.3443325446293684\n",
      "train loss:1.3929848224780008\n",
      "train loss:1.3937379567285721\n",
      "train loss:1.7320179476928708\n",
      "train loss:1.3054854152081474\n",
      "train loss:1.3066686968259693\n",
      "train loss:1.6649762792991456\n",
      "train loss:0.8427106830527007\n",
      "train loss:1.383279194877376\n",
      "train loss:1.2836499787758562\n",
      "train loss:0.8796312821453075\n",
      "train loss:1.5216949228446688\n",
      "train loss:1.6933195479900776\n",
      "train loss:1.0476355467026346\n",
      "train loss:1.8832099712002275\n",
      "train loss:1.483225624053404\n",
      "train loss:1.0098279187735748\n",
      "train loss:2.0588469851229827\n",
      "train loss:1.284695480649223\n",
      "train loss:2.0498164697151604\n",
      "train loss:1.4987088287569768\n",
      "train loss:1.3865470616381068\n",
      "train loss:1.7114938841440974\n",
      "train loss:1.7416571027341743\n",
      "train loss:1.9048475225080588\n",
      "train loss:0.9838990438967757\n",
      "train loss:1.567273380186656\n",
      "train loss:2.037644960904744\n",
      "train loss:1.2761161682362414\n",
      "train loss:1.4938394171320026\n",
      "train loss:1.3133871627043228\n",
      "train loss:1.1492105465013962\n",
      "train loss:1.2271864255089302\n",
      "train loss:1.636688458940905\n",
      "train loss:1.2478000894967916\n",
      "train loss:1.382779909223815\n",
      "train loss:2.008892742215151\n",
      "train loss:1.4090275168541466\n",
      "train loss:1.8684303778180762\n",
      "train loss:1.6046070914784702\n",
      "train loss:1.2629319911772874\n",
      "train loss:1.9146644257645762\n",
      "train loss:1.552910863165701\n",
      "train loss:1.51105123198118\n",
      "train loss:1.2621677012216477\n",
      "train loss:1.537898101282162\n",
      "train loss:1.6203422700702574\n",
      "train loss:1.2683521721527948\n",
      "train loss:1.01890868749087\n",
      "train loss:1.500386105254829\n",
      "train loss:1.1101447730623202\n",
      "train loss:1.327884126004782\n",
      "train loss:1.562133944955613\n",
      "train loss:1.9406961751199538\n",
      "train loss:1.5119466746246275\n",
      "train loss:1.441006801367592\n",
      "train loss:1.1514120878249297\n",
      "train loss:1.6102440385793717\n",
      "train loss:1.7296785300144761\n",
      "train loss:0.8691376305684253\n",
      "train loss:1.682292664897372\n",
      "train loss:1.117168497087642\n",
      "train loss:2.12904553661938\n",
      "train loss:1.1279802746667975\n",
      "train loss:1.3802409774491629\n",
      "train loss:1.1497398772327674\n",
      "train loss:1.150021024208152\n",
      "train loss:1.7593885828637414\n",
      "train loss:1.867097419791623\n",
      "train loss:0.946328675134238\n",
      "train loss:1.77745104185492\n",
      "train loss:1.823369612706061\n",
      "train loss:1.512795453033644\n",
      "train loss:1.4353196064722389\n",
      "train loss:1.5444513248540888\n",
      "train loss:1.1844850629220585\n",
      "train loss:1.3007181789157083\n",
      "train loss:1.6031036782565997\n",
      "train loss:1.0698137026927257\n",
      "train loss:1.1149115303034476\n",
      "train loss:1.3700185001400291\n",
      "train loss:1.6740013340293984\n",
      "train loss:1.5798761423156182\n",
      "train loss:1.106987363196529\n",
      "train loss:1.453253932217032\n",
      "train loss:1.293538648451896\n",
      "train loss:1.1062634913626663\n",
      "train loss:1.6581919188178127\n",
      "train loss:1.9848508209079356\n",
      "train loss:1.2474001999564566\n",
      "train loss:1.8250124496701479\n",
      "train loss:1.1874081731501527\n",
      "train loss:2.08078437319596\n",
      "train loss:1.088474075987945\n",
      "train loss:1.4660914234991265\n",
      "train loss:1.5214756864488956\n",
      "train loss:1.2147011405569135\n",
      "train loss:1.8752581700061242\n",
      "train loss:1.4545049936466903\n",
      "train loss:1.6967208575047164\n",
      "train loss:1.5868264188381649\n",
      "train loss:1.4694927999320557\n",
      "train loss:1.25375657337728\n",
      "train loss:1.7056269817497411\n",
      "train loss:1.708820830219467\n",
      "train loss:0.9262631690297731\n",
      "train loss:1.292823936306377\n",
      "train loss:1.5990481458418342\n",
      "train loss:1.6298399624488238\n",
      "train loss:1.1227754579015374\n",
      "train loss:1.9645532495773008\n",
      "train loss:1.2104840664319019\n",
      "train loss:1.6896091452871118\n",
      "train loss:1.5705507759642594\n",
      "train loss:1.521561800255341\n",
      "train loss:1.5459733318469326\n",
      "train loss:0.723412509796731\n",
      "train loss:0.7691438212728017\n",
      "train loss:1.6093799656381216\n",
      "train loss:1.081366181704394\n",
      "train loss:1.376727302871203\n",
      "train loss:1.6531952677471604\n",
      "train loss:2.000174251222338\n",
      "train loss:1.246497334713067\n",
      "train loss:1.5255643225235764\n",
      "train loss:1.317462199229751\n",
      "train loss:1.150483611487885\n",
      "train loss:1.4282454034653376\n",
      "train loss:1.5286205686280352\n",
      "train loss:1.6519396049083384\n",
      "train loss:1.4397224792813632\n",
      "train loss:1.8001637390828666\n",
      "train loss:1.5694023537017663\n",
      "train loss:1.411774541409026\n",
      "train loss:1.4071598821104405\n",
      "train loss:1.7405157015941106\n",
      "train loss:1.9725530836249159\n",
      "train loss:1.5070106356297328\n",
      "train loss:1.2381305112446772\n",
      "train loss:1.323974422352501\n",
      "train loss:1.3864284077858096\n",
      "train loss:2.0345370387596313\n",
      "train loss:1.5887776777939826\n",
      "train loss:1.343580856006041\n",
      "train loss:2.2286807079375395\n",
      "train loss:1.8006518028206677\n",
      "train loss:1.9294989300085885\n",
      "train loss:2.0802064392851944\n",
      "train loss:1.5264510294032692\n",
      "train loss:1.576550117904612\n",
      "train loss:1.5139617805665702\n",
      "train loss:1.790577115168507\n",
      "train loss:2.081228463176033\n",
      "train loss:1.630102342149304\n",
      "train loss:1.5560040669022053\n",
      "train loss:1.9030334059191325\n",
      "train loss:1.1767109004643237\n",
      "train loss:1.5607071425815173\n",
      "train loss:1.4875387971559741\n",
      "train loss:1.8723579208346064\n",
      "train loss:1.816254341923114\n",
      "train loss:1.0907553586636016\n",
      "train loss:1.842512175876628\n",
      "train loss:1.609541095454319\n",
      "train loss:1.258777286233514\n",
      "train loss:1.567436348179279\n",
      "train loss:1.2957447843772016\n",
      "train loss:1.8847394918188258\n",
      "train loss:1.021425930090466\n",
      "train loss:1.5985217825578961\n",
      "train loss:1.0669697971351044\n",
      "train loss:1.4803804938731702\n",
      "train loss:1.9773923056432365\n",
      "train loss:1.6552982866633628\n",
      "train loss:0.882992602381683\n",
      "train loss:1.5597253808550875\n",
      "train loss:1.8800661984642038\n",
      "train loss:1.520969434114642\n",
      "train loss:1.9548160779718695\n",
      "train loss:1.3061476991123102\n",
      "train loss:1.4988047519200483\n",
      "train loss:1.8175118612959351\n",
      "train loss:1.4677121646224853\n",
      "train loss:1.0296905280506519\n",
      "train loss:1.6075162170433486\n",
      "train loss:1.4742641507962024\n",
      "train loss:1.275183666672377\n",
      "train loss:1.6872223796072767\n",
      "train loss:1.3583594012930118\n",
      "train loss:1.7125950235173455\n",
      "train loss:1.9645130023598714\n",
      "train loss:1.521289041921123\n",
      "train loss:1.5859826879813588\n",
      "train loss:0.9369276245857284\n",
      "train loss:1.4025814375384498\n",
      "train loss:1.3460916917955132\n",
      "train loss:1.5569666787399445\n",
      "train loss:1.3522169662754515\n",
      "train loss:1.5144909010298164\n",
      "train loss:1.1995210745178162\n",
      "train loss:1.5064179943913456\n",
      "train loss:1.639142838053941\n",
      "train loss:1.3087029560038856\n",
      "train loss:1.7886113436056672\n",
      "train loss:1.793932859868378\n",
      "train loss:1.4990609198825404\n",
      "train loss:1.5328496581689703\n",
      "train loss:1.5191181078514666\n",
      "train loss:1.2106167617520618\n",
      "train loss:1.4261244627316603\n",
      "train loss:1.2292099845233437\n",
      "train loss:1.5409746002095175\n",
      "train loss:1.634117779384551\n",
      "train loss:1.0936758818345418\n",
      "train loss:1.4543367343824798\n",
      "train loss:1.3541490755413053\n",
      "train loss:1.2293217330814707\n",
      "train loss:1.3271785621316725\n",
      "train loss:1.4213426685141073\n",
      "train loss:1.8316663635375678\n",
      "train loss:1.2065053013359657\n",
      "train loss:1.2480871704686942\n",
      "train loss:1.4751248300750783\n",
      "train loss:1.360757752275434\n",
      "train loss:1.4166312558957999\n",
      "train loss:1.9843692596242382\n",
      "train loss:1.5544144906741744\n",
      "train loss:1.927777120392862\n",
      "train loss:1.1044163353434397\n",
      "train loss:0.7910874904145891\n",
      "train loss:1.4166864441727962\n",
      "train loss:1.3406995727395432\n",
      "train loss:1.1036868425117592\n",
      "train loss:1.7915435054142943\n",
      "train loss:0.9206097135711984\n",
      "train loss:1.4184245888086193\n",
      "train loss:1.4491452057969698\n",
      "train loss:1.2152463072557969\n",
      "train loss:1.5686201038315106\n",
      "train loss:1.667035296736082\n",
      "train loss:1.5533206244606312\n",
      "train loss:1.0884850419258068\n",
      "train loss:1.083001673607225\n",
      "train loss:1.2491633541726208\n",
      "train loss:1.2148321737312315\n",
      "train loss:1.351527161748569\n",
      "train loss:1.7191077859702013\n",
      "train loss:1.62497298214739\n",
      "train loss:1.7138299443002705\n",
      "train loss:1.894463033647252\n",
      "train loss:1.3721898141045639\n",
      "train loss:1.412381751792585\n",
      "train loss:1.2409877014691986\n",
      "train loss:1.8212396245727251\n",
      "train loss:1.4603062709007435\n",
      "train loss:1.4740863696363067\n",
      "train loss:1.321127350212954\n",
      "train loss:1.5829153968167013\n",
      "train loss:1.1215902731789094\n",
      "train loss:1.404471960091638\n",
      "train loss:1.0179668216376787\n",
      "train loss:1.9093429774028088\n",
      "train loss:1.5381878505533275\n",
      "train loss:1.4905871529648298\n",
      "train loss:1.3093272507642726\n",
      "train loss:0.7769134955632865\n",
      "train loss:1.2126459441657018\n",
      "train loss:1.2880886466341468\n",
      "train loss:1.007966403051069\n",
      "train loss:1.2784427677555623\n",
      "train loss:1.190448435472982\n",
      "train loss:1.768333419414145\n",
      "train loss:1.5941647522046287\n",
      "train loss:1.707453676758148\n",
      "train loss:0.9922612266238418\n",
      "train loss:1.0523108253434947\n",
      "train loss:1.6709079624333811\n",
      "train loss:1.252136102591814\n",
      "train loss:1.570526113500555\n",
      "train loss:1.2980869007393894\n",
      "train loss:1.1830205396755065\n",
      "train loss:1.1515020889568315\n",
      "train loss:1.657867196615174\n",
      "train loss:1.3942013020983435\n",
      "train loss:1.5333631207948233\n",
      "train loss:1.439749901284378\n",
      "train loss:1.1348423511850814\n",
      "train loss:1.3665341704998961\n",
      "train loss:1.6100335131294012\n",
      "train loss:1.5804277751029976\n",
      "train loss:1.3792393513745942\n",
      "train loss:2.0266027742959554\n",
      "train loss:1.3631698022750869\n",
      "train loss:2.0280116812101974\n",
      "train loss:1.59206262039617\n",
      "train loss:1.5964686944718944\n",
      "train loss:1.5341745827017834\n",
      "train loss:1.7220817169328704\n",
      "train loss:1.3122076400834808\n",
      "train loss:0.7279646718936459\n",
      "train loss:1.3914713572920605\n",
      "train loss:1.5793529978567018\n",
      "train loss:1.3785732898790821\n",
      "train loss:1.8457013437656307\n",
      "train loss:1.7460702448784151\n",
      "train loss:0.9343876952551312\n",
      "train loss:1.1063446126503798\n",
      "train loss:1.939865321732579\n",
      "train loss:1.658913264159859\n",
      "train loss:1.5350090778451757\n",
      "train loss:1.5055298744531354\n",
      "train loss:1.5507899844240862\n",
      "train loss:1.628146066565962\n",
      "train loss:1.2135687563590145\n",
      "train loss:1.1801822468498453\n",
      "train loss:1.249752602140949\n",
      "train loss:1.3937416202693187\n",
      "train loss:1.3988195899294493\n",
      "train loss:1.562145162140669\n",
      "train loss:1.6378248416981396\n",
      "train loss:1.2308436107789016\n",
      "train loss:1.4710287437169085\n",
      "train loss:1.0228577715002511\n",
      "train loss:1.4274772868424297\n",
      "train loss:1.8082743072131517\n",
      "train loss:1.3790466076899726\n",
      "train loss:1.2544209925465661\n",
      "train loss:0.6900235811356843\n",
      "train loss:1.6629932197624868\n",
      "train loss:0.7897970033776154\n",
      "train loss:1.3911524233582981\n",
      "train loss:1.3648279627910924\n",
      "train loss:1.463037091493678\n",
      "train loss:1.5296563592859127\n",
      "train loss:0.7417805747097488\n",
      "train loss:1.2573717570891794\n",
      "train loss:0.8588893294632122\n",
      "train loss:1.4526921334843717\n",
      "train loss:1.4738032769519276\n",
      "train loss:1.8436023390515452\n",
      "train loss:1.1273633455031855\n",
      "train loss:1.2381916226954168\n",
      "train loss:1.5325416879949862\n",
      "train loss:1.8676420320667884\n",
      "train loss:1.8076958324720689\n",
      "train loss:1.571830463437325\n",
      "train loss:1.3249805948900468\n",
      "train loss:1.1477191926661967\n",
      "train loss:1.4653617073416652\n",
      "train loss:1.6793408992620051\n",
      "train loss:1.7206713597049395\n",
      "train loss:1.3160371367779975\n",
      "train loss:1.2003704152151295\n",
      "train loss:0.8825999050227068\n",
      "train loss:1.383705409969759\n",
      "train loss:1.3695467506469798\n",
      "train loss:1.5992698919752595\n",
      "train loss:0.8324516053562656\n",
      "train loss:1.1695838810710726\n",
      "train loss:1.4228395927081834\n",
      "train loss:1.1396187406332656\n",
      "train loss:1.4613545937138823\n",
      "train loss:1.4951376386006938\n",
      "train loss:1.3018312570423494\n",
      "train loss:1.55198318732769\n",
      "train loss:1.132352131112585\n",
      "train loss:1.7353092603098\n",
      "train loss:1.3225481337536413\n",
      "train loss:1.311187077785662\n",
      "train loss:1.5601210246525907\n",
      "train loss:1.7379489962148262\n",
      "train loss:1.6062853265995263\n",
      "train loss:1.7144991531879974\n",
      "train loss:1.6972377945844723\n",
      "train loss:1.5206853620422667\n",
      "train loss:1.4056717873733258\n",
      "train loss:1.4609586698912616\n",
      "train loss:1.297236438110073\n",
      "train loss:1.3863851321977116\n",
      "train loss:1.9673887115828035\n",
      "train loss:0.9205563214423869\n",
      "train loss:1.3849507634037523\n",
      "train loss:1.3771432458081223\n",
      "train loss:1.1436325538671397\n",
      "train loss:1.645444476933643\n",
      "train loss:1.7493892504650685\n",
      "train loss:1.4337067304323352\n",
      "train loss:1.5339745766624717\n",
      "train loss:1.5167502833556397\n",
      "train loss:1.6741128772023532\n",
      "train loss:0.8434013880225271\n",
      "train loss:1.3269714014005958\n",
      "train loss:1.3905097434187426\n",
      "train loss:1.8063686343420025\n",
      "train loss:1.800149517517453\n",
      "train loss:1.4445973631748703\n",
      "train loss:1.3723849373195924\n",
      "train loss:1.1667767903633777\n",
      "train loss:1.7192584991480715\n",
      "train loss:1.4184311745393865\n",
      "train loss:1.9048892445658911\n",
      "train loss:1.0877787678848985\n",
      "train loss:1.002029624686795\n",
      "train loss:1.1387206346326368\n",
      "train loss:1.843059715560201\n",
      "train loss:1.5570010669511032\n",
      "train loss:1.8752646792240486\n",
      "train loss:1.5346964530277467\n",
      "train loss:1.4059296638590595\n",
      "train loss:1.2587248227453565\n",
      "train loss:1.6948573958063489\n",
      "train loss:1.3616610057267038\n",
      "train loss:1.8379061869336013\n",
      "train loss:1.198036618796041\n",
      "train loss:1.542862899104153\n",
      "train loss:1.4827180160172495\n",
      "train loss:1.5818170938070302\n",
      "train loss:1.2876198257907714\n",
      "train loss:1.6250615203328684\n",
      "train loss:1.5321102272480673\n",
      "train loss:1.5969892180806906\n",
      "train loss:1.5531025893375383\n",
      "train loss:1.6952665836769651\n",
      "train loss:1.8423915953678525\n",
      "train loss:2.137073517855987\n",
      "train loss:1.3710378800704688\n",
      "train loss:1.772471367251158\n",
      "train loss:1.5432212271827779\n",
      "train loss:1.515371755300101\n",
      "train loss:1.714750892844498\n",
      "train loss:1.3077886853000233\n",
      "train loss:1.6802512442838489\n",
      "train loss:1.0956695058729653\n",
      "train loss:1.2461721676441728\n",
      "train loss:1.3235459275050292\n",
      "train loss:1.5373599683917394\n",
      "train loss:1.2017896690451102\n",
      "train loss:1.3443534937326291\n",
      "train loss:1.4617408614345708\n",
      "train loss:1.0741765537194783\n",
      "train loss:1.4845357620264954\n",
      "train loss:1.6544650311811417\n",
      "train loss:1.6804381313332208\n",
      "train loss:1.4468016568397446\n",
      "train loss:1.729540416628652\n",
      "train loss:1.0035170431862608\n",
      "train loss:1.6822236233875114\n",
      "train loss:1.55992764999058\n",
      "train loss:1.710307955588189\n",
      "train loss:0.5365269610096807\n",
      "train loss:1.4160369982504313\n",
      "train loss:1.6145772904938571\n",
      "train loss:1.3544872466279347\n",
      "train loss:1.7634707999319104\n",
      "train loss:1.4382688924557674\n",
      "train loss:1.475531484889721\n",
      "train loss:1.7500761473014095\n",
      "train loss:1.5042956739270033\n",
      "train loss:1.8879782581783622\n",
      "train loss:1.714214479440941\n",
      "train loss:1.4462979352606509\n",
      "train loss:1.4955028434942066\n",
      "train loss:1.4041846064633738\n",
      "train loss:1.1181832996907806\n",
      "train loss:1.3888852777727554\n",
      "train loss:1.2569360553688251\n",
      "train loss:1.3898911742106506\n",
      "train loss:1.1361170773856866\n",
      "train loss:1.6145156220194798\n",
      "train loss:0.9089055436574242\n",
      "train loss:1.7609893964216365\n",
      "train loss:1.472282481840476\n",
      "train loss:1.6129134389399489\n",
      "train loss:1.478691532512356\n",
      "train loss:1.289452254211661\n",
      "train loss:1.6733925795597835\n",
      "train loss:1.4415358031231333\n",
      "train loss:1.5204492388766173\n",
      "train loss:1.5747501735166312\n",
      "train loss:1.4706191162879934\n",
      "train loss:1.2823021247556254\n",
      "train loss:1.237630707041555\n",
      "train loss:1.7654101361062455\n",
      "train loss:1.5744711662562323\n",
      "train loss:1.573993916701375\n",
      "train loss:1.2284430147258854\n",
      "train loss:1.3808337932470924\n",
      "train loss:1.4970387597378527\n",
      "train loss:1.5119243066294947\n",
      "train loss:1.2444743250090817\n",
      "train loss:1.2846887753639158\n",
      "train loss:1.4654092048805347\n",
      "train loss:1.083587515080041\n",
      "train loss:1.1662772632690723\n",
      "train loss:1.1663735861084086\n",
      "train loss:1.4329256481522537\n",
      "train loss:1.7338644408289448\n",
      "train loss:1.5189461383674183\n",
      "train loss:1.3740441589764436\n",
      "train loss:1.2163246225894542\n",
      "train loss:1.9134491213013838\n",
      "train loss:1.3779342667569874\n",
      "train loss:1.2292859988517144\n",
      "train loss:1.1307193195793608\n",
      "train loss:1.4355122119319499\n",
      "train loss:1.6800100001355478\n",
      "train loss:1.1740973471805687\n",
      "train loss:1.2818220590511278\n",
      "train loss:1.0210825400998522\n",
      "train loss:1.647590064519171\n",
      "train loss:1.6982821406891244\n",
      "train loss:1.265332222543686\n",
      "train loss:1.659764582237909\n",
      "train loss:1.630737700049659\n",
      "train loss:1.2471575542300617\n",
      "train loss:1.3936695869000608\n",
      "train loss:1.5132411633339384\n",
      "train loss:1.64281845887153\n",
      "train loss:1.9443774051958873\n",
      "train loss:1.9347645890610448\n",
      "train loss:1.1281897668263297\n",
      "train loss:1.6229595958685128\n",
      "train loss:1.0285798918567644\n",
      "train loss:1.443208822020575\n",
      "train loss:1.2227041465872648\n",
      "train loss:1.797108044341917\n",
      "train loss:1.2005927851383325\n",
      "train loss:1.650637113366664\n",
      "train loss:1.1688109924482872\n",
      "train loss:1.971501696435562\n",
      "train loss:1.420531804897299\n",
      "train loss:1.3910085488144581\n",
      "train loss:1.3415700499447394\n",
      "train loss:1.5096075808510672\n",
      "train loss:1.9275183944328198\n",
      "train loss:0.8430999139174198\n",
      "train loss:1.3474202681522576\n",
      "train loss:1.5647120330830924\n",
      "train loss:1.2689743930440098\n",
      "train loss:1.3437947051769537\n",
      "train loss:1.4164091217221644\n",
      "train loss:1.2541059417355045\n",
      "train loss:1.2482641610123926\n",
      "train loss:1.3845541162313155\n",
      "train loss:1.6125234263557986\n",
      "train loss:1.4289074926833139\n",
      "train loss:1.4667738120812213\n",
      "train loss:1.8993444779030237\n",
      "train loss:1.6523116131406461\n",
      "train loss:1.1141728259465187\n",
      "train loss:1.5906151388108878\n",
      "train loss:1.3901294374333755\n",
      "train loss:1.461041115396292\n",
      "train loss:1.6281535625287205\n",
      "train loss:1.5518231946679513\n",
      "train loss:1.9155894309040065\n",
      "train loss:1.6716416878235276\n",
      "train loss:1.0864050204655042\n",
      "train loss:1.828171477343894\n",
      "train loss:1.5894264396110431\n",
      "train loss:1.5079280939812834\n",
      "train loss:1.5619399094184727\n",
      "train loss:1.0736191098377446\n",
      "train loss:1.663534858929319\n",
      "train loss:1.484364752829117\n",
      "train loss:1.711463501427631\n",
      "train loss:1.4497082844490279\n",
      "train loss:1.638838167484728\n",
      "train loss:1.2390694810249383\n",
      "train loss:1.4980162104790378\n",
      "train loss:1.3673397959017624\n",
      "train loss:1.2953251833325001\n",
      "train loss:1.2768414551573375\n",
      "train loss:0.8060070308801544\n",
      "train loss:1.2555561587467\n",
      "train loss:0.854648856055328\n",
      "train loss:1.4880302913917185\n",
      "train loss:1.6971702989719144\n",
      "train loss:1.3000724772931505\n",
      "train loss:1.1084615968406006\n",
      "train loss:1.6088546251060272\n",
      "train loss:1.5371585509746315\n",
      "train loss:1.3706967323227004\n",
      "train loss:1.3092221995369808\n",
      "train loss:1.659505195381455\n",
      "train loss:1.2712060274868302\n",
      "train loss:1.5036445719469456\n",
      "train loss:0.6253398017685153\n",
      "train loss:0.8175583192625437\n",
      "train loss:1.596237542246089\n",
      "train loss:1.0993125792883958\n",
      "train loss:1.9455133583120034\n",
      "train loss:2.068774969322424\n",
      "train loss:1.456445480599212\n",
      "train loss:1.051295361697025\n",
      "train loss:1.002007124386118\n",
      "train loss:1.3903252629581933\n",
      "train loss:1.2684846891574562\n",
      "train loss:1.4617012749288225\n",
      "train loss:1.7063568295343288\n",
      "train loss:1.4753536162640801\n",
      "train loss:1.4197408678603847\n",
      "train loss:1.2669468714915173\n",
      "train loss:1.187408782449777\n",
      "train loss:1.3990419303959762\n",
      "train loss:1.8475008774103725\n",
      "train loss:0.9025270384953806\n",
      "train loss:1.4788576422055029\n",
      "train loss:1.5568366364549928\n",
      "train loss:1.2448875866817202\n",
      "train loss:1.4210391692046045\n",
      "train loss:1.2369943383145863\n",
      "train loss:1.5186968192981065\n",
      "train loss:1.4511712456953894\n",
      "train loss:0.9874397994446606\n",
      "train loss:1.3601131174566894\n",
      "train loss:1.451531553240008\n",
      "train loss:1.4122419705168299\n",
      "train loss:1.2498471223580785\n",
      "train loss:1.5424300020025687\n",
      "train loss:0.9686812613139191\n",
      "train loss:1.1462942652806238\n",
      "train loss:2.0015915936143815\n",
      "train loss:1.879629066320946\n",
      "train loss:1.2182897485787534\n",
      "train loss:1.637118741843905\n",
      "train loss:1.5548389380817889\n",
      "train loss:2.0212110954660307\n",
      "train loss:1.4799363704349728\n",
      "train loss:1.3012518275855682\n",
      "train loss:1.2695826032417372\n",
      "train loss:1.5117260700916302\n",
      "train loss:1.6970416895916807\n",
      "train loss:1.3352566148357665\n",
      "train loss:1.6653022290629504\n",
      "train loss:1.3576972497438093\n",
      "train loss:1.585593570375368\n",
      "train loss:1.7706139143429644\n",
      "train loss:1.4835956268327508\n",
      "train loss:1.4826964627381407\n",
      "train loss:1.494707109209736\n",
      "train loss:1.6476202534919362\n",
      "train loss:1.2480132141788405\n",
      "train loss:1.3122589350096405\n",
      "train loss:1.6518016796093573\n",
      "train loss:1.6176200349002159\n",
      "train loss:1.2698316400453287\n",
      "train loss:1.374652034504995\n",
      "train loss:1.1964699269647983\n",
      "train loss:1.5194690506764457\n",
      "train loss:1.9760199047649274\n",
      "train loss:0.9796552781265925\n",
      "train loss:1.4465437443617148\n",
      "train loss:1.5551468738750212\n",
      "train loss:1.68330974681716\n",
      "train loss:1.38916157663385\n",
      "train loss:1.5738178329915091\n",
      "train loss:1.4613672664514312\n",
      "train loss:1.8509371731473774\n",
      "train loss:1.3170934958193161\n",
      "train loss:1.5597613724754944\n",
      "train loss:1.4189493510945115\n",
      "train loss:1.0112392848018052\n",
      "train loss:1.025342977771086\n",
      "train loss:1.3897354968539875\n",
      "train loss:0.9693497303625718\n",
      "train loss:1.9358433277369602\n",
      "train loss:0.9691330130870609\n",
      "train loss:1.1971685006034605\n",
      "train loss:1.5269793042536608\n",
      "train loss:1.1929025279100536\n",
      "train loss:1.4341141753668822\n",
      "train loss:1.9496791700283223\n",
      "train loss:1.131933238263973\n",
      "train loss:1.7250433607668654\n",
      "train loss:1.2365112028494372\n",
      "train loss:1.485990669972492\n",
      "train loss:1.2516428852426131\n",
      "train loss:1.4232078920761762\n",
      "train loss:1.3468985405889111\n",
      "train loss:1.7273093527986405\n",
      "train loss:1.3831563837060994\n",
      "train loss:0.971371822220045\n",
      "train loss:1.3015214854901935\n",
      "train loss:1.0032371381606384\n",
      "train loss:1.2339185505672032\n",
      "train loss:1.637362471877357\n",
      "train loss:1.2403118119525132\n",
      "train loss:1.53873813086331\n",
      "train loss:1.5680079122631378\n",
      "train loss:1.5891331847942947\n",
      "train loss:1.6185207689931864\n",
      "train loss:1.5126846349401388\n",
      "train loss:0.8494053395528329\n",
      "train loss:1.0167443274095305\n",
      "train loss:1.5031355106467175\n",
      "train loss:1.0780890161535042\n",
      "train loss:1.2535593055775833\n",
      "train loss:1.510899538758355\n",
      "train loss:1.4696068476838857\n",
      "train loss:1.5954430731627207\n",
      "train loss:1.8652660027018115\n",
      "train loss:1.3662620984969478\n",
      "train loss:1.4791855767400823\n",
      "train loss:0.9109951147919828\n",
      "train loss:2.18624013094088\n",
      "train loss:1.3987731568415083\n",
      "train loss:1.3316320182526487\n",
      "train loss:1.7116401495626459\n",
      "train loss:0.836269904816392\n",
      "train loss:1.6056584903018654\n",
      "train loss:1.4794253270304378\n",
      "train loss:0.9679400221048498\n",
      "train loss:1.796040412560642\n",
      "train loss:1.2463364133481225\n",
      "train loss:1.4896986779670098\n",
      "train loss:1.321193883909599\n",
      "train loss:1.6112938433539419\n",
      "train loss:1.77568439849559\n",
      "train loss:1.2764794015040837\n",
      "train loss:1.1446188408630806\n",
      "train loss:1.6731692707132324\n",
      "train loss:1.0556517476962788\n",
      "train loss:1.5302798976393344\n",
      "train loss:1.1380017830143427\n",
      "train loss:2.6691547504798123\n",
      "train loss:1.1877587643342054\n",
      "train loss:1.0809304097139307\n",
      "train loss:1.4623090520655775\n",
      "train loss:1.2674212364863862\n",
      "train loss:1.315106974179565\n",
      "train loss:1.3432677552370742\n",
      "train loss:1.2077991463060054\n",
      "train loss:1.2860782888710254\n",
      "train loss:1.30344718378351\n",
      "train loss:1.7067827515028473\n",
      "train loss:1.5580835730741043\n",
      "train loss:1.8530557706405013\n",
      "train loss:1.1080319538572894\n",
      "train loss:1.2739591147036624\n",
      "train loss:1.0983932928465436\n",
      "train loss:1.4682716004367335\n",
      "train loss:1.4193643625948815\n",
      "train loss:1.527856598869389\n",
      "train loss:1.1159266557274377\n",
      "train loss:0.9480382630043291\n",
      "train loss:1.2598155944510228\n",
      "train loss:1.2844108545098907\n",
      "train loss:1.8106531451706829\n",
      "train loss:1.5534249153080926\n",
      "train loss:1.5066196859011671\n",
      "train loss:1.727847465228734\n",
      "train loss:0.9304166228690736\n",
      "train loss:1.7866112642677572\n",
      "train loss:0.9802201625955457\n",
      "train loss:2.0905263707793784\n",
      "train loss:1.8196732018012973\n",
      "train loss:1.9811679201217185\n",
      "train loss:1.7604597392716805\n",
      "train loss:1.3849054626959698\n",
      "train loss:1.0502300464575978\n",
      "train loss:1.336451243103963\n",
      "train loss:1.8808940685767044\n",
      "train loss:1.701494881064497\n",
      "train loss:1.2934488014589505\n",
      "train loss:0.9044239840111169\n",
      "train loss:1.590029050470483\n",
      "train loss:1.2858451729395497\n",
      "train loss:1.4103405094613746\n",
      "train loss:1.564328327430029\n",
      "train loss:1.396240962251968\n",
      "train loss:1.472412696943564\n",
      "train loss:1.243991904372026\n",
      "train loss:1.8169779888839561\n",
      "train loss:1.7298581386640564\n",
      "train loss:1.287409499205967\n",
      "train loss:1.5644780311259465\n",
      "train loss:1.397052638888224\n",
      "train loss:1.2533198760446127\n",
      "train loss:1.5895473209252433\n",
      "train loss:1.312750399269329\n",
      "train loss:1.214061520662505\n",
      "train loss:1.6059580008157888\n",
      "train loss:1.3948049778305045\n",
      "train loss:1.2522049726000157\n",
      "train loss:1.2609093403309681\n",
      "train loss:1.6349729325309503\n",
      "train loss:1.1672944019284526\n",
      "train loss:1.4567284257678665\n",
      "train loss:1.6957759948498634\n",
      "train loss:0.9989799732414391\n",
      "train loss:1.5735391358125492\n",
      "train loss:1.186140023498893\n",
      "train loss:1.467819078963433\n",
      "train loss:1.969435512685612\n",
      "train loss:1.0681474427001638\n",
      "train loss:1.0625103717635729\n",
      "train loss:1.101468903998245\n",
      "train loss:0.911359517154288\n",
      "train loss:1.602434337969754\n",
      "train loss:1.100301103548551\n",
      "train loss:1.3062943638539586\n",
      "train loss:1.4435705204083806\n",
      "train loss:0.9644384051691463\n",
      "train loss:1.6294924388754577\n",
      "train loss:1.2244902918279448\n",
      "train loss:1.3403431703461806\n",
      "train loss:1.4508863997397847\n",
      "train loss:1.2791937810509886\n",
      "train loss:0.8019926736395198\n",
      "train loss:0.4752355557145102\n",
      "train loss:1.4592294919491993\n",
      "train loss:1.266899209290936\n",
      "train loss:1.146520940071231\n",
      "train loss:0.9220223576283745\n",
      "train loss:1.8071603590377143\n",
      "train loss:1.0996542684623059\n",
      "train loss:1.7054392754153258\n",
      "train loss:1.112720999576064\n",
      "train loss:1.7838745861931617\n",
      "train loss:1.8990643340771804\n",
      "train loss:1.6397809626424427\n",
      "train loss:1.4606530085269536\n",
      "train loss:1.5139647307538662\n",
      "train loss:1.384264999870703\n",
      "train loss:1.7149731452234593\n",
      "train loss:0.9840658507689323\n",
      "train loss:1.456181228532404\n",
      "train loss:1.73268432603885\n",
      "train loss:1.454737626528809\n",
      "train loss:1.1881910649791754\n",
      "train loss:1.2925572325101617\n",
      "train loss:1.1859614817925026\n",
      "train loss:1.1766572144563288\n",
      "train loss:1.5992051969994523\n",
      "train loss:1.0844958166837477\n",
      "train loss:1.0566446110900602\n",
      "train loss:1.354629814018691\n",
      "train loss:1.0758110821089268\n",
      "train loss:1.8252061852096595\n",
      "train loss:1.5993444984413068\n",
      "train loss:1.5120749229443944\n",
      "train loss:1.987703116716976\n",
      "train loss:1.3779523316988858\n",
      "train loss:1.6051362034914043\n",
      "train loss:1.454808016934403\n",
      "train loss:1.264220965270074\n",
      "train loss:1.0178494930459834\n",
      "train loss:1.157185587665795\n",
      "train loss:1.5086125903080905\n",
      "train loss:1.2379739953406674\n",
      "train loss:1.0204818368132884\n",
      "train loss:1.352048134857562\n",
      "train loss:1.9000702088119723\n",
      "train loss:1.1458998629746882\n",
      "train loss:1.6467500350680908\n",
      "train loss:1.513948229036853\n",
      "train loss:1.2405502480758799\n",
      "train loss:1.4152325304782143\n",
      "train loss:1.8347968961349383\n",
      "train loss:1.6480677995163564\n",
      "train loss:1.6556306581315046\n",
      "train loss:1.1725097405654727\n",
      "train loss:1.3256393033732954\n",
      "train loss:1.480582140580274\n",
      "train loss:1.7992231057106718\n",
      "train loss:1.543474503779065\n",
      "train loss:1.1997607120915386\n",
      "train loss:1.5404089066831959\n",
      "train loss:1.415648924856616\n",
      "train loss:1.0364334801903263\n",
      "train loss:1.3690625394310634\n",
      "train loss:1.352880842000279\n",
      "train loss:1.3971056345898911\n",
      "train loss:1.5887181536111725\n",
      "train loss:1.7475012820127926\n",
      "train loss:1.538882001005779\n",
      "train loss:1.5025460115278653\n",
      "train loss:1.0488483646201572\n",
      "train loss:1.6269086891604172\n",
      "train loss:1.3563022126211226\n",
      "train loss:1.3088751555375175\n",
      "train loss:1.3898704837406242\n",
      "train loss:1.701956352962703\n",
      "train loss:1.5838144172886957\n",
      "train loss:1.1953855382179008\n",
      "train loss:1.220839868833266\n",
      "train loss:1.6215576698904388\n",
      "train loss:0.8848288653126819\n",
      "train loss:1.5806210134917242\n",
      "train loss:1.1832070761847517\n",
      "train loss:1.7460792125005924\n",
      "train loss:1.963648129076353\n",
      "train loss:0.7420988742248867\n",
      "train loss:1.5983235570038983\n",
      "train loss:1.4314385102956382\n",
      "train loss:1.9827044917696104\n",
      "train loss:1.5522797820827023\n",
      "train loss:1.4034410855415236\n",
      "train loss:1.4779632287710687\n",
      "train loss:1.5849787936855277\n",
      "train loss:1.415270634277698\n",
      "train loss:1.1852914364252247\n",
      "train loss:0.7909909354275241\n",
      "train loss:1.564436520417326\n",
      "train loss:1.215576155241767\n",
      "train loss:1.4603769049663533\n",
      "train loss:1.1299837534073673\n",
      "train loss:1.8214780498713807\n",
      "train loss:1.6506353672373737\n",
      "train loss:1.2589187951212288\n",
      "train loss:1.4367239653307549\n",
      "train loss:1.5668999020181278\n",
      "train loss:0.8835501596442656\n",
      "train loss:0.9542646805797388\n",
      "train loss:1.672526784658188\n",
      "train loss:1.4649635996721186\n",
      "train loss:0.954645313102603\n",
      "train loss:1.429562555908921\n",
      "train loss:1.587664976074049\n",
      "train loss:1.264775421476323\n",
      "train loss:1.3613398818718299\n",
      "train loss:1.1497167469704925\n",
      "train loss:1.4725807664855597\n",
      "train loss:1.4346187559223238\n",
      "train loss:1.2722961918284148\n",
      "train loss:1.669748639628868\n",
      "train loss:1.490947713173921\n",
      "train loss:1.3559826722858865\n",
      "train loss:1.1402820564164926\n",
      "train loss:1.215597088717956\n",
      "train loss:1.142085328132929\n",
      "train loss:1.1269782338655312\n",
      "train loss:1.4989164511628847\n",
      "train loss:1.3195307493695307\n",
      "train loss:1.1864733683383513\n",
      "train loss:1.0831671807591445\n",
      "train loss:1.4773149096878626\n",
      "train loss:1.2843802335332621\n",
      "train loss:1.4558865384755109\n",
      "train loss:1.3178203948070129\n",
      "train loss:2.076636447702887\n",
      "train loss:1.815937390310388\n",
      "train loss:1.4231092531328853\n",
      "train loss:1.3895581538163897\n",
      "train loss:1.1220447614361564\n",
      "train loss:1.4451072395425082\n",
      "train loss:1.6817979605276947\n",
      "train loss:1.364428561419205\n",
      "train loss:2.0497388018833886\n",
      "train loss:1.7930038335223848\n",
      "train loss:1.3439737189411978\n",
      "train loss:1.4120479475521144\n",
      "train loss:1.921965845260781\n",
      "train loss:1.3880360321287821\n",
      "train loss:1.5827572258844511\n",
      "train loss:1.1096232724020927\n",
      "train loss:0.9318559071427888\n",
      "train loss:1.252739467471683\n",
      "train loss:1.9957842321687138\n",
      "train loss:1.1362994239635948\n",
      "train loss:1.2016462535170684\n",
      "train loss:1.164666970893914\n",
      "train loss:1.3834888520707498\n",
      "train loss:1.2986564272340777\n",
      "train loss:1.5543700942091017\n",
      "train loss:1.7551293512563073\n",
      "train loss:1.4538032079048981\n",
      "train loss:1.2052186585598026\n",
      "train loss:0.9765635109234869\n",
      "train loss:1.2709627431768016\n",
      "train loss:1.1755213820629824\n",
      "train loss:1.0398905385378259\n",
      "train loss:1.3203458436487252\n",
      "train loss:1.3806274762125388\n",
      "train loss:0.7706043978072568\n",
      "train loss:1.2694433025669305\n",
      "train loss:1.3522611179824313\n",
      "train loss:1.409890954673875\n",
      "train loss:1.3209122776955216\n",
      "train loss:1.3833272860868653\n",
      "train loss:0.9511841678650141\n",
      "train loss:1.5390817778785832\n",
      "train loss:1.697963140753463\n",
      "train loss:1.6474226179867393\n",
      "train loss:1.382472427765406\n",
      "train loss:1.4137762627881716\n",
      "train loss:1.5138495373363041\n",
      "train loss:1.2329485854272795\n",
      "train loss:1.0365998754511312\n",
      "train loss:1.2921970652591694\n",
      "train loss:1.184926115191705\n",
      "train loss:1.839211835473544\n",
      "train loss:1.3638230610088666\n",
      "train loss:1.8687417080481694\n",
      "train loss:1.523561199050149\n",
      "train loss:1.4424617773739357\n",
      "train loss:1.7483316574912606\n",
      "train loss:1.600973342176422\n",
      "train loss:0.9547292660186655\n",
      "train loss:1.4168083094674198\n",
      "train loss:1.3679194876336105\n",
      "train loss:1.4994877672702667\n",
      "train loss:0.7431845673192428\n",
      "train loss:1.7967332615021199\n",
      "train loss:1.650509662198255\n",
      "train loss:1.3881115409836653\n",
      "train loss:1.7240594360440422\n",
      "train loss:1.0983432955768098\n",
      "train loss:1.1105439659712262\n",
      "train loss:0.9439625882334598\n",
      "train loss:1.8805968682769272\n",
      "train loss:1.802616255501757\n",
      "train loss:1.7457763960647905\n",
      "train loss:1.3814409682419833\n",
      "train loss:2.0815173535101534\n",
      "train loss:1.7252118933271876\n",
      "train loss:1.592068295414863\n",
      "train loss:1.7218589003623097\n",
      "train loss:1.2573279912456718\n",
      "train loss:0.763518303098977\n",
      "train loss:1.636338087435211\n",
      "train loss:1.7155121490721783\n",
      "train loss:1.3292386478165388\n",
      "train loss:1.2479850640225605\n",
      "train loss:1.6370257969555866\n",
      "train loss:1.1032706928157938\n",
      "train loss:1.2510510636609942\n",
      "train loss:1.3495365490607314\n",
      "train loss:1.5133596327902583\n",
      "train loss:1.3075958867872852\n",
      "train loss:1.4826858349588186\n",
      "train loss:1.6567238859246474\n",
      "train loss:0.9257762216476333\n",
      "train loss:0.9788276040164716\n",
      "train loss:1.7932507879531827\n",
      "train loss:1.3336918782040346\n",
      "train loss:1.6102895098206484\n",
      "train loss:1.7873950821268487\n",
      "train loss:1.1836901691862667\n",
      "train loss:1.6560832533210683\n",
      "train loss:1.715613895626462\n",
      "train loss:1.3919125475280156\n",
      "train loss:1.1476652326539591\n",
      "train loss:1.1113656161701864\n",
      "train loss:1.8598000569018212\n",
      "train loss:1.292107636760326\n",
      "train loss:1.2545892077449277\n",
      "train loss:1.2920749879469156\n",
      "train loss:1.4694161641611478\n",
      "train loss:0.9308431323516377\n",
      "train loss:1.70694228855703\n",
      "train loss:1.1752792938285395\n",
      "train loss:1.012912022257692\n",
      "train loss:0.9128756360166713\n",
      "train loss:1.6014880962115476\n",
      "train loss:1.6265453699744281\n",
      "train loss:1.754721335477067\n",
      "train loss:1.5357511182179668\n",
      "train loss:2.025963731876184\n",
      "train loss:2.0056167988919156\n",
      "train loss:1.0279447877366956\n",
      "train loss:1.495817500264836\n",
      "train loss:1.0169720260149178\n",
      "train loss:1.4703174177868665\n",
      "train loss:1.623530543270001\n",
      "train loss:1.1090668483343096\n",
      "train loss:1.5360287259093297\n",
      "train loss:1.093648503055372\n",
      "train loss:1.5305725120725147\n",
      "train loss:1.196750335228515\n",
      "train loss:1.2121128543534956\n",
      "train loss:1.4678072070748824\n",
      "train loss:0.9677570037417412\n",
      "train loss:1.3169249682547226\n",
      "train loss:1.112903658836132\n",
      "train loss:1.5855269295125072\n",
      "train loss:1.1676434189783733\n",
      "train loss:1.4027565104642987\n",
      "train loss:1.0141322910803054\n",
      "train loss:1.3881840282037297\n",
      "train loss:0.8407001109988211\n",
      "train loss:1.168083306056403\n",
      "train loss:1.2241254667694288\n",
      "train loss:0.8693855569130623\n",
      "train loss:1.1694094880939763\n",
      "train loss:1.5305937027004304\n",
      "train loss:1.9339894605049515\n",
      "train loss:1.7052162501102497\n",
      "train loss:1.3195715551133145\n",
      "train loss:1.1604344707202585\n",
      "train loss:1.7001856449877688\n",
      "train loss:1.6796409922897997\n",
      "train loss:1.7450790647644858\n",
      "train loss:1.827536152354718\n",
      "train loss:1.3371909588369852\n",
      "train loss:1.7240124919566966\n",
      "train loss:1.0790724517869748\n",
      "train loss:1.0824340087232631\n",
      "train loss:0.7770227072926349\n",
      "train loss:1.3313056928691631\n",
      "train loss:1.311120994484093\n",
      "train loss:1.4481044358153674\n",
      "train loss:1.2185670546401293\n",
      "train loss:1.3015678252178047\n",
      "train loss:1.0728948974397707\n",
      "train loss:1.2030295547461705\n",
      "train loss:1.4552029847245853\n",
      "train loss:1.7599863755410865\n",
      "train loss:1.3286011712357257\n",
      "train loss:1.4616704363821047\n",
      "train loss:1.8337439344405566\n",
      "train loss:1.5219909240962688\n",
      "train loss:1.13860182400868\n",
      "train loss:1.5537358073042309\n",
      "train loss:1.1543771456806362\n",
      "train loss:1.1650905461230292\n",
      "train loss:1.2385763957459468\n",
      "train loss:1.2075101670008317\n",
      "train loss:1.1968664034554937\n",
      "train loss:1.7622848572083445\n",
      "train loss:1.1585029438124812\n",
      "train loss:1.3086784392379767\n",
      "train loss:1.5574069299104036\n",
      "train loss:1.2439210375637346\n",
      "train loss:1.376521180010051\n",
      "train loss:1.418037072864978\n",
      "train loss:0.8109109131684006\n",
      "train loss:1.1928557321756792\n",
      "train loss:1.4224019843085391\n",
      "train loss:1.371389134488663\n",
      "train loss:1.12005554057398\n",
      "train loss:1.0686858238825907\n",
      "train loss:1.8302390582682135\n",
      "train loss:1.7010938839362848\n",
      "train loss:1.051845678410906\n",
      "train loss:1.6168565622844597\n",
      "train loss:1.2252183597725064\n",
      "train loss:1.1891593606958006\n",
      "train loss:1.4380735113286869\n",
      "train loss:2.0910253791429\n",
      "train loss:1.1628733535656228\n",
      "train loss:1.0843781356460247\n",
      "train loss:1.0312201764543532\n",
      "train loss:1.0864893448515645\n",
      "train loss:1.348845202269729\n",
      "train loss:1.4700916360049558\n",
      "train loss:0.9920005541026281\n",
      "train loss:1.055521192911641\n",
      "train loss:1.1828709477019381\n",
      "train loss:1.5121707880780333\n",
      "train loss:1.3879325909882496\n",
      "train loss:1.6844143473430606\n",
      "train loss:1.1672408421915248\n",
      "train loss:1.4684612264972328\n",
      "train loss:1.284893434339971\n",
      "train loss:1.7720411838432508\n",
      "train loss:1.3577304743798049\n",
      "train loss:2.0946667070064153\n",
      "train loss:1.8353273853132197\n",
      "train loss:0.9922482439406834\n",
      "train loss:1.0272899062240475\n",
      "train loss:1.177022301246502\n",
      "train loss:1.7972455298180254\n",
      "train loss:1.5824125647488398\n",
      "train loss:1.848091985585756\n",
      "train loss:0.9123237911399272\n",
      "train loss:1.3435724884567481\n",
      "train loss:1.3665523972759783\n",
      "train loss:1.2750291739230877\n",
      "train loss:1.1328219861871212\n",
      "train loss:1.4603786016017035\n",
      "train loss:1.2928227768569887\n",
      "train loss:0.9545082576629864\n",
      "train loss:1.2623424445180684\n",
      "train loss:1.130797900944552\n",
      "train loss:1.2403960542649437\n",
      "train loss:1.1529000207525102\n",
      "train loss:1.4176085964125962\n",
      "train loss:1.1121963875797602\n",
      "train loss:2.0113012630006066\n",
      "train loss:1.5343267486195702\n",
      "train loss:2.117835287332711\n",
      "train loss:1.8211287064854855\n",
      "train loss:0.9622293464938553\n",
      "train loss:1.4570069061230444\n",
      "train loss:1.2399467511110782\n",
      "train loss:1.6717246266977663\n",
      "train loss:1.6559361642291361\n",
      "train loss:1.388034530234706\n",
      "train loss:1.463210797033422\n",
      "train loss:1.1888798926638784\n",
      "train loss:1.2927522680237193\n",
      "train loss:1.1962227236509806\n",
      "train loss:1.2488644524424317\n",
      "train loss:1.3333601532944397\n",
      "train loss:1.2172969645689535\n",
      "train loss:1.366600377250455\n",
      "train loss:1.298410359465518\n",
      "train loss:0.8197076983696523\n",
      "train loss:1.3727010587700026\n",
      "train loss:1.4371547394561197\n",
      "train loss:0.9659343983060749\n",
      "train loss:1.4992531650315426\n",
      "train loss:1.828543779471946\n",
      "train loss:1.887887028342956\n",
      "train loss:1.5994675250075712\n",
      "train loss:1.2232109850929285\n",
      "train loss:1.4896982094010807\n",
      "train loss:1.6146491217978975\n",
      "train loss:1.1793634494499525\n",
      "train loss:1.2574156685626616\n",
      "train loss:1.1661011434090556\n",
      "train loss:1.592066287150843\n",
      "train loss:1.4152972728822664\n",
      "train loss:1.2518195072317213\n",
      "train loss:1.2253603568447322\n",
      "train loss:1.3876127706413985\n",
      "train loss:1.638888474234394\n",
      "train loss:1.453590548524777\n",
      "train loss:1.4675477880795085\n",
      "train loss:1.5695210405337876\n",
      "train loss:1.5613637603216173\n",
      "train loss:1.636172555765885\n",
      "train loss:1.621429430905185\n",
      "train loss:1.7298765324566407\n",
      "train loss:1.78794128291178\n",
      "train loss:1.3172242640021736\n",
      "train loss:1.3425839991737143\n",
      "train loss:1.3688396479129061\n",
      "train loss:1.7447048711488065\n",
      "train loss:1.8513620058058784\n",
      "train loss:1.6092244871502792\n",
      "train loss:1.6687052430730163\n",
      "train loss:1.000099098829621\n",
      "train loss:1.2771311670877918\n",
      "train loss:1.3911598280279978\n",
      "train loss:1.5151879681208622\n",
      "train loss:1.2770439287131439\n",
      "train loss:1.5580693001253052\n",
      "train loss:1.6202320138760151\n",
      "train loss:1.0327258417398162\n",
      "train loss:1.6413585304294405\n",
      "train loss:1.3715807596541567\n",
      "train loss:1.4135231192600997\n",
      "train loss:1.5200130393714981\n",
      "train loss:1.0322244118492863\n",
      "train loss:1.4945313541330254\n",
      "train loss:1.3600317589503486\n",
      "train loss:1.422895449064106\n",
      "train loss:1.1678736683961026\n",
      "train loss:0.9483691675052706\n",
      "train loss:1.549387885106594\n",
      "train loss:1.4834078510166804\n",
      "train loss:1.8028283530145246\n",
      "train loss:1.2830596661761073\n",
      "train loss:1.0888285554117068\n",
      "train loss:1.6684228971142319\n",
      "train loss:1.3967297805209424\n",
      "train loss:1.4218440862008321\n",
      "train loss:1.3171380230699987\n",
      "train loss:1.421849092523384\n",
      "train loss:1.5117595195895515\n",
      "train loss:0.9851049900090425\n",
      "train loss:1.3277802116703956\n",
      "train loss:1.3054289703990007\n",
      "train loss:1.0110081118407412\n",
      "train loss:1.635244466104897\n",
      "train loss:1.5944452463548695\n",
      "train loss:1.573721495901772\n",
      "train loss:1.8622461260760232\n",
      "train loss:0.9576334484808526\n",
      "train loss:1.5331122273782687\n",
      "train loss:1.2701434703637844\n",
      "train loss:1.177687964022076\n",
      "train loss:1.328879040234733\n",
      "train loss:1.5891151550174683\n",
      "train loss:1.0925935517776515\n",
      "train loss:1.6316670875958743\n",
      "train loss:1.2948954080663355\n",
      "train loss:1.683060344901098\n",
      "train loss:1.3072972491017694\n",
      "train loss:1.2291576354186173\n",
      "train loss:1.2630845740343442\n",
      "train loss:1.7128747271205529\n",
      "train loss:1.604743955520067\n",
      "train loss:1.6009989967978644\n",
      "train loss:0.959705683702625\n",
      "train loss:1.290294039857486\n",
      "train loss:1.4790693841274063\n",
      "train loss:1.2683265347273855\n",
      "train loss:1.9137115544963925\n",
      "train loss:1.3339898237559376\n",
      "train loss:1.3217733550319828\n",
      "train loss:1.1749595781760327\n",
      "train loss:1.483814350578975\n",
      "train loss:1.0197117838994976\n",
      "train loss:1.4342734555575491\n",
      "train loss:1.704294677654119\n",
      "train loss:1.338608983718204\n",
      "train loss:1.3617407841346474\n",
      "train loss:1.8126677522262706\n",
      "train loss:2.012982430979196\n",
      "train loss:1.0043992872933283\n",
      "train loss:1.304278146904286\n",
      "train loss:1.560730139376693\n",
      "train loss:1.4107301148053435\n",
      "train loss:1.3413633116648678\n",
      "train loss:1.5719914015975807\n",
      "train loss:1.16115682152665\n",
      "train loss:1.347686868099181\n",
      "train loss:1.5139465614547492\n",
      "train loss:1.3296556672221238\n",
      "train loss:1.465428368109047\n",
      "train loss:1.4937855662664106\n",
      "train loss:1.3748120324743813\n",
      "train loss:1.6109103082174212\n",
      "train loss:1.4319866514885173\n",
      "train loss:1.1646648685694765\n",
      "train loss:1.1770480249435245\n",
      "train loss:1.1792337811168778\n",
      "train loss:1.4555468488246357\n",
      "train loss:1.4473721678512148\n",
      "train loss:1.3946467466355987\n",
      "train loss:1.504746059276043\n",
      "train loss:0.922567664441654\n",
      "train loss:1.4916875801344465\n",
      "train loss:1.1049271142913653\n",
      "train loss:1.3090639037841212\n",
      "train loss:1.0583756343380226\n",
      "train loss:1.4908570183103742\n",
      "train loss:1.2382363841462276\n",
      "train loss:1.6509299359009968\n",
      "train loss:1.4020917641274846\n",
      "train loss:1.376399598081531\n",
      "train loss:1.3165249999189768\n",
      "train loss:1.0668170895234668\n",
      "train loss:1.4075575348236826\n",
      "train loss:1.2413956491226252\n",
      "train loss:0.8542720656475769\n",
      "train loss:1.2986802594923301\n",
      "train loss:1.3715242915172077\n",
      "train loss:0.9977733285890957\n",
      "train loss:1.0917100155320365\n",
      "train loss:1.7580183551924236\n",
      "train loss:1.2920806013389377\n",
      "train loss:1.4897643591721796\n",
      "train loss:1.6901602011264818\n",
      "train loss:1.8331361635824177\n",
      "train loss:0.9084666403510573\n",
      "train loss:1.5826497513323434\n",
      "train loss:1.4636838431479937\n",
      "train loss:1.7791995765341437\n",
      "train loss:1.2540710655109022\n",
      "train loss:1.350984032032156\n",
      "train loss:0.9211856567827063\n",
      "train loss:0.9461330941459082\n",
      "train loss:1.314293083629774\n",
      "train loss:1.2232510680691664\n",
      "train loss:1.3632741220241134\n",
      "train loss:2.038465047342756\n",
      "train loss:1.6029304366302608\n",
      "train loss:1.5044210154391995\n",
      "train loss:1.3738858488883998\n",
      "train loss:1.653015634393136\n",
      "train loss:1.5795325499146489\n",
      "train loss:1.6518610676586403\n",
      "train loss:1.5896736978066834\n",
      "train loss:1.018065061829416\n",
      "train loss:1.6914571823956126\n",
      "train loss:1.4700702529899778\n",
      "train loss:1.0838956073000203\n",
      "train loss:1.2223208758990731\n",
      "train loss:1.2179888220037873\n",
      "train loss:1.3514940090661036\n",
      "train loss:1.008441948639767\n",
      "train loss:1.2226694077299478\n",
      "train loss:1.1089366398552054\n",
      "train loss:0.9576025308055935\n",
      "train loss:0.9012497146406224\n",
      "train loss:1.5495543762576993\n",
      "train loss:1.5131065708603457\n",
      "train loss:1.6403250322084912\n",
      "train loss:1.0739159794407245\n",
      "train loss:1.6293096593792762\n",
      "train loss:1.3152320943318396\n",
      "train loss:1.5404444953369425\n",
      "train loss:1.279929409067822\n",
      "train loss:1.2963189125331032\n",
      "train loss:1.4372128067301366\n",
      "train loss:1.6279970664087777\n",
      "train loss:1.888181347046498\n",
      "train loss:1.144257595703484\n",
      "train loss:1.1371186095238124\n",
      "train loss:1.1349888352149837\n",
      "train loss:1.0610512950072777\n",
      "train loss:1.5492103942170488\n",
      "train loss:1.2816689801424654\n",
      "train loss:1.5515946032366212\n",
      "train loss:1.694761734331511\n",
      "train loss:1.4350849230270042\n",
      "train loss:1.2129481292329638\n",
      "train loss:1.8683107308080011\n",
      "train loss:1.5760510552989195\n",
      "train loss:0.9834481421653383\n",
      "train loss:1.7322943263478208\n",
      "train loss:1.3197159836779062\n",
      "train loss:1.1213387578437448\n",
      "train loss:1.2076128717867074\n",
      "train loss:1.219743358952382\n",
      "train loss:1.7483574067939252\n",
      "train loss:0.8877322876334064\n",
      "train loss:1.5482230096089684\n",
      "train loss:1.3937229368206459\n",
      "train loss:1.3393138789016088\n",
      "train loss:1.6957043847443363\n",
      "train loss:1.3683742409032722\n",
      "train loss:1.3947058731488713\n",
      "train loss:1.576783499831209\n",
      "train loss:1.4868763433738865\n",
      "train loss:1.358155764051314\n",
      "train loss:1.8099082844781318\n",
      "train loss:1.2646449464380665\n",
      "train loss:1.1163175051540104\n",
      "train loss:1.3476982985541384\n",
      "train loss:1.1724692493370363\n",
      "train loss:1.7816191072819272\n",
      "train loss:1.4860091092682066\n",
      "train loss:1.426992944526291\n",
      "train loss:1.3236571415521978\n",
      "train loss:1.5697321969787514\n",
      "train loss:1.589803508548427\n",
      "train loss:1.4428265158376155\n",
      "train loss:1.6131501246068232\n",
      "train loss:0.963338404440022\n",
      "train loss:1.2120381441772152\n",
      "train loss:1.555708805470284\n",
      "train loss:1.277895639486871\n",
      "train loss:1.3692584448542748\n",
      "train loss:1.3642707475345186\n",
      "train loss:1.545399449683066\n",
      "train loss:1.681132677507343\n",
      "train loss:1.6173345414042681\n",
      "train loss:0.7313128392745656\n",
      "train loss:1.3895731382105683\n",
      "train loss:1.1639191446563224\n",
      "train loss:1.3904934878975777\n",
      "train loss:1.1914793365475664\n",
      "train loss:1.4611640940264587\n",
      "train loss:2.0241433618679134\n",
      "train loss:1.4986897529630012\n",
      "train loss:1.6245690784852482\n",
      "train loss:0.8302989410394476\n",
      "train loss:1.9701580553891005\n",
      "train loss:1.3101756389620065\n",
      "train loss:1.0284182568782514\n",
      "train loss:1.5660661508152536\n",
      "train loss:0.7612358129617528\n",
      "train loss:1.4717209799131965\n",
      "train loss:2.421758464558502\n",
      "train loss:1.1286132033547942\n",
      "train loss:2.324360023543064\n",
      "train loss:1.189662219968305\n",
      "train loss:1.7578477085516178\n",
      "train loss:1.2777964281793572\n",
      "train loss:1.5980500934193187\n",
      "train loss:1.0946270461564531\n",
      "train loss:1.6190885978807508\n",
      "train loss:1.5428163849749594\n",
      "train loss:1.7392340190680513\n",
      "train loss:1.3339802436958899\n",
      "train loss:1.6238403096323002\n",
      "train loss:1.4743782609535796\n",
      "train loss:2.0757018481485927\n",
      "train loss:1.508635776387164\n",
      "train loss:1.06322734847017\n",
      "train loss:1.0230585102609284\n",
      "train loss:1.6178124485962109\n",
      "train loss:1.2779720961745977\n",
      "train loss:1.1664137310480687\n",
      "train loss:0.9895531563572739\n",
      "train loss:0.9082425666854625\n",
      "train loss:1.119261187717142\n",
      "train loss:1.3954422654558383\n",
      "train loss:1.5955355164325835\n",
      "train loss:1.4402031032109988\n",
      "train loss:1.4269011537788108\n",
      "train loss:1.082395934908756\n",
      "train loss:1.5598313972780846\n",
      "train loss:1.0547433587997286\n",
      "train loss:1.2854194832395658\n",
      "train loss:1.8984157216237616\n",
      "train loss:1.6245801717705912\n",
      "train loss:1.4797102755463558\n",
      "train loss:1.1332716058719927\n",
      "train loss:1.2320356736985032\n",
      "train loss:1.3319009229240486\n",
      "train loss:1.6842242245122658\n",
      "train loss:1.2206060149396345\n",
      "train loss:1.2603737456378128\n",
      "train loss:1.617634315188677\n",
      "train loss:1.0125794744758148\n",
      "train loss:1.375632723384027\n",
      "train loss:1.1760743977739265\n",
      "train loss:1.7022091929525405\n",
      "train loss:1.4291668309317749\n",
      "train loss:1.4855289196981611\n",
      "train loss:1.410387747562569\n",
      "train loss:0.9682732263001794\n",
      "train loss:1.7319756985051136\n",
      "train loss:1.7749850088435302\n",
      "train loss:1.532062921114513\n",
      "train loss:0.8882780535639508\n",
      "train loss:1.1921919593855486\n",
      "train loss:1.3277630638488378\n",
      "train loss:1.7079455308656641\n",
      "train loss:1.1802233167129104\n",
      "train loss:1.197469221349038\n",
      "train loss:1.3131650202953091\n",
      "train loss:1.3563759542125988\n",
      "train loss:1.4287133478393699\n",
      "train loss:0.9980336329324789\n",
      "train loss:1.4182768733760551\n",
      "train loss:1.1535320405276432\n",
      "train loss:0.8649033890954095\n",
      "train loss:1.0539764401436662\n",
      "train loss:1.1375002348190855\n",
      "train loss:1.192186296835707\n",
      "train loss:0.8000044541978614\n",
      "train loss:0.7527948174287765\n",
      "train loss:1.408773094065702\n",
      "train loss:0.8906638043228753\n",
      "train loss:1.856287633317557\n",
      "train loss:1.2727527292962804\n",
      "train loss:1.4804865999957557\n",
      "train loss:1.417953670334237\n",
      "train loss:1.4084551743525366\n",
      "train loss:1.1494886235488186\n",
      "train loss:1.2965923312009955\n",
      "train loss:1.1210159412410539\n",
      "train loss:1.4455680981196988\n",
      "train loss:1.7676879256486853\n",
      "train loss:1.5714520561362118\n",
      "train loss:1.1686519745405284\n",
      "train loss:1.2696655567663484\n",
      "train loss:1.946602803832695\n",
      "train loss:1.5029299793744118\n",
      "train loss:1.160764039060407\n",
      "train loss:1.1899084158276374\n",
      "train loss:1.4702322703370534\n",
      "train loss:1.323408104243342\n",
      "train loss:1.673959149890306\n",
      "train loss:1.3402127598785196\n",
      "train loss:1.967144776701651\n",
      "train loss:1.1005529246361507\n",
      "train loss:1.6137836923166051\n",
      "train loss:1.2173869631727965\n",
      "train loss:1.481994678215175\n",
      "train loss:1.6171587374189529\n",
      "train loss:1.424538608951915\n",
      "train loss:1.682595844468232\n",
      "train loss:1.7432760591057126\n",
      "train loss:0.9178477534595224\n",
      "train loss:1.863133022725566\n",
      "train loss:1.6293452691498906\n",
      "train loss:1.376735209240013\n",
      "train loss:1.4049639638777924\n",
      "train loss:1.742128492471089\n",
      "train loss:1.0065806733512157\n",
      "train loss:1.4733503980296783\n",
      "train loss:1.3175641034112346\n",
      "train loss:1.7568032008413195\n",
      "train loss:0.9027091253585203\n",
      "train loss:1.3305465369870317\n",
      "train loss:1.7339544133767035\n",
      "train loss:1.370914472359361\n",
      "train loss:1.104102138397325\n",
      "train loss:1.3782756462255423\n",
      "train loss:0.7986925780316494\n",
      "train loss:0.9199209842736534\n",
      "train loss:1.5811218136648262\n",
      "train loss:1.390942266535862\n",
      "train loss:1.1091525518506526\n",
      "train loss:1.3207693024590914\n",
      "train loss:1.5264640101367344\n",
      "train loss:2.1023679855088195\n",
      "train loss:1.0072057608315839\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6617647058823529\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 20\n",
    "\n",
    "network = DeepConvNet()\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e282adaa-da9f-4341-84bb-eaec188a87b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Hue images shape: (1020, 1, 128, 128)\n",
      "Labels shape: (1020,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('G1020.csv')\n",
    "image_files = df['imageID'].tolist()  # 이미지 파일 이름이 있는 열 이름\n",
    "labels = df['binaryLabels'].values  # 레이블이 있는 열 이름\n",
    "\n",
    "# 이미지 크기 설정\n",
    "target_size = (128, 128)\n",
    "\n",
    "# cv2로 이미지 읽고 전처리\n",
    "def load_and_extract_hue(image_path, target_size):\n",
    "    image = cv2.imread(image_path)  # 이미지 읽기 (기본 BGR 형식)\n",
    "    if image is None:\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return None\n",
    "    image = cv2.resize(image, target_size)  # 이미지 크기 조정\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)  # BGR을 HSV로 변환\n",
    "    hue_channel = hsv_image[:, :, 0]  # Hue 채널만 추출\n",
    "    hue_channel = hue_channel / 180.0  # Hue 값 정규화 (0~1 범위로 스케일링, Hue 범위는 0-179)\n",
    "    return hue_channel\n",
    "\n",
    "# 모든 이미지를 불러와서 리스트에 저장\n",
    "images = [load_and_extract_hue(image_folder_path + img_path, target_size) for img_path in image_files]\n",
    "images = np.array([img for img in images if img is not None])  # None 값 제거\n",
    "\n",
    "# 차원 추가하여 (1020, 1, 128, 128) 형태로 변환\n",
    "images = images[:, np.newaxis, :, :]\n",
    "\n",
    "print(f\"Processed Hue images shape: {images.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3255c806-0f94-479b-8579-207840927145",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet3Layer:\n",
    "    def __init__(self, input_dim=(1, 128, 128), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5df4083-7104-426e-8735-60f1cf0d04d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.263087000663222\n",
      "=== epoch:1, train acc:0.3, test acc:0.35 ===\n",
      "train loss:2.148714996301905\n",
      "train loss:1.9701040603254267\n",
      "train loss:1.6794971416648246\n",
      "train loss:1.2350760931288882\n",
      "train loss:0.8739562522984807\n",
      "train loss:1.0262891402369605\n",
      "train loss:0.5296248527304634\n",
      "train loss:0.6698278664790294\n",
      "train loss:0.5340600293692731\n",
      "train loss:0.6182808194116785\n",
      "train loss:0.5859891323992127\n",
      "train loss:0.49873431883283115\n",
      "train loss:0.7067004682783964\n",
      "train loss:0.6480311971603624\n",
      "train loss:0.6870581085078735\n",
      "train loss:0.6150200085548787\n",
      "train loss:0.7231432170658286\n",
      "train loss:0.5894746201366138\n",
      "train loss:0.6198315559182638\n",
      "train loss:0.6883161421894581\n",
      "train loss:0.6886586813602908\n",
      "train loss:0.7146126021321451\n",
      "train loss:0.6405563352330242\n",
      "train loss:0.746649834033072\n",
      "train loss:0.4668000058317573\n",
      "train loss:0.30256315607433837\n",
      "train loss:0.6254141740006554\n",
      "train loss:0.6627269710366838\n",
      "train loss:0.736916152095501\n",
      "train loss:0.6079634613533484\n",
      "train loss:0.4825234069583596\n",
      "train loss:0.5317053389465258\n",
      "train loss:0.6201278541459245\n",
      "train loss:0.3324124321942148\n",
      "train loss:0.6758853123647092\n",
      "train loss:0.8731134419197035\n",
      "train loss:0.38400033011217094\n",
      "train loss:0.5043471234738561\n",
      "train loss:0.5049100522884122\n",
      "train loss:0.5164164502724418\n",
      "train loss:0.3371189435194723\n",
      "train loss:0.5212183835230377\n",
      "train loss:0.7314872373891355\n",
      "train loss:0.5064189206926236\n",
      "train loss:0.5109774086594311\n",
      "train loss:0.7102099824120679\n",
      "train loss:0.6101696308913589\n",
      "train loss:0.5392305034381658\n",
      "train loss:0.5461659779569288\n",
      "train loss:0.4897285167884216\n",
      "train loss:0.6254999313845782\n",
      "train loss:0.8955202719518361\n",
      "train loss:0.6962618786538224\n",
      "train loss:0.6067216770312033\n",
      "train loss:0.6848729477254848\n",
      "train loss:0.7405984910280725\n",
      "train loss:0.7518165829966733\n",
      "train loss:0.6380753957377452\n",
      "train loss:0.6054699304851481\n",
      "train loss:0.5830252910090963\n",
      "train loss:0.6345255363797659\n",
      "train loss:0.3310460787048555\n",
      "train loss:0.33407984093180926\n",
      "train loss:0.781095561448022\n",
      "train loss:0.7081351359346053\n",
      "train loss:1.1686256466900087\n",
      "train loss:0.5072746874334138\n",
      "train loss:0.7482571147583076\n",
      "train loss:0.4466841153090667\n",
      "train loss:0.7316511713575846\n",
      "train loss:0.5792802366859984\n",
      "train loss:0.5852624626252945\n",
      "train loss:0.6040683071932321\n",
      "train loss:0.6393034317239181\n",
      "train loss:0.7038047934479904\n",
      "train loss:0.6032644221919192\n",
      "train loss:0.5892138531885607\n",
      "train loss:0.6383578479289408\n",
      "train loss:0.685895811554212\n",
      "train loss:0.5777301520819987\n",
      "train loss:0.6162930143993293\n",
      "train loss:0.6921866131808135\n",
      "train loss:0.6113153179421286\n",
      "train loss:0.8281382244808567\n",
      "train loss:0.6156309703291587\n",
      "train loss:0.6086364982014286\n",
      "train loss:0.5214043200168996\n",
      "train loss:0.7161911178962266\n",
      "train loss:0.5984815330912217\n",
      "train loss:0.3409342604057445\n",
      "train loss:0.603087809109532\n",
      "train loss:0.5516261088087234\n",
      "train loss:0.619675541777955\n",
      "train loss:0.4992411403711811\n",
      "train loss:0.2224138545216942\n",
      "train loss:0.8201915323678897\n",
      "train loss:0.38427829964296056\n",
      "train loss:0.5549208476040014\n",
      "train loss:0.34980131755999905\n",
      "train loss:0.33204679770105383\n",
      "train loss:0.5399032257341141\n",
      "train loss:0.6733000850081287\n",
      "train loss:0.6369006798499746\n",
      "train loss:0.9685672555866273\n",
      "train loss:0.6086735205198137\n",
      "train loss:0.7586942776893426\n",
      "train loss:0.662723950479886\n",
      "train loss:0.5860234665827393\n",
      "train loss:0.6882380321936378\n",
      "train loss:0.6700017602678893\n",
      "train loss:0.6711474988027016\n",
      "train loss:0.6894555282759386\n",
      "train loss:0.6527709517942175\n",
      "train loss:0.6800954208866996\n",
      "train loss:0.6700332229472834\n",
      "train loss:0.633316240775897\n",
      "train loss:0.6286358924132649\n",
      "train loss:0.79523757846484\n",
      "train loss:0.7230666171756017\n",
      "train loss:0.5683077731683681\n",
      "train loss:0.5515388000269826\n",
      "train loss:0.8034405308530508\n",
      "train loss:0.6374529745597309\n",
      "train loss:0.5957135378058845\n",
      "train loss:0.5437575788756994\n",
      "train loss:0.8633171252257114\n",
      "train loss:0.45433607944475407\n",
      "train loss:0.3290523127058849\n",
      "train loss:0.8142744563625428\n",
      "train loss:0.385317025958864\n",
      "train loss:0.6224856598646056\n",
      "train loss:0.6375153656258107\n",
      "train loss:0.7644542802172019\n",
      "train loss:0.3894045440939606\n",
      "train loss:0.6273666195696779\n",
      "train loss:0.7408212315616441\n",
      "train loss:0.5393941102417623\n",
      "train loss:0.8593715607775005\n",
      "train loss:0.5975424066373131\n",
      "train loss:0.6703789698391731\n",
      "train loss:0.5588919393887345\n",
      "train loss:0.5217681498941344\n",
      "train loss:0.5096307857028207\n",
      "train loss:0.46188189397741664\n",
      "train loss:0.544018120638915\n",
      "train loss:0.5207743191071399\n",
      "train loss:0.48446064367340097\n",
      "train loss:0.6765564355376763\n",
      "train loss:0.33810990765291843\n",
      "train loss:0.14912253000791315\n",
      "train loss:0.6013182464854843\n",
      "train loss:0.3129115532440854\n",
      "train loss:0.6007663861390209\n",
      "train loss:0.5172188689634455\n",
      "train loss:0.9514050709934733\n",
      "train loss:0.5021589680892904\n",
      "train loss:0.8922987696290828\n",
      "train loss:0.5299248119808522\n",
      "train loss:0.7471570844447732\n",
      "train loss:0.743422347662206\n",
      "train loss:0.7328147934611351\n",
      "train loss:0.6761656418414558\n",
      "train loss:0.705998213629086\n",
      "train loss:0.6333339468308118\n",
      "train loss:0.7055297594140182\n",
      "train loss:0.6075491783141644\n",
      "train loss:0.6330791172061074\n",
      "train loss:0.6326463705070088\n",
      "train loss:0.6225405539660199\n",
      "train loss:0.604767465712863\n",
      "train loss:0.4426063786086119\n",
      "train loss:0.36080962404128236\n",
      "train loss:0.7621283173816875\n",
      "train loss:0.6806001652772424\n",
      "train loss:0.6084531869786804\n",
      "train loss:0.6652328244444392\n",
      "train loss:0.9114575582998905\n",
      "train loss:0.48307443474379597\n",
      "train loss:0.6201444803902603\n",
      "train loss:0.7371892151675481\n",
      "train loss:0.6283322082430197\n",
      "train loss:0.5690994512573017\n",
      "train loss:0.6055278675151239\n",
      "train loss:0.5353358098406179\n",
      "train loss:0.405983807538976\n",
      "train loss:0.5109725920010335\n",
      "train loss:0.22884202714990137\n",
      "train loss:0.688798074187356\n",
      "train loss:1.1068198412030104\n",
      "train loss:0.8789006182617676\n",
      "train loss:0.7690895481779061\n",
      "train loss:0.6015863508415488\n",
      "train loss:0.7133160787256261\n",
      "train loss:0.6872351075819985\n",
      "train loss:0.6822745589900152\n",
      "train loss:0.5702346424557907\n",
      "train loss:0.686847197104884\n",
      "train loss:0.6522131741909809\n",
      "train loss:0.6473757529400446\n",
      "train loss:0.649155152965586\n",
      "train loss:0.6217232336508054\n",
      "train loss:0.5999908188172822\n",
      "train loss:0.6807740724029305\n",
      "train loss:0.5877464185406311\n",
      "train loss:0.6144666848862249\n",
      "train loss:0.6775763642227728\n",
      "train loss:0.6082996235207628\n",
      "train loss:0.6028499026610927\n",
      "train loss:0.5201469329460895\n",
      "train loss:0.6973926781844544\n",
      "train loss:0.6990577459675802\n",
      "train loss:0.6966520349351548\n",
      "train loss:0.7120319655358697\n",
      "train loss:0.7109647663252229\n",
      "train loss:0.5912047101288078\n",
      "train loss:0.516918443372809\n",
      "train loss:0.7494458452213546\n",
      "train loss:0.6789784042153292\n",
      "train loss:0.6826015219031643\n",
      "train loss:0.7283150730490977\n",
      "train loss:0.5702049574695109\n",
      "train loss:0.6685638721106046\n",
      "train loss:0.5763559024721226\n",
      "train loss:0.5302394915149512\n",
      "train loss:0.6095294420531666\n",
      "train loss:0.5429018244664711\n",
      "train loss:0.526728637585832\n",
      "train loss:0.6052033606974121\n",
      "train loss:0.6957367322927592\n",
      "train loss:0.5154340516014503\n",
      "train loss:0.6097528385884013\n",
      "train loss:0.730101262400038\n",
      "train loss:0.4990628407128188\n",
      "train loss:0.3697810737368261\n",
      "train loss:0.6061527880553089\n",
      "train loss:0.7587864602631104\n",
      "train loss:0.4970672151323076\n",
      "train loss:0.6345813215669974\n",
      "train loss:0.7958480177115117\n",
      "train loss:0.4024289848037698\n",
      "train loss:0.5976499643010098\n",
      "train loss:0.5172779978084725\n",
      "train loss:0.5984025191134306\n",
      "train loss:0.5140430846032944\n",
      "train loss:0.7038927588673145\n",
      "train loss:0.32610276980376063\n",
      "train loss:0.5175669375430247\n",
      "train loss:0.7174549496840376\n",
      "train loss:0.5108269310670172\n",
      "train loss:0.3815101099512128\n",
      "train loss:0.5051179912348883\n",
      "train loss:0.6382226185164248\n",
      "train loss:0.748781675078934\n",
      "train loss:0.8588669865755401\n",
      "train loss:0.7154926079026216\n",
      "train loss:0.4140122836973946\n",
      "train loss:0.7822958957784671\n",
      "train loss:0.35843020321917995\n",
      "train loss:0.6815716178001121\n",
      "train loss:0.5828028517290498\n",
      "train loss:0.5527697731328323\n",
      "train loss:0.9755185993951617\n",
      "train loss:0.5527249682996535\n",
      "train loss:0.49997080558990825\n",
      "train loss:0.5570452830246106\n",
      "train loss:0.5512414578370705\n",
      "train loss:0.7760288719344152\n",
      "train loss:0.5296299531112679\n",
      "train loss:0.6195232433602044\n",
      "train loss:0.67401074945963\n",
      "train loss:0.6060192453733401\n",
      "train loss:0.3603291292907305\n",
      "train loss:0.6146382913554578\n",
      "train loss:0.5363560790850886\n",
      "train loss:0.5999430072486744\n",
      "train loss:0.8824992405290736\n",
      "train loss:0.5996993341592891\n",
      "train loss:0.8585214612014251\n",
      "train loss:0.6233382244821434\n",
      "train loss:0.5395691158400895\n",
      "train loss:0.5357798438620549\n",
      "train loss:0.6183924372971952\n",
      "train loss:0.5357050517365247\n",
      "train loss:0.5491432134470884\n",
      "train loss:0.7143932756346165\n",
      "train loss:0.5392804145337279\n",
      "train loss:0.43980198648438573\n",
      "train loss:0.6920349386005304\n",
      "train loss:0.4805403022525061\n",
      "train loss:0.6027046373429865\n",
      "train loss:0.7314730771518924\n",
      "train loss:0.5241318337660645\n",
      "train loss:0.6168661413569309\n",
      "train loss:0.5946997366680515\n",
      "train loss:0.914712657500159\n",
      "train loss:0.605113141721972\n",
      "train loss:0.5094792019557624\n",
      "train loss:0.5823299749723457\n",
      "train loss:0.4687385276393818\n",
      "train loss:0.669570105211263\n",
      "train loss:0.4572233390599128\n",
      "train loss:0.7558028993570414\n",
      "train loss:0.5301888441324428\n",
      "train loss:0.4302323248419566\n",
      "train loss:0.5850577719044374\n",
      "train loss:0.615524390941465\n",
      "train loss:0.806521589490923\n",
      "train loss:0.6076515843784822\n",
      "train loss:0.6934567891435697\n",
      "train loss:0.6208740053885633\n",
      "train loss:0.4299591476593861\n",
      "train loss:0.6879052941312116\n",
      "train loss:0.5781486614881786\n",
      "train loss:0.6343524315736058\n",
      "train loss:0.7702713617617422\n",
      "train loss:0.7318800581172037\n",
      "train loss:0.4876318182096597\n",
      "train loss:0.5796562469376946\n",
      "train loss:0.5417579722310063\n",
      "train loss:0.5330492802124562\n",
      "train loss:0.4708704438403502\n",
      "train loss:0.8095453915011985\n",
      "train loss:0.49883279475132253\n",
      "train loss:0.4658511588299927\n",
      "train loss:0.4069983134857945\n",
      "train loss:0.5853847255613586\n",
      "train loss:1.1388378547796805\n",
      "train loss:0.38409585770244126\n",
      "train loss:0.6463426486470911\n",
      "train loss:0.5671011066614475\n",
      "train loss:0.3832323013356448\n",
      "train loss:0.25956211337441476\n",
      "train loss:0.8918084732326674\n",
      "train loss:0.9408101142105535\n",
      "train loss:0.42731450288395134\n",
      "train loss:0.5261189612797097\n",
      "train loss:0.6796404188550891\n",
      "train loss:0.7400230229854985\n",
      "train loss:0.5295606410709455\n",
      "train loss:0.495079847997542\n",
      "train loss:0.7409107699109923\n",
      "train loss:0.5994192831643298\n",
      "train loss:0.5391996827097159\n",
      "train loss:0.4654084320675495\n",
      "train loss:0.6062265910316564\n",
      "train loss:0.44716505959188835\n",
      "train loss:0.5889848550835186\n",
      "train loss:0.5862359471169123\n",
      "train loss:0.5075181652369063\n",
      "train loss:0.639995489981936\n",
      "train loss:0.4752403269623843\n",
      "train loss:0.6537453528591934\n",
      "train loss:0.7496373233086145\n",
      "train loss:0.8958064627313382\n",
      "train loss:0.6384680671932321\n",
      "train loss:0.6892609961585977\n",
      "train loss:0.5443357181126787\n",
      "train loss:0.6099238228089704\n",
      "train loss:0.6919697290882082\n",
      "train loss:0.6470935428097305\n",
      "train loss:0.6384958928611751\n",
      "train loss:0.6714699521164487\n",
      "train loss:0.603413144117668\n",
      "train loss:0.6247047370717421\n",
      "train loss:0.7043820290220213\n",
      "train loss:0.6480606270380587\n",
      "train loss:0.5855178175624899\n",
      "train loss:0.595126931309775\n",
      "train loss:0.5075735216911219\n",
      "train loss:0.6294962858874966\n",
      "train loss:0.5934653663618079\n",
      "train loss:0.5873249981394975\n",
      "train loss:0.7560877752601517\n",
      "train loss:0.6904760742714815\n",
      "train loss:0.5268064852325994\n",
      "train loss:0.5025110921917368\n",
      "train loss:0.5789778862652397\n",
      "train loss:0.4147779235780843\n",
      "train loss:0.6617510564408696\n",
      "train loss:0.8599676521469256\n",
      "train loss:0.764209408772703\n",
      "train loss:0.6567063958558684\n",
      "train loss:0.6382439696208748\n",
      "train loss:0.6849124936899658\n",
      "train loss:0.6574614462373135\n",
      "train loss:0.6756738649969704\n",
      "train loss:0.6450007884296098\n",
      "train loss:0.5025689586402907\n",
      "train loss:0.6083463221783557\n",
      "train loss:0.5596150351088719\n",
      "train loss:0.6506079042576003\n",
      "train loss:0.6608930632401088\n",
      "train loss:0.6244613065700441\n",
      "train loss:0.4211089136097425\n",
      "train loss:0.4519441152228231\n",
      "train loss:0.5422705528189525\n",
      "train loss:0.5105996308008849\n",
      "train loss:0.6332875887946732\n",
      "train loss:0.8757076865187088\n",
      "train loss:1.0234280238511713\n",
      "train loss:0.7431070742147623\n",
      "train loss:0.5294399686386241\n",
      "train loss:0.4024751201363276\n",
      "train loss:0.49164917017465604\n",
      "train loss:0.771764906531317\n",
      "train loss:0.554222658125053\n",
      "train loss:0.5234419568032658\n",
      "train loss:0.5228507644768612\n",
      "train loss:0.44870927610126754\n",
      "train loss:0.4161584816094672\n",
      "train loss:0.6815690594234896\n",
      "train loss:0.7744304174500376\n",
      "train loss:0.4042194218089657\n",
      "train loss:0.7219894411679144\n",
      "train loss:0.49010898906784695\n",
      "train loss:0.8200735594602422\n",
      "train loss:0.5196606989840266\n",
      "train loss:0.42127960366584966\n",
      "train loss:0.4000554284618426\n",
      "train loss:0.33928224074309804\n",
      "train loss:0.6401076216963284\n",
      "train loss:0.8763953712637838\n",
      "train loss:0.7039828098033857\n",
      "train loss:0.3899636188518119\n",
      "train loss:0.6016537001888318\n",
      "train loss:0.5221026279438761\n",
      "train loss:0.47539636384172573\n",
      "train loss:0.3860132132203022\n",
      "train loss:0.6566577358753056\n",
      "train loss:0.7189189409395386\n",
      "train loss:0.49299923449938216\n",
      "train loss:0.6420846308063797\n",
      "train loss:0.5814047268207385\n",
      "train loss:0.4021013210557506\n",
      "train loss:0.4805553830485717\n",
      "train loss:0.3950625909927854\n",
      "train loss:0.2235328139096276\n",
      "train loss:0.6215318062907688\n",
      "train loss:0.23050978679009043\n",
      "train loss:0.8294036839916041\n",
      "train loss:0.5881357413933505\n",
      "train loss:0.5008114245091452\n",
      "train loss:0.6333972084381883\n",
      "train loss:0.8274130579743026\n",
      "train loss:0.86333344452293\n",
      "train loss:0.6114148339167753\n",
      "train loss:0.5473896270653464\n",
      "train loss:0.5100322165172584\n",
      "train loss:0.5227313760404614\n",
      "train loss:0.546980870792872\n",
      "train loss:0.5989761751270766\n",
      "train loss:0.6280723957829913\n",
      "train loss:0.4851932311749166\n",
      "train loss:0.5899224491765207\n",
      "train loss:0.6035524397089296\n",
      "train loss:0.5097836472074748\n",
      "train loss:0.6789235083707025\n",
      "train loss:0.5433802862999227\n",
      "train loss:0.4793509862343246\n",
      "train loss:0.48532807677110795\n",
      "train loss:0.5767399495181574\n",
      "train loss:0.4965132332974206\n",
      "train loss:0.7406911159184458\n",
      "train loss:0.8236141016356028\n",
      "train loss:0.2536137840928595\n",
      "train loss:0.8742474972514362\n",
      "train loss:0.3763755103654158\n",
      "train loss:0.5291041221037156\n",
      "train loss:0.4644808456698396\n",
      "train loss:0.5980746555432188\n",
      "train loss:0.7755852655927311\n",
      "train loss:0.5002095697544694\n",
      "train loss:0.7356961910208892\n",
      "train loss:0.6106871065201347\n",
      "train loss:0.6337057020092571\n",
      "train loss:0.5392411722757736\n",
      "train loss:0.5925864536822398\n",
      "train loss:0.4848135079023397\n",
      "train loss:0.46912984255278556\n",
      "train loss:0.4525958110623236\n",
      "train loss:0.5250667401969713\n",
      "train loss:0.6829307724225238\n",
      "train loss:0.5017990601924173\n",
      "train loss:0.4480342855146332\n",
      "train loss:0.5687198704088285\n",
      "train loss:0.3607799517850184\n",
      "train loss:0.8466485364034618\n",
      "train loss:0.6907209328242646\n",
      "train loss:0.3764807310902207\n",
      "train loss:0.5781743833956664\n",
      "train loss:0.5646054141902157\n",
      "train loss:0.5856223749946665\n",
      "train loss:0.6329910717314309\n",
      "train loss:0.8494289193463024\n",
      "train loss:0.47216897191023965\n",
      "train loss:0.5225722476368321\n",
      "train loss:0.47436634788438764\n",
      "train loss:0.4720465993642463\n",
      "train loss:0.4471427954119854\n",
      "train loss:0.6371534353351314\n",
      "train loss:0.6827520574482281\n",
      "train loss:0.5161551719918129\n",
      "train loss:0.3599206163437567\n",
      "train loss:0.5702114573741591\n",
      "train loss:0.475138690188769\n",
      "train loss:0.6130412733258919\n",
      "train loss:0.6137560715018602\n",
      "train loss:0.6490813687287401\n",
      "train loss:0.5780782144826299\n",
      "train loss:0.8404311356825345\n",
      "train loss:0.5832835953390794\n",
      "train loss:0.6211158208436505\n",
      "train loss:0.6252276908543717\n",
      "train loss:0.605168287063815\n",
      "train loss:0.621721650847661\n",
      "train loss:0.6813164425318694\n",
      "train loss:0.6465006280008391\n",
      "train loss:0.5754621811755121\n",
      "train loss:0.5871204131753404\n",
      "train loss:0.6864136250372057\n",
      "train loss:0.5299604003803917\n",
      "train loss:0.48257725747237634\n",
      "train loss:0.4304237757032558\n",
      "train loss:0.408509777365904\n",
      "train loss:0.6448376728372976\n",
      "train loss:0.5513591097088701\n",
      "train loss:0.40592972738199123\n",
      "train loss:0.18980563385012725\n",
      "train loss:0.6145251262899448\n",
      "train loss:0.4874790156827621\n",
      "train loss:0.6403302787001401\n",
      "train loss:0.5485232574058132\n",
      "train loss:0.6447928948147645\n",
      "train loss:0.474002019632621\n",
      "train loss:0.3586325387824149\n",
      "train loss:0.7139021145070783\n",
      "train loss:0.527490973664998\n",
      "train loss:0.41876086361158393\n",
      "train loss:0.5913716736610798\n",
      "train loss:0.5509649056713996\n",
      "train loss:0.6174012559114868\n",
      "train loss:0.8668599346111912\n",
      "train loss:0.5061058966150265\n",
      "train loss:0.47786748970772\n",
      "train loss:0.5855688534379292\n",
      "train loss:0.3663416078489002\n",
      "train loss:0.5207183175269287\n",
      "train loss:0.4753308751614781\n",
      "train loss:0.611224621705053\n",
      "train loss:0.33945977794811594\n",
      "train loss:0.6042592413802389\n",
      "train loss:0.5858363658987648\n",
      "train loss:0.6311324139369916\n",
      "train loss:0.7579848171969771\n",
      "train loss:0.768981692659928\n",
      "train loss:0.5961806597184103\n",
      "train loss:0.5220903671964798\n",
      "train loss:0.5529992850207055\n",
      "train loss:0.5679314565286291\n",
      "train loss:0.5263185573968463\n",
      "train loss:0.48196403788947934\n",
      "train loss:0.6328721499356008\n",
      "train loss:0.7792424270309616\n",
      "train loss:0.4630365345965265\n",
      "train loss:0.46897029054822675\n",
      "train loss:0.4552118726349327\n",
      "train loss:0.6652278290625033\n",
      "train loss:0.5706661750538523\n",
      "train loss:0.3693097420364443\n",
      "train loss:0.40098081652371187\n",
      "train loss:0.7624169686975589\n",
      "train loss:0.7211257377099549\n",
      "train loss:0.45729037649031967\n",
      "train loss:0.39100987028994455\n",
      "train loss:0.497976956983444\n",
      "train loss:0.593745070183374\n",
      "train loss:0.5472599299212303\n",
      "train loss:0.48555807065637124\n",
      "train loss:0.627691798412559\n",
      "train loss:0.4209454373265727\n",
      "train loss:0.3940420864353728\n",
      "train loss:0.30046539323611315\n",
      "train loss:0.7676027984920982\n",
      "train loss:0.4413731271399796\n",
      "train loss:0.5732300393170552\n",
      "train loss:0.5674449923084491\n",
      "train loss:0.6105804693667327\n",
      "train loss:0.5859782315626024\n",
      "train loss:0.5940010390742854\n",
      "train loss:0.7547032677601285\n",
      "train loss:0.3487728662941315\n",
      "train loss:0.7891400116531131\n",
      "train loss:0.4474340053203886\n",
      "train loss:0.5409996834329014\n",
      "train loss:0.5434786418243769\n",
      "train loss:0.5084650031123935\n",
      "train loss:0.565954185993372\n",
      "train loss:0.39757909805682284\n",
      "train loss:0.5137919988059562\n",
      "train loss:0.7532383133269693\n",
      "train loss:0.5905065929706944\n",
      "train loss:0.6181589698947768\n",
      "train loss:0.6494381561901111\n",
      "train loss:0.35717738004593\n",
      "train loss:0.6129820456015296\n",
      "train loss:0.6072096434048849\n",
      "train loss:0.5788299358079009\n",
      "train loss:0.5879746221816632\n",
      "train loss:0.44993142167865285\n",
      "train loss:0.6411130770594189\n",
      "train loss:0.43023478352036904\n",
      "train loss:0.41483091857387333\n",
      "train loss:0.6094733191618198\n",
      "train loss:0.6537268698558816\n",
      "train loss:0.4477049586677235\n",
      "train loss:0.7159247972870699\n",
      "train loss:0.5531122756536119\n",
      "train loss:0.5740351926826941\n",
      "train loss:0.560414532804887\n",
      "train loss:0.6446507683159255\n",
      "train loss:0.46874584948900744\n",
      "train loss:0.43106503477018804\n",
      "train loss:0.43503866264405133\n",
      "train loss:0.5413252117060372\n",
      "train loss:0.4651749545938273\n",
      "train loss:0.616039574482204\n",
      "train loss:0.6431432021698129\n",
      "train loss:0.3576677141580793\n",
      "train loss:0.6910651839251603\n",
      "train loss:0.522606502581871\n",
      "train loss:0.597821129027132\n",
      "train loss:0.42758230994757407\n",
      "train loss:0.5544675273704083\n",
      "train loss:0.5319450983800835\n",
      "train loss:0.46917024546932173\n",
      "train loss:0.5631560275731113\n",
      "train loss:0.5532242603389139\n",
      "train loss:0.5543933007372838\n",
      "train loss:0.4903788127911829\n",
      "train loss:0.4966651848514063\n",
      "train loss:0.4265955684466741\n",
      "train loss:0.4053317476044208\n",
      "train loss:0.39173600217580906\n",
      "train loss:0.21383529919342453\n",
      "train loss:0.4523425897245712\n",
      "train loss:0.5908537237501863\n",
      "train loss:0.34328686521868435\n",
      "train loss:0.6353234012859275\n",
      "train loss:0.65210847159874\n",
      "train loss:0.6363527704359393\n",
      "train loss:0.7212846992329964\n",
      "train loss:0.50472149354642\n",
      "train loss:0.5222661370251469\n",
      "train loss:0.3565979343218856\n",
      "train loss:0.4141083504251707\n",
      "train loss:0.7717318804589175\n",
      "train loss:0.557116078261281\n",
      "train loss:0.5764889420348858\n",
      "train loss:0.42657480863081787\n",
      "train loss:0.4308419480285221\n",
      "train loss:0.5796886047880222\n",
      "train loss:0.48603448097383345\n",
      "train loss:0.46825439990377243\n",
      "train loss:0.47225557876859253\n",
      "train loss:0.635247565311644\n",
      "train loss:0.5824812282241809\n",
      "train loss:0.6896917917351695\n",
      "train loss:0.5163411337539572\n",
      "train loss:0.4861987463142404\n",
      "train loss:0.6227065614913815\n",
      "train loss:0.5937151843209323\n",
      "train loss:0.43268840768334255\n",
      "train loss:0.34196590899388163\n",
      "train loss:0.7062505608453495\n",
      "train loss:0.25687266971286216\n",
      "train loss:0.324380815920349\n",
      "train loss:0.7442447968150078\n",
      "train loss:0.3875222767465714\n",
      "train loss:0.5354041026442963\n",
      "train loss:0.6556705175074455\n",
      "train loss:0.5021914938863815\n",
      "train loss:0.4807570747803084\n",
      "train loss:0.25065330461807683\n",
      "train loss:0.5108620054761159\n",
      "train loss:0.6045573541780069\n",
      "train loss:0.3724619932421242\n",
      "train loss:0.31873696101177373\n",
      "train loss:0.26149956614482395\n",
      "train loss:0.5170314283373577\n",
      "train loss:0.48393295601149405\n",
      "train loss:0.6614497931112767\n",
      "train loss:0.35716725835937957\n",
      "train loss:0.35093532312114367\n",
      "train loss:0.15939557306354493\n",
      "train loss:0.47313737101372\n",
      "train loss:0.31414787723698656\n",
      "train loss:0.6098708203562146\n",
      "train loss:0.4344187899331384\n",
      "train loss:0.2628161664655374\n",
      "train loss:0.4687689477419954\n",
      "train loss:0.175927278202099\n",
      "train loss:0.4261448625389729\n",
      "train loss:0.2527515708816524\n",
      "train loss:0.24887198622202672\n",
      "train loss:0.26565261093769565\n",
      "train loss:0.5238676940040058\n",
      "train loss:0.5233026525039768\n",
      "train loss:0.16168757103563297\n",
      "train loss:0.4560579637651059\n",
      "train loss:0.9203265447880085\n",
      "train loss:0.7536403635931813\n",
      "train loss:0.49810713277164387\n",
      "train loss:0.5397404614121225\n",
      "train loss:0.4850597416591861\n",
      "train loss:0.6131529863175287\n",
      "train loss:0.689131600060745\n",
      "train loss:0.82389676870733\n",
      "train loss:0.42915337454295815\n",
      "train loss:0.42161073100617885\n",
      "train loss:0.49875417816977335\n",
      "train loss:0.5093024964974349\n",
      "train loss:0.5088520511342236\n",
      "train loss:0.5464731879338892\n",
      "train loss:0.5420886024777809\n",
      "train loss:0.49542467247931726\n",
      "train loss:0.4810058748490009\n",
      "train loss:0.3093425342019086\n",
      "train loss:0.48063040840593674\n",
      "train loss:0.32089478354606343\n",
      "train loss:0.42955213973685735\n",
      "train loss:0.3918499720321712\n",
      "train loss:0.34644245621599845\n",
      "train loss:0.458640143793614\n",
      "train loss:0.4435451950757005\n",
      "train loss:0.6059749693208262\n",
      "train loss:0.370202072246853\n",
      "train loss:0.33667947466644604\n",
      "train loss:0.36868428105297024\n",
      "train loss:0.3203598513625643\n",
      "train loss:0.2410137035085783\n",
      "train loss:0.5013408044858008\n",
      "train loss:0.5909556760498991\n",
      "train loss:0.9531880401836569\n",
      "train loss:0.47729866767765844\n",
      "train loss:0.47097098389459474\n",
      "train loss:0.7948583762544511\n",
      "train loss:0.4367976922569904\n",
      "train loss:0.5059008212507388\n",
      "train loss:0.4349783666964089\n",
      "train loss:0.36219320236010466\n",
      "train loss:0.6544485954512721\n",
      "train loss:0.4310574806862542\n",
      "train loss:0.3499181222350488\n",
      "train loss:0.6083868164531985\n",
      "train loss:0.4145851057571755\n",
      "train loss:0.25932924775915817\n",
      "train loss:0.3520892844868774\n",
      "train loss:0.6605243765911377\n",
      "train loss:0.3591338036324022\n",
      "train loss:0.6019749597365602\n",
      "train loss:0.5044895591551922\n",
      "train loss:0.30223254965808477\n",
      "train loss:0.6471475654397603\n",
      "train loss:0.3276033901037086\n",
      "train loss:0.2896992755209901\n",
      "train loss:0.5003296412480494\n",
      "train loss:0.3764436315361709\n",
      "train loss:0.6562311081234051\n",
      "train loss:0.6080842562924145\n",
      "train loss:0.25074505380347145\n",
      "train loss:0.4550726589551898\n",
      "train loss:0.6103153587789506\n",
      "train loss:0.31781434089407273\n",
      "train loss:0.6140061473299633\n",
      "train loss:0.6364987137686398\n",
      "train loss:0.34827528318838274\n",
      "train loss:0.24346810401873595\n",
      "train loss:0.34683963455749683\n",
      "train loss:0.7009325357212493\n",
      "train loss:0.45063326277547355\n",
      "train loss:0.26033666220905155\n",
      "train loss:0.2872182389804244\n",
      "train loss:0.5596347304627003\n",
      "train loss:0.4350621125314418\n",
      "train loss:0.4213690444137826\n",
      "train loss:0.2789852916878541\n",
      "train loss:0.4655842526459967\n",
      "train loss:0.40279811510929503\n",
      "train loss:0.43765718504946827\n",
      "train loss:0.5061844247585368\n",
      "train loss:0.47010838560968676\n",
      "train loss:0.1622748513928452\n",
      "train loss:0.6051923587585144\n",
      "train loss:0.48593549292801086\n",
      "train loss:0.41313336468441497\n",
      "train loss:0.35638785758013125\n",
      "train loss:0.3200708416143053\n",
      "train loss:0.5709814913096454\n",
      "train loss:0.33447277424015953\n",
      "train loss:0.6184910856397877\n",
      "train loss:0.24258546526821906\n",
      "train loss:0.5237637883174402\n",
      "train loss:0.614850813094503\n",
      "train loss:0.33980592611616367\n",
      "train loss:0.4101382204810803\n",
      "train loss:0.35036147619139363\n",
      "train loss:0.8035158780824657\n",
      "train loss:0.48502130899920787\n",
      "train loss:0.33490576301903957\n",
      "train loss:0.3260781285833174\n",
      "train loss:0.3530352461913241\n",
      "train loss:0.4012616759857945\n",
      "train loss:0.49707154767652123\n",
      "train loss:0.5797782767094578\n",
      "train loss:0.36704199418994615\n",
      "train loss:0.3812437794573905\n",
      "train loss:0.39074505747777133\n",
      "train loss:0.5756032318803531\n",
      "train loss:0.7002987298732386\n",
      "train loss:0.6966667633994853\n",
      "train loss:0.37069622883744924\n",
      "train loss:0.22079751491239583\n",
      "train loss:0.30621407701543213\n",
      "train loss:0.3417697675895967\n",
      "train loss:0.24742891702532024\n",
      "train loss:0.6386609618344361\n",
      "train loss:0.22628062203833238\n",
      "train loss:0.2784789351637643\n",
      "train loss:0.14797079367293908\n",
      "train loss:0.5110247843648307\n",
      "train loss:0.33728533422151263\n",
      "train loss:0.5623466698883689\n",
      "train loss:0.5518198513656912\n",
      "train loss:0.612803899233547\n",
      "train loss:0.20039645543899764\n",
      "train loss:0.5552641620224934\n",
      "train loss:0.2714081569756474\n",
      "train loss:0.3642555956000564\n",
      "train loss:0.40090669213699853\n",
      "train loss:0.3079425675122777\n",
      "train loss:0.20739949991096288\n",
      "train loss:0.26718165380458847\n",
      "train loss:0.44646360703955035\n",
      "train loss:0.37889489971757556\n",
      "train loss:0.5791493000382332\n",
      "train loss:0.3525774822149298\n",
      "train loss:0.17308003049793563\n",
      "train loss:0.5794717481805567\n",
      "train loss:0.4926067003065766\n",
      "train loss:0.25080627882584955\n",
      "train loss:0.310614274515873\n",
      "train loss:0.35201639803741924\n",
      "train loss:0.5775217698143125\n",
      "train loss:0.4323668183386933\n",
      "train loss:0.29145019451157117\n",
      "train loss:0.21242682753598485\n",
      "train loss:0.2543444833475471\n",
      "train loss:0.5501237718420111\n",
      "train loss:0.3171296013041907\n",
      "train loss:0.4886452360321483\n",
      "train loss:0.6091494753523367\n",
      "train loss:0.3339771107471507\n",
      "train loss:0.5666112075200476\n",
      "train loss:0.5365181244269353\n",
      "train loss:0.23354352175208576\n",
      "train loss:0.7054651885952099\n",
      "train loss:0.26860604118206266\n",
      "train loss:0.6820412338574142\n",
      "train loss:0.29868625987513775\n",
      "train loss:0.32688444903187025\n",
      "train loss:0.23014794778757502\n",
      "train loss:0.39252617593019845\n",
      "train loss:0.4928010321567857\n",
      "train loss:0.6697505785530957\n",
      "train loss:0.36357764347878846\n",
      "train loss:0.2915135448361283\n",
      "train loss:0.36074742123360687\n",
      "train loss:0.6587791987751535\n",
      "train loss:1.0328369057153135\n",
      "train loss:0.23970983092044462\n",
      "train loss:0.4849160807661157\n",
      "train loss:0.42905153458740564\n",
      "train loss:0.22461464673746417\n",
      "train loss:0.45235844796147545\n",
      "train loss:0.5725206682130041\n",
      "train loss:0.3655728584547935\n",
      "train loss:0.6597128487621247\n",
      "train loss:0.5122159356366656\n",
      "train loss:0.5183076594839185\n",
      "train loss:0.2537366352409677\n",
      "train loss:0.34156565875779354\n",
      "train loss:0.3310978427161171\n",
      "train loss:0.42550690531732044\n",
      "train loss:0.39269649575743204\n",
      "train loss:0.18049255843317605\n",
      "train loss:0.3741469841492839\n",
      "train loss:0.20984888151332712\n",
      "train loss:0.27548969114127797\n",
      "train loss:0.29014198619962434\n",
      "train loss:0.213095249891959\n",
      "train loss:0.8406687130054312\n",
      "train loss:0.13736255018778454\n",
      "train loss:0.8694972867771467\n",
      "train loss:0.4175358037297531\n",
      "train loss:0.3298223273866423\n",
      "train loss:0.2459796450150639\n",
      "train loss:0.4312603905335302\n",
      "train loss:0.40330189713935327\n",
      "train loss:0.23656633087786472\n",
      "train loss:0.3522353003051242\n",
      "train loss:0.2893066075775715\n",
      "train loss:0.6163927617067231\n",
      "train loss:0.25604204945706105\n",
      "train loss:0.27800908250775924\n",
      "train loss:0.4216768024879915\n",
      "train loss:0.30860892716971083\n",
      "train loss:0.4409990814109646\n",
      "train loss:0.7995535122232151\n",
      "train loss:0.4669602029238799\n",
      "train loss:0.4892677755176423\n",
      "train loss:0.633510493061235\n",
      "train loss:0.4740195370438987\n",
      "train loss:0.3807581019685474\n",
      "train loss:0.3099297703610919\n",
      "train loss:0.4784322878180906\n",
      "train loss:0.36966809597973904\n",
      "train loss:0.5924918222629308\n",
      "train loss:0.2845993282661474\n",
      "train loss:0.15030645382932456\n",
      "train loss:0.6087546714188207\n",
      "train loss:0.741470614799664\n",
      "train loss:0.5027161551957166\n",
      "train loss:0.36639166664438083\n",
      "train loss:0.20834287952307107\n",
      "train loss:0.4022419311876564\n",
      "train loss:0.4477814205052194\n",
      "train loss:0.345190599296257\n",
      "train loss:0.5383692432015186\n",
      "train loss:0.2893277982018244\n",
      "train loss:0.2617699827075388\n",
      "train loss:0.4697026299805744\n",
      "train loss:0.5612374400995942\n",
      "train loss:0.38414510510307487\n",
      "train loss:0.7597774850316903\n",
      "train loss:0.40006854877606546\n",
      "train loss:0.3141945264737662\n",
      "train loss:0.2992648059509768\n",
      "train loss:0.3790534346296358\n",
      "train loss:0.31114849532171396\n",
      "train loss:0.5845984601675689\n",
      "train loss:0.7436589551950641\n",
      "train loss:0.44462895032308475\n",
      "train loss:0.31876715906810166\n",
      "train loss:0.293936707896597\n",
      "train loss:0.5462916551967385\n",
      "train loss:0.25416043217485673\n",
      "train loss:0.3650041358573851\n",
      "train loss:0.5513435661680383\n",
      "train loss:0.2748085018613793\n",
      "train loss:0.4645035533201722\n",
      "train loss:0.11816204572539017\n",
      "train loss:0.4790490517205555\n",
      "train loss:0.281778868558127\n",
      "train loss:0.1715825700257933\n",
      "train loss:0.4284711642953539\n",
      "train loss:0.5262241336253616\n",
      "train loss:0.19733332439950707\n",
      "train loss:0.442394154952128\n",
      "train loss:0.2259614442645371\n",
      "train loss:0.8097878827274524\n",
      "train loss:0.684374264666289\n",
      "train loss:0.3160210479756723\n",
      "train loss:0.27182105390346745\n",
      "train loss:0.4984177747965252\n",
      "train loss:0.5161439795498489\n",
      "train loss:0.29022007241141023\n",
      "train loss:0.3798028025188272\n",
      "train loss:0.27831191052863075\n",
      "train loss:0.2816584870246489\n",
      "train loss:0.38069287130810336\n",
      "train loss:0.5467455970642356\n",
      "train loss:0.2468411236113525\n",
      "train loss:0.2653160206930013\n",
      "train loss:0.6773398665927007\n",
      "train loss:0.2929699431851266\n",
      "train loss:0.23009300542476172\n",
      "train loss:0.3911956261700481\n",
      "train loss:0.3875235577281864\n",
      "train loss:0.6607198351917056\n",
      "train loss:0.4455207031933696\n",
      "train loss:0.3095918338933433\n",
      "train loss:0.3751917952117866\n",
      "train loss:0.24320044726499535\n",
      "train loss:0.33617184072140194\n",
      "train loss:0.6274985678568665\n",
      "train loss:0.39146375212796514\n",
      "train loss:0.4528079493699712\n",
      "train loss:0.5612487223517972\n",
      "train loss:0.24724318424527283\n",
      "train loss:0.3672512715024026\n",
      "train loss:0.4049603585397028\n",
      "train loss:0.18557079567234644\n",
      "train loss:0.8473094774027048\n",
      "train loss:0.4193052230755767\n",
      "train loss:1.0966058985793405\n",
      "train loss:0.3981568257025484\n",
      "train loss:0.5832545664423521\n",
      "train loss:0.43140460042025197\n",
      "train loss:0.4059532676404891\n",
      "train loss:0.4248206443273131\n",
      "train loss:0.46256365944079453\n",
      "train loss:0.3187435910600437\n",
      "train loss:0.35097721422470773\n",
      "train loss:0.46385260908988857\n",
      "train loss:0.12201306213652455\n",
      "train loss:0.2969592138879655\n",
      "train loss:0.3738394348140582\n",
      "train loss:0.5885239148959663\n",
      "train loss:0.38730073560115597\n",
      "train loss:0.1607971495871547\n",
      "train loss:0.3530770272457936\n",
      "train loss:0.5850128363737612\n",
      "train loss:0.2133287377315159\n",
      "train loss:0.43454330279977127\n",
      "train loss:0.36216574634723553\n",
      "train loss:0.5436820016972737\n",
      "train loss:0.24360362842911482\n",
      "train loss:0.5135675081303003\n",
      "train loss:0.13782866194385987\n",
      "train loss:0.5095190337208289\n",
      "train loss:0.2880696269458257\n",
      "train loss:0.6201621074984514\n",
      "train loss:0.6388309873610896\n",
      "train loss:0.7703434719879112\n",
      "train loss:0.42372485053516257\n",
      "train loss:0.3609433114559721\n",
      "train loss:0.24860539684137056\n",
      "train loss:0.348021930705436\n",
      "train loss:0.46386108252002745\n",
      "train loss:0.59084738052419\n",
      "train loss:0.4717116227178789\n",
      "train loss:0.4378090921709029\n",
      "train loss:0.28354487166949754\n",
      "train loss:0.4041933647721356\n",
      "train loss:0.3208114302343146\n",
      "train loss:0.4905829030564363\n",
      "train loss:0.27762294834783335\n",
      "train loss:0.2572724778152983\n",
      "train loss:0.3541110902003396\n",
      "train loss:0.40246150446003154\n",
      "train loss:0.3984476720383315\n",
      "train loss:0.30087689198571893\n",
      "train loss:0.360747677281685\n",
      "train loss:0.24704149870568565\n",
      "train loss:0.3078527493792398\n",
      "train loss:0.5320155691820487\n",
      "train loss:0.2980509246158135\n",
      "train loss:0.3340878540072217\n",
      "train loss:0.4144177655960757\n",
      "train loss:0.6419027023501515\n",
      "train loss:0.33423146559314076\n",
      "train loss:0.272824786629906\n",
      "train loss:0.35436692302937783\n",
      "train loss:0.35201371577427565\n",
      "train loss:0.6703342360296431\n",
      "train loss:0.23573979956995922\n",
      "train loss:0.3757109118889796\n",
      "train loss:0.35931226930497673\n",
      "train loss:0.34781211854333993\n",
      "train loss:0.22787709491998814\n",
      "train loss:0.293429977318866\n",
      "train loss:0.7725374388618507\n",
      "train loss:0.4083860863570095\n",
      "train loss:0.19083799820328223\n",
      "train loss:0.3032899661341788\n",
      "train loss:0.29649636489476927\n",
      "train loss:0.46506156618519234\n",
      "train loss:0.6921971955480692\n",
      "train loss:0.42950705155216184\n",
      "train loss:0.3470043969817487\n",
      "train loss:0.555718944270604\n",
      "train loss:0.4791136462290333\n",
      "train loss:0.21107384334680884\n",
      "train loss:0.21793930334813516\n",
      "train loss:0.1994491982087641\n",
      "train loss:0.5425355599708099\n",
      "train loss:0.28900843713563706\n",
      "train loss:0.6137636567507364\n",
      "train loss:0.5649847079243876\n",
      "train loss:0.4380762866813324\n",
      "train loss:0.38898777280215446\n",
      "train loss:0.8108970250852577\n",
      "train loss:0.3462813582249217\n",
      "train loss:0.349688379333816\n",
      "train loss:0.5427334869704209\n",
      "train loss:0.324041810362633\n",
      "train loss:0.7167396293669677\n",
      "train loss:0.4537300891678953\n",
      "train loss:0.4952799517001096\n",
      "train loss:0.463250943957718\n",
      "train loss:0.3191896697502591\n",
      "train loss:0.37617894167182286\n",
      "train loss:0.37467853125136974\n",
      "train loss:0.29421254752538106\n",
      "train loss:0.22341607464238397\n",
      "train loss:0.3541817039360657\n",
      "train loss:0.4711105708806268\n",
      "train loss:0.165498839133556\n",
      "train loss:0.2642298971748175\n",
      "train loss:0.49585432894915077\n",
      "train loss:0.1606067362718792\n",
      "train loss:0.577902144128347\n",
      "train loss:0.4004926277864125\n",
      "train loss:0.15199652535335056\n",
      "train loss:0.27220860389185514\n",
      "train loss:0.1820108000665601\n",
      "train loss:0.4317647122991892\n",
      "train loss:0.3701787275742137\n",
      "train loss:0.6028789049447055\n",
      "train loss:0.27674712126421375\n",
      "train loss:0.30386761163997456\n",
      "train loss:0.6136999560839816\n",
      "train loss:0.2884228859266313\n",
      "train loss:0.3315346086827158\n",
      "train loss:0.14171045384537986\n",
      "train loss:0.3314371987758963\n",
      "train loss:0.2424675652006137\n",
      "train loss:0.34699385192625976\n",
      "train loss:0.4707689552272766\n",
      "train loss:0.2665043650060872\n",
      "train loss:0.3095798166005309\n",
      "train loss:0.33273192194362783\n",
      "train loss:0.5336707361986953\n",
      "train loss:0.4370019542066096\n",
      "train loss:0.4432461044021915\n",
      "train loss:0.323174734721951\n",
      "train loss:0.5358379551999748\n",
      "train loss:0.20993450309220701\n",
      "train loss:0.4321816484323849\n",
      "train loss:0.3510525205843046\n",
      "train loss:0.5092542861530325\n",
      "train loss:0.28542489650312397\n",
      "train loss:0.23764302062364107\n",
      "train loss:0.47779970715937015\n",
      "train loss:0.13008085823559976\n",
      "train loss:0.2838294960348959\n",
      "train loss:0.18050010369841563\n",
      "train loss:0.2348793640515095\n",
      "train loss:0.24515718126634778\n",
      "train loss:0.2776089566349432\n",
      "train loss:0.23082898523931208\n",
      "train loss:0.39965816830747747\n",
      "train loss:0.24164004531833855\n",
      "train loss:0.33152882995719446\n",
      "train loss:0.30105533873063595\n",
      "train loss:0.34197324762543446\n",
      "train loss:0.16430719922383835\n",
      "train loss:0.11548810425013571\n",
      "train loss:0.23165595822334656\n",
      "train loss:0.3227110699051951\n",
      "train loss:0.2017816274129375\n",
      "train loss:0.8069728854404546\n",
      "train loss:0.22530225793999076\n",
      "train loss:0.21795373750934705\n",
      "train loss:0.27826980712900024\n",
      "train loss:0.17308972815771723\n",
      "train loss:0.23562072867028805\n",
      "train loss:0.4556010798243671\n",
      "train loss:0.41307902801228635\n",
      "train loss:0.25519111439884784\n",
      "train loss:0.21318157213526945\n",
      "train loss:0.21245985293025918\n",
      "train loss:0.40337165069071795\n",
      "train loss:0.49843966503340775\n",
      "train loss:0.29796717680450086\n",
      "train loss:0.2294731353264372\n",
      "train loss:0.23989983955559394\n",
      "train loss:0.2955020133761826\n",
      "train loss:0.14560569203897997\n",
      "train loss:0.1592813117598479\n",
      "train loss:0.3638966203325817\n",
      "train loss:0.3625439529904668\n",
      "train loss:0.20962027017917312\n",
      "train loss:0.22084522925006073\n",
      "train loss:0.34698356670792896\n",
      "train loss:0.1276833435129138\n",
      "train loss:0.11292150060775616\n",
      "train loss:0.5194128122458201\n",
      "train loss:0.41244713502992275\n",
      "train loss:0.2110172824706747\n",
      "train loss:0.17118701364631597\n",
      "train loss:0.38120325327575866\n",
      "train loss:0.6259928842306856\n",
      "train loss:0.39361580879922303\n",
      "train loss:0.15256581174764078\n",
      "train loss:0.20178519899681907\n",
      "train loss:0.07535986754929895\n",
      "train loss:0.31355691544847636\n",
      "train loss:0.23926755086191345\n",
      "train loss:0.33141951235033484\n",
      "train loss:0.4803364970210507\n",
      "train loss:0.46018589144125316\n",
      "train loss:0.3206316832142164\n",
      "train loss:0.31921137936652333\n",
      "train loss:0.330537094792203\n",
      "train loss:0.19947724731251892\n",
      "train loss:0.20189773209075507\n",
      "train loss:0.18944699649493266\n",
      "train loss:0.3964806158131148\n",
      "train loss:0.2797195376782301\n",
      "train loss:0.028009999998127498\n",
      "train loss:0.204642888219661\n",
      "train loss:0.24016938794602133\n",
      "train loss:0.28465290622379136\n",
      "train loss:0.48761522758034187\n",
      "train loss:0.3616117142576378\n",
      "train loss:0.24080139777208842\n",
      "train loss:0.24541472557758293\n",
      "train loss:0.34253422975514025\n",
      "train loss:0.27581392501435403\n",
      "train loss:0.643893181868785\n",
      "train loss:0.2772920536752197\n",
      "train loss:0.29818221392099664\n",
      "train loss:0.6675935373371418\n",
      "train loss:0.3473372324150305\n",
      "train loss:0.27111432796945534\n",
      "train loss:0.47050501702319514\n",
      "train loss:0.2572563923263248\n",
      "train loss:0.45546457509076427\n",
      "train loss:0.410875191965795\n",
      "train loss:0.14480067411111225\n",
      "train loss:0.20576459512003414\n",
      "train loss:0.4159056606952663\n",
      "train loss:0.36795036092298467\n",
      "train loss:0.5350144855138994\n",
      "train loss:0.33712039744929223\n",
      "train loss:0.20609463657104907\n",
      "train loss:0.15171829938353978\n",
      "train loss:0.3417551276718884\n",
      "train loss:0.32125739766076683\n",
      "train loss:0.1937739483103203\n",
      "train loss:0.4004811648431552\n",
      "train loss:0.1630522069922315\n",
      "train loss:0.08930616732623138\n",
      "train loss:0.3913279318976362\n",
      "train loss:0.32950439940493015\n",
      "train loss:0.282705752786284\n",
      "train loss:0.4641703423367738\n",
      "train loss:0.231839719489697\n",
      "train loss:0.12711133766646962\n",
      "train loss:0.4199905135106614\n",
      "train loss:0.2741962666816583\n",
      "train loss:0.6058943448546688\n",
      "train loss:0.2541857531120437\n",
      "train loss:0.29815626371247933\n",
      "train loss:0.4555745126546082\n",
      "train loss:0.2189508807125906\n",
      "train loss:0.2548392867484795\n",
      "train loss:0.2397022408667143\n",
      "train loss:0.18950725225071055\n",
      "train loss:0.18760979183225332\n",
      "train loss:0.36331847670071143\n",
      "train loss:0.13361542226538597\n",
      "train loss:0.13042392727631255\n",
      "train loss:0.5176521337533113\n",
      "train loss:0.22384600128715823\n",
      "train loss:0.2894964486567745\n",
      "train loss:0.6936606399615832\n",
      "train loss:0.4500916561672902\n",
      "train loss:0.38831666746166166\n",
      "train loss:0.264675650257807\n",
      "train loss:0.2683840793505723\n",
      "train loss:0.39050859874556565\n",
      "train loss:0.2533098099860665\n",
      "train loss:0.2566516541506835\n",
      "train loss:0.16307991587311368\n",
      "train loss:0.28853312451447405\n",
      "train loss:0.39871550019546387\n",
      "train loss:0.26095076096453157\n",
      "train loss:0.5058984821533954\n",
      "train loss:0.1969000001261086\n",
      "train loss:0.4868371580572063\n",
      "train loss:0.2704487883679566\n",
      "train loss:0.6624057404871917\n",
      "train loss:0.5200886808099993\n",
      "train loss:0.16577494246635166\n",
      "train loss:0.4400362327999544\n",
      "train loss:0.3591781233017109\n",
      "train loss:0.18713379702535854\n",
      "train loss:0.18967904239274863\n",
      "train loss:0.20207926124278278\n",
      "train loss:0.47766194427793884\n",
      "train loss:0.10890298478943468\n",
      "train loss:0.2048112290133118\n",
      "train loss:0.1878856685280737\n",
      "train loss:0.44232795616341225\n",
      "train loss:0.5694868778739831\n",
      "train loss:0.093929310919772\n",
      "train loss:0.21498018575405758\n",
      "train loss:0.1889841070351837\n",
      "train loss:0.16125255993991391\n",
      "train loss:0.19764446226673332\n",
      "train loss:0.46219808889745134\n",
      "train loss:0.20528815469674228\n",
      "train loss:0.2331704939956433\n",
      "train loss:0.27222362095953534\n",
      "train loss:0.061938436014211874\n",
      "train loss:0.4726131173543389\n",
      "train loss:0.21886771615357814\n",
      "train loss:0.28614285018808727\n",
      "train loss:0.3675644873568158\n",
      "train loss:0.29892221421108484\n",
      "train loss:0.17541347794748258\n",
      "train loss:0.10490583203329089\n",
      "train loss:0.3463047054455609\n",
      "train loss:0.2585279606088147\n",
      "train loss:0.2578729876474029\n",
      "train loss:0.35666971714074747\n",
      "train loss:0.27942065279204453\n",
      "train loss:0.2369968140563345\n",
      "train loss:0.5602796647098833\n",
      "train loss:0.3890371732606797\n",
      "train loss:0.33362776663533994\n",
      "train loss:0.44760172094013145\n",
      "train loss:0.34768258674997493\n",
      "train loss:0.09704141015988536\n",
      "train loss:0.2554224013471237\n",
      "train loss:0.23233089470104784\n",
      "train loss:0.3469158405086531\n",
      "train loss:0.06632855318563895\n",
      "train loss:0.4458361885434649\n",
      "train loss:0.3755584576737195\n",
      "train loss:0.2364067944388557\n",
      "train loss:0.21662990049849187\n",
      "train loss:0.28043621552655057\n",
      "train loss:0.10011367281616705\n",
      "train loss:0.230251502339787\n",
      "train loss:0.31183997474560476\n",
      "train loss:0.40999762661646877\n",
      "train loss:0.28403639935629094\n",
      "train loss:0.36735514316426177\n",
      "train loss:0.18683250693074216\n",
      "train loss:0.49199416493897247\n",
      "train loss:0.2947094820041486\n",
      "train loss:0.2574523660927323\n",
      "train loss:0.28258480563900085\n",
      "train loss:0.37170139145868564\n",
      "train loss:0.14538244757878724\n",
      "train loss:0.24116937526002147\n",
      "train loss:0.0693025114680576\n",
      "train loss:0.30726362011388275\n",
      "train loss:0.28369307436516594\n",
      "train loss:0.32550349468827294\n",
      "train loss:0.5886055434126303\n",
      "train loss:0.14046908162344213\n",
      "train loss:0.10828293997425176\n",
      "train loss:0.26336329626891963\n",
      "train loss:0.0536831040642015\n",
      "train loss:0.3049789298957604\n",
      "train loss:0.3888875113076148\n",
      "train loss:0.6361906151261449\n",
      "train loss:0.18166881752695865\n",
      "train loss:0.16904380289338894\n",
      "train loss:0.25194245401723\n",
      "train loss:0.33771566635651423\n",
      "train loss:0.42405558717678876\n",
      "train loss:0.4201681610024526\n",
      "train loss:0.253754972866412\n",
      "train loss:0.2687516315563113\n",
      "train loss:0.423782018417847\n",
      "train loss:0.2635514244316911\n",
      "train loss:0.18600406480899073\n",
      "train loss:0.2415540059515003\n",
      "train loss:0.16837249316364125\n",
      "train loss:0.12934502358798158\n",
      "train loss:0.17763040235899635\n",
      "train loss:0.3794409748980816\n",
      "train loss:0.3481172090879388\n",
      "train loss:0.2556633901105637\n",
      "train loss:0.24293184963323577\n",
      "train loss:0.18706468306521962\n",
      "train loss:0.5519880350371499\n",
      "train loss:0.08454152710021239\n",
      "train loss:0.40436677262205045\n",
      "train loss:0.2867202314393188\n",
      "train loss:0.08575415428953614\n",
      "train loss:0.10448590205747195\n",
      "train loss:0.17998459479649256\n",
      "train loss:0.4106250247953943\n",
      "train loss:0.29326059410874794\n",
      "train loss:0.19995856242512675\n",
      "train loss:0.4247011084886612\n",
      "train loss:0.4283142124270605\n",
      "train loss:0.2983610368938742\n",
      "train loss:0.23434827693459556\n",
      "train loss:0.29491633194477446\n",
      "train loss:0.35449473637700035\n",
      "train loss:0.276003158678644\n",
      "train loss:0.22352735373926147\n",
      "train loss:0.3305510548598229\n",
      "train loss:0.35767798094727743\n",
      "train loss:0.23941047693678938\n",
      "train loss:0.09901383825291797\n",
      "train loss:0.330955476605894\n",
      "train loss:0.5323755007156986\n",
      "train loss:0.0992962068257125\n",
      "train loss:0.11181185518288295\n",
      "train loss:0.47374149352737344\n",
      "train loss:0.23399154191882038\n",
      "train loss:0.417524841707161\n",
      "train loss:0.1566502494932483\n",
      "train loss:0.40690121382138755\n",
      "train loss:0.20844109030240315\n",
      "train loss:0.1854028178847448\n",
      "train loss:0.2129322024912242\n",
      "train loss:0.478726754005511\n",
      "train loss:0.09476406371114496\n",
      "train loss:0.21190863934510112\n",
      "train loss:0.16250568717179764\n",
      "train loss:0.22479789416694737\n",
      "train loss:0.3615175620166421\n",
      "train loss:0.11183994150304881\n",
      "train loss:0.33587671026834864\n",
      "train loss:0.052848474734259535\n",
      "train loss:0.23720408644478225\n",
      "train loss:0.2880893621068423\n",
      "train loss:0.3893785630257313\n",
      "train loss:0.2425641087118174\n",
      "train loss:0.39084598499414075\n",
      "train loss:0.1564718680830161\n",
      "train loss:0.3877285583225537\n",
      "train loss:0.27717474033669487\n",
      "train loss:0.17632210186663597\n",
      "train loss:0.15061025319327198\n",
      "train loss:0.2122405034766453\n",
      "train loss:0.17079017404022098\n",
      "train loss:0.3539101053403714\n",
      "train loss:0.22294055019465914\n",
      "train loss:0.04618304143371861\n",
      "train loss:0.5427244897952203\n",
      "train loss:0.10257405003369224\n",
      "train loss:0.18576061992349657\n",
      "train loss:0.25307573475053613\n",
      "train loss:0.10496325534566615\n",
      "train loss:0.07070449721385899\n",
      "train loss:0.29912241221977703\n",
      "train loss:0.45152566290131785\n",
      "train loss:0.20559007353130182\n",
      "train loss:0.11402460976980615\n",
      "train loss:0.1862535915163957\n",
      "train loss:0.10333960668372606\n",
      "train loss:0.26424336809388094\n",
      "train loss:0.22060243541932062\n",
      "train loss:0.13102512861143006\n",
      "train loss:0.3121518980086016\n",
      "train loss:0.2105594089413591\n",
      "train loss:0.25691119186726913\n",
      "train loss:0.27079122428267915\n",
      "train loss:0.1842834103687932\n",
      "train loss:0.1369874732189911\n",
      "train loss:0.0880485757794415\n",
      "train loss:0.30274873543299613\n",
      "train loss:0.0867591724933466\n",
      "train loss:0.0563311117025336\n",
      "train loss:0.3539199514308161\n",
      "train loss:0.16551644434361748\n",
      "train loss:0.2793883529183679\n",
      "train loss:0.3148855618481847\n",
      "train loss:0.21227391213967675\n",
      "train loss:0.16582566606147003\n",
      "train loss:0.2493749173407112\n",
      "train loss:0.2783027016264443\n",
      "train loss:0.1848753619325552\n",
      "train loss:0.0772855697463948\n",
      "train loss:0.25119223622595366\n",
      "train loss:0.14624545567114663\n",
      "train loss:0.14137943784136303\n",
      "train loss:0.12177060208994135\n",
      "train loss:0.2998788799421067\n",
      "train loss:0.13258271500976865\n",
      "train loss:0.0921209952896628\n",
      "train loss:0.1870399519638793\n",
      "train loss:0.3406294332384851\n",
      "train loss:0.09793333733549815\n",
      "train loss:0.5144241019877352\n",
      "train loss:0.17441818203349968\n",
      "train loss:0.11039344054760503\n",
      "train loss:0.24374740510422135\n",
      "train loss:0.6090791152024628\n",
      "train loss:0.19381444277704454\n",
      "train loss:0.19699437905411815\n",
      "train loss:0.5509495505279457\n",
      "train loss:0.20214087235186814\n",
      "train loss:0.13740614381115815\n",
      "train loss:0.15401002933811353\n",
      "train loss:0.38715708296528056\n",
      "train loss:0.21149656250177434\n",
      "train loss:0.2808629282904752\n",
      "train loss:0.36566227387517614\n",
      "train loss:0.3572008540077033\n",
      "train loss:0.25001428990820196\n",
      "train loss:0.12299692538523846\n",
      "train loss:0.1996571715330909\n",
      "train loss:0.14951123220524415\n",
      "train loss:0.5689842048343488\n",
      "train loss:0.09079906332457753\n",
      "train loss:0.24364426068329686\n",
      "train loss:0.2623472578069479\n",
      "train loss:0.2793784791673033\n",
      "train loss:0.13933067034119465\n",
      "train loss:0.10199143106187888\n",
      "train loss:0.1781933663632607\n",
      "train loss:0.33480279833134136\n",
      "train loss:0.18733599995949765\n",
      "train loss:0.2268422051951137\n",
      "train loss:0.31727895566007996\n",
      "train loss:0.2554248174317232\n",
      "train loss:0.13255449901002578\n",
      "train loss:0.6856528457758536\n",
      "train loss:0.11561189545892266\n",
      "train loss:0.40353187269145285\n",
      "train loss:0.18746081133325002\n",
      "train loss:0.4215375366752118\n",
      "train loss:0.18735881231068588\n",
      "train loss:0.1809484467348615\n",
      "train loss:0.21284972348493988\n",
      "train loss:0.22039797049757387\n",
      "train loss:0.04352530666411893\n",
      "train loss:0.12472301444022685\n",
      "train loss:0.13566327659717642\n",
      "train loss:0.09880207624242938\n",
      "train loss:0.2778673902796573\n",
      "train loss:0.08872294966293737\n",
      "train loss:0.1233229419219066\n",
      "train loss:0.4501000538150013\n",
      "train loss:0.22859414198789701\n",
      "train loss:0.5668154579968168\n",
      "train loss:0.32312788364070766\n",
      "train loss:0.22808398020231876\n",
      "train loss:0.06042262528289496\n",
      "train loss:0.0853170868525519\n",
      "train loss:0.14604417153020896\n",
      "train loss:0.12213126151948868\n",
      "train loss:0.2037497225265569\n",
      "train loss:0.14833582725929392\n",
      "train loss:0.16301053563042323\n",
      "train loss:0.23402233602868622\n",
      "train loss:0.1949323539074424\n",
      "train loss:0.1366965674191745\n",
      "train loss:0.1793208921368045\n",
      "train loss:0.28344442056100716\n",
      "train loss:0.16888514563328108\n",
      "train loss:0.4054972203863393\n",
      "train loss:0.28760145658414615\n",
      "train loss:0.07809560579805488\n",
      "train loss:0.15281295010605622\n",
      "train loss:0.21248102431667254\n",
      "train loss:0.23190958321560715\n",
      "train loss:0.183468516334954\n",
      "train loss:0.132942745674473\n",
      "train loss:0.2441407612610964\n",
      "train loss:0.32078976768270645\n",
      "train loss:0.24611672453868785\n",
      "train loss:0.22542422913455057\n",
      "train loss:0.16770035742440637\n",
      "train loss:0.20700741478570667\n",
      "train loss:0.05883206096743495\n",
      "train loss:0.17130312620411065\n",
      "train loss:0.13975447540103342\n",
      "train loss:0.22183923038339085\n",
      "train loss:0.10687358459730763\n",
      "train loss:0.22021287790811925\n",
      "train loss:0.11941927570281327\n",
      "train loss:0.0633319741152234\n",
      "train loss:0.2292006914830786\n",
      "train loss:0.303049786820532\n",
      "train loss:0.25699524248319383\n",
      "train loss:0.11149394004667097\n",
      "train loss:0.08216123697587817\n",
      "train loss:0.20200238942967003\n",
      "train loss:0.1777601314395505\n",
      "train loss:0.2717573825124996\n",
      "train loss:0.07251006764578385\n",
      "train loss:0.017171390067613864\n",
      "train loss:0.15102662723406707\n",
      "train loss:0.04989085196540673\n",
      "train loss:0.09555069952723948\n",
      "train loss:0.4240164003039341\n",
      "train loss:0.12639541494523807\n",
      "train loss:0.1944667538702997\n",
      "train loss:0.17102273857814027\n",
      "train loss:0.13723751591963662\n",
      "train loss:0.06633543180748594\n",
      "train loss:0.1760883806497316\n",
      "train loss:0.09696786118868188\n",
      "train loss:0.16177894880260443\n",
      "train loss:0.3028077651319593\n",
      "train loss:0.2992578712665434\n",
      "train loss:0.11095212167693722\n",
      "train loss:0.302380493857528\n",
      "train loss:0.1039632118266022\n",
      "train loss:0.12250144144422877\n",
      "train loss:0.23434374379769923\n",
      "train loss:0.12604441456489182\n",
      "train loss:0.3395439308585404\n",
      "train loss:0.2796190644457216\n",
      "train loss:0.12855685317891252\n",
      "train loss:0.10908714526700417\n",
      "train loss:0.15956472370389832\n",
      "train loss:0.0662473028527692\n",
      "train loss:0.11454167193845954\n",
      "train loss:0.2354216033674618\n",
      "train loss:0.09415539486029798\n",
      "train loss:0.27040863893793815\n",
      "train loss:0.2884100283646741\n",
      "train loss:0.03278802411908582\n",
      "train loss:0.0788300707904315\n",
      "train loss:0.19740474678470735\n",
      "train loss:0.3357188493335397\n",
      "train loss:0.1047487985594004\n",
      "train loss:0.04771333372789475\n",
      "train loss:0.14758289566554283\n",
      "train loss:0.2002810131774117\n",
      "train loss:0.018611790022037913\n",
      "train loss:0.276738967515455\n",
      "train loss:0.12242432280080347\n",
      "train loss:0.11089620727839063\n",
      "train loss:0.39331628393168233\n",
      "train loss:0.30913722699682344\n",
      "train loss:0.14722981931037826\n",
      "train loss:0.2380382445436981\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.5490196078431373\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet3Layer(input_dim=(1,128,128), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9971934b-fb47-40f3-837d-acf8670b8ab2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,100) (10,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      3\u001b[0m network \u001b[38;5;241m=\u001b[39m ConvNet7Layer(input_dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m128\u001b[39m), \n\u001b[0;32m      4\u001b[0m                         conv_param \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter_num\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m30\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstride\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m},\n\u001b[0;32m      5\u001b[0m                         hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, weight_init_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(network, x_train, y_train, x_test, y_test,\n\u001b[0;32m      7\u001b[0m                   epochs\u001b[38;5;241m=\u001b[39mmax_epochs, mini_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      8\u001b[0m                   optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer_param\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.001\u001b[39m},\n\u001b[0;32m      9\u001b[0m                   evaluate_sample_num_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\4학년 2학기\\딥러닝\\과제\\common\\trainer.py:71\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[1;32m---> 71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step()\n\u001b[0;32m     73\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39maccuracy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_test)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\4학년 2학기\\딥러닝\\과제\\common\\trainer.py:44\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_train[batch_mask]\n\u001b[0;32m     42\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_train[batch_mask]\n\u001b[1;32m---> 44\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mgradient(x_batch, t_batch)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mparams, grads)\n\u001b[0;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mloss(x_batch, t_batch)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\4학년 2학기\\딥러닝\\과제\\mymethod\\neural_network.py:311\u001b[0m, in \u001b[0;36mConvNet7Layer.gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;66;03m# backward\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     dout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\4학년 2학기\\딥러닝\\과제\\mymethod\\neural_network.py:281\u001b[0m, in \u001b[0;36mConvNet7Layer.loss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m--> 281\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x)\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_layer\u001b[38;5;241m.\u001b[39mforward(y, t)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\4학년 2학기\\딥러닝\\과제\\mymethod\\neural_network.py:276\u001b[0m, in \u001b[0;36mConvNet7Layer.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 276\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\4학년 2학기\\딥러닝\\과제\\common\\layers.py:57\u001b[0m, in \u001b[0;36mAffine.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m---> 57\u001b[0m out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,100) (10,) "
     ]
    }
   ],
   "source": [
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet5Layer(input_dim=(1,128,128), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "415f56fc-9be8-45b9-b099-159b918020f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepConvNet:\n",
    "    \"\"\"정확도 99% 이상의 고정밀 합성곱 신경망\n",
    "\n",
    "    네트워크 구성은 아래와 같음\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        affine - relu - dropout - affine - dropout - softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 128, 128),\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=10):\n",
    "        # 가중치 초기화===========\n",
    "        # 각 층의 뉴런 하나당 앞 층의 몇 개 뉴런과 연결되는가（TODO: 자동 계산되게 바꿀 것）\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLU를 사용할 때의 권장 초깃값\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = wight_init_scales[6] * np.random.randn(64*16*16, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = wight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b3b498c-2762-4b36-a98f-4e7cb7ecbe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.9709256806202604\n",
      "=== epoch:1, train acc:0.7, test acc:0.65 ===\n",
      "train loss:1.3984322394348672\n",
      "train loss:1.8187798311885401\n",
      "train loss:2.116549593548173\n",
      "train loss:1.7508551644139583\n",
      "train loss:2.3349759407753137\n",
      "train loss:1.94194094006544\n",
      "train loss:1.2858997999051651\n",
      "train loss:1.4445398531766183\n",
      "train loss:2.2821732060023057\n",
      "train loss:2.132841022526148\n",
      "train loss:1.5270044188065321\n",
      "train loss:1.8285390357100517\n",
      "train loss:0.8955183114415768\n",
      "train loss:1.820848480323417\n",
      "train loss:1.627589939774459\n",
      "train loss:1.469362623766903\n",
      "train loss:1.5117115772516214\n",
      "train loss:1.3978751131508467\n",
      "train loss:1.6771010936156696\n",
      "train loss:1.9361185380907777\n",
      "train loss:1.8035171376436232\n",
      "train loss:2.2461845649977845\n",
      "train loss:1.3021557875535663\n",
      "train loss:1.945649166845417\n",
      "train loss:1.190166288478663\n",
      "train loss:1.397574496770248\n",
      "train loss:1.8441630104513405\n",
      "train loss:1.5414115118522769\n",
      "train loss:1.0834246053963557\n",
      "train loss:1.2237589262792052\n",
      "train loss:1.5768207500832456\n",
      "train loss:2.0314203077347903\n",
      "train loss:1.5264926198872355\n",
      "train loss:1.8442827167986304\n",
      "train loss:1.3257516465331332\n",
      "train loss:2.575298846035502\n",
      "train loss:1.2758741544567063\n",
      "train loss:1.5273656277367413\n",
      "train loss:1.077755585431982\n",
      "train loss:1.200616689190355\n",
      "train loss:1.393176124280757\n",
      "train loss:1.7636784731664235\n",
      "train loss:1.6232635824997974\n",
      "train loss:1.6695559253293717\n",
      "train loss:1.823924721408705\n",
      "train loss:1.3317232961511056\n",
      "train loss:1.8509392515114986\n",
      "train loss:1.4150999547882168\n",
      "train loss:1.5856958764157807\n",
      "train loss:1.2702058913336138\n",
      "train loss:0.9911321910955968\n",
      "train loss:1.9520923055972559\n",
      "train loss:1.6630780106528236\n",
      "train loss:0.8230277705620775\n",
      "train loss:0.9678802741025269\n",
      "train loss:1.551954683799957\n",
      "train loss:2.3786821660240767\n",
      "train loss:1.9214799248763879\n",
      "train loss:1.587999250836424\n",
      "train loss:1.5212866400290987\n",
      "train loss:1.7176239415715124\n",
      "train loss:1.640535784610345\n",
      "train loss:1.2223631686702618\n",
      "train loss:1.7788221313541475\n",
      "train loss:1.8644478561942595\n",
      "train loss:1.886646464251801\n",
      "train loss:1.3991431802695589\n",
      "train loss:1.8684330442431805\n",
      "train loss:2.186966302374473\n",
      "train loss:1.9303175898159186\n",
      "train loss:1.3104739205288267\n",
      "train loss:1.1701434528584773\n",
      "train loss:1.889830246079932\n",
      "train loss:1.5017778355237468\n",
      "train loss:1.65496494552611\n",
      "train loss:1.2603927654920108\n",
      "train loss:1.1436192188375784\n",
      "train loss:1.208309167406854\n",
      "train loss:2.0424533386601156\n",
      "train loss:1.4754904665531574\n",
      "train loss:1.656719946791183\n",
      "train loss:2.650046320208642\n",
      "train loss:0.8842501117657156\n",
      "train loss:1.5800755154692026\n",
      "train loss:1.3009219732198294\n",
      "train loss:1.3628476446618991\n",
      "train loss:1.1945743299340787\n",
      "train loss:1.422771738921594\n",
      "train loss:1.9951486189461023\n",
      "train loss:1.6718888850232907\n",
      "train loss:1.9377893894819351\n",
      "train loss:1.744048426082816\n",
      "train loss:1.5707118455703175\n",
      "train loss:2.058742212033967\n",
      "train loss:1.6363581882534934\n",
      "train loss:1.7165529232642833\n",
      "train loss:1.3156219715290853\n",
      "train loss:1.4970439272298566\n",
      "train loss:1.3926321792964704\n",
      "train loss:1.4298488469046506\n",
      "train loss:1.8662581613403642\n",
      "train loss:1.7586425101448337\n",
      "train loss:0.8863477457579874\n",
      "train loss:1.7005638736208692\n",
      "train loss:1.152324454702821\n",
      "train loss:1.8842834628441814\n",
      "train loss:1.8698356602768402\n",
      "train loss:1.0073576063451124\n",
      "train loss:1.8847287290810784\n",
      "train loss:2.216229151953646\n",
      "train loss:1.6766077575112193\n",
      "train loss:1.4834574331758952\n",
      "train loss:1.6817389107890381\n",
      "train loss:1.6526726818294688\n",
      "train loss:1.6691993550795794\n",
      "train loss:1.8096187758633555\n",
      "train loss:1.6138936998347237\n",
      "train loss:1.3200256078208485\n",
      "train loss:1.5125078365970224\n",
      "train loss:1.4717971741080287\n",
      "train loss:1.7164899247430974\n",
      "train loss:2.0335610117802805\n",
      "train loss:1.3343287831773218\n",
      "train loss:1.3124614424974834\n",
      "train loss:1.3136299227148696\n",
      "train loss:1.565072369050509\n",
      "train loss:1.2281143363783804\n",
      "train loss:1.2473491167271176\n",
      "train loss:1.6930022006884968\n",
      "train loss:2.007926318377578\n",
      "train loss:1.366631089983401\n",
      "train loss:1.187787634319346\n",
      "train loss:1.2626656416295807\n",
      "train loss:1.7641617512571113\n",
      "train loss:0.9193750648306886\n",
      "train loss:1.0627431643166845\n",
      "train loss:1.7738551817699908\n",
      "train loss:1.3127156207462216\n",
      "train loss:1.5984719388456352\n",
      "train loss:1.3568816564899073\n",
      "train loss:1.6247983603911322\n",
      "train loss:1.6129751827904244\n",
      "train loss:1.2975783429309047\n",
      "train loss:1.3901327412058282\n",
      "train loss:1.5426713753231396\n",
      "train loss:1.757919740345575\n",
      "train loss:1.309482669216998\n",
      "train loss:1.915161550192948\n",
      "train loss:1.5097339461703057\n",
      "train loss:1.1426697076612533\n",
      "train loss:0.5230550149931957\n",
      "train loss:1.706370883973727\n",
      "train loss:1.188876084054074\n",
      "train loss:1.8389212199218101\n",
      "train loss:1.3773283157081904\n",
      "train loss:1.4015362119522692\n",
      "train loss:1.4570146186743211\n",
      "train loss:1.9397897056416529\n",
      "train loss:1.4564655132294335\n",
      "train loss:0.969413433186778\n",
      "train loss:1.8083488902650884\n",
      "train loss:1.4689508550057482\n",
      "train loss:1.1128477563386698\n",
      "train loss:1.3215972596304781\n",
      "train loss:1.7759052330137894\n",
      "train loss:1.789132865240894\n",
      "train loss:1.1467697640936767\n",
      "train loss:1.6534695708406493\n",
      "train loss:1.3562243114482462\n",
      "train loss:1.397230123950623\n",
      "train loss:0.835382897917101\n",
      "train loss:1.1994764453270865\n",
      "train loss:0.9614396882362162\n",
      "train loss:1.7368520114204145\n",
      "train loss:1.2145533276320164\n",
      "train loss:0.9350535990107323\n",
      "train loss:1.2827784764171122\n",
      "train loss:1.394171618847525\n",
      "train loss:1.1713975158829333\n",
      "train loss:1.6466000027104322\n",
      "train loss:0.9697510113091269\n",
      "train loss:1.1416507071171018\n",
      "train loss:1.693529293805372\n",
      "train loss:0.7337387609002243\n",
      "train loss:1.4133042456747993\n",
      "train loss:1.8915175726189382\n",
      "train loss:1.2788603502755327\n",
      "train loss:1.3785759962368664\n",
      "train loss:1.440070258141819\n",
      "train loss:1.8619800349438773\n",
      "train loss:1.1044876378686275\n",
      "train loss:2.308756264032509\n",
      "train loss:1.3311767248367992\n",
      "train loss:0.9380766948985974\n",
      "train loss:1.6712341814135023\n",
      "train loss:1.6126916649922873\n",
      "train loss:1.0477715753772503\n",
      "train loss:1.5025865972816903\n",
      "train loss:1.4857515468637597\n",
      "train loss:1.4867332363550065\n",
      "train loss:1.1884206952648722\n",
      "train loss:1.912947490606567\n",
      "train loss:1.8130182746788261\n",
      "train loss:1.2623888958268623\n",
      "train loss:1.0911997313484787\n",
      "train loss:1.734973202526703\n",
      "train loss:1.1180324004764406\n",
      "train loss:1.4852116976762384\n",
      "train loss:1.318665289609948\n",
      "train loss:1.7744069359924062\n",
      "train loss:0.6515492244329554\n",
      "train loss:1.4608059279539427\n",
      "train loss:1.3743355908965351\n",
      "train loss:1.6041136986289377\n",
      "train loss:1.3902084502889065\n",
      "train loss:1.3144531552707488\n",
      "train loss:1.1376893028678736\n",
      "train loss:1.9643608391336205\n",
      "train loss:1.14592093054485\n",
      "train loss:1.7088105854972002\n",
      "train loss:1.8766840423418067\n",
      "train loss:1.8705547058531224\n",
      "train loss:0.5794354495384921\n",
      "train loss:1.3333832242265278\n",
      "train loss:0.9222657316514988\n",
      "train loss:1.1455928955841308\n",
      "train loss:1.679975780976891\n",
      "train loss:1.8971176711549997\n",
      "train loss:1.4232184877532617\n",
      "train loss:1.278605568634031\n",
      "train loss:1.5070695557679161\n",
      "train loss:1.9171507721829777\n",
      "train loss:1.38289953815831\n",
      "train loss:0.9969466603453261\n",
      "train loss:1.8229293082124003\n",
      "train loss:1.47669515735822\n",
      "train loss:1.2377788563668486\n",
      "train loss:1.1983277072032705\n",
      "train loss:1.4559613623768337\n",
      "train loss:1.669547639167217\n",
      "train loss:1.348916614989355\n",
      "train loss:1.3142456377369358\n",
      "train loss:1.0736174181064713\n",
      "train loss:1.1780693058195397\n",
      "train loss:1.4322299551767355\n",
      "train loss:1.7182725795948819\n",
      "train loss:0.9598243186245178\n",
      "train loss:1.4763262610315302\n",
      "train loss:1.0316606933645407\n",
      "train loss:1.250054781971934\n",
      "train loss:0.7592399417080122\n",
      "train loss:1.3753503766617248\n",
      "train loss:2.0779759436829357\n",
      "train loss:0.8483161540074423\n",
      "train loss:1.6592776414778925\n",
      "train loss:1.3244837792660404\n",
      "train loss:1.6400737980206148\n",
      "train loss:1.5674293249063491\n",
      "train loss:1.5618122573738997\n",
      "train loss:1.2314843550862606\n",
      "train loss:1.5257695593698908\n",
      "train loss:1.432233946824498\n",
      "train loss:1.5889349746488677\n",
      "train loss:1.7596198873547184\n",
      "train loss:1.1254889305711393\n",
      "train loss:1.4147358658340994\n",
      "train loss:1.445444872934425\n",
      "train loss:1.6034534221774766\n",
      "train loss:1.1368340230326308\n",
      "train loss:1.237728783597459\n",
      "train loss:1.5140436865798688\n",
      "train loss:1.5643591193226805\n",
      "train loss:2.0017856291333245\n",
      "train loss:1.2231339365418339\n",
      "train loss:1.589350412237637\n",
      "train loss:1.4954501880758833\n",
      "train loss:1.3948540471396955\n",
      "train loss:2.0243311894538474\n",
      "train loss:0.8287618256294864\n",
      "train loss:1.6530439885273243\n",
      "train loss:1.0303544041750707\n",
      "train loss:1.1325245428676811\n",
      "train loss:1.873398589123666\n",
      "train loss:1.4260358438857461\n",
      "train loss:0.677737754686786\n",
      "train loss:2.078491488137014\n",
      "train loss:2.0281134659940787\n",
      "train loss:1.7839476202710176\n",
      "train loss:1.6135260921563312\n",
      "train loss:1.5013367236142827\n",
      "train loss:2.2375464409354953\n",
      "train loss:1.317045547456579\n",
      "train loss:1.093425905113421\n",
      "train loss:2.044134845087805\n",
      "train loss:1.3594568428067437\n",
      "train loss:0.8529067561863835\n",
      "train loss:1.7074632166466235\n",
      "train loss:2.1163288273339953\n",
      "train loss:1.3080067869364278\n",
      "train loss:0.5006986043509521\n",
      "train loss:1.1199285907571073\n",
      "train loss:1.8515008229008658\n",
      "train loss:1.6037949480664762\n",
      "train loss:1.398318643718575\n",
      "train loss:1.4025828556765467\n",
      "train loss:1.2544635647680202\n",
      "train loss:1.9070151376047222\n",
      "train loss:1.7240043674709409\n",
      "train loss:1.6350634897872027\n",
      "train loss:1.7677875852550613\n",
      "train loss:1.3512721114115451\n",
      "train loss:1.7824109792447071\n",
      "train loss:1.9962099072230017\n",
      "train loss:1.3223759757451152\n",
      "train loss:1.9962957628192062\n",
      "train loss:1.5616550372077023\n",
      "train loss:0.896180578267273\n",
      "train loss:1.3503151965020286\n",
      "train loss:1.5177828931429425\n",
      "train loss:1.3214816119787334\n",
      "train loss:1.674742522350423\n",
      "train loss:2.0715876832057645\n",
      "train loss:1.312001551932798\n",
      "train loss:1.6226146611839696\n",
      "train loss:0.9117110416626015\n",
      "train loss:1.2880876356908764\n",
      "train loss:1.3269724567213386\n",
      "train loss:1.570641882484996\n",
      "train loss:2.169009134447212\n",
      "train loss:1.4059839379585912\n",
      "train loss:1.7745496338494533\n",
      "train loss:1.550175012113593\n",
      "train loss:1.6575545725940635\n",
      "train loss:1.435516533781103\n",
      "train loss:1.4303907183530233\n",
      "train loss:1.1639282149280459\n",
      "train loss:1.4003768502076126\n",
      "train loss:1.2982032853278132\n",
      "train loss:1.2708513126672778\n",
      "train loss:1.4336400124730166\n",
      "train loss:1.5443627368688069\n",
      "train loss:1.64456259821169\n",
      "train loss:1.038016939162039\n",
      "train loss:1.811925850471121\n",
      "train loss:1.5752930265787746\n",
      "train loss:1.4380796513198326\n",
      "train loss:1.9168870314469941\n",
      "train loss:1.1279359289609325\n",
      "train loss:1.5622436098478027\n",
      "train loss:1.3860455955006468\n",
      "train loss:1.2517234165189435\n",
      "train loss:1.3968673667315363\n",
      "train loss:1.5581005678665047\n",
      "train loss:1.689047523154251\n",
      "train loss:0.9450108348614592\n",
      "train loss:1.7669379143703434\n",
      "train loss:1.3385269249155833\n",
      "train loss:1.3456372566567847\n",
      "train loss:1.6345784822236844\n",
      "train loss:1.0005116781105081\n",
      "train loss:1.1458842498015112\n",
      "train loss:1.3350827188229042\n",
      "train loss:1.7167232597820115\n",
      "train loss:1.3157966785618869\n",
      "train loss:1.3528982087101127\n",
      "train loss:1.2224797867702564\n",
      "train loss:1.21740666912632\n",
      "train loss:1.5643705993179593\n",
      "train loss:1.5466909611079707\n",
      "train loss:1.7050373032225807\n",
      "train loss:1.0877433582695586\n",
      "train loss:1.2365473941854188\n",
      "train loss:1.5624717686239944\n",
      "train loss:1.1065672752709061\n",
      "train loss:1.7222664742396128\n",
      "train loss:1.7579374939797277\n",
      "train loss:1.2932766276588632\n",
      "train loss:1.4995890928412536\n",
      "train loss:1.570784413067389\n",
      "train loss:1.242321561580773\n",
      "train loss:1.6372462365588327\n",
      "train loss:1.5729957321735968\n",
      "train loss:0.9553705093294402\n",
      "train loss:1.8408117163389193\n",
      "train loss:1.1308516482676958\n",
      "train loss:1.1301485663007538\n",
      "train loss:1.3081658163480419\n",
      "train loss:0.9131961129192072\n",
      "train loss:1.6014825306644869\n",
      "train loss:1.536025284274499\n",
      "train loss:1.068952899361598\n",
      "train loss:1.702070902277552\n",
      "train loss:0.757158104359771\n",
      "train loss:1.6230620536279523\n",
      "train loss:1.4071600593782134\n",
      "train loss:1.0210447573023917\n",
      "train loss:1.7204094456678565\n",
      "train loss:1.1020392841872386\n",
      "train loss:1.7907238516992567\n",
      "train loss:1.8769550393222083\n",
      "train loss:1.5730435083589633\n",
      "train loss:1.886351832741171\n",
      "train loss:1.461511242115804\n",
      "train loss:1.2902023518932952\n",
      "train loss:1.146363609319736\n",
      "train loss:1.2577666164268377\n",
      "train loss:0.9709388857904443\n",
      "train loss:1.4934662008379924\n",
      "train loss:1.4104636804656718\n",
      "train loss:1.7655800533459824\n",
      "train loss:1.1395002240507344\n",
      "train loss:1.294407786192176\n",
      "train loss:1.427964239688234\n",
      "train loss:1.2245480342754882\n",
      "train loss:1.0331283841414993\n",
      "train loss:1.573952320500194\n",
      "train loss:1.6214686749783915\n",
      "train loss:1.4881967730051016\n",
      "train loss:1.5313626903543693\n",
      "train loss:1.2897242931375168\n",
      "train loss:1.511384718261219\n",
      "train loss:1.407028405121825\n",
      "train loss:1.114825043261162\n",
      "train loss:1.6037711616249504\n",
      "train loss:1.9514698939902537\n",
      "train loss:1.3254913544116245\n",
      "train loss:1.0991842064604003\n",
      "train loss:1.4410693591526305\n",
      "train loss:1.7893233485167925\n",
      "train loss:1.455955686692207\n",
      "train loss:1.2795415391730998\n",
      "train loss:1.0525234897601894\n",
      "train loss:1.4945086321732037\n",
      "train loss:0.963686782787826\n",
      "train loss:1.4243273838984967\n",
      "train loss:1.3067569524710867\n",
      "train loss:1.0597483205922786\n",
      "train loss:1.4744634901680027\n",
      "train loss:1.1995903138061377\n",
      "train loss:1.4239627788810392\n",
      "train loss:1.2572330066438098\n",
      "train loss:1.6787299559297484\n",
      "train loss:1.676223100012234\n",
      "train loss:1.5204734748778486\n",
      "train loss:1.1915416067818092\n",
      "train loss:1.0134494212331762\n",
      "train loss:1.9435998831752144\n",
      "train loss:1.6276589913704427\n",
      "train loss:1.4086230748940276\n",
      "train loss:1.2232533133776402\n",
      "train loss:0.8404764071754494\n",
      "train loss:1.6752024016590212\n",
      "train loss:1.6813398488988607\n",
      "train loss:1.034302407332001\n",
      "train loss:1.5167172111804113\n",
      "train loss:1.425647711897557\n",
      "train loss:1.8872548960353606\n",
      "train loss:0.9602652559921687\n",
      "train loss:1.4030124022010662\n",
      "train loss:1.520955782390907\n",
      "train loss:1.2441773177477695\n",
      "train loss:0.4320412446998018\n",
      "train loss:1.130036660074675\n",
      "train loss:1.298674748989827\n",
      "train loss:0.4939227649981369\n",
      "train loss:2.3664948432873296\n",
      "train loss:1.7977063597706011\n",
      "train loss:0.9403191768990272\n",
      "train loss:1.0589271793952695\n",
      "train loss:1.9089249617660553\n",
      "train loss:1.851720419392984\n",
      "train loss:1.8736160698107025\n",
      "train loss:1.7165143436850883\n",
      "train loss:1.0487759788903475\n",
      "train loss:1.0416455709055055\n",
      "train loss:1.6105085321427897\n",
      "train loss:1.3201132012804013\n",
      "train loss:1.3050425732292974\n",
      "train loss:1.3099179529741884\n",
      "train loss:1.5173051263222486\n",
      "train loss:0.8195639160528756\n",
      "train loss:1.520752325990149\n",
      "train loss:1.5033185471637815\n",
      "train loss:1.0406702181071261\n",
      "train loss:1.2035074461653594\n",
      "train loss:1.5934712321390143\n",
      "train loss:1.5238856555179217\n",
      "train loss:2.09699810885747\n",
      "train loss:1.5972836174089864\n",
      "train loss:1.6409973188180296\n",
      "train loss:1.3682454649751696\n",
      "train loss:1.3253449077667443\n",
      "train loss:1.397497152232817\n",
      "train loss:1.7016702747629946\n",
      "train loss:1.3918092482308961\n",
      "train loss:1.2375204901560273\n",
      "train loss:1.1639017043727449\n",
      "train loss:1.6140518796209313\n",
      "train loss:1.825548534869109\n",
      "train loss:1.5204174032957236\n",
      "train loss:1.2311924009810287\n",
      "train loss:1.3249556354928704\n",
      "train loss:1.0497826916519037\n",
      "train loss:1.6980856990832636\n",
      "train loss:1.4016385736619106\n",
      "train loss:1.58835046987477\n",
      "train loss:1.6920364808127677\n",
      "train loss:1.9538412763011883\n",
      "train loss:1.6313288558901617\n",
      "train loss:1.3668843859207138\n",
      "train loss:1.3663026251536126\n",
      "train loss:1.4111533766521251\n",
      "train loss:1.151747966802342\n",
      "train loss:1.384131551167966\n",
      "train loss:1.1540110206070762\n",
      "train loss:1.6351130185515594\n",
      "train loss:1.2410524988550933\n",
      "train loss:1.447050694624727\n",
      "train loss:1.5240346823509974\n",
      "train loss:2.317020385639548\n",
      "train loss:1.3197310696755102\n",
      "train loss:1.015679968358054\n",
      "train loss:1.0420345405610307\n",
      "train loss:1.3097765569542883\n",
      "train loss:1.426286151850755\n",
      "train loss:1.376450820744972\n",
      "train loss:1.1093349203945224\n",
      "train loss:1.264449205703499\n",
      "train loss:1.4928063731813288\n",
      "train loss:1.4611947007319166\n",
      "train loss:1.5486561841903728\n",
      "train loss:1.691214838748244\n",
      "train loss:1.0014351449461507\n",
      "train loss:1.5276503778883952\n",
      "train loss:0.9657197236566377\n",
      "train loss:0.9984552895462977\n",
      "train loss:1.6581348258794644\n",
      "train loss:1.18848323227121\n",
      "train loss:0.9189997885862065\n",
      "train loss:1.0738156252406565\n",
      "train loss:1.7150706913472837\n",
      "train loss:1.181833324633241\n",
      "train loss:0.8379807386175842\n",
      "train loss:0.9380347405163911\n",
      "train loss:1.340221547148422\n",
      "train loss:1.5858629391009553\n",
      "train loss:1.6816108673331602\n",
      "train loss:1.4682778401059011\n",
      "train loss:1.3610528966766193\n",
      "train loss:1.3754048202323599\n",
      "train loss:2.0612902784731464\n",
      "train loss:1.6848741592842835\n",
      "train loss:1.4440966909565085\n",
      "train loss:1.4906593520934153\n",
      "train loss:2.3554404693945763\n",
      "train loss:1.8876201562114159\n",
      "train loss:1.4153107159501177\n",
      "train loss:1.6263417344084978\n",
      "train loss:1.5068762173098111\n",
      "train loss:1.4333574126125284\n",
      "train loss:1.4653110137158534\n",
      "train loss:1.5707476398143543\n",
      "train loss:1.7128253344884108\n",
      "train loss:1.1844333475276727\n",
      "train loss:0.9063881344597187\n",
      "train loss:1.4237877404294612\n",
      "train loss:1.1069386901660965\n",
      "train loss:1.8248044351505281\n",
      "train loss:1.656386363086582\n",
      "train loss:1.1709917855448158\n",
      "train loss:1.6743040116459031\n",
      "train loss:1.501146334378863\n",
      "train loss:0.8025523664564295\n",
      "train loss:1.5563289193226897\n",
      "train loss:1.6436662359745882\n",
      "train loss:0.8922275499393052\n",
      "train loss:1.042383607118122\n",
      "train loss:2.2534533957615306\n",
      "train loss:1.9954269306636832\n",
      "train loss:1.4895469035290716\n",
      "train loss:1.4283083189974854\n",
      "train loss:1.2582899303398363\n",
      "train loss:1.0470798141876863\n",
      "train loss:1.321290570356569\n",
      "train loss:1.4799258073910868\n",
      "train loss:1.1274687514984059\n",
      "train loss:1.178952311536886\n",
      "train loss:0.7333000097497737\n",
      "train loss:1.195367018649192\n",
      "train loss:1.5615185463962649\n",
      "train loss:1.6947337463836434\n",
      "train loss:1.6285142748623955\n",
      "train loss:1.493059811294587\n",
      "train loss:1.5507777415346344\n",
      "train loss:1.3842001426056723\n",
      "train loss:1.5857067546363095\n",
      "train loss:1.068173960603056\n",
      "train loss:1.53850998262679\n",
      "train loss:1.278609943785726\n",
      "train loss:1.182970072245414\n",
      "train loss:1.3803526948784672\n",
      "train loss:1.353548302784\n",
      "train loss:1.7149077509575208\n",
      "train loss:1.3745930909683004\n",
      "train loss:1.1817569977203788\n",
      "train loss:0.5904039358584237\n",
      "train loss:1.1259001803475315\n",
      "train loss:1.1771973774685778\n",
      "train loss:1.541526545504639\n",
      "train loss:1.090142848510898\n",
      "train loss:1.3027078275566144\n",
      "train loss:1.1252746066263042\n",
      "train loss:0.9421535650993172\n",
      "train loss:1.6462062328756146\n",
      "train loss:1.564183682917402\n",
      "train loss:1.2288573792002528\n",
      "train loss:1.033282897395708\n",
      "train loss:1.4937425619395268\n",
      "train loss:1.3679809911483551\n",
      "train loss:1.75126778655567\n",
      "train loss:1.2191036367079762\n",
      "train loss:1.3025327620540748\n",
      "train loss:1.5412155305245339\n",
      "train loss:1.0643838610071934\n",
      "train loss:1.5177549940536204\n",
      "train loss:1.6397018391150127\n",
      "train loss:0.9304209102802592\n",
      "train loss:1.5169933652857719\n",
      "train loss:1.5510373910567166\n",
      "train loss:1.674177785408745\n",
      "train loss:1.4851000589466747\n",
      "train loss:1.6916797982456107\n",
      "train loss:0.9977065808415129\n",
      "train loss:1.2822424026323525\n",
      "train loss:1.3458720604297805\n",
      "train loss:1.50373474793764\n",
      "train loss:1.3415425975841737\n",
      "train loss:1.4653107559925413\n",
      "train loss:1.3313840031545197\n",
      "train loss:1.1038825624516815\n",
      "train loss:1.1713540889851077\n",
      "train loss:1.3748691408186973\n",
      "train loss:1.6518350907793677\n",
      "train loss:1.2113462663110979\n",
      "train loss:1.505212482416513\n",
      "train loss:1.5572492672738585\n",
      "train loss:1.3593654954258672\n",
      "train loss:1.2380058120161141\n",
      "train loss:1.065243423266081\n",
      "train loss:1.532932663651333\n",
      "train loss:1.7321176400410558\n",
      "train loss:1.7012349112447858\n",
      "train loss:1.7520651609165234\n",
      "train loss:1.5739994610025683\n",
      "train loss:2.2380185217613926\n",
      "train loss:1.1588788849777847\n",
      "train loss:1.3847244440486643\n",
      "train loss:1.311767171575767\n",
      "train loss:1.112241809534169\n",
      "train loss:1.2628991662762186\n",
      "train loss:1.5677682812985672\n",
      "train loss:1.1972421803962743\n",
      "train loss:1.3123606754547712\n",
      "train loss:1.7564695589732573\n",
      "train loss:0.9305863021249978\n",
      "train loss:1.3837665687806155\n",
      "train loss:1.9655000994626832\n",
      "train loss:1.2091318802447415\n",
      "train loss:1.7439383265991348\n",
      "train loss:1.3303109782088272\n",
      "train loss:1.8647220734275216\n",
      "train loss:1.510105364392397\n",
      "train loss:1.2853592217138443\n",
      "train loss:1.1627043043183876\n",
      "train loss:1.1672334362636974\n",
      "train loss:1.7838484175153042\n",
      "train loss:1.2695122054212593\n",
      "train loss:1.5599343859489037\n",
      "train loss:1.6058358316168282\n",
      "train loss:1.4909786405493217\n",
      "train loss:1.235538405641371\n",
      "train loss:1.0414966624991417\n",
      "train loss:1.307511527574797\n",
      "train loss:1.4973813138537309\n",
      "train loss:1.5286771328777435\n",
      "train loss:1.6338877583580338\n",
      "train loss:0.9570071420975512\n",
      "train loss:1.2592458735930911\n",
      "train loss:1.361986833402449\n",
      "train loss:0.8321496706146106\n",
      "train loss:0.8077821637397878\n",
      "train loss:1.6090187398989968\n",
      "train loss:1.7065378017430617\n",
      "train loss:1.5525039705014252\n",
      "train loss:1.222924814992679\n",
      "train loss:1.8862970528514524\n",
      "train loss:1.7000957262208427\n",
      "train loss:1.6001714640973401\n",
      "train loss:1.437301953137929\n",
      "train loss:1.6824153737571386\n",
      "train loss:0.907450358729454\n",
      "train loss:1.7779172145061914\n",
      "train loss:1.3171073327517777\n",
      "train loss:1.9178704943745355\n",
      "train loss:1.817136805793346\n",
      "train loss:1.358775062166582\n",
      "train loss:1.6516855138402324\n",
      "train loss:0.9765650183224748\n",
      "train loss:0.7209497000811327\n",
      "train loss:1.5752178988718466\n",
      "train loss:1.0342013833009094\n",
      "train loss:1.3319661586272424\n",
      "train loss:1.3316111083699647\n",
      "train loss:1.9627487888242925\n",
      "train loss:1.1488714930909842\n",
      "train loss:1.4998079623546352\n",
      "train loss:0.6767654542337033\n",
      "train loss:0.9596131209646466\n",
      "train loss:1.160510937376511\n",
      "train loss:1.6441625835417242\n",
      "train loss:1.1705914454467086\n",
      "train loss:1.359394139261203\n",
      "train loss:1.4325232848558038\n",
      "train loss:1.4349606817259255\n",
      "train loss:1.5800934285733697\n",
      "train loss:1.2341011537702826\n",
      "train loss:1.5602819068713767\n",
      "train loss:1.3756265839653814\n",
      "train loss:1.4567751047241428\n",
      "train loss:1.4161166683669006\n",
      "train loss:1.5561334934887276\n",
      "train loss:1.2229354626867956\n",
      "train loss:1.7950051800991766\n",
      "train loss:0.9945176935087302\n",
      "train loss:1.185350312011323\n",
      "train loss:1.5029566932646932\n",
      "train loss:1.3193179513531808\n",
      "train loss:0.8866901971772908\n",
      "train loss:1.3437588789533046\n",
      "train loss:1.1148073625900097\n",
      "train loss:1.269929610366457\n",
      "train loss:1.061686634138923\n",
      "train loss:1.6286925613844532\n",
      "train loss:1.2542222344213316\n",
      "train loss:1.3921999801121232\n",
      "train loss:0.9620187414856629\n",
      "train loss:1.9632484682780245\n",
      "train loss:1.659955833393966\n",
      "train loss:1.4174087852063706\n",
      "train loss:1.1666059759024519\n",
      "train loss:0.5544499874233547\n",
      "train loss:1.2123753861215998\n",
      "train loss:1.7465146349857665\n",
      "train loss:1.548537199542331\n",
      "train loss:1.5371212843891118\n",
      "train loss:1.5054238456969469\n",
      "train loss:1.240669857704672\n",
      "train loss:1.0540849483667507\n",
      "train loss:1.3847393641882975\n",
      "train loss:1.1783176672250353\n",
      "train loss:1.2598737157671684\n",
      "train loss:1.464877947540002\n",
      "train loss:1.9484058773576183\n",
      "train loss:0.9747583701906745\n",
      "train loss:1.71295765651464\n",
      "train loss:1.5527174876991565\n",
      "train loss:1.4485763687656754\n",
      "train loss:1.0602043002159036\n",
      "train loss:1.0746839175513203\n",
      "train loss:1.5702575686234048\n",
      "train loss:1.2897096426567485\n",
      "train loss:1.5507314092563036\n",
      "train loss:1.752245810672218\n",
      "train loss:1.5097990109750747\n",
      "train loss:1.0918718496992128\n",
      "train loss:0.8998539853489633\n",
      "train loss:1.5230201210339505\n",
      "train loss:1.490938317705721\n",
      "train loss:1.3514610363433084\n",
      "train loss:1.109166482886457\n",
      "train loss:1.4859624303407044\n",
      "train loss:1.1968311042476267\n",
      "train loss:2.153341647094654\n",
      "train loss:1.746088151073139\n",
      "train loss:1.1214719759896556\n",
      "train loss:1.18918804938326\n",
      "train loss:1.6258596092048596\n",
      "train loss:1.4591561559497443\n",
      "train loss:1.427503276880875\n",
      "train loss:1.2529880082736855\n",
      "train loss:0.8776920222468112\n",
      "train loss:2.0602645886318993\n",
      "train loss:1.2319492877784746\n",
      "train loss:1.4661617735914627\n",
      "train loss:1.1505717607153871\n",
      "train loss:1.3809323518108965\n",
      "train loss:1.2813985192598696\n",
      "train loss:1.7602521516932677\n",
      "train loss:1.7914246124384028\n",
      "train loss:1.748753738303042\n",
      "train loss:1.0932611797897014\n",
      "train loss:1.4510554352678293\n",
      "train loss:1.7916562900914657\n",
      "train loss:1.3248997198523402\n",
      "train loss:1.2123596094584697\n",
      "train loss:0.8623065942767003\n",
      "train loss:1.2932405062687862\n",
      "train loss:1.2924637714026592\n",
      "train loss:1.272671078024197\n",
      "train loss:1.358400926679444\n",
      "train loss:1.3471231308887879\n",
      "train loss:1.4028424528285528\n",
      "train loss:1.2165553000149063\n",
      "train loss:1.2701574796897837\n",
      "train loss:1.8431150582663829\n",
      "train loss:0.9805022747280822\n",
      "train loss:1.4427320299546724\n",
      "train loss:1.0309542708686108\n",
      "train loss:1.0728606837324235\n",
      "train loss:1.0345546445710996\n",
      "train loss:1.4727526115945246\n",
      "train loss:0.9943091771502732\n",
      "train loss:0.9658801925088903\n",
      "train loss:1.3990243400994793\n",
      "train loss:1.180767095305457\n",
      "train loss:1.1438688327314934\n",
      "train loss:1.3477844779449533\n",
      "train loss:1.6623568537818543\n",
      "train loss:1.2251822145798579\n",
      "train loss:1.2771321859390798\n",
      "train loss:1.4310121495470338\n",
      "train loss:1.3760250048640084\n",
      "train loss:1.5154063858908684\n",
      "train loss:1.248854520825852\n",
      "train loss:0.8420211325945178\n",
      "train loss:1.4274363813743223\n",
      "train loss:1.2863898261594646\n",
      "train loss:1.154375677915844\n",
      "train loss:1.5224740287469265\n",
      "train loss:1.574123635369657\n",
      "train loss:1.621082336093938\n",
      "train loss:1.056832848234033\n",
      "train loss:0.9398244968552729\n",
      "train loss:1.5344434638602498\n",
      "train loss:1.2385290594119494\n",
      "train loss:1.2652619402098229\n",
      "train loss:1.7664109630567086\n",
      "train loss:2.3617057903265506\n",
      "train loss:0.8513274646948725\n",
      "train loss:1.797904650499128\n",
      "train loss:1.416234145827521\n",
      "train loss:1.0155538390280914\n",
      "train loss:1.7021251744730548\n",
      "train loss:1.6707689008854192\n",
      "train loss:1.3804011472911655\n",
      "train loss:1.6369323472792519\n",
      "train loss:1.7340458167518222\n",
      "train loss:1.0584025905797756\n",
      "train loss:1.5719084804059582\n",
      "train loss:0.8452049902939247\n",
      "train loss:1.2943603636477814\n",
      "train loss:1.2709401671280645\n",
      "train loss:1.207357911263815\n",
      "train loss:1.2430291746293078\n",
      "train loss:1.0257246034389051\n",
      "train loss:1.6514134126761832\n",
      "train loss:1.6998704457791256\n",
      "train loss:1.4913629452501207\n",
      "train loss:1.5469617915014013\n",
      "train loss:1.1881147322214332\n",
      "train loss:1.2243675593532195\n",
      "train loss:1.4706341561978375\n",
      "train loss:0.8941407433405318\n",
      "train loss:1.1029483556988322\n",
      "train loss:1.8745592154847959\n",
      "train loss:1.106520799741369\n",
      "train loss:1.7380078691626704\n",
      "train loss:0.8568134927291078\n",
      "train loss:1.3527320798875255\n",
      "train loss:1.4407904842128427\n",
      "train loss:1.203175303360148\n",
      "train loss:1.7683337831353299\n",
      "train loss:1.5023916038946639\n",
      "train loss:1.0782878452151496\n",
      "train loss:1.5543019821044708\n",
      "train loss:1.4400046153197206\n",
      "train loss:1.0935005659988934\n",
      "train loss:1.3233248625358862\n",
      "train loss:1.0848913733812127\n",
      "train loss:1.1552925616979202\n",
      "train loss:1.462278213309145\n",
      "train loss:1.5126342919010125\n",
      "train loss:1.3321868695505894\n",
      "train loss:1.5072958914964991\n",
      "train loss:1.3476536251041953\n",
      "train loss:1.845672338315324\n",
      "train loss:1.6437061599385856\n",
      "train loss:1.2197344337225111\n",
      "train loss:1.1531403023465443\n",
      "train loss:0.9513687742597764\n",
      "train loss:0.8872239926389168\n",
      "train loss:1.2762864766202118\n",
      "train loss:1.3400026721326315\n",
      "train loss:1.2672391044067859\n",
      "train loss:1.4944237306482522\n",
      "train loss:1.507942223417761\n",
      "train loss:1.2165402292993623\n",
      "train loss:0.900263126888879\n",
      "train loss:1.474183979944948\n",
      "train loss:1.3270284977537192\n",
      "train loss:1.1092619418966794\n",
      "train loss:1.2868420679627819\n",
      "train loss:1.4256942772244214\n",
      "train loss:1.959824294018095\n",
      "train loss:1.6816727055060816\n",
      "train loss:1.4567483871306148\n",
      "train loss:1.800461274636747\n",
      "train loss:0.8470921067944926\n",
      "train loss:0.8319173304539677\n",
      "train loss:0.8532186262204675\n",
      "train loss:0.7629922007338058\n",
      "train loss:1.662247121392486\n",
      "train loss:1.3706385394020455\n",
      "train loss:0.7196783567307349\n",
      "train loss:1.31171825295008\n",
      "train loss:3.1400389780560616\n",
      "train loss:1.4511502471233573\n",
      "train loss:1.1849370083868425\n",
      "train loss:0.7912858198050189\n",
      "train loss:1.249544607162837\n",
      "train loss:1.1724223059903058\n",
      "train loss:1.2400615049766002\n",
      "train loss:1.7993515300855012\n",
      "train loss:1.6106511813977638\n",
      "train loss:1.254060027754083\n",
      "train loss:1.4649825106939902\n",
      "train loss:1.4923014386864002\n",
      "train loss:1.2776473847120857\n",
      "train loss:1.3706010164614506\n",
      "train loss:0.85297349309662\n",
      "train loss:1.1500951783776614\n",
      "train loss:1.513894801948072\n",
      "train loss:1.2895265936396203\n",
      "train loss:1.9509841191087927\n",
      "train loss:1.0466525358874228\n",
      "train loss:1.464494732791011\n",
      "train loss:1.1204085749081765\n",
      "train loss:1.1855021264546817\n",
      "train loss:0.8100874659812748\n",
      "train loss:1.4465395154820326\n",
      "train loss:1.0745367537534882\n",
      "train loss:0.7560419728676309\n",
      "train loss:1.7795256575219007\n",
      "train loss:0.3754278438948908\n",
      "train loss:0.6474307789876638\n",
      "train loss:1.2765086547267228\n",
      "train loss:1.2932731076247241\n",
      "train loss:1.5943687916503482\n",
      "train loss:1.2513968355962868\n",
      "train loss:0.9395622573132731\n",
      "train loss:1.5040158498637788\n",
      "train loss:1.7404817479071357\n",
      "train loss:1.683333923203294\n",
      "train loss:1.4088328074933447\n",
      "train loss:1.7292860641572332\n",
      "train loss:1.6433523749925196\n",
      "train loss:1.4763552246108207\n",
      "train loss:1.1018804723070055\n",
      "train loss:1.359525001455993\n",
      "train loss:1.4547395754695667\n",
      "train loss:1.5863256986753103\n",
      "train loss:1.4452057463163928\n",
      "train loss:1.551757841141043\n",
      "train loss:0.9437532558634765\n",
      "train loss:1.1140691239467913\n",
      "train loss:1.790844927792429\n",
      "train loss:1.2259175080036235\n",
      "train loss:1.2681120131054855\n",
      "train loss:1.639021574507034\n",
      "train loss:1.0441507442026494\n",
      "train loss:0.782871130109843\n",
      "train loss:0.9771925001875887\n",
      "train loss:1.327753142649729\n",
      "train loss:0.8339232383524233\n",
      "train loss:1.1893767985431998\n",
      "train loss:1.4662242719341774\n",
      "train loss:1.3145085633134554\n",
      "train loss:1.6670763800436155\n",
      "train loss:0.9573140017444143\n",
      "train loss:1.0902732691305985\n",
      "train loss:1.4100511711278507\n",
      "train loss:1.5375465328622397\n",
      "train loss:1.727186097409119\n",
      "train loss:1.042372409331985\n",
      "train loss:1.040554934409253\n",
      "train loss:0.9691768667571277\n",
      "train loss:1.565266240688806\n",
      "train loss:1.3354635820589795\n",
      "train loss:1.145970583025088\n",
      "train loss:1.6494403798834483\n",
      "train loss:1.1838156752962385\n",
      "train loss:1.1845331497364613\n",
      "train loss:1.2041033349840842\n",
      "train loss:1.3154044059324934\n",
      "train loss:1.294485651878348\n",
      "train loss:1.2525736627337163\n",
      "train loss:1.1862168144847236\n",
      "train loss:1.215178096346755\n",
      "train loss:1.4099252917917648\n",
      "train loss:1.2840792421884264\n",
      "train loss:1.3267075595099895\n",
      "train loss:1.2309963038761729\n",
      "train loss:1.3126775878254737\n",
      "train loss:0.9098404955537088\n",
      "train loss:1.0976478504985174\n",
      "train loss:1.651665100270602\n",
      "train loss:1.8217548491441822\n",
      "train loss:1.3217931729572396\n",
      "train loss:1.842735612385918\n",
      "train loss:0.8722195588098327\n",
      "train loss:1.54218106682604\n",
      "train loss:1.166213865877784\n",
      "train loss:0.9595513967721263\n",
      "train loss:1.2672964017313455\n",
      "train loss:1.2854167135345058\n",
      "train loss:0.9738728448527351\n",
      "train loss:0.8882533006932822\n",
      "train loss:1.3602017730856786\n",
      "train loss:1.1791598310727687\n",
      "train loss:1.6388645968017408\n",
      "train loss:1.0040368967673943\n",
      "train loss:1.3974174208153982\n",
      "train loss:1.1414381054398037\n",
      "train loss:1.4096089497081024\n",
      "train loss:0.8626793933832895\n",
      "train loss:0.8336838266886591\n",
      "train loss:1.5544921021816305\n",
      "train loss:1.1114394886926942\n",
      "train loss:0.610855262092335\n",
      "train loss:1.1931587254103906\n",
      "train loss:0.6037521982096195\n",
      "train loss:0.9752899078146579\n",
      "train loss:1.5848998535063188\n",
      "train loss:1.511561907553709\n",
      "train loss:1.263886871054321\n",
      "train loss:1.4248616470107565\n",
      "train loss:1.2529565815757624\n",
      "train loss:1.3198075191234389\n",
      "train loss:0.9631486015785656\n",
      "train loss:1.6785721220958751\n",
      "train loss:1.594527927387823\n",
      "train loss:2.04565299405608\n",
      "train loss:1.3595314487552839\n",
      "train loss:1.5262254684495797\n",
      "train loss:1.3075710379983723\n",
      "train loss:1.6143640393049257\n",
      "train loss:1.5875724190354645\n",
      "train loss:1.1836149190285823\n",
      "train loss:1.7284574385306537\n",
      "train loss:1.2764464942400668\n",
      "train loss:1.146021059883269\n",
      "train loss:0.8797946642444131\n",
      "train loss:1.5597746483794444\n",
      "train loss:1.275872018222248\n",
      "train loss:1.6951577843571315\n",
      "train loss:1.2477193579214507\n",
      "train loss:1.1887297969380017\n",
      "train loss:1.425156488819541\n",
      "train loss:1.3067662491934222\n",
      "train loss:1.4314074503853775\n",
      "train loss:1.2451216635920208\n",
      "train loss:0.6982703288342567\n",
      "train loss:0.4734277220000175\n",
      "train loss:1.1113250631447147\n",
      "train loss:1.5107618405101277\n",
      "train loss:1.5822055084694906\n",
      "train loss:0.8662149270592966\n",
      "train loss:1.602615537823926\n",
      "train loss:1.7263552767127934\n",
      "train loss:1.407617537672828\n",
      "train loss:0.8875507262738047\n",
      "train loss:1.70215527013285\n",
      "train loss:1.011703996551435\n",
      "train loss:1.6465026106765535\n",
      "train loss:1.5550210838596743\n",
      "train loss:1.5179466814868676\n",
      "train loss:1.6696723262385675\n",
      "train loss:0.8205134654095005\n",
      "train loss:1.2652847914591763\n",
      "train loss:1.294036990671628\n",
      "train loss:1.5402035542086674\n",
      "train loss:1.6698462760584298\n",
      "train loss:1.9484012577514616\n",
      "train loss:1.403764349818857\n",
      "train loss:1.4494833847820519\n",
      "train loss:1.0354980832764156\n",
      "train loss:1.8431656376221124\n",
      "train loss:1.5950659401870881\n",
      "train loss:1.2696869378341256\n",
      "train loss:0.9518238561386694\n",
      "train loss:1.0423101249169453\n",
      "train loss:0.9901594320992254\n",
      "train loss:1.5196643208942788\n",
      "train loss:1.1240942966288658\n",
      "train loss:1.4884002025182503\n",
      "train loss:1.7954460041699636\n",
      "train loss:1.1601822155138242\n",
      "train loss:1.1814946747053252\n",
      "train loss:1.5975768412707834\n",
      "train loss:1.3690390439529396\n",
      "train loss:1.7429718109273904\n",
      "train loss:1.1157028215851197\n",
      "train loss:1.4706035224010103\n",
      "train loss:1.3380518345024996\n",
      "train loss:1.3475017850100994\n",
      "train loss:0.9385408514256464\n",
      "train loss:1.4327570326325512\n",
      "train loss:1.7191271660659038\n",
      "train loss:1.5500730495166253\n",
      "train loss:1.1062349758201762\n",
      "train loss:1.3884265109093827\n",
      "train loss:1.260612680078426\n",
      "train loss:1.7935534316353567\n",
      "train loss:1.56257172747559\n",
      "train loss:1.2918519417846697\n",
      "train loss:1.1520479836894328\n",
      "train loss:1.3249712108122256\n",
      "train loss:1.351454789203301\n",
      "train loss:1.3267095046870196\n",
      "train loss:0.8951650536458358\n",
      "train loss:1.4834679649731293\n",
      "train loss:1.578595754679649\n",
      "train loss:1.8732279621884893\n",
      "train loss:1.5027633910174965\n",
      "train loss:1.2960096669253311\n",
      "train loss:0.8128519458525728\n",
      "train loss:1.1995826917727768\n",
      "train loss:0.860656662741218\n",
      "train loss:1.0919620739006137\n",
      "train loss:1.284117663993594\n",
      "train loss:1.040055371220792\n",
      "train loss:1.5905765287737348\n",
      "train loss:0.9994085938150004\n",
      "train loss:1.0885556496398252\n",
      "train loss:1.676381639762893\n",
      "train loss:1.139154569569614\n",
      "train loss:1.7040153606268038\n",
      "train loss:1.2108684191249104\n",
      "train loss:2.020751880585275\n",
      "train loss:1.5572322439861315\n",
      "train loss:1.2175216101375168\n",
      "train loss:1.3805073425530452\n",
      "train loss:1.6110596921259117\n",
      "train loss:1.903440187180702\n",
      "train loss:1.1987422530889156\n",
      "train loss:1.0029204817541144\n",
      "train loss:1.4838605397407971\n",
      "train loss:1.769438995052201\n",
      "train loss:1.1229580361457467\n",
      "train loss:0.7804093541934745\n",
      "train loss:1.5129620120412703\n",
      "train loss:0.9150748690022962\n",
      "train loss:0.9506863279749205\n",
      "train loss:1.527764463386243\n",
      "train loss:1.162382555257289\n",
      "train loss:1.1098071681502002\n",
      "train loss:1.813566990437263\n",
      "train loss:0.9550271318730379\n",
      "train loss:1.449804069784467\n",
      "train loss:1.7115206815134694\n",
      "train loss:1.5747220719441668\n",
      "train loss:1.234496778686888\n",
      "train loss:1.3766918056883513\n",
      "train loss:0.8053584959838986\n",
      "train loss:1.5493186137419894\n",
      "train loss:1.8164111398336338\n",
      "train loss:1.7790356748448304\n",
      "train loss:1.212552041946723\n",
      "train loss:2.033107747526924\n",
      "train loss:0.9276568883288482\n",
      "train loss:0.9071903350462776\n",
      "train loss:1.1159011420749716\n",
      "train loss:1.2561820565142647\n",
      "train loss:1.3374151571756059\n",
      "train loss:1.2696645220183593\n",
      "train loss:1.234809031830002\n",
      "train loss:0.9228283154411316\n",
      "train loss:1.342274648114176\n",
      "train loss:1.4195001680177899\n",
      "train loss:1.660614149100639\n",
      "train loss:1.778986124088577\n",
      "train loss:1.049176153735042\n",
      "train loss:1.7352828075722766\n",
      "train loss:1.2092581365409163\n",
      "train loss:0.9274143597305418\n",
      "train loss:1.5377616233640021\n",
      "train loss:1.9236833709646632\n",
      "train loss:1.1092593755951605\n",
      "train loss:1.794249596506256\n",
      "train loss:1.3102734814899568\n",
      "train loss:1.4650955786855064\n",
      "train loss:1.1959999783103517\n",
      "train loss:2.0012497200911827\n",
      "train loss:0.8527637489011717\n",
      "train loss:1.8997740187807408\n",
      "train loss:0.9124225112505979\n",
      "train loss:1.286853626178464\n",
      "train loss:1.1714067164674338\n",
      "train loss:1.6832741816895962\n",
      "train loss:1.123817053578264\n",
      "train loss:2.0597955446522747\n",
      "train loss:1.2534822179265541\n",
      "train loss:1.578083679316004\n",
      "train loss:1.0698915626521424\n",
      "train loss:1.4393748690212478\n",
      "train loss:1.0412285924575886\n",
      "train loss:0.9454597418856642\n",
      "train loss:1.698507061635377\n",
      "train loss:1.4866939106037966\n",
      "train loss:0.7918330595732128\n",
      "train loss:1.4040486202248816\n",
      "train loss:1.5958298028766769\n",
      "train loss:0.77565163822765\n",
      "train loss:1.1912328784041868\n",
      "train loss:1.5130410446356188\n",
      "train loss:1.2014748729088245\n",
      "train loss:1.1865923415476958\n",
      "train loss:1.2853235151900932\n",
      "train loss:1.3175715739159224\n",
      "train loss:1.093466492802325\n",
      "train loss:1.2934971499778425\n",
      "train loss:1.3155547130189462\n",
      "train loss:1.8706459229201045\n",
      "train loss:1.338793592109276\n",
      "train loss:1.3425848733931016\n",
      "train loss:1.5178449174323114\n",
      "train loss:1.4339561664787204\n",
      "train loss:1.0013923801100284\n",
      "train loss:1.5415058157095043\n",
      "train loss:1.0494827464803957\n",
      "train loss:1.2468289024070993\n",
      "train loss:1.1883301872459358\n",
      "train loss:1.1263523512627311\n",
      "train loss:1.703585622923909\n",
      "train loss:2.1368238638717707\n",
      "train loss:1.7531320808537205\n",
      "train loss:1.597250688023304\n",
      "train loss:1.3975732631382445\n",
      "train loss:1.2565705453864973\n",
      "train loss:1.7599976496754688\n",
      "train loss:0.9512119803838768\n",
      "train loss:1.3440299921533776\n",
      "train loss:1.104944436054347\n",
      "train loss:1.5224229336892106\n",
      "train loss:1.5202139265008152\n",
      "train loss:0.8547189250663492\n",
      "train loss:1.3489318223500326\n",
      "train loss:1.3198544394782994\n",
      "train loss:1.5505245709152837\n",
      "train loss:1.6986881121553403\n",
      "train loss:1.5903788093883535\n",
      "train loss:0.9852165637717016\n",
      "train loss:1.0986809692175084\n",
      "train loss:1.390756155976724\n",
      "train loss:1.4121737334682147\n",
      "train loss:1.1962232648224558\n",
      "train loss:0.5663153982192201\n",
      "train loss:1.6880390201406796\n",
      "train loss:1.131809317104557\n",
      "train loss:1.3524436357724667\n",
      "train loss:1.567492997522931\n",
      "train loss:0.9645570354238359\n",
      "train loss:1.2726952056269425\n",
      "train loss:1.925832663732038\n",
      "train loss:1.1136443824080495\n",
      "train loss:1.0785968819927603\n",
      "train loss:1.1528902484892352\n",
      "train loss:1.2845056822223209\n",
      "train loss:1.0786012940312624\n",
      "train loss:1.4079535613067735\n",
      "train loss:1.1903445612974677\n",
      "train loss:1.2033706759698368\n",
      "train loss:1.3799471017934255\n",
      "train loss:1.717753634934386\n",
      "train loss:1.158359581007735\n",
      "train loss:1.0642461750472576\n",
      "train loss:1.3649968280249998\n",
      "train loss:1.253984227944861\n",
      "train loss:1.3243959920607828\n",
      "train loss:0.9272155198338841\n",
      "train loss:1.2043162086184196\n",
      "train loss:1.313924270174617\n",
      "train loss:1.6354502659975902\n",
      "train loss:1.3512887445788564\n",
      "train loss:1.6499228178688206\n",
      "train loss:1.6755728134583194\n",
      "train loss:1.5906169603778983\n",
      "train loss:1.1131838805287853\n",
      "train loss:1.4485097800625089\n",
      "train loss:0.9611399553222121\n",
      "train loss:1.4480465761356087\n",
      "train loss:1.3203244626612973\n",
      "train loss:1.6106038872957253\n",
      "train loss:1.3250326620236597\n",
      "train loss:1.0485428636124234\n",
      "train loss:1.4819577772914485\n",
      "train loss:1.1252465030102816\n",
      "train loss:1.6063026194713097\n",
      "train loss:1.0909583068204518\n",
      "train loss:1.9712602056885324\n",
      "train loss:0.8542765434821012\n",
      "train loss:1.1855998817623117\n",
      "train loss:1.0023920443438208\n",
      "train loss:1.1613763024015016\n",
      "train loss:1.1071679212584975\n",
      "train loss:1.2398882758409904\n",
      "train loss:1.0404095062716818\n",
      "train loss:0.9566546034426009\n",
      "train loss:1.6343813857355183\n",
      "train loss:1.1284968021888495\n",
      "train loss:0.82951965712776\n",
      "train loss:1.4328471353913073\n",
      "train loss:1.717667824032197\n",
      "train loss:1.0716680869181026\n",
      "train loss:1.5129409841271193\n",
      "train loss:1.192450625220265\n",
      "train loss:1.2252059468871273\n",
      "train loss:1.3535630935595377\n",
      "train loss:0.8297239193948351\n",
      "train loss:1.637496973644073\n",
      "train loss:0.8656095920991687\n",
      "train loss:0.6680455839659425\n",
      "train loss:1.3135588018102915\n",
      "train loss:0.9284900347786769\n",
      "train loss:1.012395708577127\n",
      "train loss:0.9462991829347439\n",
      "train loss:0.9118032665173672\n",
      "train loss:1.0296034475526294\n",
      "train loss:1.1477142756086336\n",
      "train loss:1.0136959982495892\n",
      "train loss:1.298842284250593\n",
      "train loss:1.263751256426625\n",
      "train loss:1.0180525837023666\n",
      "train loss:1.1908905874093427\n",
      "train loss:1.494309455228277\n",
      "train loss:1.7032188174332394\n",
      "train loss:1.1456992772584818\n",
      "train loss:1.1970587475398529\n",
      "train loss:1.0703029241935542\n",
      "train loss:1.291510681810655\n",
      "train loss:1.5556936026745694\n",
      "train loss:1.3542256067177354\n",
      "train loss:1.582985483595999\n",
      "train loss:1.547361981286\n",
      "train loss:1.1006583936950762\n",
      "train loss:1.026491393294897\n",
      "train loss:1.1743488614619726\n",
      "train loss:1.7561531823658918\n",
      "train loss:1.253466498967505\n",
      "train loss:1.4243687715306028\n",
      "train loss:1.4349817390704154\n",
      "train loss:1.3872741718288633\n",
      "train loss:1.226856512566027\n",
      "train loss:1.028263149673903\n",
      "train loss:1.1095410497881613\n",
      "train loss:1.8836333949107054\n",
      "train loss:0.8838437370756621\n",
      "train loss:1.6787938197119352\n",
      "train loss:1.6833289724140026\n",
      "train loss:2.4512320198778292\n",
      "train loss:1.0351921667453001\n",
      "train loss:1.136419919237016\n",
      "train loss:1.606204066549672\n",
      "train loss:1.4713503697574493\n",
      "train loss:1.8258651047362175\n",
      "train loss:2.0377760608774693\n",
      "train loss:1.4100933533994984\n",
      "train loss:1.0667941848119114\n",
      "train loss:1.3628661785932448\n",
      "train loss:1.0318735531076186\n",
      "train loss:0.896517880811649\n",
      "train loss:0.9384084999801606\n",
      "train loss:1.2599840781044018\n",
      "train loss:0.9165774420332731\n",
      "train loss:1.4920079334643765\n",
      "train loss:1.2838277408105607\n",
      "train loss:1.8495704796171857\n",
      "train loss:1.116907499230066\n",
      "train loss:0.6648993080890293\n",
      "train loss:1.2493214300954785\n",
      "train loss:1.3158716870878187\n",
      "train loss:1.702381068418689\n",
      "train loss:0.8729528531176417\n",
      "train loss:0.8673681234812648\n",
      "train loss:0.9315354449769012\n",
      "train loss:1.8607001249092883\n",
      "train loss:1.2625644537759046\n",
      "train loss:0.9058353425829389\n",
      "train loss:1.5446720912226042\n",
      "train loss:1.1037196938749347\n",
      "train loss:1.3925308352415366\n",
      "train loss:1.1156152764247913\n",
      "train loss:1.2019128430247237\n",
      "train loss:1.602952385605628\n",
      "train loss:1.5492084157065873\n",
      "train loss:1.1421774593874043\n",
      "train loss:0.8350030457196249\n",
      "train loss:1.0988219376151496\n",
      "train loss:1.2039273643036028\n",
      "train loss:1.114105508876542\n",
      "train loss:1.230689237163848\n",
      "train loss:1.514863004957956\n",
      "train loss:1.1353512082034722\n",
      "train loss:1.2377539278406198\n",
      "train loss:1.4660329022064995\n",
      "train loss:1.1064630369083315\n",
      "train loss:1.2327466102974185\n",
      "train loss:1.3570824588696768\n",
      "train loss:1.2344286535286002\n",
      "train loss:1.0809615755948618\n",
      "train loss:1.4690160070993712\n",
      "train loss:1.454592046120817\n",
      "train loss:1.4662724083368417\n",
      "train loss:1.4174712434349748\n",
      "train loss:1.49398367956289\n",
      "train loss:1.0882986549154068\n",
      "train loss:1.4428551446996503\n",
      "train loss:0.9465311388103496\n",
      "train loss:1.1749389869246496\n",
      "train loss:1.2696076532898317\n",
      "train loss:0.8126425899407707\n",
      "train loss:1.4298885428787045\n",
      "train loss:1.0871754050357372\n",
      "train loss:1.0864439654212514\n",
      "train loss:1.4338993904744903\n",
      "train loss:1.2612582791765419\n",
      "train loss:1.1758029592830173\n",
      "train loss:1.3033507022205835\n",
      "train loss:0.7947525723409786\n",
      "train loss:1.312175635300473\n",
      "train loss:1.4992340845067493\n",
      "train loss:1.4983227935728918\n",
      "train loss:0.9074707851412744\n",
      "train loss:0.78364262760595\n",
      "train loss:0.8344958739384015\n",
      "train loss:1.356228651483645\n",
      "train loss:1.0543309713721891\n",
      "train loss:1.0037934277593799\n",
      "train loss:1.7752870620275314\n",
      "train loss:1.4846028611650282\n",
      "train loss:1.2365917603591108\n",
      "train loss:1.0935230444685802\n",
      "train loss:1.4212842834031905\n",
      "train loss:1.4933812433525095\n",
      "train loss:1.1865429343477476\n",
      "train loss:1.3695155046480887\n",
      "train loss:1.768923962750069\n",
      "train loss:0.720857821362549\n",
      "train loss:0.9710600748002653\n",
      "train loss:1.1861258351179276\n",
      "train loss:1.1065843122354102\n",
      "train loss:1.3090988708833329\n",
      "train loss:1.5570799704894982\n",
      "train loss:1.635704538188288\n",
      "train loss:0.9791582925841997\n",
      "train loss:1.5171871151146392\n",
      "train loss:0.9550829496666443\n",
      "train loss:1.3440678422967265\n",
      "train loss:0.8796255658536172\n",
      "train loss:1.0606396865503327\n",
      "train loss:1.5756428640831048\n",
      "train loss:1.224245482512139\n",
      "train loss:1.293258126271907\n",
      "train loss:1.5065920500792358\n",
      "train loss:1.0713603710009907\n",
      "train loss:1.2631889497208635\n",
      "train loss:1.4168921295892702\n",
      "train loss:0.7368099506111099\n",
      "train loss:1.5041130061809782\n",
      "train loss:1.1846707640899021\n",
      "train loss:1.1221758122767058\n",
      "train loss:1.6570986093456763\n",
      "train loss:1.2235845019525744\n",
      "train loss:1.4970521257990805\n",
      "train loss:1.4773240777307344\n",
      "train loss:0.9190727366056677\n",
      "train loss:1.3463612011530548\n",
      "train loss:0.7036010201641764\n",
      "train loss:0.9270856968997958\n",
      "train loss:1.2882140052327222\n",
      "train loss:1.573947169535377\n",
      "train loss:1.2697057675719887\n",
      "train loss:0.8584257893377719\n",
      "train loss:0.7545100228075146\n",
      "train loss:1.3324657246532596\n",
      "train loss:1.1041824949978831\n",
      "train loss:1.2203767337945282\n",
      "train loss:1.3343994574952929\n",
      "train loss:0.6386967150542001\n",
      "train loss:0.7315437539968407\n",
      "train loss:0.9391935886832018\n",
      "train loss:1.37506602022754\n",
      "train loss:1.0626811825079772\n",
      "train loss:0.8851381068992822\n",
      "train loss:1.6693590609550342\n",
      "train loss:0.8488443427679135\n",
      "train loss:1.4737150501264202\n",
      "train loss:0.9456553324783925\n",
      "train loss:1.1202787992894931\n",
      "train loss:0.9020513402432954\n",
      "train loss:1.145622502241117\n",
      "train loss:0.8609301562045738\n",
      "train loss:1.211443959889039\n",
      "train loss:1.0013006519803043\n",
      "train loss:1.0797268363834407\n",
      "train loss:1.7410909394604137\n",
      "train loss:0.9852876473726248\n",
      "train loss:1.2833934708288495\n",
      "train loss:1.3051890836236828\n",
      "train loss:1.0986332112479247\n",
      "train loss:1.2732959347596808\n",
      "train loss:0.6591543719510922\n",
      "train loss:1.3369839423022964\n",
      "train loss:1.3615086968886143\n",
      "train loss:1.1039956229961267\n",
      "train loss:1.239726585737475\n",
      "train loss:1.0836477123033834\n",
      "train loss:0.7293625620000438\n",
      "train loss:1.344711682894167\n",
      "train loss:1.025358974786325\n",
      "train loss:1.0674740194227916\n",
      "train loss:1.3072524822970313\n",
      "train loss:1.6099095190889023\n",
      "train loss:1.1278355138820855\n",
      "train loss:1.4069926036477916\n",
      "train loss:1.2729273981550429\n",
      "train loss:0.7328475769952193\n",
      "train loss:0.6424722523430887\n",
      "train loss:1.0509600798127114\n",
      "train loss:1.844618271946515\n",
      "train loss:1.3941646274790622\n",
      "train loss:0.8376575114828901\n",
      "train loss:1.2501590124168718\n",
      "train loss:1.0771773384428658\n",
      "train loss:1.1562212901020583\n",
      "train loss:1.362044418623857\n",
      "train loss:1.3101611692206938\n",
      "train loss:1.235150963541066\n",
      "train loss:1.381187325312179\n",
      "train loss:1.3255531749918275\n",
      "train loss:0.8076651674633164\n",
      "train loss:1.473517709598211\n",
      "train loss:1.15581486743593\n",
      "train loss:0.8665518281186546\n",
      "train loss:0.7034950809700502\n",
      "train loss:1.4668225394291299\n",
      "train loss:1.0383476358658568\n",
      "train loss:1.2698758631855107\n",
      "train loss:0.9348421400808353\n",
      "train loss:1.2930071492198238\n",
      "train loss:1.3236548240672028\n",
      "train loss:1.398861844020703\n",
      "train loss:1.6581500239766183\n",
      "train loss:1.1446349760498347\n",
      "train loss:1.506340230124836\n",
      "train loss:1.7239922695171592\n",
      "train loss:1.3206603420440632\n",
      "train loss:1.4840100086905088\n",
      "train loss:1.556496862596387\n",
      "train loss:1.0613615306298492\n",
      "train loss:0.41190803955010963\n",
      "train loss:1.3789923196614564\n",
      "train loss:1.4551891789180698\n",
      "train loss:1.2912426730377846\n",
      "train loss:0.35852410249979244\n",
      "train loss:1.4781092984898945\n",
      "train loss:0.8454291660743616\n",
      "train loss:1.4125224183938674\n",
      "train loss:0.9368313465053992\n",
      "train loss:1.6347117976327048\n",
      "train loss:0.932364763262471\n",
      "train loss:1.300502937280815\n",
      "train loss:1.1987435387104257\n",
      "train loss:1.393712413784054\n",
      "train loss:1.1173058226723478\n",
      "train loss:1.3101593144415218\n",
      "train loss:0.7483237857212692\n",
      "train loss:1.51323567015269\n",
      "train loss:1.4944952182542157\n",
      "train loss:1.3359186405273042\n",
      "train loss:1.0906822005281211\n",
      "train loss:1.397833167413455\n",
      "train loss:1.253989452051437\n",
      "train loss:1.1593667364077134\n",
      "train loss:1.0585449073827387\n",
      "train loss:0.42506284948548617\n",
      "train loss:0.897637989488452\n",
      "train loss:1.0006326200238145\n",
      "train loss:0.9864127079757768\n",
      "train loss:1.3875751698480767\n",
      "train loss:1.5888932823876287\n",
      "train loss:1.5659271123869962\n",
      "train loss:1.401663986813249\n",
      "train loss:0.940127370106236\n",
      "train loss:0.908647233186049\n",
      "train loss:0.611359029092912\n",
      "train loss:1.1214632437164833\n",
      "train loss:0.6829057130009433\n",
      "train loss:1.3600597507529362\n",
      "train loss:1.5899907747315196\n",
      "train loss:1.0582136752515512\n",
      "train loss:1.0272851951318662\n",
      "train loss:2.0281485649565854\n",
      "train loss:1.4602402107119612\n",
      "train loss:1.6241667678629486\n",
      "train loss:0.7692217911499599\n",
      "train loss:0.8911723377272793\n",
      "train loss:1.3586712939011203\n",
      "train loss:1.0212062698913333\n",
      "train loss:0.5624972874008283\n",
      "train loss:1.4672018727126712\n",
      "train loss:1.5148822366340564\n",
      "train loss:1.5926572575294553\n",
      "train loss:1.4231851755375104\n",
      "train loss:0.8179796407144396\n",
      "train loss:1.3945498319643508\n",
      "train loss:1.3086233980443427\n",
      "train loss:0.5118415735179156\n",
      "train loss:0.5264863199850094\n",
      "train loss:0.6958782664167373\n",
      "train loss:1.3083038406675054\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6421568627450981\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 20\n",
    "\n",
    "network = DeepConvNet()\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1fa4e140-ea27-4aea-8988-2c81697d72e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.3333333333333333\n",
      "혼동 행렬:\n",
      " [[  2 136]\n",
      " [  0  66]]\n",
      "분류 보고서:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.03       138\n",
      "           1       0.33      1.00      0.49        66\n",
      "\n",
      "    accuracy                           0.33       204\n",
      "   macro avg       0.66      0.51      0.26       204\n",
      "weighted avg       0.78      0.33      0.18       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mymethod.functions import *\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "x_train_reshaped = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], -1)\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train_reshaped, y_train)\n",
    "y_pred = model.predict(x_test_reshaped)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"정확도:\", accuracy)\n",
    "print(\"혼동 행렬:\\n\", conf_matrix)\n",
    "print(\"분류 보고서:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "21e952d4-c916-4143-8b48-607dfce95002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.5980392156862745\n",
      "혼동 행렬:\n",
      " [[96 42]\n",
      " [40 26]]\n",
      "분류 보고서:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.70       138\n",
      "           1       0.38      0.39      0.39        66\n",
      "\n",
      "    accuracy                           0.60       204\n",
      "   macro avg       0.54      0.54      0.54       204\n",
      "weighted avg       0.60      0.60      0.60       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_reshaped = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], -1)\n",
    "model = LinearDiscriminantAnalysis()\n",
    "model.fit(x_train_reshaped, y_train)\n",
    "y_pred = model.predict(x_test_reshaped)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"정확도:\", accuracy)\n",
    "print(\"혼동 행렬:\\n\", conf_matrix)\n",
    "print(\"분류 보고서:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d1c8a3f-d64e-4203-83ad-db82d035512f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6764705882352942\n",
      "혼동 행렬:\n",
      " [[138   0]\n",
      " [ 66   0]]\n",
      "분류 보고서:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      1.00      0.81       138\n",
      "           1       0.00      0.00      0.00        66\n",
      "\n",
      "    accuracy                           0.68       204\n",
      "   macro avg       0.34      0.50      0.40       204\n",
      "weighted avg       0.46      0.68      0.55       204\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hotmi\\OneDrive\\Desktop\\4학년 2학기\\딥러닝\\과제\\mymethod\\functions.py:87: RuntimeWarning: divide by zero encountered in log\n",
      "  class_conditional = -0.5 * np.sum(np.log(2. * np.pi * self.variances[cls])) - 0.5 * np.sum(((x - self.means[cls]) ** 2) / (self.variances[cls]))\n",
      "C:\\Users\\hotmi\\OneDrive\\Desktop\\4학년 2학기\\딥러닝\\과제\\mymethod\\functions.py:87: RuntimeWarning: invalid value encountered in divide\n",
      "  class_conditional = -0.5 * np.sum(np.log(2. * np.pi * self.variances[cls])) - 0.5 * np.sum(((x - self.means[cls]) ** 2) / (self.variances[cls]))\n",
      "C:\\Users\\hotmi\\OneDrive\\Desktop\\4학년 2학기\\딥러닝\\과제\\mymethod\\functions.py:87: RuntimeWarning: divide by zero encountered in divide\n",
      "  class_conditional = -0.5 * np.sum(np.log(2. * np.pi * self.variances[cls])) - 0.5 * np.sum(((x - self.means[cls]) ** 2) / (self.variances[cls]))\n",
      "C:\\Users\\hotmi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\hotmi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\hotmi\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "x_train_reshaped = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], -1)\n",
    "model = GaussianNaiveBayes()\n",
    "model.fit(x_train_reshaped, y_train)\n",
    "y_pred = model.predict(x_test_reshaped)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"정확도:\", accuracy)\n",
    "print(\"혼동 행렬:\\n\", conf_matrix)\n",
    "print(\"분류 보고서:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1b4d3cf0-0f9b-47de-9486-9f78b61494de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6225\n",
      "LDA Accuracy: 0.6618\n",
      "Gaussian Classifier Accuracy: 0.4167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "x_train_reshaped = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "# Logistic Regression model\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(x_train_reshaped, y_train)\n",
    "y_pred_logistic = logistic_model.predict(x_test_reshaped)\n",
    "logistic_accuracy = accuracy_score(y_test, y_pred_logistic)\n",
    "print(f\"Logistic Regression Accuracy: {logistic_accuracy:.4f}\")\n",
    "\n",
    "# Linear Discriminant Analysis (LDA) model\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(x_train_reshaped, y_train)\n",
    "y_pred_lda = lda_model.predict(x_test_reshaped)\n",
    "lda_accuracy = accuracy_score(y_test, y_pred_lda)\n",
    "print(f\"LDA Accuracy: {lda_accuracy:.4f}\")\n",
    "\n",
    "# Gaussian Naive Bayes classifier\n",
    "gaussian_model = GaussianNB()\n",
    "gaussian_model.fit(x_train_reshaped, y_train)\n",
    "y_pred_gaussian = gaussian_model.predict(x_test_reshaped)\n",
    "gaussian_accuracy = accuracy_score(y_test, y_pred_gaussian)\n",
    "print(f\"Gaussian Classifier Accuracy: {gaussian_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "31643681-888d-4554-819e-90f306fcdab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hotmi\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.6018 - loss: 1.5149 - val_accuracy: 0.5122 - val_loss: 1.2399\n",
      "Epoch 2/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7167 - loss: 0.7358 - val_accuracy: 0.7256 - val_loss: 0.8204\n",
      "Epoch 3/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8376 - loss: 0.3822 - val_accuracy: 0.6341 - val_loss: 0.9786\n",
      "Epoch 4/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8567 - loss: 0.3852 - val_accuracy: 0.5915 - val_loss: 0.9949\n",
      "Epoch 5/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8611 - loss: 0.3844 - val_accuracy: 0.6341 - val_loss: 0.9125\n",
      "Epoch 6/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9367 - loss: 0.2319 - val_accuracy: 0.6829 - val_loss: 0.9385\n",
      "Epoch 7/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9647 - loss: 0.1694 - val_accuracy: 0.6768 - val_loss: 0.9957\n",
      "Epoch 8/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9768 - loss: 0.1170 - val_accuracy: 0.7012 - val_loss: 1.0864\n",
      "Epoch 9/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9773 - loss: 0.1015 - val_accuracy: 0.7012 - val_loss: 1.1088\n",
      "Epoch 10/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9653 - loss: 0.1030 - val_accuracy: 0.6524 - val_loss: 1.1050\n",
      "Epoch 11/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9910 - loss: 0.0730 - val_accuracy: 0.6829 - val_loss: 1.1166\n",
      "Epoch 12/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9996 - loss: 0.0403 - val_accuracy: 0.6707 - val_loss: 1.1138\n",
      "Epoch 13/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9993 - loss: 0.0384 - val_accuracy: 0.6829 - val_loss: 1.1185\n",
      "Epoch 14/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0318 - val_accuracy: 0.7073 - val_loss: 1.1834\n",
      "Epoch 15/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9982 - loss: 0.0270 - val_accuracy: 0.7073 - val_loss: 1.1994\n",
      "Epoch 16/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0243 - val_accuracy: 0.7073 - val_loss: 1.1821\n",
      "Epoch 17/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0174 - val_accuracy: 0.7073 - val_loss: 1.2068\n",
      "Epoch 18/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0142 - val_accuracy: 0.6890 - val_loss: 1.1884\n",
      "Epoch 19/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0137 - val_accuracy: 0.7134 - val_loss: 1.2604\n",
      "Epoch 20/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0115 - val_accuracy: 0.7012 - val_loss: 1.2467\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "TensorFlow Model Accuracy: 0.6569\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(x_train_reshaped )\n",
    "X_test = scaler.transform(x_test_reshaped)\n",
    "\n",
    "# Build a simple TensorFlow model for binary classification\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"TensorFlow Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1203721b-26da-4a8e-970d-223c1d5808a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hotmi\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - accuracy: 0.6180 - loss: 2.1614 - val_accuracy: 0.6951 - val_loss: 0.6749\n",
      "Epoch 2/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.7053 - loss: 0.6262 - val_accuracy: 0.6951 - val_loss: 0.6505\n",
      "Epoch 3/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.7342 - loss: 0.5569 - val_accuracy: 0.6829 - val_loss: 0.6285\n",
      "Epoch 4/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.7635 - loss: 0.5049 - val_accuracy: 0.6890 - val_loss: 0.6681\n",
      "Epoch 5/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.7958 - loss: 0.4496 - val_accuracy: 0.7073 - val_loss: 0.6817\n",
      "Epoch 6/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.8948 - loss: 0.3280 - val_accuracy: 0.6890 - val_loss: 0.7100\n",
      "Epoch 7/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.9486 - loss: 0.2351 - val_accuracy: 0.6402 - val_loss: 0.9414\n",
      "Epoch 8/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.9831 - loss: 0.0874 - val_accuracy: 0.6768 - val_loss: 1.4132\n",
      "Epoch 9/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.9945 - loss: 0.0209 - val_accuracy: 0.7195 - val_loss: 2.2756\n",
      "Epoch 10/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.9996 - loss: 0.0105 - val_accuracy: 0.6707 - val_loss: 1.8222\n",
      "Epoch 11/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.6402 - val_loss: 1.8657\n",
      "Epoch 12/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.6585 - val_loss: 2.1053\n",
      "Epoch 13/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 8.0755e-04 - val_accuracy: 0.6707 - val_loss: 2.5368\n",
      "Epoch 14/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 4.3808e-04 - val_accuracy: 0.6707 - val_loss: 2.6721\n",
      "Epoch 15/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.2862e-04 - val_accuracy: 0.6707 - val_loss: 2.6462\n",
      "Epoch 16/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.8562e-04 - val_accuracy: 0.6707 - val_loss: 2.7587\n",
      "Epoch 17/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.6003e-04 - val_accuracy: 0.6707 - val_loss: 2.8058\n",
      "Epoch 18/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.3597e-04 - val_accuracy: 0.6707 - val_loss: 2.8816\n",
      "Epoch 19/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.0610e-04 - val_accuracy: 0.6707 - val_loss: 2.9076\n",
      "Epoch 20/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 8.1383e-05 - val_accuracy: 0.6768 - val_loss: 2.9489\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 231ms/step\n",
      "TensorFlow Model Accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "# Build a deeper TensorFlow model for binary classification with CNN layers\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(X_train.shape[1], 1)),\n",
    "    tf.keras.layers.Reshape((X_train.shape[1], 1)),\n",
    "    tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"TensorFlow Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dfcf2e85-b860-4ff0-a533-0b4cc3169cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 212ms/step\n",
      "TensorFlow Model Accuracy: 0.6520\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"TensorFlow Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "95898dcc-e60c-47d9-b8b0-16d8a55b0311",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet3Layer:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e05ab2f-4d34-4e8c-a46d-34afa3ace375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.257426039307062\n",
      "=== epoch:1, train acc:0.7, test acc:0.65 ===\n",
      "train loss:2.125987266181375\n",
      "train loss:1.9140892729886096\n",
      "train loss:1.6465471107620977\n",
      "train loss:1.2182149193305463\n",
      "train loss:0.7561317569931127\n",
      "train loss:0.6494452691191397\n",
      "train loss:0.3185993602909669\n",
      "train loss:0.7839215184733372\n",
      "train loss:0.6640310813006123\n",
      "train loss:0.6191931950734172\n",
      "train loss:0.7018489944556301\n",
      "train loss:0.7169834316683076\n",
      "train loss:0.6158929755513463\n",
      "train loss:0.7341478276676666\n",
      "train loss:0.46689367882428395\n",
      "train loss:0.31916244772681585\n",
      "train loss:1.0542891742719567\n",
      "train loss:0.6676881008335608\n",
      "train loss:0.6938733897954155\n",
      "train loss:0.874185316581882\n",
      "train loss:0.9207606929821551\n",
      "train loss:0.7652238697430231\n",
      "train loss:0.6705939235926883\n",
      "train loss:0.35420071287070487\n",
      "train loss:0.9157949831653678\n",
      "train loss:0.9132488553334198\n",
      "train loss:0.7877228972928596\n",
      "train loss:0.8779605339484778\n",
      "train loss:0.8726320690004755\n",
      "train loss:0.4519996007485122\n",
      "train loss:0.6572411596094346\n",
      "train loss:0.6893052093990033\n",
      "train loss:0.6696173949039996\n",
      "train loss:0.9070665515387202\n",
      "train loss:0.7568406278136206\n",
      "train loss:0.8476330714626871\n",
      "train loss:0.719626754143639\n",
      "train loss:0.6782468122949632\n",
      "train loss:0.6759799487087855\n",
      "train loss:0.5891922135151118\n",
      "train loss:0.6908202678467182\n",
      "train loss:0.5203794955213915\n",
      "train loss:0.620722168139205\n",
      "train loss:0.7173328485589593\n",
      "train loss:0.49831038668509703\n",
      "train loss:0.6339543975377648\n",
      "train loss:0.38942535791781296\n",
      "train loss:0.5091707842431277\n",
      "train loss:0.5330785233540525\n",
      "train loss:0.6643363384188073\n",
      "train loss:0.6428471461600077\n",
      "train loss:1.0682729500705748\n",
      "train loss:0.8046252599352893\n",
      "train loss:0.5178674830046253\n",
      "train loss:0.7621400855497101\n",
      "train loss:0.5358156792143698\n",
      "train loss:0.6057019771918952\n",
      "train loss:0.5607690115512787\n",
      "train loss:0.45025954370295473\n",
      "train loss:0.5502463952192793\n",
      "train loss:0.5372145700090698\n",
      "train loss:0.5114716378836202\n",
      "train loss:0.376021870917973\n",
      "train loss:0.6451447094952085\n",
      "train loss:0.6218214054602682\n",
      "train loss:0.6669811671190625\n",
      "train loss:0.7938014178011179\n",
      "train loss:0.7263833058887004\n",
      "train loss:0.7290647703637718\n",
      "train loss:0.4238436458456425\n",
      "train loss:0.6160154355565678\n",
      "train loss:0.6247267214279054\n",
      "train loss:0.6867613202384395\n",
      "train loss:0.6261594874052625\n",
      "train loss:0.6206521747756721\n",
      "train loss:0.44889019840448147\n",
      "train loss:0.6082469063019399\n",
      "train loss:0.5419831430519793\n",
      "train loss:0.5269468651795968\n",
      "train loss:0.32963470680767304\n",
      "train loss:0.38910803540213956\n",
      "train loss:0.5281693969109302\n",
      "train loss:0.8937932099803744\n",
      "train loss:0.4751888845136591\n",
      "train loss:0.835361058319906\n",
      "train loss:1.032049271505956\n",
      "train loss:0.6009819752198002\n",
      "train loss:0.6052967280878058\n",
      "train loss:0.5256792773849318\n",
      "train loss:0.5330237678056211\n",
      "train loss:0.8175036454147948\n",
      "train loss:0.41735006252842777\n",
      "train loss:0.4858153246212531\n",
      "train loss:0.5614460245530907\n",
      "train loss:0.6900265274407078\n",
      "train loss:0.5396377174676898\n",
      "train loss:0.46657348927531545\n",
      "train loss:0.5208518221889793\n",
      "train loss:0.7716137009882553\n",
      "train loss:0.7104962363971181\n",
      "train loss:0.4109135589362789\n",
      "train loss:0.5249478180792562\n",
      "train loss:0.6288743162409072\n",
      "train loss:0.5113833849233747\n",
      "train loss:0.6376198800504499\n",
      "train loss:0.6176868272226014\n",
      "train loss:0.9515860107648946\n",
      "train loss:0.6195687140769738\n",
      "train loss:0.6179309458670443\n",
      "train loss:0.6113436864077499\n",
      "train loss:0.68601825250723\n",
      "train loss:0.6789307658251563\n",
      "train loss:0.6844915697732998\n",
      "train loss:0.6215852690095114\n",
      "train loss:0.6121327604415066\n",
      "train loss:0.7175470648284528\n",
      "train loss:0.7370055736996177\n",
      "train loss:0.6957063912972303\n",
      "train loss:0.6566322910062102\n",
      "train loss:0.6213095240882445\n",
      "train loss:0.626218140003299\n",
      "train loss:0.5894919392013305\n",
      "train loss:0.6642697815301288\n",
      "train loss:0.594481924947946\n",
      "train loss:0.6733988938651747\n",
      "train loss:0.5357520141223395\n",
      "train loss:0.4407871246566516\n",
      "train loss:0.7064754512559868\n",
      "train loss:0.9515028003763952\n",
      "train loss:0.6222850928541287\n",
      "train loss:0.5220577019986593\n",
      "train loss:0.3891567176121955\n",
      "train loss:0.6013107173906505\n",
      "train loss:0.4596776701811067\n",
      "train loss:0.4838969355276075\n",
      "train loss:0.34841159040754477\n",
      "train loss:0.5088136846918518\n",
      "train loss:0.355808333449693\n",
      "train loss:0.5046979041945033\n",
      "train loss:0.5253353739348197\n",
      "train loss:0.5168922395233112\n",
      "train loss:0.5409130863483287\n",
      "train loss:0.6722445845323326\n",
      "train loss:0.80671040684323\n",
      "train loss:0.7461821693171651\n",
      "train loss:0.7201153106862253\n",
      "train loss:0.6978743772985798\n",
      "train loss:0.6184398161118151\n",
      "train loss:0.7017466653010742\n",
      "train loss:0.6476000528632334\n",
      "train loss:0.6357634391754705\n",
      "train loss:0.6420542322998661\n",
      "train loss:0.5950930630090819\n",
      "train loss:0.7165932199771181\n",
      "train loss:0.6779750901661874\n",
      "train loss:0.6593311385941332\n",
      "train loss:0.698922984825302\n",
      "train loss:0.6778607212474126\n",
      "train loss:0.5930865918179328\n",
      "train loss:0.5753952044146501\n",
      "train loss:0.4790735203160904\n",
      "train loss:0.6037360363105552\n",
      "train loss:0.5519068251720232\n",
      "train loss:0.5223778519749147\n",
      "train loss:0.7330172958092108\n",
      "train loss:0.6165038502991489\n",
      "train loss:0.7665648999141067\n",
      "train loss:0.6639529032905224\n",
      "train loss:0.47569040617262387\n",
      "train loss:0.6025993032909366\n",
      "train loss:0.3793873728488403\n",
      "train loss:0.3681482119865204\n",
      "train loss:0.48325125417788345\n",
      "train loss:0.5048518278980232\n",
      "train loss:0.5036267571194581\n",
      "train loss:0.6369426473307338\n",
      "train loss:0.5971962049782223\n",
      "train loss:0.37911543941526094\n",
      "train loss:0.8675476355516443\n",
      "train loss:0.6301604766207511\n",
      "train loss:0.5247192549506846\n",
      "train loss:0.7904703427105939\n",
      "train loss:0.44085440255364505\n",
      "train loss:0.45815172469407733\n",
      "train loss:0.6152458038479189\n",
      "train loss:0.6034062512528069\n",
      "train loss:0.6131835693015734\n",
      "train loss:0.5982120572879521\n",
      "train loss:0.7328692281283737\n",
      "train loss:0.5304293277310366\n",
      "train loss:0.6909510255410988\n",
      "train loss:0.541102581480038\n",
      "train loss:0.536152578428233\n",
      "train loss:0.5508488561551615\n",
      "train loss:0.42306394302818734\n",
      "train loss:0.40549026950938394\n",
      "train loss:0.981646851039136\n",
      "train loss:0.6018415791329343\n",
      "train loss:0.6184856625443373\n",
      "train loss:0.6080970189703142\n",
      "train loss:0.40609789155318704\n",
      "train loss:0.379660744553662\n",
      "train loss:0.37441462108319123\n",
      "train loss:0.638084467096262\n",
      "train loss:0.8627515466382363\n",
      "train loss:0.7070232849326481\n",
      "train loss:0.8031054636946848\n",
      "train loss:0.4192827274843173\n",
      "train loss:0.5355839870823377\n",
      "train loss:0.697282272045522\n",
      "train loss:0.6027926928584295\n",
      "train loss:0.5315314204069846\n",
      "train loss:0.4647934378690084\n",
      "train loss:0.6618645396094831\n",
      "train loss:0.761186915776055\n",
      "train loss:0.5245248836897541\n",
      "train loss:0.7581513566350071\n",
      "train loss:0.6241975066735789\n",
      "train loss:0.6112205729014791\n",
      "train loss:0.7216993403557033\n",
      "train loss:0.494419091630106\n",
      "train loss:0.6140390737756098\n",
      "train loss:0.4700604779156694\n",
      "train loss:0.5535270469009952\n",
      "train loss:0.5141671981197858\n",
      "train loss:0.6055269096367347\n",
      "train loss:0.5011882138516475\n",
      "train loss:0.496431851470739\n",
      "train loss:0.49532548816332256\n",
      "train loss:0.9528162401731175\n",
      "train loss:0.6320541390825752\n",
      "train loss:0.4979598820443198\n",
      "train loss:0.919289122356032\n",
      "train loss:0.586380025962863\n",
      "train loss:0.5180643906737952\n",
      "train loss:0.5392356011570314\n",
      "train loss:0.6075619244645358\n",
      "train loss:0.8641366939301636\n",
      "train loss:0.7443323515830633\n",
      "train loss:0.5975858758080137\n",
      "train loss:0.6053291595353282\n",
      "train loss:0.6013428765093126\n",
      "train loss:0.6433629972149693\n",
      "train loss:0.6386585031622622\n",
      "train loss:0.5740845949079907\n",
      "train loss:0.6803644204467345\n",
      "train loss:0.7179204812378049\n",
      "train loss:0.5642380351211976\n",
      "train loss:0.5476751166051865\n",
      "train loss:0.5470389339508632\n",
      "train loss:0.5314889977602496\n",
      "train loss:0.41388375330321486\n",
      "train loss:0.6013887656409145\n",
      "train loss:0.7838971143741079\n",
      "train loss:0.3685975516404777\n",
      "train loss:0.34746807166847316\n",
      "train loss:0.4679228219037842\n",
      "train loss:0.8348936893858628\n",
      "train loss:0.3736490932204194\n",
      "train loss:0.663477601832316\n",
      "train loss:0.9396429906507304\n",
      "train loss:0.7408401753093721\n",
      "train loss:0.6523075844643006\n",
      "train loss:0.5405784385174144\n",
      "train loss:0.47398731033388736\n",
      "train loss:0.6089400174484096\n",
      "train loss:0.4311560329892317\n",
      "train loss:0.723717767144584\n",
      "train loss:0.49083832033785296\n",
      "train loss:0.6811194198618943\n",
      "train loss:0.7328600275373891\n",
      "train loss:0.7130322872594107\n",
      "train loss:0.6242824832277971\n",
      "train loss:0.49787316387358926\n",
      "train loss:0.623317952593016\n",
      "train loss:0.7447078357998735\n",
      "train loss:0.4728627435253059\n",
      "train loss:0.3723432399287589\n",
      "train loss:0.6922579264438504\n",
      "train loss:0.2929656217020024\n",
      "train loss:0.5901272863970083\n",
      "train loss:0.4920718190397556\n",
      "train loss:0.34321002060862105\n",
      "train loss:0.8058749369133427\n",
      "train loss:0.3299572600158479\n",
      "train loss:0.889040183878936\n",
      "train loss:0.9235162106681433\n",
      "train loss:0.814084485113413\n",
      "train loss:0.6146315467244582\n",
      "train loss:0.4316508881809916\n",
      "train loss:0.6767059982534402\n",
      "train loss:0.4879263766125015\n",
      "train loss:0.6443638696889515\n",
      "train loss:0.5738684537685719\n",
      "train loss:0.49262764180024343\n",
      "train loss:0.6458468351193007\n",
      "train loss:0.7327479231432176\n",
      "train loss:0.5884739863446632\n",
      "train loss:0.5417923485703566\n",
      "train loss:0.5658889060395869\n",
      "train loss:0.5291248423060604\n",
      "train loss:0.720211385197099\n",
      "train loss:1.0005218215053193\n",
      "train loss:0.43493281780542503\n",
      "train loss:0.539330599886464\n",
      "train loss:0.5728427101490482\n",
      "train loss:0.40946240326419564\n",
      "train loss:0.5921580476370917\n",
      "train loss:0.5392446843748927\n",
      "train loss:0.6279252918558418\n",
      "train loss:0.8304787026305774\n",
      "train loss:0.6226465731643822\n",
      "train loss:0.7189922575341388\n",
      "train loss:0.7214874900731746\n",
      "train loss:0.7168202484053958\n",
      "train loss:0.4820007396378446\n",
      "train loss:0.6078727438341569\n",
      "train loss:0.6235298237847878\n",
      "train loss:0.5671066467676936\n",
      "train loss:0.4786555225336778\n",
      "train loss:0.7565872646955475\n",
      "train loss:0.5708396259222168\n",
      "train loss:0.7957679090290622\n",
      "train loss:0.4894627298999971\n",
      "train loss:0.5148534181042169\n",
      "train loss:0.6912872243616294\n",
      "train loss:0.5450669307822991\n",
      "train loss:0.7911560057523297\n",
      "train loss:0.5798609065142057\n",
      "train loss:0.5962388163623384\n",
      "train loss:0.5993761210875893\n",
      "train loss:0.8600596604543549\n",
      "train loss:0.6222598825887133\n",
      "train loss:0.6174658593569479\n",
      "train loss:0.6408337436798094\n",
      "train loss:0.6110631667917608\n",
      "train loss:0.5195821014666966\n",
      "train loss:0.6816732824971709\n",
      "train loss:0.889529261892503\n",
      "train loss:0.6589546165920979\n",
      "train loss:0.7292463196868741\n",
      "train loss:0.6166051427440132\n",
      "train loss:0.6732276021234893\n",
      "train loss:0.5743445038340919\n",
      "train loss:0.6284284467240173\n",
      "train loss:0.5842872854933898\n",
      "train loss:0.7733560915781429\n",
      "train loss:0.5620574687800957\n",
      "train loss:0.6166499308725137\n",
      "train loss:0.5400241480183763\n",
      "train loss:0.7005160012342587\n",
      "train loss:0.6839174726034056\n",
      "train loss:0.3486770978620326\n",
      "train loss:0.9310761448227017\n",
      "train loss:0.496266419867471\n",
      "train loss:0.6160793620552016\n",
      "train loss:0.7503877876836109\n",
      "train loss:0.5226950623356851\n",
      "train loss:0.5223275383814692\n",
      "train loss:0.7070308895798798\n",
      "train loss:0.5010343721998785\n",
      "train loss:0.6705510926522047\n",
      "train loss:0.42060842970975704\n",
      "train loss:0.5843941264161707\n",
      "train loss:0.7820892776115909\n",
      "train loss:0.6946708135761924\n",
      "train loss:0.5247665892850735\n",
      "train loss:0.6075934759315778\n",
      "train loss:0.6718567694283012\n",
      "train loss:0.5214837971377426\n",
      "train loss:0.6116727054172878\n",
      "train loss:0.4032462274842528\n",
      "train loss:0.6954604376206108\n",
      "train loss:0.5538034880595986\n",
      "train loss:0.6167880835395355\n",
      "train loss:0.8176214602618079\n",
      "train loss:0.6168343059884982\n",
      "train loss:0.8448230681389972\n",
      "train loss:0.635238669647771\n",
      "train loss:0.5450682949232937\n",
      "train loss:0.5672731430249404\n",
      "train loss:0.6776161007221988\n",
      "train loss:0.6435121176550188\n",
      "train loss:0.5688381766765234\n",
      "train loss:0.5352124664912266\n",
      "train loss:0.6504268830414425\n",
      "train loss:0.5746786082676487\n",
      "train loss:0.4660504818903731\n",
      "train loss:0.43416498913252066\n",
      "train loss:0.38975494870515026\n",
      "train loss:0.7166203225153238\n",
      "train loss:0.8797760805080259\n",
      "train loss:0.2211911244934514\n",
      "train loss:0.7342349814071888\n",
      "train loss:0.6481385981598675\n",
      "train loss:0.5994696766344119\n",
      "train loss:0.502645083085436\n",
      "train loss:0.6624713634911374\n",
      "train loss:0.7432421872897825\n",
      "train loss:0.506525522188358\n",
      "train loss:0.516773633770067\n",
      "train loss:0.5048017050799851\n",
      "train loss:0.6839668491231267\n",
      "train loss:0.4949072049773432\n",
      "train loss:0.6334268433634722\n",
      "train loss:0.7643780078540123\n",
      "train loss:0.5385403753987663\n",
      "train loss:0.49147230554155985\n",
      "train loss:0.6044814777088727\n",
      "train loss:0.6718356328221555\n",
      "train loss:0.5257803025634104\n",
      "train loss:0.4987289190280717\n",
      "train loss:0.6983834514875606\n",
      "train loss:0.4985346149745918\n",
      "train loss:0.5180753702991513\n",
      "train loss:0.4911404401278693\n",
      "train loss:0.6154392253803005\n",
      "train loss:0.7966673473023369\n",
      "train loss:0.7310971935486842\n",
      "train loss:0.42921509281080167\n",
      "train loss:0.42685406578796714\n",
      "train loss:0.2891162988804924\n",
      "train loss:0.6944899603965412\n",
      "train loss:0.8337809946402899\n",
      "train loss:0.6237565421105059\n",
      "train loss:0.5478188635998072\n",
      "train loss:0.5485385415454991\n",
      "train loss:0.6056982915862035\n",
      "train loss:0.4873573009577436\n",
      "train loss:0.7057454696869783\n",
      "train loss:0.4095971902980654\n",
      "train loss:0.4426276229280438\n",
      "train loss:0.38037318555245586\n",
      "train loss:0.6116889936307286\n",
      "train loss:0.6088604290651787\n",
      "train loss:0.46291509997478875\n",
      "train loss:0.34389660341922235\n",
      "train loss:0.6033491616736535\n",
      "train loss:0.6266148151429439\n",
      "train loss:0.7337323116700395\n",
      "train loss:0.30605099879956765\n",
      "train loss:0.5407054836683435\n",
      "train loss:0.3251836989368496\n",
      "train loss:0.6869373957242756\n",
      "train loss:0.5618507817968075\n",
      "train loss:0.6123828970429538\n",
      "train loss:0.6374914835374839\n",
      "train loss:0.6117806846652469\n",
      "train loss:0.3703201688722408\n",
      "train loss:0.8622644232128076\n",
      "train loss:0.5900677113344289\n",
      "train loss:0.5596980359361726\n",
      "train loss:0.6041908009839145\n",
      "train loss:0.553171895064703\n",
      "train loss:0.7136436405361166\n",
      "train loss:0.6977394578911016\n",
      "train loss:0.5216995874714077\n",
      "train loss:0.6696561093428887\n",
      "train loss:0.5547916022163193\n",
      "train loss:0.5084422611105704\n",
      "train loss:0.5706078174008413\n",
      "train loss:0.5489533926737301\n",
      "train loss:0.743168609705163\n",
      "train loss:0.6396469415753702\n",
      "train loss:0.7898337552717256\n",
      "train loss:0.5820277478161037\n",
      "train loss:0.5810470501752432\n",
      "train loss:0.417420043750479\n",
      "train loss:0.6081949051865733\n",
      "train loss:0.6928037003846652\n",
      "train loss:0.7746231316803807\n",
      "train loss:0.6150713573474011\n",
      "train loss:0.6878603569278534\n",
      "train loss:0.5888873715848522\n",
      "train loss:0.6178108210403763\n",
      "train loss:0.6360972164556847\n",
      "train loss:0.5324906837598904\n",
      "train loss:0.5112780322452373\n",
      "train loss:0.5315563815629887\n",
      "train loss:0.5528504803815508\n",
      "train loss:0.5075565472215605\n",
      "train loss:0.6164489824832203\n",
      "train loss:0.32840854849307904\n",
      "train loss:0.7267129068461882\n",
      "train loss:0.5233971946084953\n",
      "train loss:0.5167702385340407\n",
      "train loss:0.2675068796000652\n",
      "train loss:0.1279299059698226\n",
      "train loss:0.0842741614916746\n",
      "train loss:0.284711010329436\n",
      "train loss:0.783626326817058\n",
      "train loss:0.21991998887154343\n",
      "train loss:0.6576137364362037\n",
      "train loss:0.32138643684609647\n",
      "train loss:1.2892670034021427\n",
      "train loss:0.5606963627004525\n",
      "train loss:0.5415354698583437\n",
      "train loss:0.3932305183869334\n",
      "train loss:0.602514901893535\n",
      "train loss:0.5291704488688664\n",
      "train loss:0.6102675123412138\n",
      "train loss:0.6774864960525664\n",
      "train loss:0.6470006942384627\n",
      "train loss:0.626785202595581\n",
      "train loss:0.6591684702764379\n",
      "train loss:0.6583219912748541\n",
      "train loss:0.6660178550361977\n",
      "train loss:0.6363527099022447\n",
      "train loss:0.6684482658943036\n",
      "train loss:0.6402855549301574\n",
      "train loss:0.6330820799106411\n",
      "train loss:0.565163183330694\n",
      "train loss:0.6323628422072813\n",
      "train loss:0.6157348871738362\n",
      "train loss:0.7137014656747644\n",
      "train loss:0.417595495448739\n",
      "train loss:0.4206221339216776\n",
      "train loss:0.5163877069637203\n",
      "train loss:0.5003598272360987\n",
      "train loss:0.4884207789487923\n",
      "train loss:0.5190300406330299\n",
      "train loss:0.6732152407149817\n",
      "train loss:0.5324388930886601\n",
      "train loss:0.7746960207251263\n",
      "train loss:0.7603847727603996\n",
      "train loss:0.3743635796802854\n",
      "train loss:0.5963776906528773\n",
      "train loss:0.7821465774540435\n",
      "train loss:0.7009396267657173\n",
      "train loss:0.4610297070168756\n",
      "train loss:0.5989846134971535\n",
      "train loss:0.5219790949105898\n",
      "train loss:0.6823925503311296\n",
      "train loss:0.5940927565112604\n",
      "train loss:0.6380985286785059\n",
      "train loss:0.6339761487918764\n",
      "train loss:0.48637217294568763\n",
      "train loss:0.4820797404699914\n",
      "train loss:0.5228692101262038\n",
      "train loss:0.4976951351767115\n",
      "train loss:0.448508803645251\n",
      "train loss:0.5233744055482669\n",
      "train loss:0.4239565080427086\n",
      "train loss:0.34119016585778406\n",
      "train loss:0.7228517543383832\n",
      "train loss:0.5102603772110775\n",
      "train loss:0.5289508483883927\n",
      "train loss:0.620594599068963\n",
      "train loss:0.5033164334802583\n",
      "train loss:0.6905075676278096\n",
      "train loss:0.6800925466291856\n",
      "train loss:0.5555964343279485\n",
      "train loss:0.5443592945461481\n",
      "train loss:0.344109470083355\n",
      "train loss:0.5150869819907022\n",
      "train loss:0.658402229103269\n",
      "train loss:0.5350461489789844\n",
      "train loss:0.5628192915124025\n",
      "train loss:0.643478375481927\n",
      "train loss:0.32350674296608667\n",
      "train loss:0.5672205918620923\n",
      "train loss:0.5184473276794497\n",
      "train loss:0.5976593373725599\n",
      "train loss:0.6261583723543966\n",
      "train loss:0.7041348701055962\n",
      "train loss:0.4930271510751261\n",
      "train loss:0.5993202708196181\n",
      "train loss:0.6081568992525076\n",
      "train loss:0.5035863773349574\n",
      "train loss:0.5288737190461583\n",
      "train loss:0.6715419596299773\n",
      "train loss:0.656673074254442\n",
      "train loss:0.6646098454209473\n",
      "train loss:0.6148826690576048\n",
      "train loss:0.5469563275105463\n",
      "train loss:0.6208586302739985\n",
      "train loss:0.48005217082013474\n",
      "train loss:0.5180202533339568\n",
      "train loss:0.5103214821037677\n",
      "train loss:0.6450928171523131\n",
      "train loss:0.7132206527278333\n",
      "train loss:0.45312990091096167\n",
      "train loss:0.5394856827033735\n",
      "train loss:0.5185886296622495\n",
      "train loss:0.5810691309555087\n",
      "train loss:0.5217185495418258\n",
      "train loss:0.817071796098972\n",
      "train loss:0.5088805850670319\n",
      "train loss:0.6584524899865742\n",
      "train loss:0.5884105470795067\n",
      "train loss:0.5358229714812905\n",
      "train loss:0.5745420399487903\n",
      "train loss:0.5302401872337466\n",
      "train loss:0.61073047987128\n",
      "train loss:0.5344304513909648\n",
      "train loss:0.3252325596360722\n",
      "train loss:0.4598905517578785\n",
      "train loss:0.655009533074096\n",
      "train loss:0.6386244131533088\n",
      "train loss:0.4813352807546032\n",
      "train loss:0.6557890096543184\n",
      "train loss:0.5135881749040603\n",
      "train loss:0.5699069845745944\n",
      "train loss:0.7260301881439544\n",
      "train loss:0.5888493282891157\n",
      "train loss:0.3428938450405467\n",
      "train loss:0.4993725846699514\n",
      "train loss:0.41278573048000194\n",
      "train loss:0.3360707136499243\n",
      "train loss:0.6251137591163992\n",
      "train loss:0.6491431427847536\n",
      "train loss:0.5220584194733919\n",
      "train loss:0.35479683546153346\n",
      "train loss:0.7435345031235286\n",
      "train loss:0.5358118528183687\n",
      "train loss:0.7173747090975422\n",
      "train loss:0.5706430948951422\n",
      "train loss:0.3189243559740451\n",
      "train loss:0.35579651048224126\n",
      "train loss:0.5603030611903022\n",
      "train loss:0.40671540389865035\n",
      "train loss:0.3907473718631914\n",
      "train loss:0.46075961684292643\n",
      "train loss:0.32399832242283444\n",
      "train loss:0.3656692550829746\n",
      "train loss:0.5890634568702702\n",
      "train loss:0.4609369317372022\n",
      "train loss:0.5942992770894634\n",
      "train loss:0.4854198132954476\n",
      "train loss:0.34082872085546867\n",
      "train loss:0.3932305534491617\n",
      "train loss:0.4039082461761881\n",
      "train loss:0.2573062455090226\n",
      "train loss:0.578402389689963\n",
      "train loss:0.6229317987255734\n",
      "train loss:0.44496555738042487\n",
      "train loss:0.6128123992090637\n",
      "train loss:0.5168514602465479\n",
      "train loss:0.7317420783675064\n",
      "train loss:0.4209214121846423\n",
      "train loss:0.6623781552282499\n",
      "train loss:0.4751539808084718\n",
      "train loss:0.4632005354152162\n",
      "train loss:0.5818571497239217\n",
      "train loss:0.32112096750930047\n",
      "train loss:0.5694698381874264\n",
      "train loss:0.37592027430700525\n",
      "train loss:0.6042178161253793\n",
      "train loss:0.5699411289173691\n",
      "train loss:0.6006936026720858\n",
      "train loss:0.23008793444411607\n",
      "train loss:0.5621768442227384\n",
      "train loss:0.7829944967534412\n",
      "train loss:0.5767225430620713\n",
      "train loss:0.4293196425978468\n",
      "train loss:0.43563571185969446\n",
      "train loss:0.6756940219497883\n",
      "train loss:0.4724555583934669\n",
      "train loss:0.5591629848389681\n",
      "train loss:0.4332082083429259\n",
      "train loss:0.5575840433575044\n",
      "train loss:0.6272206223981756\n",
      "train loss:0.5376270225338702\n",
      "train loss:0.4673511600875527\n",
      "train loss:0.5770401199010555\n",
      "train loss:0.6102542102917827\n",
      "train loss:0.3533509444183084\n",
      "train loss:0.3254794200838669\n",
      "train loss:0.7486594684628545\n",
      "train loss:0.24361807726170479\n",
      "train loss:0.6205387062245175\n",
      "train loss:0.6310685820073421\n",
      "train loss:0.615806901580801\n",
      "train loss:0.7416015000626589\n",
      "train loss:0.2543809899016675\n",
      "train loss:0.5651748917184564\n",
      "train loss:0.4131548658735814\n",
      "train loss:0.966343353556041\n",
      "train loss:0.640374724573171\n",
      "train loss:0.5689224158560495\n",
      "train loss:0.2538228099485461\n",
      "train loss:0.3649539748126577\n",
      "train loss:0.3519016217736101\n",
      "train loss:0.525448637750995\n",
      "train loss:0.5240248060701724\n",
      "train loss:0.46949335856446\n",
      "train loss:0.359371150436198\n",
      "train loss:0.45587381430213203\n",
      "train loss:0.5246822691108814\n",
      "train loss:0.24845714291944368\n",
      "train loss:0.42717591705329533\n",
      "train loss:0.4760690490657938\n",
      "train loss:0.6772391015404138\n",
      "train loss:0.7685738454304711\n",
      "train loss:0.3033149479643502\n",
      "train loss:0.32516824776345715\n",
      "train loss:0.34157486833809686\n",
      "train loss:0.3615979498666576\n",
      "train loss:0.5642437540113511\n",
      "train loss:0.924157115859636\n",
      "train loss:0.4567888105435708\n",
      "train loss:0.38113704876815435\n",
      "train loss:0.54364925190691\n",
      "train loss:0.5121941566796133\n",
      "train loss:0.4560728713121932\n",
      "train loss:0.5894429627376471\n",
      "train loss:0.4491848817597631\n",
      "train loss:0.586586116451891\n",
      "train loss:0.41535306144233475\n",
      "train loss:0.41963666669421673\n",
      "train loss:0.5038114212667459\n",
      "train loss:0.5146917734568296\n",
      "train loss:0.29941662275556835\n",
      "train loss:0.4188900871024511\n",
      "train loss:0.6944079961567201\n",
      "train loss:0.52947380225851\n",
      "train loss:0.5028013650271705\n",
      "train loss:0.41022085222610205\n",
      "train loss:0.35189641374916675\n",
      "train loss:0.2584062679732849\n",
      "train loss:0.1921245112949306\n",
      "train loss:0.506614529501576\n",
      "train loss:0.6467633504688118\n",
      "train loss:0.1812752386290829\n",
      "train loss:0.49235166447110296\n",
      "train loss:0.3704426488311553\n",
      "train loss:0.29496676184759485\n",
      "train loss:0.1785066975008211\n",
      "train loss:0.10996938097953868\n",
      "train loss:0.3781953997679057\n",
      "train loss:0.5601481145704115\n",
      "train loss:0.48792725991600294\n",
      "train loss:0.3799321117838323\n",
      "train loss:0.3504954966629531\n",
      "train loss:0.722993942060598\n",
      "train loss:0.8173827690028304\n",
      "train loss:0.6719278251772232\n",
      "train loss:0.515020924887591\n",
      "train loss:0.6809122250065125\n",
      "train loss:0.3877335784207977\n",
      "train loss:0.5295877761342922\n",
      "train loss:0.6252287798917269\n",
      "train loss:0.4918385779072395\n",
      "train loss:0.5687417651954079\n",
      "train loss:0.4717723104263779\n",
      "train loss:0.5001215382262492\n",
      "train loss:0.6213570130943114\n",
      "train loss:0.504308206265891\n",
      "train loss:0.5360828110144256\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 13\u001b[0m\n\u001b[0;32m      6\u001b[0m network \u001b[38;5;241m=\u001b[39m ConvNet3Layer(input_dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m128\u001b[39m), \n\u001b[0;32m      7\u001b[0m                         conv_param \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter_num\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m30\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstride\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m},\n\u001b[0;32m      8\u001b[0m                         hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, weight_init_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(network, x_train, y_train, x_test, y_test,\n\u001b[0;32m     10\u001b[0m                   epochs\u001b[38;5;241m=\u001b[39mmax_epochs, mini_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     11\u001b[0m                   optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer_param\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.001\u001b[39m},\n\u001b[0;32m     12\u001b[0m                   evaluate_sample_num_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\4학년 2학기\\딥러닝\\과제\\common\\trainer.py:71\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[1;32m---> 71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step()\n\u001b[0;32m     73\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39maccuracy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_test)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\4학년 2학기\\딥러닝\\과제\\common\\trainer.py:44\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_train[batch_mask]\n\u001b[0;32m     42\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_train[batch_mask]\n\u001b[1;32m---> 44\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mgradient(x_batch, t_batch)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mparams, grads)\n\u001b[0;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mloss(x_batch, t_batch)\n",
      "Cell \u001b[1;32mIn[54], line 68\u001b[0m, in \u001b[0;36mConvNet3Layer.gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# backward\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     dout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[54], line 39\u001b[0m, in \u001b[0;36mConvNet3Layer.loss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 39\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_layer\u001b[38;5;241m.\u001b[39mforward(y, t)\n",
      "Cell \u001b[1;32mIn[54], line 34\u001b[0m, in \u001b[0;36mConvNet3Layer.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m---> 34\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\4학년 2학기\\딥러닝\\과제\\common\\layers.py:54\u001b[0m, in \u001b[0;36mAffine.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# 텐서 대응\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_x_shape \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m---> 54\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     57\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet3Layer(input_dim=(1,128,128), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf935d79-8c12-4b35-965d-6d2b83281b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet3Layer(input_dim=(1,128,128), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=200, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4051d1ee-01c8-48cf-9a89-f11fc845b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet3Layer(input_dim=(1,128,128), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=400, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9754c84-dd27-4e48-81a5-f612d0410cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = ConvNet3Layer(input_dim=(1,128,128), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=800, output_size=10, weight_init_std=0.01)\n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=10,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=100)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7401ea9f-e961-4fe7-9ff7-014f77b27094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
